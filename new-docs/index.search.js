var relearn_search_index=[{content:`The Toolstack runs in an environment on a server (host) that has:
Physical hardware. The Xen hypervisor. The control domain (domain 0): the priviledged domain that the Toolstack runs in. Other, mostly unpriviledged domains, usually for guests (VMs). The Toolstack relies on various bits of software inside the control domain, and directly communicates with most of these:
Linux kernel including drivers for hardware and Xen paravirtualised devices (e.g. netback and blkback). Interacts through /sys and /proc, udev scripts, xenstore, … CentOS distibution including userspace tools and libraries. systemd, networking tools, … Xen-specific libraries, especially libxenctrl (a.k.a. libxc) xenstored: a key-value pair configuration database Accessible from all domains on a host, which makes it useful for inter-domain communication. The control domain has access to the entire xenstore database, while other domains only see sub-trees that are specific to that domain. Used for connecting VM disks and network interfaces, and other VM configuration options. Used for VM status reporting, e.g. the capabilities of the PV drivers (if installed), the IP address, etc. SM: Storage Manager plugins which connect xapi’s internal storage interfaces to the control APIs of external storage systems. stunnel: a daemon which decodes TLS and forwards traffic to xapi (and the other way around). Open vSwitch (OVS): a virtual network switch, used to connect VMs to network interfaces. The OVS offers several networking features that xapi takes advantage of. QEMU: emulation of various bits of hardware DEMU: emulation of Nvidia vGPUs xenguest emu-manager pvsproxy xenconsoled: allows access to guest consoles. This is common to all Xen hosts. The Toolstack also interacts with software that runs inside the guests:
PV drivers The guest agent `,description:"",tags:null,title:"Environment",uri:"/new-docs/toolstack/high-level/environment/index.html"},{content:`The XAPI Toolstack forms the main control plane of a pool of XenServer hosts. It allow the administrator to:
Configure the hardware resources of XenServer hosts: storage, networking, graphics, memory. Create, configure and destroy VMs and their virtual resources. Control the lifecycle of VMs. Monitor the status of hosts, VMs and related resources. To this, the Toolstack:
Exposes an API that can be accessed by external clients over HTTP(s). Exposes a CLI. Ensures that physical resources are configured when needed, and VMs receive the resources they require. Implements various features to help the administrator manage their systems. Monitors running VMs. Records metrics about physical and virtual resources. `,description:"",tags:null,title:"Responsibilities",uri:"/new-docs/toolstack/responsibilities/index.html"},{content:" Responsibilities High-level architecture Environment Daemons Interfaces Features Disaster Recovery Event handling in the Control Plane - Xapi, Xenopsd and Xenstore High-Availability NUMA Snapshots vGPU Xapi Storage Migration ",description:"",tags:null,title:"The XAPI Toolstack",uri:"/new-docs/toolstack/index.html"},{content:`The Toolstack consists of a set of co-operating daemons:
xapi manages clusters of hosts, co-ordinating access to shared storage and networking. xenopsd a low-level “domain manager” which takes care of creating, suspending, resuming, migrating, rebooting domains by interacting with Xen via libxc and libxl. xcp-rrdd a performance counter monitoring daemon which aggregates “datasources” defined via a plugin API and records history for each. There are various rrdd-plugin daemons: xcp-rrdd-gpumon xcp-rrdd-iostat xcp-rrdd-squeezed xcp-rrdd-xenpm xcp-networkd a host network manager which takes care of configuring interfaces, bridges and OpenVSwitch instances squeezed a daemon in charge of VM memory management xapi-storage-script for storage manipulation over SMAPIv3 message-switch exchanges messages between the daemons on a host xapi-guard forwards uefi and vtpm persistence calls from domains to xapi v6d controls which features are enabled. forkexecd a helper daemon that assists the above daemons with executing binaries and scripts xhad The High-Availability daemon perfmon a daemon which monitors performance counters and sends “alerts” if values exceed some pre-defined threshold mpathalert a daemon which monitors “storage paths” and sends “alerts” if paths fail and need repair wsproxy handles access to VM consoles `,description:"",tags:null,title:"Daemons",uri:"/new-docs/toolstack/high-level/daemons/index.html"},{content:`Xapi is the xapi-project host and cluster manager.
Xapi is responsible for:
providing a stable interface (the XenAPI) allowing one client to manage multiple hosts hosting the “xe” CLI authenticating users and applying role-based access control locking resources (in particular disks) allowing storage to be managed through plugins planning and coping with host failures (“High Availability”) storing VM and host configuration generating alerts managing software patching Principles The XenAPI interface must remain backwards compatible, allowing older clients to continue working Xapi delegates all Xenstore/libxc/libxl access to Xenopsd, so Xapi could be run in an unprivileged helper domain Xapi delegates the low-level storage manipulation to SM plugins. Xapi delegates setting up host networking to xcp-networkd. Xapi delegates monitoring performance counters to xcp-rrdd. Overview The following diagram shows the internals of Xapi:
The top of the diagram shows the XenAPI clients: XenCenter, XenOrchestra, OpenStack and CloudStack using XenAPI and HTTP GET/PUT over ports 80 and 443 to talk to xapi. These XenAPI (JSON-RPC or XML-RPC over HTTP POST) and HTTP GET/PUT are always authenticated using either PAM (by default using the local passwd and group files) or through Active Directory.
The APIs are classified into categories:
coordinator-only: these are the majority of current APIs. The coordinator should be called and relied upon to forward the call to the right place with the right locks held. normally-local: these are performance special cases such as disk import/export and console connection which are sent directly to hosts which have the most efficient access to the data. emergency: these deal with scenarios where the coordinator is offline If the incoming API call should be resent to the coordinator than a XenAPI HOST_IS_SLAVE error message containing the coordinator’s IP is sent to the client.
Once past the initial checks, API calls enter the “message forwarding” layer which
locks resources (via the current_operations mechanism) decides which host should execute the request. If the request should run locally then a direct function call is used; otherwise the message forwarding code makes a synchronous API call to a specific other host. Note: Xapi currently employs a “thread per request” model which causes one full POSIX thread to be created for every request. Even when a request is forwarded the full thread persists, blocking for the result to become available.
If the XenAPI call is a VM lifecycle operation then it is converted into a Xenopsd API call and forwarded over a Unix domain socket. Xapi and Xenopsd have similar notions of cancellable asynchronous “tasks”, so the current Xapi task (all operations run in the context of a task) is bound to the Xenopsd task, so cancellation is passed through and progress updates are received.
If the XenAPI call is a storage operation then the “storage access” layer
verifies that the storage objects are in the correct state (SR attached/detached; VDI attached/activated read-only/read-write) invokes the relevant operation in the Storage Manager API (SMAPI) v2 interface; depending on the type of SR: uses the SMAPIv2 to SMAPIv1 converter to generate the necessary command-line to talk to the SMAPIv1 plugin (EXT, NFS, LVM etc) and to execute it uses the SMAPIv2 to SMAPIv3 converter daemon xapi-storage-script to exectute the necessary SMAPIv3 command (GFS2) persists the state of the storage objects (including the result of a VDI.attach call) to persistent storage Internally the SMAPIv1 plugins use privileged access to the Xapi database to directly set fields (e.g. VDI.virtual_size) that would be considered read/only to other clients. The SMAPIv1 plugins also rely on Xapi for
knowledge of all hosts which may access the storage locking of disks within the resource pool safely executing code on other hosts via the “Xapi plugin” mechanism The Xapi database contains Host and VM metadata and is shared pool-wide. The coordinator keeps a copy in memory, and all other nodes remote queries to the coordinator. The database associates each object with a generation count which is used to implement the XenAPI event.next and event.from APIs. The database is routinely asynchronously flushed to disk in XML format. If the “redo-log” is enabled then all database writes are made synchronously as deltas to a shared block device. Without the redo-log, recent updates may be lost if Xapi is killed before a flush.
High-Availability refers to planning for host failure, monitoring host liveness and then following-through on the plans. Xapi defers to an external host liveness monitor called xhad. When xhad confirms that a host has failed – and has been isolated from the storage – then Xapi will restart any VMs which have failed and which have been marked as “protected” by HA. Xapi can also impose admission control to prevent the pool becoming too overloaded to cope with n arbitrary host failures.
The xe CLI is implemented in terms of the XenAPI, but for efficiency the implementation is linked directly into Xapi. The xe program remotes its command-line to Xapi, and Xapi sends back a series of simple commands (prompt for input; print line; fetch file; exit etc).
`,description:"",tags:null,title:"Xapi",uri:"/new-docs/xapi/index.html"},{content:`The XAPI Toolstack manages a cluster of hosts, network switches and storage on behalf of clients such as XenCenter and Xen Orchestra.
The most fundamental concept is of a Resource pool: the whole cluster managed as a single entity. The following diagram shows a cluster of hosts running xapi, all sharing some storage:
At any time, at most one host is known as the pool coordinator (formerly known as “master”) and is responsible for coordination and locking resources within the pool. When a pool is first created a coordinator host is chosen. The coordinator role can be transferred
on user request in an orderly fashion (xe pool-designate-new-master) on user request in an emergency (xe pool-emergency-transition-to-master) automatically if HA is enabled on the cluster. All hosts expose an HTTP, XML-RPC and JSON-RPC interface running on port 80 and with TLS on port 443, but control operations will only be processed on the coordinator host. Attempts to send a control operation to another host will result in a XenAPI redirect error message. For efficiency the following operations are permitted on non-coordinator hosts:
querying performance counters (and their history) connecting to VNC consoles import/export (particularly when disks are on local storage) Since the coordinator host acts as coordinator and lock manager, the other hosts will often talk to the coordinator. Non-coordinator hosts will also talk to each other (over the same HTTP and RPC channels) to
transfer VM memory images (VM migration) mirror disks (storage migration) Note that some types of shared storage (in particular all those using vhd) require coordination for disk GC and coalesce. This coordination is currently done by xapi and hence it is not possible to share this kind of storage between resource pools.
The following diagram shows the software running on a single host. Note that all hosts run the same software (although not necessarily the same version, if we are in the middle of a rolling update).
The XAPI Toolstack expects the host to be running Xen on x86. The Xen hypervisor partitions the host into Domains, some of which can have privileged hardware access, and the rest are unprivileged guests. The XAPI Toolstack normally runs all of its components in the privileged initial domain, Domain 0, also known as “the control domain”. However there is experimental code which supports “driver domains” allowing storage and networking drivers to be isolated in their own domains.
Environment Daemons Interfaces `,description:"",tags:null,title:"High-level architecture",uri:"/new-docs/toolstack/high-level/index.html"},{content:`Xenopsd is the VM manager of the XAPI Toolstack. Xenopsd is responsible for:
Starting, stopping, rebooting, suspending, resuming, migrating VMs. (Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices. Setting up VM consoles. Running bootloaders. Setting QoS parameters. Configuring SMBIOS tables. Handling crashes. etc. Check out the full features list.
The code is in ocaml/xenopsd.
Principles Do no harm: Xenopsd should never touch domains/VMs which it hasn’t been asked to manage. This means that it can co-exist with other VM managers such as ‘xl’ and ’libvirt’. Be independent: Xenopsd should be able to work in isolation. In particular the loss of some other component (e.g. the network) should not by itself prevent VMs being managed locally (including shutdown and reboot). Asynchronous by default: Xenopsd exposes task monitoring and offers cancellation for all operations. Xenopsd ensures that the system is always in a manageable state after an operation has been cancelled. Avoid state duplication: where another component owns some state, Xenopsd will always defer to it. We will avoid creating out-of-sync caches of this state. Be debuggable: Xenopsd will expose diagnostic APIs and tools to allow its internal state to be inspected and modified. `,description:"",tags:null,title:"Xenopsd",uri:"/new-docs/xenopsd/index.html"},{content:`The xcp-networkd daemon (hereafter simply called “networkd”) is a component in the xapi toolstack that is responsible for configuring network interfaces and virtual switches (bridges) on a host.
The code is in ocaml/networkd.
Principles Distro-agnostic. Networkd is meant to work on at least CentOS/RHEL as well a Debian/Ubuntu based distros. It therefore should not use any network configuration features specific to those distros.
Stateless. By default, networkd should not maintain any state. If you ask networkd anything about a network interface or bridge, or any other network sub-system property, it will always query the underlying system (e.g. an IP address), rather than returning any cached state. However, if you want networkd to configure networking at host boot time, the you can ask it to remember your configuration you have set for any interface or bridge you choose.
Idempotent. It should be possible to call any networkd function multiple times without breaking things. For example, calling a function to set an IP address on an interface twice in a row should have the same outcome as calling it just once.
Do no harm. Networkd should only configure what you ask it to configure. This means that it can co-exist with other network managers.
Usage Networkd is a daemon that is typically started at host-boot time. In the same way as the other daemons in the xapi toolstack, it is controlled by RPC requests. It typically receives requests from the xapi daemon, on behalf of which it configures host networking.
Networkd’s RCP API is fully described by the network_interface.ml file. The API has two main namespaces: Interface and Bridge, which are implemented in two modules in network_server.ml.
In line with other xapi daemons, all API functions take an argument of type debug_info (a string) as their first argument. The debug string appears in any log lines that are produced as a side effort of calling the function.
Network Interface API The Interface API has functions to query and configure properties of Linux network devices, such as IP addresses, and bringing them up or down. Most Interface functions take a name string as a reference to a network interface as their second argument, which is expected to be the name of the Linux network device. There is also a special function, called Interface.make_config, that is able to configure a number of interfaces at once. It takes an argument called config of type (iface * interface_config_t) list, where iface is an interface name, and interface_config_t is a compound type containing the full configuration for an interface (as far as networkd is able to configure them), currently defined as follows:
type interface_config_t = { ipv4_conf: ipv4; ipv4_gateway: Unix.inet_addr option; ipv6_conf: ipv6; ipv6_gateway: Unix.inet_addr option; ipv4_routes: (Unix.inet_addr * int * Unix.inet_addr) list; dns: Unix.inet_addr list * string list; mtu: int; ethtool_settings: (string * string) list; ethtool_offload: (string * string) list; persistent_i: bool; }When the function returns, it should have completely configured the interface, and have brought it up. The idempotency principle applies to this function, which means that it can be used to successively modify interface properties; any property that has not changed will effectively be ignored. In fact, Interface.make_config is the main function that xapi uses to configure interfaces, e.g. as a result of a PIF.plug or a PIF.reconfigure_ip call.
Also note the persistent property in the interface config. When an interface is made “persistent”, this means that any configuration that is set on it is remembered by networkd, and the interface config is written to disk. When networkd is started, it will read the persistent config and call Interface.make_config on it in order to apply it (see Startup below).
The full networkd API should be documented separately somewhere on this site.
Bridge API The Bridge API functions are all about the management of virtual switches, also known as “bridges”. The shape of the Bridge API roughly follows that of the Open vSwitch in that it treats a bridge as a collection of “ports”, where a port can contain one or more “interfaces”.
NIC bonding and VLANs are all configured on the Bridge level. There are functions for creating and destroying bridges, adding and removing ports, and configuring bonds and VLANs. Like interfaces, bridges and ports are addressed by name in the Bridge functions. Analogous to the Interface function with the same name, there is a Bridge.make_config function, and bridges can be made persistent.
type port_config_t = { interfaces: iface list; bond_properties: (string * string) list; bond_mac: string option; } type bridge_config_t = { ports: (port * port_config_t) list; vlan: (bridge * int) option; bridge_mac: string option; other_config: (string * string) list; persistent_b: bool; }Backends Networkd currently has two different backends: the “Linux bridge” backend and the “Open vSwitch” backend. The former is the “classic” backend based on the bridge module that is available in the Linux kernel, plus additional standard Linux functionality for NIC bonding and VLANs. The latter backend is newer and uses the Open vSwitch (OVS) for bridging as well as other functionality. Which backend is currently in use is defined by the file /etc/xensource/network.conf, which is read by networkd when it starts. The choice of backend (currently) only affects the Bridge API: every function in it has a separate implementation for each backend.
Low-level Interfaces Networkd uses standard networking commands and interfaces that are available in most modern Linux distros, rather than relying on any distro-specific network tools (see the distro-agnostic principle). These are tools such as ip (iproute2), dhclient and brctl, as well as the sysfs files system, and netlink sockets. To control the OVS, the ovs-* command line tools are used. All low-level functions are called from network_utils.ml.
Configuration on Startup Networkd, periodically as well as on shutdown, writes the current configuration of all bridges and interfaces (see above) in a JSON format to a file called networkd.db (currently in /var/lib/xcp). The contents of the file are completely described by the following type:
type config_t = { interface_config: (iface * interface_config_t) list; bridge_config: (bridge * bridge_config_t) list; gateway_interface: iface option; dns_interface: iface option; }The gateway_interface and dns_interface in the config are global host-level options to define from which interfaces the default gateway and DNS configuration is taken. This is especially important when multiple interfaces are configured by DHCP.
When networkd starts up, it first reads network.conf to determine the network backend. It subsequently attempts to parse networkd.db, and tries to call Bridge.make_config and Interface.make_config on it, with a special options to only apply the config for persistent bridges and interfaces, as well as bridges related to those (for example, if a VLAN bridge is configured, then also its parent bridge must be configured).
Networkd also supports upgrades from older versions of XenServer that used a network configuration script called interface-configure. If networkd.db is not found on startup, then networkd attempts to call this tool (via the /etc/init.d/management-interface script) in order to set up networking at boot time. This is normally followed immediately by a call from xapi instructing networkd to take over.
Finally, if no network config (old or new) is found on disk at all, networkd looks for a XenServer “firstboot” data file, which is written by XenServer’s host installer, and tries to apply it to set up the management interface.
Monitoring Besides the ability to configure bridges and network interfaces, networkd has facilities for monitoring interfaces and bonds. When networkd starts, a monitor thread is started, which does several things (see network_monitor_thread.ml):
Every 5 seconds, it gathers send/receive counters and link state of all network interfaces. It then writes these stats to a shared-memory file, to be picked up by other components such as xcp-rrdd and xapi (see documentation about “xenostats” elsewhere). It monitors NIC bonds, and sends alerts through xapi in case of link state changes within a bond. It uses ip monitor address to watch for an IP address changes, and if so, it calls xapi (Host.signal_networking_change) for it to update the IP addresses of the PIFs in its database that were configured by DHCP. `,description:"",tags:null,title:"Networkd",uri:"/new-docs/xcp-networkd/index.html"},{content:" Disaster Recovery Event handling in the Control Plane - Xapi, Xenopsd and Xenstore High-Availability NUMA Snapshots vGPU Xapi Storage Migration ",description:"",tags:null,title:"Features",uri:"/new-docs/toolstack/features/index.html"},{content:`Squeezed is the XAPI Toolstack’s host memory manager (aka balloon driver). Squeezed uses ballooning to move memory between running VMs, to avoid wasting host memory.
Principles Avoid wasting host memory: unused memory should be put to use by returning it to VMs. Memory should be shared in proportion to the configured policy. Operate entirely at the level of domains (not VMs), and be independent of Xen toolstack. `,description:"",tags:null,title:"Squeezed",uri:"/new-docs/squeezed/index.html"},{content:`The xapi-guard daemon is the component in the xapi toolstack that is responsible for handling persistence requests from VMs (domains). Currently these are UEFI vars and vTPM updates.
The code is in ocaml/xapi-guard. When the daemon managed only with UEFI updates it was called varstored-guard. Some files and package names still use the previous name.
Principles Calls from domains must be limited in privilege to do certain API calls, and to read and write from their corresponding VM in xapi’s database only. Xenopsd is able to control xapi-guard through message switch, this access is not limited. Listening to domain socket is restored whenever the daemon restarts to minimize disruption of running domains. Disruptions to requests when xapi is unavailable is minimized. The startup procedure is not blocked by the availability of xapi, and write requests from domains must not fail because xapi is unavailable. Overview Xapi-guard forwards calls from domains to xapi to persist UEFI variables, and update vTPMs. To do this, it listens to 1 socket per service (varstored, or swtpm) per domain. To create these sockets before the domains are running, it listens to a message-switch socket. This socket listens to calls from xenopsd, which orchestrates the domain creation.
To protect the domains from xapi being unavailable transiently, xapi-guard provides an on-disk cache for vTPM writes. This cache acts as a buffer and stores the requests temporarily until xapi can be contacted again. This situation usually happens when xapi is being restarted as part of an update. SWTPM, the vTPM daemon, reads the contents of the TPM from xapi-guard on startup, suspend, and resume. During normal operation SWTPM does not send read requests from xapi-guard.
Structure The cache module consists of two Lwt threads, one that writes to disk, and another one that reads from disk. The writer is triggered when a VM writes to the vTPM. It never blocks if xapi is unreachable, but responds as soon as the data has been stored either by xapi or on the local disk, such that the VM receives a timely response to the write request. Both try to send the requests to xapi, depending on the state, to attempt write all the cached data back to xapi, and stop using the cache. The threads communicate through a bounded queue, this is done to limit the amount of memory used. This queue is a performance optimisation, where the writer informs the reader precisely which are the names of the cache files, such that the reader does not need to list the cache directory. And a full queue does not mean data loss, just a loss of performance; vTPM writes are still cached.
This means that the cache operates in three modes:
Direct: during normal operation the disk is not used at all Engaged: both threads use the queue to order events Disengaged: A thread dumps request to disk while the other reads the cache until it’s empty --- title: Cache State --- stateDiagram-v2 Disengaged note right of Disengaged Writer doesn't add requests to queue Reader reads from cache and tries to push to xapi end note Direct note left of Direct Writer bypasses cache, send to xapi Reader waits end note Engaged note right of Engaged Writer writes to cache and adds requests to queue Reader reads from queue and tries to push to xapi end note [*] --> Disengaged Disengaged --> Disengaged : Reader pushed pending TPMs to xapi, in the meantime TPMs appeared in the cache Disengaged --> Direct : Reader pushed pending TPMs to xapi, cache is empty Direct --> Direct : Writer receives TPM, sent to xapi Direct --> Engaged : Writer receives TPM, error when sent to xapi Engaged --> Direct : Reader sent TPM to xapi, finds an empty queue Engaged --> Engaged : Writer receives TPM, queue is not full Engaged --> Disengaged : Writer receives TPM, queue is fullStartup At startup, there’s a dedicated routine to transform the existing contents of the cache. This is currently done because the timestamp reference change on each boot. This means that the existing contents might have timestamps considered more recent than timestamps of writes coming from running events, leading to missing content updates. This must be avoided and instead the updates with offending timestamps are renamed to a timestamp taken from the current timestamp, ensuring a consistent ordering. The routine is also used to keep a minimal file tree: unrecognised files are deleted, temporary files created to ensure atomic writes are left untouched, and empty directories are deleted. This mechanism can be changed in the future to migrate to other formats.
`,description:"",tags:null,title:"Xapi-guard",uri:"/new-docs/xapi-guard/index.html"},{content:`Helpful guides for xapi developers.
How to add.... Adding a Class to the API Adding a field to the API Adding a function to the API Adding a XenAPI extension `,description:"",tags:null,title:"Guides",uri:"/new-docs/xapi/guides/index.html"},{content:`This document describes how to add a new class to the data model that defines the Xen Server API. It complements two other documents that describe how to extend an existing class:
Adding a Field Adding a Function As a running example, we will use the addition of a class that is part of the design for the PVS Direct feature. PVS Direct introduces proxies that serve VMs with disk images. This class was added via commit CP-16939 to Xen API.
Example: PVS_server In the world of Xen Server, each important concept like a virtual machine, interface, or users is represented by a class in the data model. A class defines methods and instance variables. At runtime, all class instances are held in an in-memory database. For example, part of [PVS Direct] is a class PVS_server, representing a resource that provides block-level data for virtual machines. The design document defines it to have the following important properties:
Fields (string set) addresses (RO/constructor) IPv4 addresses of the server.
(int) first_port (RO/constructor) First UDP port accepted by the server.
(int) last_port (RO/constructor) Last UDP port accepted by the server.
(PVS_farm ref) farm (RO/constructor) Link to the farm that this server is included in. A PVS_server object must always have a valid farm reference; the PVS_server will be automatically GC’ed by xapi if the associated PVS_farm object is removed.
(string) uuid (R0/runtime) Unique identifier/object reference. Allocated by the server.
Methods (or Functions) (PVS_server ref) introduce (string set addresses, int first_port, int last_port, PVS_farm ref farm) Introduce a new PVS server into the farm. Allowed at any time, even when proxies are in use. The proxies will be updated automatically.
(void) forget (PVS_server ref self) Remove a PVS server from the farm. Allowed at any time, even when proxies are in use. The proxies will be updated automatically.
Implementation Overview The implementation of a class is distributed over several files:
ocaml/idl/datamodel.ml – central class definition ocaml/idl/datamodel_types.ml – definition of releases ocaml/xapi/cli_frontend.ml – declaration of CLI operations ocaml/xapi/cli_operations.ml – implementation of CLI operations ocaml/xapi/records.ml – getters and setters ocaml/xapi/OMakefile – refers to xapi_pvs_farm.ml ocaml/xapi/api_server.ml – refers to xapi_pvs_farm.ml ocaml/xapi/message_forwarding.ml ocaml/xapi/xapi_pvs_farm.ml – implementation of methods, new file Data Model The data model ocaml/idl/datamodel.ml defines the class. To keep the name space tidy, most helper functions are grouped into an internal module:
(* datamodel.ml *) let schema_minor_vsn = 103 (* line 21 -- increment this *) let _pvs_farm = "PVS_farm" (* line 153 *) module PVS_farm = struct (* line 8658 *) let lifecycle = [Prototyped, rel_dundee_plus, ""] let introduce = call ~name:"introduce" ~doc:"Introduce new PVS farm" ~result:(Ref _pvs_farm, "the new PVS farm") ~params: [ String,"name","name of the PVS farm" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let forget = call ~name:"forget" ~doc:"Remove a farm's meta data" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ] ~errs:[ Api_errors.pvs_farm_contains_running_proxies; Api_errors.pvs_farm_contains_servers; ] ~lifecycle ~allowed_roles:_R_POOL_OP () let set_name = call ~name:"set_name" ~doc:"Update the name of the PVS farm" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ; String, "value", "name to be used" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let add_cache_storage = call ~name:"add_cache_storage" ~doc:"Add a cache SR for the proxies on the farm" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ; Ref _sr, "value", "SR to be used" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let remove_cache_storage = call ~name:"remove_cache_storage" ~doc:"Remove a cache SR for the proxies on the farm" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ; Ref _sr, "value", "SR to be removed" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let obj = let null_str = Some (VString "") in let null_set = Some (VSet []) in create_obj (* <---- creates class *) ~name: _pvs_farm ~descr:"machines serving blocks of data for provisioning VMs" ~doccomments:[] ~gen_constructor_destructor:false ~gen_events:true ~in_db:true ~lifecycle ~persist:PersistEverything ~in_oss_since:None ~messages_default_allowed_roles:_R_POOL_OP ~contents: [ uid _pvs_farm ~lifecycle ; field ~qualifier:StaticRO ~lifecycle ~ty:String "name" ~default_value:null_str "Name of the PVS farm. Must match name configured in PVS" ; field ~qualifier:DynamicRO ~lifecycle ~ty:(Set (Ref _sr)) "cache_storage" ~default_value:null_set ~ignore_foreign_key:true "The SR used by PVS proxy for the cache" ; field ~qualifier:DynamicRO ~lifecycle ~ty:(Set (Ref _pvs_server)) "servers" "The set of PVS servers in the farm" ; field ~qualifier:DynamicRO ~lifecycle ~ty:(Set (Ref _pvs_proxy)) "proxies" "The set of proxies associated with the farm" ] ~messages: [ introduce ; forget ; set_name ; add_cache_storage ; remove_cache_storage ] () end let pvs_farm = PVS_farm.obj The class is defined by a call to create_obj and it defines the fields and messages (methods) belonging to the class. Each field has a name, a type, and some meta information. Likewise, each message (or method) is created by call that describes its parameters.
The PVS_farm has additional getter and setter methods for accessing its fields. These are not declared here as part of the messages but are automatically generated.
To make sure the new class is actually used, it is important to enter it into two lists:
(* datamodel.ml *) let all_system = (* line 8917 *) [ ... vgpu_type; pvs_farm; ... ] let expose_get_all_messages_for = [ (* line 9097 *) ... _pvs_farm; _pvs_server; _pvs_proxy; When a field refers to another object that itself refers back to it, these two need to be entered into the all_relations list. For example, _pvs_server refers to a _pvs_farm value via "farm", which, in turn, refers to the _pvs_server value via its "servers" field.
let all_relations = [ (* ... *) (_sr, "introduced_by"), (_dr_task, "introduced_SRs"); (_pvs_server, "farm"), (_pvs_farm, "servers"); (_pvs_proxy, "farm"), (_pvs_farm, "proxies"); ] CLI Conventions The CLI provides access to objects from the command line. The following conventions exist for naming fields:
A field in the data model uses an underscore (_) but a hyphen (-) in the CLI: what is cache_storage in the data model becomes cache-storage in the CLI.
When a field contains a reference or multiple, like proxies, it becomes proxy-uuids in the CLI because references are always referred to by their UUID.
CLI Getters and Setters All fields can be read from the CLI and some fields can also be set via the CLI. These getters and setters are mostly generated automatically and need to be connected to the CLI through a function in ocaml/xapi/records.ml. Note that field names here use the naming convention for the CLI:
(* ocaml/xapi/records.ml *) let pvs_farm_record rpc session_id pvs_farm = let _ref = ref pvs_farm in let empty_record = ToGet (fun () -> Client.PVS_farm.get_record rpc session_id !_ref) in let record = ref empty_record in let x () = lzy_get record in { setref = (fun r -> _ref := r ; record := empty_record) ; setrefrec = (fun (a,b) -> _ref := a; record := Got b) ; record = x ; getref = (fun () -> !_ref) ; fields= [ make_field ~name:"uuid" ~get:(fun () -> (x ()).API.pVS_farm_uuid) () ; make_field ~name:"name" ~get:(fun () -> (x ()).API.pVS_farm_name) ~set:(fun name -> Client.PVS_farm.set_name rpc session_id !_ref name) () ; make_field ~name:"cache-storage" ~get:(fun () -> (x ()).API.pVS_farm_cache_storage |> List.map get_uuid_from_ref |> String.concat "; ") ~add_to_set:(fun sr_uuid -> let sr = Client.SR.get_by_uuid rpc session_id sr_uuid in Client.PVS_farm.add_cache_storage rpc session_id !_ref sr) ~remove_from_set:(fun sr_uuid -> let sr = Client.SR.get_by_uuid rpc session_id sr_uuid in Client.PVS_farm.remove_cache_storage rpc session_id !_ref sr) () ; make_field ~name:"server-uuids" ~get:(fun () -> (x ()).API.pVS_farm_servers |> List.map get_uuid_from_ref |> String.concat "; ") ~get_set:(fun () -> (x ()).API.pVS_farm_servers |> List.map get_uuid_from_ref) () ; make_field ~name:"proxy-uuids" ~get:(fun () -> (x ()).API.pVS_farm_proxies |> List.map get_uuid_from_ref |> String.concat "; ") ~get_set:(fun () -> (x ()).API.pVS_farm_proxies |> List.map get_uuid_from_ref) () ] } CLI Interface to Methods Methods accessible from the CLI are declared in ocaml/xapi/cli_frontend.ml. Each declaration refers to the real implementation of the method, like Cli_operations.PVS_far.introduce:
(* cli_frontend.ml *) let rec cmdtable_data : (string*cmd_spec) list = (* ... *) "pvs-farm-introduce", { reqd=["name"]; optn=[]; help="Introduce new PVS farm"; implementation=No_fd Cli_operations.PVS_farm.introduce; flags=[]; }; "pvs-farm-forget", { reqd=["uuid"]; optn=[]; help="Forget a PVS farm"; implementation=No_fd Cli_operations.PVS_farm.forget; flags=[]; }; CLI Implementation of Methods Each CLI operation that is not a getter or setter has an implementation in cli_operations.ml which is implemented in terms of the real implementation:
(* cli_operations.ml *) module PVS_farm = struct let introduce printer rpc session_id params = let name = List.assoc "name" params in let ref = Client.PVS_farm.introduce ~rpc ~session_id ~name in let uuid = Client.PVS_farm.get_uuid rpc session_id ref in printer (Cli_printer.PList [uuid]) let forget printer rpc session_id params = let uuid = List.assoc "uuid" params in let ref = Client.PVS_farm.get_by_uuid ~rpc ~session_id ~uuid in Client.PVS_farm.forget rpc session_id ref end Fields that should show up in the CLI interface by default are declared in the gen_cmds value:
(* cli_operations.ml *) let gen_cmds rpc session_id = let mk = make_param_funs in List.concat [ (*...*) ; Client.Pool.(mk get_all get_all_records_where get_by_uuid pool_record "pool" [] ["uuid";"name-label";"name-description";"master" ;"default-SR"] rpc session_id) ; Client.PVS_farm.(mk get_all get_all_records_where get_by_uuid pvs_farm_record "pvs-farm" [] ["uuid";"name";"cache-storage";"server-uuids"] rpc session_id) Error messages Error messages used by an implementation are introduced in two files:
(* ocaml/xapi-consts/api_errors.ml *) let pvs_farm_contains_running_proxies = "PVS_FARM_CONTAINS_RUNNING_PROXIES" let pvs_farm_contains_servers = "PVS_FARM_CONTAINS_SERVERS" let pvs_farm_sr_already_added = "PVS_FARM_SR_ALREADY_ADDED" let pvs_farm_sr_is_in_use = "PVS_FARM_SR_IS_IN_USE" let sr_not_in_pvs_farm = "SR_NOT_IN_PVS_FARM" let pvs_farm_cant_set_name = "PVS_FARM_CANT_SET_NAME" (* ocaml/idl/datamodel.ml *) (* PVS errors *) error Api_errors.pvs_farm_contains_running_proxies ["proxies"] ~doc:"The PVS farm contains running proxies and cannot be forgotten." (); error Api_errors.pvs_farm_contains_servers ["servers"] ~doc:"The PVS farm contains servers and cannot be forgotten." (); error Api_errors.pvs_farm_sr_already_added ["farm"; "SR"] ~doc:"Trying to add a cache SR that is already associated with the farm" (); error Api_errors.sr_not_in_pvs_farm ["farm"; "SR"] ~doc:"The SR is not associated with the farm." (); error Api_errors.pvs_farm_sr_is_in_use ["farm"; "SR"] ~doc:"The SR is in use by the farm and cannot be removed." (); error Api_errors.pvs_farm_cant_set_name ["farm"] ~doc:"The name of the farm can't be set while proxies are active." () Method Implementation The implementation of methods lives in a module in ocaml/xapi:
(* ocaml/xapi/api_server.ml *) module PVS_farm = Xapi_pvs_farm The file below is typically a new file and needs to be added to ocaml/xapi/OMakefile.
(* ocaml/xapi/xapi_pvs_farm.ml *) module D = Debug.Make(struct let name = "xapi_pvs_farm" end) module E = Api_errors let api_error msg xs = raise (E.Server_error (msg, xs)) let introduce ~__context ~name = let pvs_farm = Ref.make () in let uuid = Uuid.to_string (Uuid.make_uuid ()) in Db.PVS_farm.create ~__context ~ref:pvs_farm ~uuid ~name ~cache_storage:[]; pvs_farm (* ... *) Messages received on a slave host may or may not be executed there. In the simple case, each methods executes locally:
(* ocaml/xapi/message_forwarding.ml *) module PVS_farm = struct let introduce ~__context ~name = info "PVS_farm.introduce %s" name; Local.PVS_farm.introduce ~__context ~name let forget ~__context ~self = info "PVS_farm.forget"; Local.PVS_farm.forget ~__context ~self let set_name ~__context ~self ~value = info "PVS_farm.set_name %s" value; Local.PVS_farm.set_name ~__context ~self ~value let add_cache_storage ~__context ~self ~value = info "PVS_farm.add_cache_storage"; Local.PVS_farm.add_cache_storage ~__context ~self ~value let remove_cache_storage ~__context ~self ~value = info "PVS_farm.remove_cache_storage"; Local.PVS_farm.remove_cache_storage ~__context ~self ~value end `,description:"",tags:null,title:"Adding a Class to the API",uri:"/new-docs/xapi/guides/howtos/add-class/index.html"},{content:`This page describes how to add a field to XenAPI. A field is a parameter of a class that can be used in functions and read from the API.
Bumping the database schema version Whenever a field is added to or removed from the API, its schema version needs to be increased. XAPI needs this fundamental procedure in order to be able to detect that an automatic database upgrade is necessary or to find out that the new schema is incompatible with the existing database. If the schema version is not bumped, XAPI will start failing in unpredictable ways. Note that bumping the version is not necessary when adding functions, only when adding fields.
The current version number is kept at the top of the file ocaml/idl/datamodel_common.ml in the variables schema_major_vsn and schema_minor_vsn, of which only the latter should be incremented (the major version only exists for historical reasons). When moving to a new XenServer release, also update the variable last_release_schema_minor_vsn to the schema version of the last release. To keep track of the schema versions of recent XenServer releases, the file contains variables for these, such as miami_release_schema_minor_vsn. After starting a new version of Xapi on an existing server, the database is automatically upgraded if the schema version of the existing database matches the value of last_release_schema_*_vsn in the new Xapi.
As an example, the patch below shows how the schema version was bumped when the new API fields used for ActiveDirectory integration were added:
--- a/ocaml/idl/datamodel.ml Tue Nov 11 16:17:48 2008 +0000 +++ b/ocaml/idl/datamodel.ml Tue Nov 11 15:53:29 2008 +0000 @@ -15,17 +15,20 @@ open Datamodel_types open Datamodel_types (* IMPORTANT: Please bump schema vsn if you change/add/remove a _field_. You do not have to dump vsn if you change/add/remove a message *) let schema_major_vsn = 5 -let schema_minor_vsn = 55 +let schema_minor_vsn = 56 (* Historical schema versions just in case this is useful later *) let rio_schema_major_vsn = 5 let rio_schema_minor_vsn = 19 +let miami_release_schema_major_vsn = 5 +let miami_release_schema_minor_vsn = 35 + (* the schema vsn of the last release: used to determine whether we can upgrade or not.. *) let last_release_schema_major_vsn = 5 -let last_release_schema_minor_vsn = 35 +let last_release_schema_minor_vsn = 55 Setting the schema hash In the ocaml/idl/schematest.ml there is the last_known_schema_hash This needs to be updated to be the next hash after the schema version was bumped. Get the new hash by running make test and you will receive the correct hash in the error message.
Adding the new field to some existing class ocaml/idl/datamodel.ml Add a new “field” line to the class in the file ocaml/idl/datamodel.ml or ocaml/idl/datamodel_[class].ml. The new field might require a suitable default value. This default value is used in case the user does not provide a value for the field.
A field has a number of parameters:
The lifecycle parameter, which shows how the field has evolved over time. The qualifier parameter, which controls access to the field. The following values are possible: Value Meaning StaticRO Field is set statically at install-time. DynamicRO Field is computed dynamically at run time. RW Field is read/write. The ty parameter for the type of the field. The default_value parameter. The name of the field. A documentation string. Example of a field in the pool class:
field ~lifecycle:[Published, rel_orlando, "Controls whether HA is enabled"] ~qualifier:DynamicRO ~ty:Bool ~default_value:(Some (VBool false)) "ha_enabled" "true if HA is enabled on the pool, false otherwise"; See datamodel_types.ml for information about other parameters.
Changing Constructors Adding a field would change the constructors for the class – functions Db.*.create – and therefore, any references to these in the code need to be updated. In the example, the argument ~ha_enabled:false should be added to any call to Db.Pool.create.
Examples of where these calls can be found is in ocaml/tests/common/test_common.ml and ocaml/xapi/xapi_[class].ml.
CLI Records If you want this field to show up in the CLI (which you probably do), you will also need to modify the Records module, in the file ocaml/xapi-cli-server/records.ml. Find the record function for the class which you have modified, add a new entry to the fields list using make_field. This type can be found in the same file.
The only required parameters are name and get (and unit, of course ). If your field is a map or set, then you will need to pass in get_{map,set}, and optionally set_{map,set}, if it is a RW field. The hidden parameter is useful if you don’t want this field to show up in a *_params_list call. As an example, here is a field that we’ve just added to the SM class:
make_field ~name:"versioned-capabilities" ~get:(fun () -> Record_util.s2sm_to_string "; " (x ()).API.sM_versioned_capabilities) ~get_map:(fun () -> (x ()).API.sM_versioned_capabilities) ~hidden:true (); Testing The new fields can be tested by copying the newly compiled xapi binary to a test box. After the new xapi service is started, the file /var/log/xensource.log in the test box should contain a few lines reporting the successful upgrade of the metadata schema in the test box:
[...|xapi] Db has schema major_vsn=5, minor_vsn=57 (current is 5 58) (last is 5 57) [...|xapi] Database schema version is that of last release: attempting upgrade [...|sql] attempting to restore database from /var/xapi/state.db [...|sql] finished parsing xml [...|sql] writing db as xml to file '/var/xapi/state.db'. [...|xapi] Database upgrade complete, restarting to use new db Making this field accessible as a CLI attribute XenAPI functions to get and set the value of the new field are generated automatically. It requires some extra work, however, to enable such operations in the CLI.
The CLI has commands such as host-param-list and host-param-get. To make a new field accessible by these commands, the file xapi-cli-server/records.ml needs to be edited. For the pool.ha-enabled field, the pool_record function in this file contains the following (note the convention to replace underscores by hyphens in the CLI):
let pool_record rpc session_id pool = ... [ ... make_field ~name:"ha-enabled" ~get:(fun () -> string_of_bool (x ()).API.pool_ha_enabled) (); ... ]} NB: the ~get parameter must return a string so include a relevant function to convert the type of the field into a string i.e. string_of_bool
See xapi-cli-server/records.ml for examples of handling field types other than Bool.
`,description:"",tags:null,title:"Adding a field to the API",uri:"/new-docs/xapi/guides/howtos/add-field/index.html"},{content:`This page describes how to add a function to XenAPI.
Add message to API The file idl/datamodel.ml is a description of the API, from which the marshalling and handler code is generated.
In this file, the create_obj function is used to define a class which may contain fields and support operations (known as “messages”). For example, the identifier host is defined using create_obj to encapsulate the operations which can be performed on a host.
In order to add a function to the API, we need to add a message to an existing class. This entails adding a function in idl/datamodel.ml or one of the other datamodel files to describe the new message and adding it to the class’s list of messages. In this example, we are adding to idl/datamodel_host.ml.
The function to describe the new message will look something like the following:
let host_price_of = call ~flags:[\`Session] ~name:"price_of" ~in_oss_since:None ~in_product_since:rel_orlando ~params:[(Ref _host, "host", "The host containing the price information"); (String, "item", "The item whose price is queried")] ~result:(Float, "The price of the item") ~doc:"Returns the price of a named item." ~allowed_roles:_R_POOL_OP () By convention, the name of the function is formed from the name of the class and the name of the message: host and price_of, in the example. An entry for host_price_of is added to the messages of the host class:
let host = create_obj ... ~messages: [... host_price_of; ] ... The parameters passed to call are all optional (except ~name and ~in_product_since).
The ~flags parameter is used to set conditions for the use of the message. For example, \`Session is used to indicate that the call must be made in the presence of an existing session.
The value of the ~in_product_since parameter is a string taken from idl/datamodel_types.ml indicates the XenServer release in which this message was first introduced.
The ~params parameter describes a list of the formal parameters of the message. Each parameter is described by a triple. The first component of the triple is the type (from type ty in idl/datamodel_types.ml); the second is the name of the parameter, and the third is a human-readable description of the parameter. The first triple in the list is conventionally the instance of the class on which the message will operate. In the example, this is a reference to the host.
Similarly, the ~result describes the message’s return type, although this is permitted to merely be a single value rather than a list of values. If no ~result is specified, the default is unit.
The ~doc parameter describes what the message is doing.
The bool ~hide_from_docs parameter prevents the message from being included in the documentation when generated.
The bool ~pool_internal parameter is used to indicate if the message should be callable by external systems or only internal hosts.
The ~errs parameter is a list of possible exceptions that the message can raise.
The parameter ~lifecycle takes in an array of (Status, version, doc) to indicate the lifecycle of the message type. This takes over from ~in_oss_since which indicated the release that the message type was introduced. NOTE: Leave this parameter empty, it will be populated on build.
The ~allowed_roles parameter is used for access control (see below).
Compiling xen-api.(hg|git) will cause the code corresponding to this message to be generated and output in ocaml/xapi/server.ml. In the example above, a section handling an incoming call host.price_of appeared in ocaml/xapi/server.ml. However, after this was generated, the rest of the build failed because this call expects a price_of function in the Host object.
Expected values in parameter ~in_product_since In the example above, the value of the parameter ~in_product_since informs that the message host_price_of was added during the rel_orlando release cycle. If a new release cycle is required, then it needs to be added in the file idl/datamodel_types.ml. The patch below shows how the new rel_george release identifier was added. Any class, message, etc. added during the rel_george release cycle should contain ~in_product_since:rel_george entries. (obs: the release and upgrade infrastructure can handle only one new rel_* identifier – in this case, rel_george – in each release)
--- a/ocaml/idl/datamodel_types.ml Tue Nov 11 15:17:48 2008 +0000 +++ b/ocaml/idl/datamodel_types.ml Tue Nov 11 15:53:29 2008 +0000 @@ -27,14 +27,13 @@ (* useful constants for product vsn tracking *) let oss_since_303 = Some "3.0.3" +let rel_george = "george" let rel_orlando = "orlando" let rel_orlando_update_1 = "orlando-update-1" let rel_symc = "symc" let rel_miami = "miami" let rel_rio = "rio" -let release_order = [engp:rel_rio; rel_miami; rel_symc; rel_orlando; rel_orlando_update_1] +let release_order = [engp:rel_rio; rel_miami; rel_symc; rel_orlando; rel_orlando_update_1; rel_george] Update expose_get_all_messages_for list If you are adding a new class, do not forget to add your new class _name to the expose_get_all_messages_for list, at the bottom of datamodel.ml, in order to have automatically generated get_all and get_all_records functions attached to it.
Update the RBAC field containing the roles expected to use the new API call After the RBAC integration, Xapi provides by default a set of static roles associated to the most common subject tasks.
The api calls associated with each role are defined by a new ~allowed_roles parameter in each api call, which specifies the list of static roles that should be able to execute the call. The possible roles for this list is one of the following names, defined in datamodel.ml:
role_pool_admin role_pool_operator role_vm_power_admin role_vm_admin role_vm_operator role_read_only So, for instance,
~allowed_roles:[role_pool_admin,role_pool_operator] (* this is not the recommended usage, see example below *) would be a valid list (though it is not the recommended way of using allowed_roles, see below), meaning that subjects belonging to either role_pool_admin or role_pool_operator can execute the api call.
The RBAC requirements define a policy where the roles in the list above are supposed to be totally-ordered by the set of api-calls associated with each of them. That means that any api-call allowed to role_pool_operator should also be in role_pool_admin; any api-call allowed to role_vm_power_admin should also be in role_pool_operator and also in role_pool_admin; and so on. Datamodel.ml provides shortcuts for expressing these totally-ordered set of roles policy associated with each api-call:
_R_POOL_ADMIN, equivalent to [role_pool_admin] _R_POOL_OP, equivalent to [role_pool_admin,role_pool_operator] _R_VM_POWER_ADMIN, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin] _R_VM_ADMIN, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin] _R_VM_OP, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin,role_vm_op] _R_READ_ONLY, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin,role_vm_op,role_read_only] The ~allowed_roles parameter should use one of the shortcuts in the list above, instead of directly using a list of roles, because the shortcuts above make sure that the roles in the list are in a total order regarding the api-calls permission sets. Creating an api-call with e.g. allowed_roles:[role_pool_admin,role_vm_admin] would be wrong, because that would mean that a pool_operator cannot execute the api-call that a vm_admin can, breaking the total-order policy expected in the RBAC 1.0 implementation. In the future, this requirement might be relaxed.
So, the example above should instead be used as:
~allowed_roles:_R_POOL_OP (* recommended usage via pre-defined totally-ordered role lists *) and so on.
How to determine the correct role of a new api-call: if only xapi should execute the api-call, ie. it is an internal call: _R_POOL_ADMIN if it is related to subject, role, external-authentication: _R_POOL_ADMIN if it is related to accessing Dom0 (via console, ssh, whatever): _R_POOL_ADMIN if it is related to the pool object: R_POOL_OP if it is related to the host object, licenses, backups, physical devices: _R_POOL_OP if it is related to managing VM memory, snapshot/checkpoint, migration: _R_VM_POWER_ADMIN if it is related to creating, destroying, cloning, importing/exporting VMs: _R_VM_ADMIN if it is related to starting, stopping, pausing etc VMs or otherwise accessing/manipulating VMs: _R_VM_OP if it is related to being able to login, manipulate own tasks and read values only: _R_READ_ONLY Update message forwarding The “message forwarding” layer describes the policy of whether an incoming API call should be forwarded to another host (such as another member of the pool) or processed on the host which receives the call. This policy may be non-trivial to describe and so cannot be auto-generated from the data model.
In xapi/message_forwarding.ml, add a function to the relevant module to describe this policy. In the running example, we add the following function to the Host module:
let price_of ~__context ~host ~item = info "Host.price_of for item %s" item; let local_fn = Local.Host.price_of ~host ~item in do_op_on ~local_fn ~__context ~host (fun session_id rpc -> Client.Host.price_of ~rpc ~session_id ~host ~item) After the ~__context parameter, the parameters of this new function should match the parameters we specified for the message. In this case, that is the host and the item to query the price of.
The do_op_on function takes a function to execute locally and a function to execute remotely and performs one of these operations depending on whether the given host is the local host.
The local function references Local.Host.price_of, which is a function we will write in the next step.
Implement the function Now we write the function to perform the logic behind the new API call. For a host-based call, this will reside in xapi/xapi_host.ml. For other classes, other files with similar names are used.
We add the following function to xapi/xapi_host.ml:
let price_of ~__context ~host ~item = if item = "fish" then 3.14 else 0.00 We also need to add the function to the interface xapi/xapi_host.mli:
val price_of : __context:Context.t -> host:API.ref_host -> item:string -> float Congratulations, you’ve added a function to the API!
Add the operation to the CLI Edit xapi-cli-server/cli_frontend.ml. Add a block to the definition of cmdtable_data as in the following example:
"host-price-of", { reqd=["host-uuid"; "item"]; optn=[]; help="Find out the price of an item on a certain host."; implementation= No_fd Cli_operations.host_price_of; flags=[]; }; Include here the following:
The names of required (reqd) and optional (optn) parameters.
A description to be displayed when calling xe help <cmd> in the help field.
The implementation should use With_fd if any communication with the client is necessary (for example, showing the user a warning, sending the contents of a file, etc.) Otherwise, No_fd can be used as above.
The flags field can be used to set special options:
Vm_selectors: adds a “vm” parameter for the name of a VM (rather than a UUID) Host_selectors: adds a “host” parameter for the name of a host (rather than a UUID) Standard: includes the command in the list of common commands displayed by xe help Neverforward: Hidden: Deprecated of string list: Now we must implement Cli_operations.host_price_of. This is done in xapi-cli-server/cli_operations.ml. This function typically extracts the parameters and forwards them to the internal implementation of the function. Other arbitrary code is permitted. For example:
let host_price_of printer rpc session_id params = let host = Client.Host.get_by_uuid rpc session_id (List.assoc "host-uuid" params) in let item = List.assoc "item" params in let price = string_of_float (Client.Host.price_of ~rpc ~session_id ~host ~item) in printer (Cli_printer.PList [price]) Tab Completion in the CLI The CLI features tab completion for many of its commands’ parameters. Tab completion is implemented in the file ocaml/xe-cli/bash-completion, which is installed on the host as /etc/bash_completion.d/cli, and is done on a parameter-name rather than on a command-name basis. The main portion of the bash-completion file is a case statement that contains a section for each of the parameters that benefit from completion. There is also an entry that catches all parameter names ending at -uuid, and performs an automatic lookup of suitable UUIDs. The host-uuid parameter of our new host-price-of command therefore automatically gains completion capabilities.
Executing the CLI operation Recompile xapi with the changes described above and install it on a test machine.
Execute the following command to see if the function exists:
xe help host-price-of Invoke the function itself with the following command:
xe host-price-of host-uuid=<tab> item=fish and you should find out the price of fish.
`,description:"",tags:null,title:"Adding a function to the API",uri:"/new-docs/xapi/guides/howtos/add-function/index.html"},{content:`A XenAPI extension is a new RPC which is implemented as a separate executable (i.e. it is not part of xapi) but which still benefits from xapi parameter type-checking, multi-language stub generation, documentation generation, authentication etc. An extension can be backported to previous versions by simply adding the implementation, without having to recompile xapi itself.
A XenAPI extension is in two parts:
a declaration in the xapi datamodel. This must use the ~forward_to:(Extension "filename") parameter. The filename must be unique, and should be the same as the XenAPI call name. an implementation executable in the dom0 filesystem with path /etc/xapi.d/extensions/filename To define an extension First write the declaration in the datamodel. The act of specifying the types and writing the documentation will help clarify the intended meaning of the call.
Second create a prototype of your implementation and put an executable file in /etc/xapi.d/extensions/filename. The calling convention is:
the file must be executable xapi will parse the XMLRPC call arguments received over the network and check the session_id is valid xapi will execute the named executable the XMLRPC call arguments will be sent to the executable on stdin and stdin will be closed afterwards the executable will run and print an XMLRPC response on stdout xapi will read the response and return it to the client. See the basic example.
Second make a pull request containing only the datamodel definitions (it is not necessary to include the prototype too). This will attract review comments which will help you improve your API further. Once the pull request is merged, then the API call name and extension are officially yours and you may use them on any xapi version which supports the extension mechanism.
Packaging your extension Your extension /etc/xapi.d/extensions/filename (and dependencies) should be packaged for your target distribution (for XenServer dom0 this would be a CentOS RPM). Once the package is unpacked on the target machine, the extension should be immediately callable via the XenAPI, provided the xapi version supports the extension mechanism. Note the xapi version does not need to know about the specific extension in advance: it will always look in /etc/xapi.d/extensions/ for all RPC calls whose name it does not recognise.
Limitations On type-checking
if the xapi version is new enough to know about your specific extension: xapi will type-check the call arguments for you if the xapi version is too old to know about your specific extension: the extension will still be callable but the arguments will not be type-checked. On access control
if the xapi version is new enough to know about your specific extension: you can declare that a user must have a particular role (e.g. ‘VM admin’) if the xapi version is too old to know about your specific extension: the extension will still be callable but the client must have the ‘Pool admin’ role. Since a xapi which knows about your specific extension is stricter than an older xapi, it’s a good idea to develop against the new xapi and then test older xapi versions later.
`,description:"",tags:null,title:"Adding a XenAPI extension",uri:"/new-docs/xapi/guides/howtos/add-api-extension/index.html"},{content:`Squeezed is responsible for managing the memory on a single host. Squeezed “balances” memory between VMs according to a policy written to Xenstore.
The following diagram shows the internals of Squeezed:
At the center of squeezed is an abstract model of a Xen host. The model includes:
The amount of already-used host memory (used by fixed overheads such as Xen and the crash kernel). Per-domain memory policy specifically dynamic-min and dynamic-max which together describe a range, within which the domain’s actual used memory should remain. Per-domain calibration data which allows us to compute the necessary balloon target value to achive a particular memory usage value. Squeezed is a single-threaded program which receives commands from xenopsd over a Unix domain socket. When Xenopsd wishes to start a new VM, squeezed will be asked to create a “reservation”. Note this is different to the Xen notion of a reservation. A squeezed reservation consists of an amount of memory squeezed will guarantee to keep free labelled with an id. When Xenopsd later creates the domain to notionally use the reservation, the reservation is “transferred” to the domain before the domain is built.
Squeezed will also wake up every 30s and attempt to rebalance the memory on a host. This is useful to correct imbalances caused by balloon drivers temporarily failing to reach their targets. Note that ballooning is fundamentally a co-operative process, so squeezed must handle cases where the domains refuse to obey commands.
The “output” of squeezed is a list of “actions” which include:
Set domain x’s memory/target to a new value. Set the maxmem of a domain to a new value (as a hard limit beyond which the domain cannot allocate). `,description:"",tags:null,title:"Architecture",uri:"/new-docs/squeezed/architecture/index.html"},{content:`Xenopsd instances run on a host and manage VMs on behalf of clients. This picture shows 3 different Xenopsd instances: 2 named “xenopsd-xc” and 1 named “xenopsd-xenlight”.
Each instance is responsible for managing a disjoint set of VMs. Clients should never ask more than one Xenopsd to manage the same VM. Managing a VM means:
handling start/shutdown/suspend/resume/migrate/reboot allowing devices (disks, nics, PCI cards, vCPUs etc) to be manipulated providing updates to clients when things change (reboots, console becomes available, guest agent says something etc). For a full list of features, consult the features list.
Each Xenopsd instance has a unique name on the host. A typical name is
org.xen.xcp.xenops.classic org.xen.xcp.xenops.xenlight A higher-level tool, such as xapi will associate VMs with individual Xenopsd names.
Running multiple Xenopsds is necessary because
The virtual hardware supported by different technologies (libxc, libxl, qemu) is expected to be different. We can guarantee the virtual hardware is stable across a rolling upgrade by running the VM on the old Xenopsd. We can then switch Xenopsds later over a VM reboot when the VM admin is happy with it. If the VM admin is unhappy then we can reboot back to the original Xenopsd again. The suspend/resume/migrate image formats will differ across technologies (again libxc vs libxl) and it will be more reliable to avoid switching technology over a migrate. In the future different security domains may have different Xenopsd instances providing even stronger isolation guarantees between domains than is possible today. Communication with Xenopsd is handled through a Xapi-global library: xcp-idl. This library supports
message framing: by default using HTTP but a binary framing format is available message encoding: by default we use JSON but XML is also available RPCs over Unix domain sockets and persistent queues. This library allows the communication details to be changed without having to change all the Xapi clients and servers.
Xenopsd has a number of “backends” which perform the low-level VM operations such as (on Xen) “create domain” “hotplug disk” “destroy domain”. These backends contain all the hypervisor-specific code including
connecting to Xenstore opening the libxc /proc/xen/privcmd interface initialising libxl contexts The following diagram shows the internal structure of Xenopsd:
At the top of the diagram two client RPC have been sent: one to start a VM and the other to fetch the latest events. The RPCs are all defined in xcp-idl/xen/xenops_interface.ml. The RPCs are received by the Xenops_server module and decomposed into “micro-ops” (labelled “μ op”). These micro ops represent actions like
create a Xen domain (recall a Xen domain is an empty shell with no memory) build a Xen domain: this is where the kernel or hvmloader is copied in launch a device model: this is where a qemu instance is started (if one is required) hotplug a device: this involves writing the frontend and backend trees to Xenstore unpause a domain (recall a Xen domain is created in the paused state) Each of these micro-ops is represented by a function call in a “backend plugin” interface. The micro-ops are enqueued in queues, one queue per VM. There is a thread pool (whose size can be changed dynamically by the admin) which pulls micro-ops from the VM queues and calls the corresponding backend function.
The active backend (there can only be one backend per Xenopsd instance) executes the micro-ops. The Xenops_server_xen backend in the picture above talks to libxc, libxl and qemu to create and destroy domains. The backend also talks to other Xapi services, in particular
it registers datasources with xcp-rrdd, telling xcp-rrdd to measure I/O throughput and vCPU utilisation it reserves memory for new domains by talking to squeezed it makes disks available by calling SMAPIv2 VDI.{at,de}tach, VDI.{,de}activate it launches subprocesses by talking to forkexecd (avoiding problems with accidental fd capture) Xenopsd backends are also responsible for monitoring running VMs. In the Xenops_server_xen backend this is done by watching Xenstore for
@releaseDomain watch events device hotplug status changes When such an event happens (for example: @releaseDomain sent when a domain requests a reboot) the corresponding operation does not happen inline. Instead the event is rebroadcast upwards to Xenops_server as a signal (for example: “VM id needs some attention”) and a “VM_stat” micro-op is queued in the appropriate queue. Xenopsd does not allow operations to run on the same VM in parallel and enforces this by:
pushing all operations pertaining to a VM to the same queue associating each VM queue to at-most-one worker pool thread The event takes the form “VM id needs some attention” and not “VM id needs to be rebooted” because, by the time the queue is flushed, the VM may well now be in a different state. Perhaps rather than being rebooted it now needs to be shutdown; or perhaps the domain is now in a good state because the reboot has already happened. The signals sent by the backend to the Xenops_server are a bit like event channel notifications in the Xen ring protocols: they are requests to ask someone to perform work, they don’t themselves describe the work that needs to be done.
An implication of this design is that it should always be possible to answer the question, “what operation should be performed to get the VM into a valid state?”. If an operation is cancelled half-way through or if Xenopsd is suddenly restarted, it will ask the question about all the VMs and perform the necessary operations. The operations must be designed carefully to make this work. For example if Xenopsd is restarted half-way through starting a VM, it must be obvious on restart that the VM should either be forcibly shutdown or rebooted to make it a valid state again. Note: we don’t demand that operations are performed as transactions; we only demand that the state they leave the system be “sensible” in the sense that the admin will recognise it and be able to continue their work.
Sometimes this can be achieved through careful ordering of side-effects within the operations, taking advantage of artifacts of the system such as:
a domain which has not been fully created will have total vCPU time = 0 and will be paused. If we see one of these we should reboot it because it may not be fully intact. In the absense of “tells” from the system, operations are expected to journal their intentions and support restart after failure.
There are three categories of metadata associated with VMs:
system metadata: this is created as a side-effect of starting VMs. This includes all the information about active disks and nics stored in Xenstore and the list of running domains according to Xen. VM: this is the configuration to use when the VM is started or rebooted. This is like a “config file” for the VM. VmExtra: this is the runtime configuration of the VM. When VM configuration is changed it often cannot be applied immediately; instead the VM continues to run with the previous configuration. We need to track the runtime configuration of the VM in order for suspend/resume and migrate to work. It is also useful to be able to tell a client, “on next reboot this value will be x but currently it is x-1”. VM and VmExtra metadata is stored by Xenopsd in the domain 0 filesystem, in a simple directory hierarchy.
`,description:"",tags:null,title:"Architecture",uri:"/new-docs/xenopsd/architecture/index.html"},{content:"",description:"",tags:null,title:"Categories",uri:"/new-docs/categories/index.html"},{content:"",description:"",tags:null,title:"Database",uri:"/new-docs/xapi/database/index.html"},{content:`Squeezed is a single host memory ballooning daemon. It helps by:
Allowing VM memory to be adjusted dynamically without having to reboot; and Avoiding wasting memory by keeping everything fully utilised, while retaining the ability to take memory back to start new VMs. Squeezed currently includes a simple Ballooning policy which serves as a useful default. The policy is written with respect to an abstract Xen memory model, which is based on a number of assumptions about the environment, for example that most domains have co-operative balloon drivers. In theory the policy could be replaced later with something more sophisticated (for example see [xenballoond](https://github.com/avsm/xen-unstable/blob/master/tools/xenballoon/ xenballoond.README)).
The Toolstack interface is used by Xenopsd to free memory for starting new VMs. Although the only known client is Xenopsd, the interface can in theory be used by other clients. Multiple clients can safely use the interface at the same time.
The internal structure consists of a single-thread event loop. To see how it works end-to-end, consult the example.
No software is ever perfect; to understand the flaws in Squeezed, please consult the list of issues.
Environmental assumptions The Squeezed daemon runs within a Xen domain 0 and communicates to xenstored via a Unix domain socket. Therefore Squeezed is granted full access to xenstore, enabling it to modify every domain’s memory/target.
The Squeezed daemon calls setmaxmem in order to cap the amount of memory a domain can use. This relies on a patch to xen which allows maxmem to be set lower than totpages See Section maxmem for more information.
The Squeezed daemon assumes that only domains which write control/feature-balloon into xenstore can respond to ballooning requests. It will not ask any other domains to balloon.
The Squeezed daemon assumes that the memory used by a domain is: (i) that listed in domain_getinfo as totpages; (ii) shadow as given by shadow_allocation_get; and (iii) a small (few KiB) of miscellaneous Xen structures (e.g. for domains, vcpus) which are invisible.
The Squeezed daemon assumes that a domain which is created with a particular memory/target (and startmem, to within rounding error) will reach a stable value of totpages before writing control/feature-balloon. The daemon writes this value to memory/memory-offset for future reference.
The Squeezed daemon does not know or care exactly what causes the difference between totpages and memory/target and it does not expect it to remain constant across Xen releases. It only expects the value to remain constant over the lifetime of a domain. The Squeezed daemon assumes that the balloon driver has hit its target when difference between memory/target and totpages equals the memory-offset value.
Corrollary: to make a domain with a responsive balloon driver currenty using totpages allocate or free x it suffices to set memory/target to x+totpages-memoryoffset and wait for the balloon driver to finish. See Section memory model for more detail. The Squeezed daemon must maintain a “slush fund” of memory (currently 9MiB) which it must prevent any domain from allocating. Since (i) some Xen operations (such as domain creation) require memory within a physical address range (e.g. less than 4GiB) and (ii) since Xen preferentially allocates memory outside these ranges, it follows that by preventing guests from allocating all host memory (even transiently) we guarantee that memory from within these special ranges is always available. Squeezed operates in two phases: first causing memory to be freed; and second causing memory to be allocated.
The Squeezed daemon assumes that it may set memory/target to any value within range: memory/dynamic-max to memory/dynamic-min
The Squeezed daemon assumes that the probability of a domain booting successfully may be increased by setting memory/target closer to memory/static-max.
The Squeezed daemon assumes that, if a balloon driver has not made any visible progress after 5 seconds, it is effectively inactive. Active domains will be expected to pick up the slack.
Toolstack interface The toolstack interface introduces the concept of a reservation. A reservation is: an amount of host free memory tagged with an associated reservation id. Note this is an internal Squeezed concept and Xen is completely unaware of it. When the daemon is moving memory between domains, it always aims to keep
where s is the size of the “slush fund” (currently 9MiB) and is the amount corresponding to the ith reservation.
As an aside: Earlier versions of Squeezed always associated memory with a Xen domain. Unfortunately this required domains to be created before memory was freed which was problematic because domain creation requires small amounts of contiguous frames. Rather than implement some form of memory defragmentation, Squeezed and Xenopsd were modified to free memory before creating a domain. This necessitated making memory reservations first-class stand-alone entities.
Once a reservation is made (and the corresponding memory is freed), it can be transferred to a domain created by a toolstack. This associates the reservation with that domain so that, if the domain is destroyed, the reservation is also freed. Note that Squeezed is careful not to count both a domain’s reservation and its totpages during e.g. domain building: instead it considers the domain’s allocation to be the maximum of reservation and totpages.
The size of a reservation may either be specified exactly by the caller or the caller may provide a memory range. If a range is provided the daemon will allocate at least as much as the minimum value provided and as much as possible up to the maximum. By allocating as much memory as possible to the domain, the probability of a successful boot is increased.
Clients of the Squeezed provide a string name when they log in. All untransferred reservations made by a client are automatically deleted when a client logs in. This prevents memory leaks where a client crashes and loses track of its own reservation ids.
The interface looks like this:
string session_id login( string client_name ) string reservation_id reserve_memory( string client_name, int kib ) int amount, string reservation_id reserve_memory_range( string client_name, int min, int max ) void delete_reservation( string client_name, string reservation_id ) void transfer_reservation_to_domain( string client_name, string reservation_id, int domid ) The Xenopsd code in pseudocode works as follows:
r_id = reserve_memory_range("xenopsd", min, max); try: d = domain_create() transfer_reservation_to_domain("xenopsd", r_id, d) with: delete_reservation("xenopsd", r_id) The interface is currently implemented using a trivial RPC protocol over a Unix domain socket in domain 0.
Ballooning policy This section describes the very simple default policy currently built-into Squeezed.
Every domain has a pair of values written into xenstore: memory/dynamic-min and memory/dynamic-max with the following meanings:
memory/dynamic-min the lowest value that Squeezed is allowed to set memory/target. The administrator should make this as low as possible but high enough to ensure that the applications inside the domain actually work. memory/dynamic-max the highest value that Squeezed is allowed to set memory/target. This can be used to dynamically cap the amount of memory a domain can use. If all balloon drivers are responsive then Squeezed daemon allocates memory proportionally, so that each domain has the same value of: So:
if memory is plentiful then all domains will have memory/target=memory/dynamic-max
if memory is scarce then all domains will have memory/target=memory/dynamic-min
Note that the values of memory/target suggested by the policy are ideal values. In many real-life situations (e.g. when a balloon driver fails to make progress and is declared inactive) the memory/target values will be different.
Note that, by default, domain 0 has memory/dynamic-min=memory/dynamic-max, effectively disabling ballooning. Clearly a more sophisticated policy would be required here since ballooning down domain 0 as extra domains are started would be counterproductive while backends and control interfaces remain in domain 0.
The memory model Squeezed considers a ballooning-aware domain (i.e. one which has written the feature-balloon flag into xenstore) to be completely described by the parameters:
dynamic-min: policy value written to memory/dynamic-min in xenstore by a toolstack (see Section Ballooning policy)
dynamic-max: policy value written to memory/dynamic-max in xenstore by a toolstack (see Section Ballooning policy)
target: balloon driver target written to memory/target in xenstore by Squeezed.
totpages: instantaneous number of pages used by the domain as returned by the hypercall domain_getinfo
memory-offset: constant difference between target and totpages when the balloon driver believes no ballooning is necessary: where memory-offset = totpages - target when the balloon driver believes it has reached its target.
maxmem: upper limit on totpages: where totpages <= maxmem
For convenience we define a adjusted-target to be the target value necessary to cause a domain currently using totpages to maintain this value indefinitely so adjusted-target = totpages - memory-offset.
The Squeezed daemon believes that:
a domain should be ballooning iff adjusted-target <> target (unless it has become inactive)
a domain has hit its target iff adjusted-target = target (to within 1 page);
if a domain has target = x then, when ballooning is complete, it will have totpages = memory-offset + x; and therefore
to cause a domain to free y it sufficies to set target := totpages - memory-offset - y.
The Squeezed daemon considers non-ballooning aware domains (i.e. those which have not written feature-balloon) to be represented by pairs of:
totpages: instantaneous number of pages used by the domain as returned by domain_getinfo
reservation: memory initially freed for this domain by Squeezed after a transfer_reservation_to_domid call
Note that non-ballooning aware domains will always have startmem = target since the domain will not be instructed to balloon. Since a domain which is being built will have 0 <= totpages <= reservation, Squeezed computes and subtracts this from its model of the host’s free memory, ensuring that it doesn’t accidentally reallocate this memory for some other purpose.
The Squeezed daemon believes that:
all guest domains start out as non-ballooning aware domains where target=reservation=startmem$;
some guest domains become ballooning-aware during their boot sequence i.e. when they write feature-balloon
The Squeezed daemon considers a host to be represented by:
ballooning domains: a set of domains which Squeezed will instruct to balloon;
other domains: a set of booting domains and domains which have no balloon drivers (or whose balloon drivers have failed)
a “slush fund” of low memory required for Xen
physinfo.free_pages total amount of memory instantanously free (including both free_pages and scrub_pages)
reservations: batches of free memory which are not (yet) associated with any domain
The Squeezed daemon considers memory to be unused (i.e. not allocated for any useful purpose) if it is neither in use by a domain nor reserved.
The main loop The main loop is triggered by either:
the arrival of an allocation request on the toolstack interface; or
the policy engine – polled every 10s – deciding that a target adjustment is needed.
Each iteration of the main loop generates the following actions:
Domains which were active but have failed to make progress towards their target in 5s are declared inactive. These domains then have: maxmem set to the minimum of target and totpages.
Domains which were inactive but have started to make progress towards their target are declared active. These domains then have: maxmem set to target.
Domains which are currently active have new targets computed according to the policy (see Section Ballooning policy). Note that inactive domains are ignored and not expected to balloon.
Note that domains remain classified as inactive only during one run of the main loop. Once the loop has terminated all domains are optimistically assumed to be active again. Therefore should a domain be classified as inactive once, it will get many later chances to respond.
The targets are set in two phases. The maxmem is used to prevent domains suddenly allocating more memory than we want them to.
The main loop has a notion of a host free memory “target”, similar to the existing domain memory target. When we are trying to free memory (e.g. for starting a new VM), the host free memory “target” is increased. When we are trying to distribute memory among guests (e.g. after a domain has shutdown and freed lots of memory), the host free memory “target” is low. Note the host free memory “target” is always at least several MiB to ensure that some host free memory with physical address less than 4GiB is free (see Two phase target setting for related information).
The main loop terminates when all active domains have reached their targets (this could be because all domains responded or because they all wedged and became inactive); and the policy function hasn’t suggested any new target changes. There are three possible results:
Success if the host free memory is near enough its “target”;
Failure if the operation is simply impossible within the policy limits (i.e. dynamic_min values are too high;
Failure if the operation failed because one or more domains became inactive and this prevented us from reaching our host free memory “target”.
Note that, since only active domains have their targets set, the system effectively rewards domains which refuse to free memory (inactive) and punishes those which do free memory (active). This effect is countered by signalling to the admin which domains/VMs aren’t responding so they can take corrective action. To achieve this, the daemon monitors the list of inactive domains and if a domain is inactive for more than 20s it writes a flag into xenstore memory/uncooperative. This key can be monitored and used to generate an alert, if desired.
Two phase target setting The following diagram shows how a system with two domains can evolve if domain memory/target values are increased for some domains and decreased for others, at the same time. Each graph shows two domains (domain 1 and domain 2) and a host. For a domain, the square box shows its adjusted-totpages and the arrow indicates the direction of the memory/target. For the host the square box indicates total free memory. Note the highlighted state where the host’s free memory is temporarily exhausted
In the initial state (at the top of the diagram), there are two domains, one which has been requested to use more memory and the other requested to use less memory. In effect the memory is to be transferred from one domain to the other. In the final state (at the bottom of the diagram), both domains have reached their respective targets, the memory has been transferred and the host free memory is at the same value it was initially. However the system will not move atomically from the initial state to the final: there are a number of possible transient in-between states, two of which have been drawn in the middle of the diagram. In the left-most transient state the domain which was asked to free memory has freed all the memory requested: this is reflected in the large amount of host memory free. In the right-most transient state the domain which was asked to allocate memory has allocated all the memory requested: now the host’s free memory has hit zero.
If the host’s free memory hits zero then Xen has been forced to give all memory to guests, including memory less than 4GiB which is critical for allocating certain structures. Even if we ask a domain to free memory via the balloon driver there is no guarantee that it will free the useful memory. This leads to an annoying failure mode where operations such as creating a domain free due to ENOMEM despite the fact that there is apparently lots of memory free.
The solution to this problem is to adopt a two-phase memory/target setting policy. The Squeezed daemon forces domains to free memory first before allowing domains to allocate, in-effect forcing the system to move through the left-most state in the diagram above.
Use of maxmem The Xen domain maxmem value is used to limit memory allocations by the domain. The rules are:
if the domain has never been run and is paused then maxmem is set to reservation (reservations were described in the Toolstack interface section above);
these domains are probably still being built and we must let them allocate their startmem
FIXME: this “never been run” concept pre-dates the feature-balloon flag: perhaps we should use the feature-balloon flag instead.
if the domain is running and the balloon driver is thought to be working then maxmem is set to target; and
there may be a delay between lowering a target and the domain noticing so we prevent the domain from allocating memory when it should in fact be deallocating. if the domain is running and the balloon driver is thought to be inactive then maxmem is set to the minimum of target and actual.
if the domain is using more memory than it should then we allow it to make progress down towards its target; however
if the domain is using less memory than it should then we must prevent it from suddenly waking up and allocating more since we have probably just given it to someone else
FIXME: should we reduce the target to leave the domain in a neutral state instead of asking it to allocate and fail forever?
Example operation The diagram shows an initial system state comprising 3 domains on a single host. The state is not ideal; the domains each have the same policy settings (dynamic-min and dynamic-max) and yet are using differing values of adjusted-totpages. In addition the host has more memory free than desired. The second diagram shows the result of computing ideal target values and the third diagram shows the result after targets have been set and the balloon drivers have responded.
The scenario above includes 3 domains (domain 1, domain 2, domain 3) on a host. Each of the domains has a non-ideal adjusted-totpages value.
Recall we also have the policy constraint that: dynamic-min <= target <= dynamic-max Hypothetically if we reduce target by target-dynamic-min (i.e. by setting target to dynamic-min) then we should reduce totpages by the same amount, freeing this much memory on the host. In the upper-most graph in the diagram above, the total amount of memory which would be freed if we set each of the 3 domain’s target to dynamic-min is: d1 + d2 + d3. In this hypothetical situation we would now have x + s + d1 + d2 + d3 free on the host where s is the host slush fund and x is completely unallocated. Since we always want to keep the host free memory above s, we are free to return x + d1 + d2 + d3 to guests. If we use the default built-in proportional policy then, since all domains have the same dynamic-min and dynamic-max, each gets the same fraction of this free memory which we call g: For each domain, the ideal balloon target is now target = dynamic-min + g. Squeezed does not set all the targets at once: this would allow the allocating domains to race with the deallocating domains, potentially allowing all low memory to be allocated. Therefore Squeezed sets the targets in two phases.
The structure of the daemon Squeezed is a single-threaded daemon which is started by an init.d script. It sits waiting for incoming requests on its toolstack interface and checks every 10s whether all domain targets are set to the ideal values (recall the Ballooning policy). If an allocation request arrives or if the domain targets require adjusting then it calls into the module squeeze_xen.ml.
The module src/squeeze_xen.ml contains code which inspects the state of the host (through hypercalls and reading xenstore) and creates a set of records describing the current state of the host and all the domains. Note this snapshot of state is not atomic – it is pieced together from multiple hypercalls and xenstore reads – we assume that the errors generated are small and we ignore them. These records are passed into the squeeze.ml module where they are processed and converted into a list of actions i.e. (i) updates to memory/target and; (ii) declarations that particular domains have become inactive or active. The rationale for separating the Xen interface from the main ballooning logic was to make testing easier: the module test/squeeze_test.ml contains a simple simulator which allows various edge-cases to be checked.
Issues If a linux domU kernel has the netback, blkback or blktap modules then they away pages via alloc_empty_pages_and_pagevec() during boot. This interacts with the balloon driver to break the assumption that, reducing the target by x from a neutral value should free x amount of memory.
Polling the state of the host (particular the xenstore contents) is a bit inefficient. Perhaps we should move the policy values dynamic_min and dynamic_max to a separate place in the xenstore tree and use watches instead.
The memory values given to the domain builder are in units of MiB. We may wish to similarly quantise the target value or check that the memory-offset calculation still works.
The Xen patch queue reintroduces the lowmem emergency pool. This was an attempt to prevent guests from allocating lowmem before we switched to a two-phase target setting procedure. This patch can probably be removed.
It seems unnecessarily evil to modify an inactive domain’s maxmem leaving maxmem less than target, causing the guest to attempt allocations forwever. It’s probably neater to move the target at the same time.
Declaring a domain active just because it makes small amounts of progress shouldn’t be enough. Otherwise a domain could free 1 byte (or maybe 1 page) every 5s.
Likewise, declaring a domain “uncooperative” only if it has been inactive for 20s means that a domain could alternate between inactive for 19s and active for 1s and not be declared “uncooperative”.
Document history Version Date Change 0.2 10th Nov 2014 Update to markdown 0.1 9th Nov 2009 Initial version `,description:"",tags:null,title:"Design",uri:"/new-docs/squeezed/design/index.html"},{content:"",description:"",tags:null,title:"Design",uri:"/new-docs/xenopsd/design/index.html"},{content:`The HA feature will restart VMs after hosts have failed, but what happens if a whole site (e.g. datacenter) is lost? A disaster recovery configuration is shown in the following diagram:
We rely on the storage array’s built-in mirroring to replicate (synchronously or asynchronously: the admin’s choice) between the primary and the secondary site. When DR is enabled the VM disk data and VM metadata are written to the storage server and mirrored. The secondary site contains the other side of the data mirror and a set of hosts, which may be powered off.
In normal operation, the DR feature allows a “dry-run” recovery where a host on the secondary site checks that it can indeed see all the VM disk data and metadata. This should be done regularly, so that admins are familiar with the process.
After a disaster, the admin breaks the mirror on the secondary site and triggers a remote power-on of the offline hosts (either using an out-of-band tool or the built-in host power-on feature of xapi). The pool master on the secondary site can connect to the storage and extract all the VM metadata. Finally the VMs can all be restarted.
When the primary site is fully recovered, the mirror can be re-synchronised and the VMs can be moved back.
`,description:"",tags:null,title:"Disaster Recovery",uri:"/new-docs/toolstack/features/DR/index.html"},{content:`Introduction Xapi, xenopsd and xenstore use a number of different events to obtain indications that some state changed in dom0 or in the guests. The events are used as an efficient alternative to polling all these states periodically.
xenstore provides a very configurable approach in which each and any key can be watched individually by a xenstore client. Once the value of a watched key changes, xenstore will indicate to the client that the value for that key has changed. An ocaml xenstore client library provides a way for ocaml programs such as xenopsd, message-cli and rrdd to provide high-level ocaml callback functions to watch specific key. It’s very common, for instance, for xenopsd to watch specific keys in the xenstore keyspace of a guest and then after receiving events for some or all of them, read other keys or subkeys in xenstored to update its internal state mirroring the state of guests and its devices (for instance, if the guest has pv drivers and specific frontend devices have established connections with the backend devices in dom0). xapi also provides a very configurable event mechanism in which the xenapi can be used to provide events whenever a xapi object (for instance, a VM, a VBD etc) changes state. This event mechanism is very reliable and is extensively used by XenCenter to provide real-time update on the XenCenter GUI. xenopsd provides a somewhat less configurable event mechanism, where it always provides signals for all objects (VBDs, VMs etc) whose state changed (so it’s not possible to select a subset of objects to watch for as in xenstore or in xapi). It’s up to the xenopsd client (eg. xapi) to receive these events and then filter out or act on each received signal by calling back xenopsd and asking it information for the specific signalled object. The main use in xapi for the xenopsd signals is to update xapi’s database of the current state of each object controlled by xenopsd (VBDs, VMs etc). Given a choice between polling states and receiving events when the state change, we should in general opt for receiving events in the code in order to avoid adding bottlenecks in dom0 that will prevent the scalability of XenServer to many VMs and virtual devices.
Xapi Sending events from the xenapi A xenapi user client, such as XenCenter, the xe-cli or a python script, can register to receive events from XAPI for specific objects in the XAPI DB. XAPI will generate events for those registered clients whenever the corresponding XAPI DB object changes.
This small python scripts shows how to register a simple event watch loop for XAPI:
import XenAPI session = XenAPI.Session("http://xshost") session.login_with_password("username","password") session.xenapi.event.register(["VM","pool"]) # register for events in the pool and VM objects while True: try: events = session.xenapi.event.next() # block until a xapi event on a xapi DB object is available for event in events: print "received event op=%s class=%s ref=%s" % (event['operation'], event['class'], event['ref']) if event['class'] == 'vm' and event['operatoin'] == 'mod': vm = event['snapshot'] print "xapi-event on vm: vm_uuid=%s, power_state=%s, current_operation=%s" % (vm['uuid'],vm['name_label'],vm['power_state'],vm['current_operations'].values()) except XenAPI.Failure, e: if len(e.details) > 0 and e.details[0] == 'EVENTS_LOST': session.xenapi.event.unregister(["VM","pool"]) session.xenapi.event.register(["VM","pool"]) Receiving events from xenopsd Xapi receives all events from xenopsd via the function xapi_xenops.events_watch() in its own independent thread. This is a single-threaded function that is responsible for handling all of the signals sent by xenopsd. In some situations with lots of VMs and virtual devices such as VBDs, this loop may saturate a single dom0 vcpu, which will slow down handling all of the xenopsd events and may cause the xenopsd signals to accumulate unboundedly in the worst case in the updates queue in xenopsd (see Figure 1).
The function xapi_xenops.events_watch() calls xenops_client.UPDATES.get() to obtain a list of (barrier, barrier_events), and then it process each one of the barrier_event, which can be one of the following events:
Vm id: something changed in this VM, run xapi_xenops.update_vm() to query xenopsd about its state. The function update_vm() will update power_state, allowed_operations, console and guest_agent state in the xapi DB. Vbd id: something changed in this VM, run xapi_xenops.update_vbd() to query xenopsd about its state. The function update_vbd() will update currently_attached and connected in the xapi DB. Vif id: something changed in this VM, run xapi_xenops.update_vif() to query xenopsd about its state. The function update_vif() will update activate and plugged state of in the xapi DB. Pci id: something changed in this VM, run xapi_xenops.update_pci() to query xenopsd about its state. Vgpu id: something changed in this VM, run xapi_xenops.update_vgpu() to query xenopsd about its state. Task id: something changed in this VM, run xapi_xenops.update_task() to query xenopsd about its state. The function update_task() will update the progress of the task in the xapi DB using the information of the task in xenopsd. All the xapi_xenops.update_X() functions above will call Xenopsd_client.X.stat() functions to obtain the current state of X from xenopsd:
There are a couple of optimisations while processing the events in xapi_xenops.events_watch():
if an event X=(vm_id,dev_id) (eg. Vbd dev_id) has already been processed in a barrier_events, it’s not processed again. A typical value for X is eg. “<vm_uuid>.xvda” for a VBD. if Events_from_xenopsd.are_supressed X, then this event is ignored. Events are supressed if VM X.vm_id is migrating away from the host Barriers When xapi needs to execute (and to wait for events indicating completion of) a xapi operation (such as VM.start and VM.shutdown) containing many xenopsd sub-operations (such as VM.start – to force xenopsd to change the VM power_state, and VM.stat, VBD.stat, VIF.stat etc – to force the xapi DB to catch up with the xenopsd new state for these objects), xapi sends to the xenopsd input queue a barrier, indicating that xapi will then block and only continue execution of the barred operation when xenopsd returns the barrier. The barrier should only be returned when xenopsd has finished the execution of all the operations requested by xapi (such as VBD.stat and VM.stat in order to update the state of the VM in the xapi database after a VM.start has been issued to xenopsd). A recent problem has been detected in the xapi_xenops.events_watch() function: when it needs to process many VM_check_state events, this may push for later the processing of barriers associated with a VM.start, delaying xapi in reporting (via a xapi event) that the VM state in the xapi DB has reached the running power_state. This needs further debugging, and is probably one of the reasons in CA-87377 why in some conditions a xapi event reporting that the VM power_state is running (causing it to go from yellow to green state in XenCenter) is taking so long to be returned, way after the VM is already running.
Xenopsd Xenopsd has a few queues that are used by xapi to store commands to be executed (eg. VBD.stat) and update events to be picked up by xapi. The main ones, easily seen at runtime by running the following command in dom0, are:
# xenops-cli diagnostics --queue=org.xen.xapi.xenops.classic { queues: [ # XENOPSD INPUT QUEUE ... stuff that still needs to be processed by xenopsd VM.stat VBD.stat VM.start VM.shutdown VIF.plug etc ] workers: [ # XENOPSD WORKER THREADS ... which stuff each worker thread is processing ] updates: { updates: [ # XENOPSD OUTPUT QUEUE ... signals from xenopsd that need to be picked up by xapi VM_check_state VBD_check_state etc ] } tasks: [ # XENOPSD TASKS ... state of each known task, before they are manually deleted after completion of the task ] }Sending events to xapi Whenever xenopsd changes the state of a XenServer object such as a VBD or VM, or when it receives an event from xenstore indicating that the states of these objects have changed (perhaps because either a guest or the dom0 backend changed the state of a virtual device), it creates a signal for the corresponding object (VM_check_state, VBD_check_state etc) and send it up to xapi. Xapi will then process this event in its xapi_xenops.events_watch() function.
These signals may need to wait a long time to be processed if the single-threaded xapi_xenops.events_watch() function is having difficulties (ie taking a long time) to process previous signals in the UPDATES queue from xenopsd. Receiving events from xenstore Xenopsd watches a number of keys in xenstore, both in dom0 and in each guest. Xenstore is responsible to send watch events to xenopsd whenever the watched keys change state. Xenopsd uses a xenstore client library to make it easier to create a callback function that is called whenever xenstore sends these events.
Xenopsd also needs to complement sometimes these watch events with polling of some values. An example is the @introduceDomain event in xenstore (handled in xenopsd/xc/xenstore_watch.ml), which indicates that a new VM has been created. This event unfortunately does not indicate the domid of the VM, and xenopsd needs to query Xen (via libxc) which domains are now available in the host and compare with the previous list of known domains, in order to figure out the domid of the newly introduced domain.
It is not good practice to poll xenstore for changes of values. This will add a large overhead to both xenstore and xenopsd, and decrease the scalability of XenServer in terms of number of VMs/host and virtual devices per VM. A much better approach is to rely on the watch events of xenstore to indicate when a specific value has changed in xenstore.
Xenstore Sending events to xenstore clients If a xenstore client has created watch events for a key, then xenstore will send events to this client whenever this key changes state.
Receiving events from xenstore clients Xenstore clients indicate to xenstore that something state changed by writing to some xenstore key. This may or may not cause xenstore to create watch events for the corresponding key, depending on if other xenstore clients have watches on this key.
`,description:"",tags:null,title:"Event handling in the Control Plane - Xapi, Xenopsd and Xenstore",uri:"/new-docs/toolstack/features/events/index.html"},{content:` ids rather than data; inherently coalescable blocking poll + async operations implies a client needs 2 connections coarse granularity similarity and differences with: XenAPI, event channels, xenstore watches https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467
`,description:"",tags:null,title:"Events",uri:"/new-docs/xenopsd/design/Events/index.html"},{content:"General Pluggable backends including xc: drives Xen via libxc and xenguest simulator: simulates operations for component-testing Supports running multiple instances and backends on the same host, looking after different sets of VMs Extensive configuration via command-line (see manpage) and config file Command-line tool for easy VM administration and troubleshooting User-settable degree of concurrency to get VMs started quickly VMs VM start/shutdown/reboot VM suspend/resume/checkpoint/migrate VM pause/unpause VM s3suspend/s3resume customisable SMBIOS tables for OEM-locked VMs hooks for 3rd party extensions: pre-start pre-destroy post-destroy pre-reboot per-VM xenguest replacement suppression of VM reboot loops live vCPU hotplug and unplug vCPU to pCPU affinity setting vCPU QoS settings (weight and cap for the Xen credit2 scheduler) DMC memory-ballooning support support for storage driver domains live update of VM shadow memory guest-initiated disk/nic hotunplug guest-initiated disk eject force disk/nic unplug support for ‘surprise-removable’ devices disk QoS configuration nic QoS configuration persistent RTC two-way guest agent communication for monitoring and control network carrier configuration port-locking for nics text and VNC consoles over TCP and Unix domain sockets PV kernel and ramdisk whitelisting configurable VM videoram programmable action-after-crash behaviour including: shutting down the VM, taking a crash dump or leaving the domain paused for inspection ability to move nics between bridges/switches advertises the VM memory footprints PCI passthrough support for discrete emulators (e.g. ‘demu’) PV keyboard and mouse qemu stub domains cirrus and stdvga graphics cards HVM serial console (useful for debugging) support for vGPU workaround for ‘spurious page faults’ kernel bug workaround for ‘machine address size’ kernel bug Hosts CPUid masking for heterogenous pools: reports true features and current features Host console reading Hypervisor version and capabilities reporting Host CPU querying APIs versioned json-rpc API with feature advertisements clients can disconnect, reconnect and easily resync with the latest VM state without losing updates all operations have task control including asychronous cancellation: for both subprocesses and xenstore watches progress updates subtasks per-task debug logs asynchronous event watching API advertises VM metrics memory usage balloon driver co-operativeness shadow memory usage domain ids channel passing (via sendmsg(2)) for efficient memory image copying ",description:"",tags:null,title:"Features",uri:"/new-docs/xenopsd/features/index.html"},{content:'Overview In this document we will use the VM.pool_migrate request to illustrate the interaction between various components within the XAPI toolstack during migration. However this schema can be applied to other requests as well.\nNot all parts of the Xapi toolstack are shown here as not all are involved in the migration process. For instance you won’t see the squeezed nor mpathalert two daemons that belong to the toolstack but don’t participate in the migration of a VM.\nAnatomy of a VM migration Migration is initiated by a Xapi client that sends VM.pool_migrate, an RPC XML request. The Xen API server handles this request and dispatches it to the server. The server is generated using XAPI IDL and requests are wrapped whithin a context, either to be forwarded to a host or executed locally. Broadly, the context follows RBAC rules. The executed function is related to the message of the request (refer to XenAPI Reference). In the case of the migration you can refer to ocaml/idl/datamodel_vm.ml. The server will dispatch the operation to server helpers, executing the operation synchronously or asynchronously and returning the RPC answer. Message forwarding decides if operation must be executed by another host of the pool and then forward the call or if is executed locally. When executed locally the high-level migration operation is send to the Xenopsd daemon by posting a message on a known queue on the message switch. Xenopsd will get the command and will split it into several atomic operations that will be run by the xenopsd backend. Xenopsd with its backend can then access xenstore or execute hypercall to interact with xen a server the micro operation. A diagram is worth a thousand words flowchart TD %% First we are starting by a XAPI client that is sending an XML-RPC request client((Xapi client)) -. sends RPC XML request .-> xapi_server{"`Dispatch RPC **api_server.ml**`"} style client stroke:#CAFEEE,stroke-width:4px %% XAPI Toolstack internals subgraph "Xapi Toolstack (master of the pool)" style server stroke:#BAFA00,stroke-width:4px,stroke-dasharray: 5 5 xapi_server --dispatch call (ie VM.pool_migrate)--> server("`Auto generated using *IDL* **server.ml**`") server --do_dispatch (ie VM.pool_migrate)--> server_helpers["`server helpers **server_helpers.ml**`"] server_helpers -- call management (ie xapi_vm_migrate.ml)--> message_forwarding["`check where to run the call **message_forwarding.ml**`"] message_forwarding -- execute locally --> vm_management["`VM Mgmt like **xapi_vm_migrate.ml**`"] vm_management -- Call --> xapi_xenops["`Transform xenops see (**xapi_xenops.ml**)`"] xapi_xenops <-- Post following IDL model (see xenops_interface.ml) --> msg_switch subgraph "Message Switch Daemon" msg_switch[["Queues"]] end subgraph "Xenopsd Daemon" msg_switch <-- Push/Pop on org.xen.xapi.xenopsd.classic --> xenopsd_server xenopsd_server["`Xenposd *frontend* get & split high level opertion into atomics`"] o-- linked at compile time --o xenopsd_backend end end %% Xenopsd backend is accessing xen and xenstore xenopsd_backend["`Xenopsd *backend* Backend XC (libxenctrl)`"] -. access to .-> xen_hypervisor["Xen hypervisor & xenstore"] style xen_hypervisor stroke:#BEEF00,stroke-width:2px %% Can send request to the host where call must be executed message_forwarding -.forward call to .-> elected_host["Host where call must be executed"] style elected_host stroke:#B0A,stroke-width:4px',description:"",tags:null,title:"From RPC migration request to xapi internals",uri:"/new-docs/xapi/walkthroughs/migration_overview/index.html"},{content:`High-Availability (HA) tries to keep VMs running, even when there are hardware failures in the resource pool, when the admin is not present. Without HA the following may happen:
during the night someone spills a cup of coffee over an FC switch; then VMs running on the affected hosts will lose access to their storage; then business-critical services will go down; then monitoring software will send a text message to an off-duty admin; then the admin will travel to the office and fix the problem by restarting the VMs elsewhere. With HA the following will happen:
during the night someone spills a cup of coffee over an FC switch; then VMs running on the affected hosts will lose access to their storage; then business-critical services will go down; then the HA software will determine which hosts are affected and shut them down; then the HA software will restart the VMs on unaffected hosts; then services are restored; then on the next working day the admin can arrange for the faulty switch to be replaced. HA is designed to handle an emergency and allow the admin time to fix failures properly.
Example The following diagram shows an HA-enabled pool, before and after a network link between two hosts fails.
When HA is enabled, all hosts in the pool
exchange periodic heartbeat messages over the network send heartbeats to a shared storage device. attempt to acquire a “master lock” on the shared storage. HA is designed to recover as much as possible of the pool after a single failure i.e. it removes single points of failure. When some subset of the pool suffers a failure then the remaining pool members
figure out whether they are in the largest fully-connected set (the “liveset”); if they are not in the largest set then they “fence” themselves (i.e. force reboot via the hypervisor watchdog) elect a master using the “master lock” restart all lost VMs. After HA has recovered a pool, it is important that the original failure is addressed because the remaining pool members may not be able to cope with any more failures.
Design HA must never violate the following safety rules:
there must be at most one master at all times. This is because the master holds the VM and disk locks. there must be at most one instance of a particular VM at all times. This is because starting the same VM twice will result in severe filesystem corruption. However to be useful HA must:
detect failures quickly; minimise the number of false-positives in the failure detector; and make the failure handling logic as robust as possible. The implementation difficulty arises when trying to be both useful and safe at the same time.
Terminology We use the following terminology:
fencing: also known as I/O fencing, refers to the act of isolating a host from network and storage. Once a host has been fenced, any VMs running there cannot generate side-effects observable to a third party. This means it is safe to restart the running VMs on another node without violating the safety-rule and running the same VM simultaneously in two locations. heartbeating: exchanging status updates with other hosts at regular pre-arranged intervals. Heartbeat messages reveal that hosts are alive and that I/O paths are working. statefile: a shared disk (also known as a “quorum disk”) on the “Heartbeat” SR which is mapped as a block device into every host’s domain 0. The shared disk acts both as a channel for heartbeat messages and also as a building block of a Pool master lock, to prevent multiple hosts becoming masters in violation of the safety-rule (a dangerous situation also known as “split-brain”). management network: the network over which the XenAPI XML/RPC requests flow and also used to send heartbeat messages. liveset: a per-Host view containing a subset of the Hosts in the Pool which are considered by that Host to be alive i.e. responding to XenAPI commands and running the VMs marked as resident_on there. When a Host b leaves the liveset as seen by Host a it is safe for Host a to assume that Host b has been fenced and to take recovery actions (e.g. restarting VMs), without violating either of the safety-rules. properly shared SR: an SR which has field shared=true; and which has a PBD connecting it to every enabled Host in the Pool; and where each of these PBDs has field currently_attached set to true. A VM whose disks are in a properly shared SR could be restarted on any enabled Host, memory and network permitting. properly shared Network: a Network which has a PIF connecting it to every enabled Host in the Pool; and where each of these PIFs has field currently_attached set to true. A VM whose VIFs connect to properly shared Networks could be restarted on any enabled Host, memory and storage permitting. agile: a VM is said to be agile if all disks are in properly shared SRs and all network interfaces connect to properly shared Networks. unprotected: an unprotected VM has field ha_always_run set to false and will never be restarted automatically on failure or have reconfiguration actions blocked by the HA overcommit protection. best-effort: a best-effort VM has fields ha_always_run set to true and ha_restart_priority set to best-effort. A best-effort VM will only be restarted if (i) the failure is directly observed; and (ii) capacity exists for an immediate restart. No more than one restart attempt will ever be made. protected: a VM is said to be protected if it will be restarted by HA i.e. has field ha_always_run set to true and field ha_restart_priority not set to \`best-effort. survival rule 1: describes the situation where hosts survive because they are in the largest network partition with statefile access. This is the normal state of the xhad daemon. survival rule 2: describes the situation where all hosts have lost access to the statefile but remain alive while they can all see each-other on the network. In this state any further failure will cause all nodes to self-fence. This state is intended to cope with the system-wide temporary loss of the storage service underlying the statefile. Assumptions We assume:
All I/O used for monitoring the health of hosts (i.e. both storage and network-based heartbeating) is along redundant paths, so that it survives a single hardware failure (e.g. a broken switch or an accidentally-unplugged cable). It is up to the admin to ensure their environment is setup correctly. The hypervisor watchdog mechanism will be able to guarantee the isolation of nodes, once communication has been lost, within a pre-arranged time period. Therefore no active power fencing equipment is required. VMs may only be marked as protected if they are fully agile i.e. able to run on any host, memory permitting. No additional constraints of any kind may be specified e.g. it is not possible to make “CPU reservations”. Pools are assumed to be homogenous with respect to CPU type and presence of VT/SVM support (also known as “HVM”). If a Pool is created with non-homogenous hosts using the --force flag then the additional constraints will not be noticed by the VM failover planner resulting in runtime failures while trying to execute the failover plans. No attempt will ever be made to shutdown or suspend “lower” priority VMs to guarantee the survival of “higher” priority VMs. Once HA is enabled it is not possible to reconfigure the management network or the SR used for storage heartbeating. VMs marked as protected are considered to have failed if they are offline i.e. the VM failure handling code is level-sensitive rather than edge-sensitive. VMs marked as best-effort are considered to have failed only when the host where they are resident is declared offline i.e. the best-effort VM failure handling code is edge-sensitive rather than level-sensitive. A single restart attempt is attempted and if this fails no further start is attempted. HA can only be enabled if all Pool hosts are online and actively responding to requests. when HA is enabled the database is configured to write all updates to the “Heartbeat” SR, guaranteeing that VM configuration changes are not lost when a host fails. Components The implementation is split across the following components:
xhad: the cluster membership daemon maintains a quorum of hosts through network and storage heartbeats xapi: used to configure the HA policy i.e. which network and storage to use for heartbeating and which VMs to restart after a failure. xen: the Xen watchdog is used to reliably fence the host when the host has been (partially or totally) isolated from the cluster To avoid a “split-brain”, the cluster membership daemon must “fence” (i.e. isolate) nodes when they are not part of the cluster. In general there are 2 approaches:
cut the power of remote hosts which you can’t talk to on the network any more. This is the approach taken by most open-source clustering software since it is simpler. However it has the downside of requiring the customer buy more hardware and set it up correctly. rely on the remote hosts using a watchdog to cut their own power (i.e. halt or reboot) after a timeout. This relies on the watchdog being reliable. Most other people don’t trust the Linux watchdog; after all the Linux kernel is highly threaded, performs a lot of (useful) functions and kernel bugs which result in deadlocks do happen. We use the Xen watchdog because we believe that the Xen hypervisor is simple enough to reliably fence the host (via triggering a reboot of domain 0 which then triggers a host reboot). xhad xhad is the cluster membership daemon: it exchanges heartbeats with the other nodes to determine which nodes are still in the cluster (the “live set”) and which nodes have definitely failed (through watchdog fencing). When a host has definitely failed, xapi will unlock all the disks and restart the VMs according to the HA policy.
Since Xapi is a critical part of the system, the xhad also acts as a Xapi watchdog. It polls Xapi every few seconds and checks if Xapi can respond. If Xapi seems to have failed then xhad will restart it. If restarts continue to fail then xhad will consider the host to have failed and self-fence.
xhad is configured via a simple config file written on each host in /etc/xensource/xhad.conf. The file must be identical on each host in the cluster. To make changes to the file, HA must be disabled and then re-enabled afterwards. Note it may not be possible to re-enable HA depending on the configuration change (e.g. if a host has been added but that host has a broken network configuration then this will block HA enable).
The xhad.conf file is written in XML and contains
pool-wide configuration: this includes a list of all hosts which should be in the liveset and global timeout information local host configuration: this identifies the local host and described which local network interface and block device to use for heartbeating. The following is an example xhad.conf file:
<?xml version="1.0" encoding="utf-8"?> <xhad-config version="1.0"> <!--pool-wide configuration--> <common-config> <GenerationUUID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</GenerationUUID> <UDPport>694</UDPport> <!--for each host, specify host UUID, and IP address--> <host> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</HostID> <IPaddress>xxx.xxx.xxx.xx1</IPaddress> </host> <host> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</HostID> <IPaddress>xxx.xxx.xxx.xx2</IPaddress> </host> <host> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</HostID> <IPaddress>xxx.xxx.xxx.xx3</IPaddress> </host> <!--optional parameters [sec] --> <parameters> <HeartbeatInterval>4</HeartbeatInterval> <HeartbeatTimeout>30</HeartbeatTimeout> <StateFileInterval>4</StateFileInterval> <StateFileTimeout>30</StateFileTimeout> <HeartbeatWatchdogTimeout>30</HeartbeatWatchdogTimeout> <StateFileWatchdogTimeout>45</StateFileWatchdogTimeout> <BootJoinTimeout>90</BootJoinTimeout> <EnableJoinTimeout>90</EnableJoinTimeout> <XapiHealthCheckInterval>60</XapiHealthCheckInterval> <XapiHealthCheckTimeout>10</XapiHealthCheckTimeout> <XapiRestartAttempts>1</XapiRestartAttempts> <XapiRestartTimeout>30</XapiRestartTimeout> <XapiLicenseCheckTimeout>30</XapiLicenseCheckTimeout> </parameters> </common-config> <!--local host configuration--> <local-config> <localhost> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx2</HostID> <HeartbeatInterface> xapi1</HeartbeatInterface> <HeartbeatPhysicalInterface>bond0</HeartbeatPhysicalInterface> <StateFile>/dev/statefiledevicename</StateFile> </localhost> </local-config> </xhad-config>The fields have the following meaning:
GenerationUUID: a UUID generated each time HA is reconfigured. This allows xhad to tell an old host which failed; had been removed from the configuration; repaired and then restarted that the world has changed while it was away. UDPport: the port number to use for network heartbeats. It’s important to allow this traffic through the firewall and to make sure the same port number is free on all hosts (beware of portmap services occasionally binding to it). HostID: a UUID identifying a host in the pool. We would normally use xapi’s notion of a host uuid. IPaddress: any IP address on the remote host. We would normally use xapi’s notion of a management network. HeartbeatTimeout: if a heartbeat packet is not received for this many seconds, then xhad considers the heartbeat to have failed. This is the user-supplied “HA timeout” value, represented below as T. T must be bigger than 10; we would normally use 60s. StateFileTimeout: if a storage update is not seen for a host for this many seconds, then xhad considers the storage heartbeat to have failed. We would normally use the same value as the HeartbeatTimeout T. HeartbeatInterval: interval between heartbeat packets sent. We would normally use a value 2 <= t <= 6, derived from the user-supplied HA timeout via t = (T + 10) / 10 StateFileInterval: interval betwen storage updates (also known as “statefile updates”). This would normally be set to the same value as HeartbeatInterval. HeartbeatWatchdogTimeout: If the host does not send a heartbeat for this amount of time then the host self-fences via the Xen watchdog. We normally set this to T. StateFileWatchdogTimeout: If the host does not update the statefile for this amount of time then the host self-fences via the Xen watchdog. We normally set this to T+15. BootJoinTimeout: When the host is booting and joining the liveset (i.e. the cluster), consider the join a failure if it takes longer than this amount of time. We would normally set this to T+60. EnableJoinTimeout: When the host is enabling HA for the first time, consider the enable a failure if it takes longer than this amount of time. We would normally set this to T+60. XapiHealthCheckInterval: Interval between “health checks” where we run a script to check whether Xapi is responding or not. XapiHealthCheckTimeout: Number of seconds to wait before assuming that Xapi has deadlocked during a “health check”. XapiRestartAttempts: Number of Xapi restarts to attempt before concluding Xapi has permanently failed. XapiRestartTimeout: Number of seconds to wait for a Xapi restart to complete before concluding it has failed. XapiLicenseCheckTimeout: Number of seconds to wait for a Xapi license check to complete before concluding that xhad should terminate. In addition to the config file, Xhad exposes a simple control API which is exposed as scripts:
ha_set_pool_state (Init | Invalid): sets the global pool state to “Init” (before starting HA) or “Invalid” (causing all other daemons who can see the statefile to shutdown) ha_start_daemon: if the pool state is “Init” then the daemon will attempt to contact other daemons and enable HA. If the pool state is “Active” then the host will attempt to join the existing liveset. ha_query_liveset: returns the current state of the cluster. ha_propose_master: returns whether the current node has been elected pool master. ha_stop_daemon: shuts down the xhad on the local host. Note this will not disarm the Xen watchdog by itself. ha_disarm_fencing: disables fencing on the local host. ha_set_excluded: when a host is being shutdown cleanly, record the fact that the VMs have all been shutdown so that this host can be ignored in future cluster membership calculations. Fencing Xhad continuously monitors whether the host should remain alive, or if it should self-fence. There are two “survival rules” which will keep a host alive; if neither rule applies (or if xhad crashes or deadlocks) then the host will fence. The rules are:
Xapi is running; the storage heartbeats are visible; this host is a member of the “best” partition (as seen through the storage heartbeats) Xapi is running; the storage is inaccessible; all hosts which should be running (i.e. not those “excluded” by being cleanly shutdown) are online and have also lost storage access (as seen through the network heartbeats). where the “best” partition is the largest one if that is unique, or if there are multiple partitions of the same size then the one containing the lowest host uuid is considered best.
The first survival rule is the “normal” case. The second rule exists only to prevent the storage from becoming a single point of failure: all hosts can remain alive until the storage is repaired. Note that if a host has failed and has not yet been repaired, then the storage becomes a single point of failure for the degraded pool. HA removes single point of failures, but multiple failures can still cause problems. It is important to fix failures properly after HA has worked around them.
xapi Xapi is responsible for
exposing an interface for setting HA policy creating VDIs (disks) on shared storage for heartbeating and storing the pool database arranging for these disks to be attached on host boot, before the “SRmaster” is online configuring and managing the xhad heartbeating daemon The HA policy APIs include
methods to determine whether a VM is agile i.e. can be restarted in principle on any host after a failure planning for a user-specified number of host failures and enforcing access control restarting failed protected VMs in policy order The HA policy settings are stored in the Pool database which is written (synchronously) to a VDI in the same SR that’s being used for heartbeating. This ensures that the database can be recovered after a host fails and the VMs are recovered.
Xapi stores 2 settings in its local database:
ha_disable_failover_actions: this is set to false when we want nodes to be able to recover VMs – this is the normal case. It is set to true during the HA disable process to prevent a split-brain forming while HA is only partially enabled. ha_armed: this is set to true to tell Xapi to start Xhad during host startup and wait to join the liveset. Disks on shared storage The regular disk APIs for creating, destroying, attaching, detaching (etc) disks need the SRmaster (usually but not always the Pool master) to be online to allow the disks to be locked. The SRmaster cannot be brought online until the host has joined the liveset. Therefore we have a cyclic dependency: joining the liveset needs the statefile disk to be attached but attaching a disk requires being a member of the liveset already.
The dependency is broken by adding an explicit “unlocked” attach storage API called VDI_ATTACH_FROM_CONFIG. Xapi uses the VDI_GENERATE_CONFIG API during the HA enable operation and stores away the result. When the system boots the VDI_ATTACH_FROM_CONFIG is able to attach the disk without the SRmaster.
The role of Host.enabled The Host.enabled flag is used to mean, “this host is ready to start VMs and should be included in failure planning”. The VM restart planner assumes for simplicity that all protected VMs can be started anywhere; therefore all involved networks and storage must be properly shared. If a host with an unplugged PBD were to become enabled then the corresponding SR would cease to be properly shared, all the VMs would cease to be agile and the VM restart logic would fail.
To ensure the VM restart logic always works, great care is taken to make sure that Hosts may only become enabled when their networks and storage are properly configured. This is achieved by:
when the master boots and initialises its database it sets all Hosts to dead and disabled and then signals the HA background thread (signal_database_state_valid) to wake up from sleep and start processing liveset information (and potentially setting hosts to live) when a slave calls Pool.hello (i.e. after the slave has rebooted), the master sets it to disabled, allowing it a grace period to plug in its storage; when a host (master or slave) successfully plugs in its networking and storage it calls consider_enabling_host which checks that the preconditions are met and then sets the host to enabled; and when a slave notices its database connection to the master restart (i.e. after the master xapi has just restarted) it calls consider_enabling_host} The steady-state When HA is enabled and all hosts are running normally then each calls ha_query_liveset every 10s.
Slaves check to see if the host they believe is the master is alive and has the master lock. If another node has become master then the slave will rewrite its pool.conf and restart. If no node is the master then the slave will call on_master_failure, proposing itself and, if it is rejected, checking the liveset to see which node acquired the lock.
The master monitors the liveset and updates the Host_metrics.live flag of every host to reflect the liveset value. For every host which is not in the liveset (i.e. has fenced) it enumerates all resident VMs and marks them as Halted. For each protected VM which is not running, the master computes a VM restart plan and attempts to execute it. If the plan fails then a best-effort VM.start call is attempted. Finally an alert is generated if the VM could not be restarted.
Note that XenAPI heartbeats are still sent when HA is enabled, even though they are not used to drive the values of the Host_metrics.live field. Note further that, when a host is being shutdown, the host is immediately marked as dead and its host reference is added to a list used to prevent the Host_metrics.live being accidentally reset back to live again by the asynchronous liveset query. The Host reference is removed from the list when the host restarts and calls Pool.hello.
Planning and overcommit The VM failover planning code is sub-divided into two pieces, stored in separate files:
binpack.ml: contains two algorithms for packing items of different sizes (i.e. VMs) into bins of different sizes (i.e. Hosts); and xapi_ha_vm_failover.ml: interfaces between the Pool database and the binpacker; also performs counterfactual reasoning for overcommit protection. The input to the binpacking algorithms are configuration values which represent an abstract view of the Pool:
type ('a, 'b) configuration = { hosts: ('a * int64) list; (** a list of live hosts and free memory *) vms: ('b * int64) list; (** a list of VMs and their memory requirements *) placement: ('b * 'a) list; (** current VM locations *) total_hosts: int; (** total number of hosts in the pool 'n' *) num_failures: int; (** number of failures to tolerate 'r' *) }Note that:
the memory required by the VMs listed in placement has already been substracted from the total memory of the hosts; it doesn’t need to be subtracted again. the free memory of each host has already had per-host miscellaneous overheads subtracted from it, including that used by unprotected VMs, which do not appear in the VM list. the total number of hosts in the pool (total_hosts) is a constant for any particular invocation of HA. the number of failures to tolerate (num_failures) is the user-settable value from the XenAPI Pool.ha_host_failures_to_tolerate. There are two algorithms which satisfy the interface:
sig plan_always_possible: ('a, 'b) configuration -> bool; get_specific_plan: ('a, 'b) configuration -> 'b list -> ('b * 'a) list endThe function get_specific_plan takes a configuration and a list of Hosts which have failed. It returns a VM restart plan represented as a VM to Host association list. This is the function called by the background HA VM restart thread on the master.
The function plan_always_possible returns true if every sequence of Host failures of length num_failures (irrespective of whether all hosts failed at once, or in multiple separate episodes) would result in calls to get_specific_plan which would allow all protected VMs to be restarted. This function is heavily used by the overcommit protection logic as well as code in XenCenter which aims to maximise failover capacity using the counterfactual reasoning APIs:
Pool.ha_compute_max_host_failures_to_tolerate Pool.ha_compute_hypothetical_max_host_failures_to_tolerateThere are two binpacking algorithms: the more detailed but expensive algorithmm is used for smaller/less complicated pool configurations while the less detailed, cheaper algorithm is used for the rest. The choice between algorithms is based only on total_hosts (n) and num_failures (r). Note that the choice of algorithm will only change if the number of Pool hosts is varied (requiring HA to be disabled and then enabled) or if the user requests a new num_failures target to plan for.
The expensive algorithm uses an exchaustive search with a “biggest-fit-decreasing” strategy that takes the biggest VMs first and allocates them to the biggest remaining Host. The implementation keeps the VMs and Hosts as sorted lists throughout. There are a number of transformations to the input configuration which are guaranteed to preserve the existence of a VM to host allocation (even if the actual allocation is different). These transformations which are safe are:
VMs may be removed from the list VMs may have their memory requirements reduced Hosts may be added Hosts may have additional memory added. The cheaper algorithm is used for larger Pools where the state space to search is too large. It uses the same “biggest-fit-decreasing” strategy with the following simplifying approximations:
every VM that fails is as big as the biggest the number of VMs which fail due to a single Host failure is always the maximum possible (even if these are all very small VMs) the largest and most capable Hosts fail An informal argument that these approximations are safe is as follows: if the maximum number of VMs fail, each of which is size of the largest and we can find a restart plan using only the smaller hosts then any real failure:
can never result in the failure of more VMs; can never result in the failure of bigger VMs; and can never result in less host capacity remaining. Therefore we can take this almost-certainly-worse-than-worst-case failure plan and:
replace the remaining hosts in the worst case plan with the real remaining hosts, which will be the same size or larger; and replace the failed VMs in the worst case plan with the real failed VMs, which will be fewer or the same in number and smaller or the same in size. Note that this strategy will perform best when each host has the same number of VMs on it and when all VMs are approximately the same size. If one very big VM exists and a lot of smaller VMs then it will probably fail to find a plan. It is more tolerant of differing amounts of free host memory.
Overcommit protection Overcommit protection blocks operations which would prevent the Pool being able to restart protected VMs after host failure. The Pool may become unable to restart protected VMs in two general ways: (i) by running out of resource i.e. host memory; and (ii) by altering host configuration in such a way that VMs cannot be started (or the planner thinks that VMs cannot be started).
API calls which would change the amount of host memory currently in use (VM.start, VM.resume, VM.migrate etc) have been modified to call the planning functions supplying special “configuration change” parameters. Configuration change values represent the proposed operation and have type
type configuration_change = { (** existing VMs which are leaving *) old_vms_leaving: (API.ref_host * (API.ref_VM * API.vM_t)) list; (** existing VMs which are arriving *) old_vms_arriving: (API.ref_host * (API.ref_VM * API.vM_t)) list; (** hosts to pretend to disable *) hosts_to_disable: API.ref_host list; (** new number of failures to consider *) num_failures: int option; (** new VMs to restart *) new_vms_to_protect: API.ref_VM list; }A VM migration will be represented by saying the VM is “leaving” one host and “arriving” at another. A VM start or resume will be represented by saying the VM is “arriving” on a host.
Note that no attempt is made to integrate the overcommit protection with the general VM.start host chooser as this would be quite expensive.
Note that the overcommit protection calls are written as asserts called within the message forwarder in the master, holding the main forwarding lock.
API calls which would change the system configuration in such a way as to prevent the HA restart planner being able to guarantee to restart protected VMs are also blocked. These calls include:
VBD.create: where the disk is not in a properly shared SR VBD.insert: where the CDROM is local to a host VIF.create: where the network is not properly shared PIF.unplug: when the network would cease to be properly shared PBD.unplug: when the storage would cease to be properly shared Host.enable: when some network or storage would cease to be properly shared (e.g. if this host had a broken storage configuration) xen The Xen hypervisor has per-domain watchdog counters which, when enabled, decrement as time passes and can be reset from a hypercall from the domain. If the domain fails to make the hypercall and the timer reaches zero then the domain is immediately shutdown with reason reboot. We configure Xen to reboot the host when domain 0 enters this state.
High-level operations Enabling HA Before HA can be enabled the admin must take care to configure the environment properly. In particular:
NIC bonds should be available for network heartbeats; multipath should be configured for the storage heartbeats; all hosts should be online and fully-booted. The XenAPI client can request a specific shared SR to be used for storage heartbeats, otherwise Xapi will use the Pool’s default SR. Xapi will use VDI_GENERATE_CONFIG to ensure the disk will be attached automatically on system boot before the liveset has been joined.
Note that extra effort is made to re-use any existing heartbeat VDIS so that
if HA is disabled with some hosts offline, when they are rebooted they stand a higher chance of seeing a well-formed statefile with an explicit invalid state. If the VDIs were destroyed on HA disable then hosts which boot up later would fail to attach the disk and it would be harder to distinguish between a temporary storage failure and a permanent HA disable. the heartbeat SR can be created on expensive low-latency high-reliability storage and made as small as possible (to minimise infrastructure cost), safe in the knowledge that if HA enables successfully once, it won’t run out of space and fail to enable in the future. The Xapi-to-Xapi communication looks as follows:
The Xapi Pool master calls Host.ha_join_liveset on all hosts in the pool simultaneously. Each host runs the ha_start_daemon script which starts Xhad. Each Xhad starts exchanging heartbeats over the network and storage defined in the xhad.conf.
Joining a liveset The Xhad instances exchange heartbeats and decide which hosts are in the “liveset” and which have been fenced.
After joining the liveset, each host clears the “excluded” flag which would have been set if the host had been shutdown cleanly before – this is only needed when a host is shutdown cleanly and then restarted.
Xapi periodically queries the state of xhad via the ha_query_liveset command. The state will be Starting until the liveset is fully formed at which point the state will be Online.
When the ha_start_daemon script returns then Xapi will decide whether to stand for master election or not. Initially when HA is being enabled and there is a master already, this node will be expected to stand unopposed. Later when HA notices that the master host has been fenced, all remaining hosts will stand for election and one of them will be chosen.
Shutting down a host When a host is to be shutdown cleanly, it can be safely “excluded” from the pool such that a future failure of the storage heartbeat will not cause all pool hosts to self-fence (see survival rule 2 above). When a host is “excluded” all other hosts know that the host does not consider itself a master and has no resources locked i.e. no VMs are running on it. An excluded host will never allow itself to form part of a “split brain”.
Once a host has given up its master role and shutdown any VMs, it is safe to disable fencing with ha_disarm_fencing and stop xhad with ha_stop_daemon. Once the daemon has been stopped the “excluded” bit can be set in the statefile via ha_set_excluded and the host safely rebooted.
Restarting a host When a host restarts after a failure Xapi notices that ha_armed is set in the local database. Xapi
runs the attach-static-vdis script to attach the statefile and database VDIs. This can fail if the storage is inaccessible; Xapi will retry until it succeeds. runs the ha_start_daemon to join the liveset, or determine that HA has been cleanly disabled (via setting the state to Invalid). In the special case where Xhad fails to access the statefile and the host used to be a slave then Xapi will try to contact the previous master and find out
who the new master is; whether HA is enabled on the Pool or not. If Xapi can confirm that HA was disabled then it will disarm itself and join the new master. Otherwise it will keep waiting for the statefile to recover.
In the special case where the statefile has been destroyed and cannot be recovered, there is an emergency HA disable API the admin can use to assert that HA really has been disabled, and it’s not simply a connectivity problem. Obviously this API should only be used if the admin is totally sure that HA has been disabled.
Disabling HA There are 2 methods of disabling HA: one for the “normal” case when the statefile is available; and the other for the “emergency” case when the statefile has failed and can’t be recovered.
Disabling HA cleanly HA can be shutdown cleanly when the statefile is working i.e. when hosts are alive because of survival rule 1. First the master Xapi tells the local Xhad to mark the pool state as “invalid” using ha_set_pool_state. Every xhad instance will notice this state change the next time it performs a storage heartbeat. The Xhad instances will shutdown and Xapi will notice that HA has been disabled the next time it attempts to query the liveset.
If a host loses access to the statefile (or if none of the hosts have access to the statefile) then HA can be disabled uncleanly.
Disabling HA uncleanly The Xapi master first calls Host.ha_disable_failover_actions on each host which sets ha_disable_failover_decisions in the lcoal database. This prevents the node rebooting, gaining statefile access, acquiring the master lock and restarting VMs when other hosts have disabled their fencing (i.e. a “split brain”).
Once the master is sure that no host will suddenly start recovering VMs it is safe to call Host.ha_disarm_fencing which runs the script ha_disarm_fencing and then shuts down the Xhad with ha_stop_daemon.
Add a host to the pool We assume that adding a host to the pool is an operation the admin will perform manually, so it is acceptable to disable HA for the duration and to re-enable it afterwards. If a failure happens during this operation then the admin will take care of it by hand.
`,description:"",tags:null,title:"High-Availability",uri:"/new-docs/toolstack/features/HA/index.html"},{content:`There are a number of hook points at which xenopsd may execute certain scripts. These scripts are found in hook-specific directories of the form /etc/xapi.d/<hookname>/. All executable scripts in these directories are run with the following arguments:
<script.sh> -reason <reason> -vmuuid <uuid of VM> The scripts are executed in filename-order. By convention, the filenames are usually of the form 10resetvdis.
The hook points are:
vm-pre-shutdown vm-pre-migrate vm-post-migrate (Dundee only) vm-pre-start vm-pre-reboot vm-pre-resume vm-post-resume (Dundee only) vm-post-destroy and the reason codes are:
clean-shutdown hard-shutdown clean-reboot hard-reboot suspend source -- passed to pre-migrate hook on source host destination -- passed to post-migrate hook on destination (Dundee only) none For example, in order to execute a script on VM shutdown, it would be sufficient to create the script in the post-destroy hook point:
/etc/xapi.d/vm-post-destroy/01myscript.sh containing
#!/bin/bash echo I was passed $@ > /tmp/output And when, for example, VM e30d0050-8f15-e10d-7613-cb2d045c8505 is shut-down, the script is executed:
[vagrant@localhost ~]$ sudo xe vm-shutdown --force uuid=e30d0050-8f15-e10d-7613-cb2d045c8505 [vagrant@localhost ~]$ cat /tmp/output I was passed -vmuuid e30d0050-8f15-e10d-7613-cb2d045c8505 -reason hard-shutdown `,description:"",tags:null,title:"Hooks",uri:"/new-docs/xenopsd/design/hooks/index.html"},{content:`Memory is used for many things:
the hypervisor code: this is the Xen executable itself the hypervisor heap: this is needed for per-domain structures and per-vCPU structures the crash kernel: this is needed to collect information after a host crash domain RAM: this is the memory the VM believes it has shadow memory: for HVM guests running on hosts without hardware assisted paging (HAP) Xen uses shadow to optimise page table updates. For all guests shadow is used during live migration for tracking the memory transfer. video RAM for the virtual graphics card Some of these are constants (e.g. hypervisor code) while some depend on the VM configuration (e.g. domain RAM). Xapi calls the constants “host overhead” and the variables due to VM configuration as “VM overhead”. There is no low-level API to query this information, therefore xapi will sample the host overheads at system boot time and model the per-VM overheads.
Host overhead The host overhead is not managed by xapi, instead it is sampled. After the host boots and before any VMs start, xapi asks Xen how much memory the host has in total, and how much memory is currently free. Xapi subtracts the free from the total and stores this as the host overhead.
VM overhead The inputs to the model are
VM.memory_static_max: the maximum amount of RAM the domain will be able to use VM.HVM_shadow_multiplier: allows the shadow memory to be increased VM.VCPUs_max: the maximum number of vCPUs the domain will be able to use First the shadow memory is calculated, in MiB
Second the VM overhead is calculated, in MiB
Memory required to start a VM If ballooning is disabled, the memory required to start a VM is the same as the VM overhead above.
If ballooning is enabled then the memory calculation above is modified to use the VM.memory_dynamic_max rather than the VM.memory_static_max.
Memory required to migrate a VM If ballooning is disabled, the memory required to receive a migrating VM is the same as the VM overhead above.
If ballooning is enabled, then the VM will first be ballooned down to VM.memory_dynamic_min and then it will be migrated across. If the VM fails to balloon all the way down, then correspondingly more memory will be required on the receiving side.
`,description:"",tags:null,title:"Host memory accounting",uri:"/new-docs/xapi/memory/index.html"},{content:"",description:"",tags:null,title:"How to add....",uri:"/new-docs/xapi/guides/howtos/index.html"},{content:`Communication between the Toolstack daemon is built upon libraries from a component called xapi-idl.
Abstracts communication between daemons over the message-switch using JSON/RPC. Contains the definition of the interfaces exposed by the daemons (except xapi). `,description:"",tags:null,title:"Interfaces",uri:"/new-docs/toolstack/high-level/interfaces/index.html"},{content:" sequenceDiagram autonumber participant tx as sender participant rx0 as receiver thread 0 participant rx1 as receiver thread 1 participant rx2 as receiver thread 2 activate tx tx->>rx0: VM.import_metadata tx->>tx: Squash memory to dynamic-min tx->>rx1: HTTP /migrate/vm activate rx1 rx1->>rx1: VM_receive_memory<br/>VM_create (00000001)<br/>VM_restore_vifs rx1->>tx: handshake (control channel)<br/>Synchronisation point 1 tx->>rx2: HTTP /migrate/mem activate rx2 rx2->>tx: handshake (memory channel)<br/>Synchronisation point 1-mem tx->>rx1: handshake (control channel)<br/>Synchronisation point 1-mem ACK rx2->>rx1: memory fd tx->>rx1: VM_save/VM_restore<br/>Synchronisation point 2 tx->>tx: VM_rename rx1->>rx2: exit deactivate rx2 tx->>rx1: handshake (control channel)<br/>Synchronisation point 3 rx1->>rx1: VM_rename<br/>VM_restore_devices<br/>VM_unpause<br/>VM_set_domain_action_request rx1->>tx: handshake (control channel)<br/>Synchronisation point 4 deactivate rx1 tx->>tx: VM_shutdown<br/>VM_remove deactivate tx ",description:"",tags:null,title:"Live Migration Sequence Diagram",uri:"/new-docs/xenopsd/walkthroughs/live-migration/index.html"},{content:`In the present version of XenServer, metadata changes resulting in writes to the database are not persisted in non-volatile storage. Hence, in case of failure, up to five minutes’ worth of metadata changes could be lost. The Metadata-on-LUN feature addresses the issue by ensuring that all database writes are retained. This will be used to improve recovery from failure by storing incremental deltas which can be re-applied to an old version of the database to bring it more up-to-date. An implication of this is that clients will no longer be required to perform a ‘pool-sync-database’ to protect critical writes, because all writes will be implicitly protected.
This is implemented by saving descriptions of all persistent database writes to a LUN when HA is active. Upon xapi restart after failure, such as on master fail-over, these descriptions are read and parsed to restore the latest version of the database.
Layout on block device It is useful to store the database on the block device as well as the deltas, so that it is unambiguous on recovery which version of the database the deltas apply to.
The content of the block device will be structured as shown in the table below. It consists of a header; the rest of the device is split into two halves.
Length (bytes) Description Header 16 Magic identifier 1 ASCII NUL 1 Validity byte First half database 36 UUID as ASCII string 16 Length of database as decimal ASCII (as specified) Database (binary data) 16 Generation count as decimal ASCII 36 UUID as ASCII string First half deltas 16 Length of database delta as decimal ASCII (as specified) Database delta (binary data) 16 Generation count as decimal ASCII 36 UUID as ASCII string Second half database 36 UUID as ASCII string 16 Length of database as decimal ASCII (as specified) Database (binary data) 16 Generation count as decimal ASCII 36 UUID as ASCII string Second half deltas 16 Length of database delta as decimal ASCII (as specified) Database delta (binary data) 16 Generation count as decimal ASCII 36 UUID as ASCII string After the header, one or both halves may be devoid of content. In a half which contains a database, there may be zero or more deltas (repetitions of the last three entries in each half).
The structure of the device is split into two halves to provide double-buffering. In case of failure during write to one half, the other half remains intact.
The magic identifier at the start of the file protect against attempting to treat a different device as a redo log.
The validity byte is a single \`ascii character indicating the state of the two halves. It can take the following values:
Byte Description 0 Neither half is valid 1 First half is valid 2 Second half is valid The use of lengths preceding data sections permit convenient reading. The constant repetitions of the UUIDs act as nonces to protect against reading in invalid data in the case of an incomplete or corrupt write.
Architecture The I/O to and from the block device may involve long delays. For example, if there is a network problem, or the iSCSI device disappears, the I/O calls may block indefinitely. It is important to isolate this from xapi. Hence, I/O with the block device will occur in a separate process.
Xapi will communicate with the I/O process via a UNIX domain socket using a simple text-based protocol described below. The I/O process will use to ensure that it can always accept xapi’s requests with a guaranteed upper limit on the delay. Xapi can therefore communicate with the process using blocking I/O.
Xapi will interact with the I/O process in a best-effort fashion. If it cannot communicate with the process, or the process indicates that it has not carried out the requested command, xapi will continue execution regardless. Redo-log entries are idempotent (modulo the raising of exceptions in some cases) so it is of little consequence if a particular entry cannot be written but others can. If xapi notices that the process has died, it will attempt to restart it.
The I/O process keeps track of a pointer for each half indicating the position at which the next delta will be written in that half.
Protocol Upon connection to the control socket, the I/O process will attempt to connect to the block device. Depending on whether this is successful or unsuccessful, one of two responses will be sent to the client.
connect|ack_ if it is successful; or
connect|nack|<length>|<message> if it is unsuccessful, perhaps because the block device does not exist or cannot be read from. The <message> is a description of the error; the <length> of the message is expressed using 16 digits of decimal ascii.
The former message indicates that the I/O process is ready to receive commands. The latter message indicates that commands can not be sent to the I/O process.
There are three commands which xapi can send to the I/O process. These are described below, with a high level description of the operational semantics of the I/O process’ actions, and the corresponding responses. For ease of parsing, each command is ten bytes in length.
Write database Xapi requests that a new database is written to the block device, and sends its content using the data socket.
Command: : writedb___|<uuid>|<generation-count>|<length> The UUID is expressed as 36 ASCII characters. The length of the data and the generation-count are expressed using 16 digits of decimal ASCII. Semantics: Read the validity byte. If one half is valid, we will use the other half. If no halves are valid, we will use the first half. Read the data from the data socket and write it into the chosen half. Set the pointer for the chosen half to point to the position after the data. Set the validity byte to indicate the chosen half is valid. Response: : writedb|ack_ in case of successful write; or writedb|nack|<length>|<message> otherwise. For error messages, the length of the message is expressed using 16 digits of decimal ascii. In particular, the error message for timeouts is the string Timeout. Write database delta Xapi sends a description of a database delta to append to the block device.
Command: : writedelta|<uuid>|<generation-count>|<length>|<data> The UUID is expressed as 36 ASCII characters. The length of the data and the generation-count are expressed using 16 digits of decimal ASCII. Semantics: Read the validity byte to establish which half is valid. If neither half is valid, return with a nack. If the half’s pointer is set, seek to that position. Otherwise, scan through the half and stop at the position after the last write. Write the entry. Update the half’s pointer to point to the position after the entry. Response: : writedelta|ack_ in case of successful append; or writedelta|nack|<length>|<message> otherwise. For error messages, the length of the message is expressed using 16 digits of decimal ASCII. In particular, the error message for timeouts is the string Timeout. Read log Xapi requests the contents of the log.
Command: : read______
Semantics: Read the validity byte to establish which half is valid. If neither half is valid, return with an end. Attempt to read the database from the current half. If this is successful, continue in that half reading entries up to the position of the half’s pointer. If the pointer is not set, read until a record of length zero is found or the end of the half is reached. Otherwise—if the attempt to the read the database was not successful—switch to using the other half and try again from step 2. Finally output an end. Response: : read|nack_|<length>|<message> in case of error; or read|db___|<generation-count>|<length>|<data> for a database record, then a sequence of zero or more read|delta|<generation-count>|<length>|<data> for each delta record, then read|end__ For each record, and for error messages, the length of the data or message is expressed using 16 digits of decimal ascii. In particular, the error message for timeouts is the string Timeout. Re-initialise log Xapi requests that the block device is re-initialised with a fresh redo-log.
Command: : empty_____\\
Semantics: : 1. Set the validity byte to indicate that neither half is valid.
Response: : empty|ack_ in case of successful re-initialisation; or empty|nack|<length>|<message> otherwise. For error messages, the length of the message is expressed using 16 digits of decimal ASCII. In particular, the error message for timeouts is the string Timeout. Impact on xapi performance The implementation of the feature causes a slow-down in xapi of around 6% in the general case. However, if the LUN becomes inaccessible this can cause a slow-down of up to 25% in the worst case.
The figure below shows the result of testing four configurations, counting the number of database writes effected through a command-line ‘xe pool-param-set’ call.
The first and second configurations are xapi without the Metadata-on-LUN feature, with HA disabled and enabled respectively.
The third configuration shows xapi with the Metadata-on-LUN feature using a healthy LUN to which all database writes can be successfully flushed.
The fourth configuration shows xapi with the Metadata-on-LUN feature using an inaccessible LUN for which all database writes fail.
Testing strategy The section above shows how xapi performance is affected by this feature. The sections below describe the dev-testing which has already been undertaken, and propose how this feature will impact on regression testing.
Dev-testing performed A variety of informal tests have been performed as part of the development process:
Enable HA. Confirm LUN starts being used to persist database writes.
Enable HA, disable HA. Confirm LUN stops being used.
Enable HA, kill xapi on master, restart xapi on master. Confirm that last database write before kill is successfully restored on restart.
Repeatedly enable and disable HA. Confirm that no file descriptors are leaked (verified by counting the number of descriptors in /proc/pid/fd/).
Enable HA, reboot the master. Due to HA, a slave becomes the master (or this can be forced using ‘xe pool-emergency-transition-to-master’). Confirm that the new master starts is able to restore the database from the LUN from the point the old master left off, and begins to write new changes to the LUN.
Enable HA, disable the iSCSI volume. Confirm that xapi continues to make progress, although database writes are not persisted.
Enable HA, disable and enable the iSCSI volume. Confirm that xapi begins to use the LUN when the iSCSI volume is re-enabled and subsequent writes are persisted.
These tests have been undertaken using an iSCSI target VM and a real iSCSI volume on lannik. In these scenarios, disabling the iSCSI volume consists of stopping the VM and unmapping the LUN, respectively.
Proposed new regression test A new regression test is proposed to confirm that all database writes are persisted across failure.
There are three types of database modification to test: row creation, field-write and row deletion. Although these three kinds of write could be tested in separate tests, the means of setting up the pre-conditions for a field-write and a row deletion require a row creation, so it is convenient to test them all in a single test.
Start a pool containing three hosts.
Issue a CLI command on the master to create a row in the database, e.g.
xe network-create name-label=a.
Forcefully power-cycle the master.
On fail-over, issue a CLI command on the new master to check that the row creation persisted:
xe network-list name-label=a,
confirming that the returned string is non-empty.
Issue a CLI command on the master to modify a field in the new row in the database:
xe network-param-set uuid=<uuid> name-description=abcd,
where <uuid> is the UUID returned from step 2.
Forcefully power-cycle the master.
On fail-over, issue a CLI command on the new master to check that the field-write persisted:
xe network-param-get uuid=<uuid> param-name=name-description,
where <uuid> is the UUID returned from step 2. The returned string should contain
abcd.
Issue a CLI command on the master to delete the row from the database:
xe network-destroy uuid=<uuid>,
where <uuid> is the UUID returned from step 2.
Forcefully power-cycle the master.
On fail-over, issue a CLI command on the new master to check that the row does not exist:
xe network-list name-label=a,
confirming that the returned string is empty.
Impact on existing regression tests The Metadata-on-LUN feature should mean that there is no need to perform an ‘xe pool-sync-database’ operation in existing HA regression tests to ensure that database state persists on xapi failure.
`,description:"",tags:null,title:"Metadata-on-LUN",uri:"/new-docs/xapi/database/redo-log/index.html"},{content:`NUMA in a nutshell Systems that contain more than one CPU socket are typically built on a Non-Uniform Memory Architecture (NUMA) 12. In a NUMA system each node has fast, lower latency access to local memory.
In the diagram 3 above we have 4 NUMA nodes:
2 of those are due to 2 separate physical packages (sockets) a further 2 is due to Sub-NUMA-Clustering (aka Nodes Per Socket for AMD) where the L3 cache is split The L3 cache is shared among multiple cores, but cores 0-5 have lower latency access to one part of it, than cores 6-11, and this is also reflected by splitting memory addresses into 4 31GiB ranges in total.
In the diagram the closer the memory is to the core, the lower the access latency:
per-core caches: L1, L2 per-package shared cache: L3 (local part), L3 (remote part) local NUMA node (to a group of cores, e.g. L#0 P#0), node 0 remote NUMA node in same package (L#1 P#2), node 1 remote NUMA node in other packages (L#2 P#1 and ‘L#3P#3’), node 2 and 3 The NUMA distance matrix Accessing remote NUMA node in the other package has to go through a shared interconnect, which has lower bandwidth than the direct connections, and also a bottleneck if both cores have to access remote memory: the bandwidth for a single core is effectively at most half.
This is reflected in the NUMA distance/latency matrix. The units are arbitrary, and by convention access latency to the local NUMA node is given distance ‘10’.
Relative latency matrix by logical indexes:
index 0 2 1 3 0 10 21 11 21 2 21 10 21 11 1 11 21 10 21 3 21 11 21 10 This follows the latencies described previously:
fast access to local NUMA node memory (by definition), node 0, cost 10 slightly slower access latency to the other NUMA node in same package, node 1, cost 11 twice as slow access latency to remote NUMA memory in the other physical package (socket): nodes 2 and 3, cost 21 There is also I/O NUMA where a cost is similarly associated to where a PCIe is plugged in, but exploring that is future work (it requires exposing NUMA topology to the Dom0 kernel to benefit from it), and for simplicity the diagram above does not show it.
Advantages of NUMA NUMA does have advantages though: if each node accesses only its local memory, then each node can independently achieve maximum throughput.
For best performance we should:
minimize the amount of interconnect bandwidth we are using run code that accesses memory allocated on the closest NUMA node maximize the number of NUMA nodes that we use in the system as a whole If a VM’s memory and vCPUs can entirely fit within a single NUMA node then we should tell Xen to prefer to allocate memory from and run the vCPUs on a single NUMA node.
Xen vCPU soft-affinity The Xen scheduler supports 2 kinds of constraints:
hard pinning: a vCPU may only run on the specified set of pCPUs and nowhere else soft pinning: a vCPU is preferably run on the specified set of pCPUs, but if they are all busy then it may run elsewhere The former is useful if you want strict separation, but it can potentially leave part of the system idle while another part is bottlenecked with lots of vCPUs all competing for the same limited set of pCPUs.
Xen does not migrate workloads between NUMA nodes on its own (the Linux kernel does), although it is possible to achieve a similar effect with explicit migration. However migration introduces additional delays and is best avoided for entire VMs.
The latter (soft pinning) is preferred: running a workload now, even on a potentially suboptimal pCPU (higher NUMA latency) is still better than not running it at all and waiting until a pCPU is freed up.
Xen will also allocate memory for the VM according to the vCPU (soft) pinning: if the vCPUs are pinned only to NUMA nodes A and B, then it will allocate the VM’s memory from NUMA nodes A and B (in a round-robin way, resulting in interleaving).
By default (no pinning) it will interleave memory from all NUMA nodes, which provides average performance, but individual tasks’ performance may be significantly higher or lower depending on which NUMA node the application may have “landed” on. Furthermore restarting processes will speed them up or slow them down as address space randomization picks different memory regions inside a VM.
Note that this is not the worst case: the worst case would be for memory to be allocated on one NUMA node, but the vCPU always running on the furthest away NUMA node.
Best effort NUMA-aware memory allocation for VMs By default Xen stripes the VM’s memory accross all NUMA nodes of the host, which means that every VM has to go through all the interconnects. The goal here is to find a better allocation than the default, not necessarily an optimal allocation. An optimal allocation would require knowing what VMs you would start/create in the future, and planning across hosts too.
Overall we want to balance the VMs across NUMA nodes, such that we use all NUMA nodes to take advantage of the maximum memory bandwidth available on the system. For now this proposed balancing will be done only by balancing memory usage: always heuristically allocating VMs on the NUMA node that has the most available memory. Note that this allocation has a race condition for now when multiple VMs are booted in parallel, because we don’t wait until Xen has constructed the domain for each one (that’d serialize domain construction, which is currently parallel). This may be improved in the future by having an API to query Xen where it has allocated the memory, and to explicitly ask it to place memory on a given NUMA node (instead of best_effort).
If a VM doesn’t fit into a single node then it is not so clear what the best approach is. One criteria to consider is minimizing the NUMA distance between the nodes chosen for the VM. Large NUMA systems may not be fully connected in a mesh requiring multiple hops to each a node, or even have assymetric links, or links with different bitwidth. These tradeoff should be approximatively reflected in the ACPI SLIT tables, as a matrix of distances between nodes. It is possible that 3 NUMA nodes have a smaller average/maximum distance than 2, so we need to consider all possibilities.
For N nodes there would be 2^N possibilities, so [Topology.NUMA.candidates] limits the number of choices to 65520+N (full set of 2^N possibilities for 16 NUMA nodes, and a reduced set of choices for larger systems).
[Topology.NUMA.candidates] is a sorted sequence of node sets, in ascending order of maximum/average distances. Once we’ve eliminated the candidates not suitable for this VM (that do not have enough total memory/pCPUs) we are left with a monotonically increasing sequence of nodes. There are still multiple possibilities with same average distance. This is where we consider our second criteria - balancing - and pick the node with most available free memory.
Once a suitable set of NUMA nodes are picked we compute the CPU soft affinity as the union of the CPUs from all these NUMA nodes. If we didn’t find a solution then we let Xen use its default allocation.
The “distances” between NUMA nodes may not all be equal, e.g. some nodes may have shorter links to some remote NUMA nodes, while others may have to go through multiple hops to reach it. See page 13 in 4 for a diagram of an AMD Opteron 6272 system.
Limitations and tradeoffs Booting multiple VMs in parallel will result in potentially allocating both on the same NUMA node (race condition) When we’re about to run out of host memory we’ll fall back to striping memory again, but the soft affinity mask won’t reflect that (this needs an API to query Xen on where it has actually placed the VM, so we can fix up the mask accordingly) XAPI is not aware of NUMA balancing across a pool, and choses hosts purely based on total amount of free memory, even if a better NUMA placement could be found on another host Very large (>16 NUMA nodes) systems may only explore a limited number of choices (fit into a single node vs fallback to full interleaving) The exact VM placement is not yet controllable Microbenchmarks with a single VM on a host show both performance improvements and regressions on memory bandwidth usage: previously a single VM may have been able to take advantage of the bandwidth of both NUMA nodes if it happened to allocate memory from the right places, whereas now it’ll be forced to use just a single node. As soon as you have more than 1 VM that is busy on a system enabling NUMA balancing should almost always be an improvement though. it is not supported to combine hard vCPU masks with soft affinity: if hard affinities are used then no NUMA scheduling is done by the toolstack and we obey exactly what the user has asked for with hard affinities. This shouldn’t affect other VMs since the memory used by hard-pinned VMs will still be reflected in overall less memory available on individual NUMA nodes. Corner case: the ACPI standard allows certain NUMA nodes to be unreachable (distance 0xFF = -1 in the Xen bindings). This is not supported and will cause an exception to be raised. If this is an issue in practice the NUMA matrix could be pre-filtered to contain only reachable nodes. NUMA nodes with 0 CPUs are accepted (it can result from hard affinity pinnings) NUMA balancing is not considered during HA planning Dom0 is a single VM that needs to communicate with all other VMs, so NUMA balancing is not applied to it (we’d need to expose NUMA topology to the Dom0 kernel so it can better allocate processes) IO NUMA is out of scope for now XAPI datamodel design New API field: Host.numa_affinity_policy. Choices: default_policy, any, best_effort. On upgrade the field is set to default_policy Changes in the field only affect newly (re)booted VMs, for changes to take effect on existing VMs a host evacuation or reboot is needed There may be more choices in the future (e.g. strict, which requires both Xen and toolstack changes).
Meaning of the policy:
any: the Xen default where it allocated memory by striping across NUMA nodes
best_effort: the algorithm described in this document, where soft pinning is used to achieve better balancing and lower latency
default_policy: when the admin hasn’t expressed a preference
Currently default_policy is treated as any, but the admin can change it, and then the system will remember that change across upgrades. If we didn’t have a default_policy then changing the “default” policy on an upgrade would be tricky: we either risk overriding an explicit choice of the admin, or existing installs cannot take advantage of the improved performance from best_effort
Future XAPI versions may change default_policy to mean best_effort. Admins can still override it to any if they wish on a host by host basis.
It is not expected that users would have to change best_effort, unless they run very specific workloads, so a pool level control is not provided at this moment.
There is also no separate feature flag: this host flag acts as a feature flag that can be set through the API without restarting the toolstack. Although obviously only new VMs will benefit.
Debugging the allocator is done by running xl vcpu-list and investigating the soft pinning masks, and by analyzing xensource.log.
Xenopsd implementation See the documentation in [softaffinity.mli] and [topology.mli].
[Softaffinity.plan] returns a [CPUSet] given a host’s NUMA allocation state and a VM’s NUMA allocation request. [Topology.CPUSet] provides helpers for operating on a set of CPU indexes. [Topology.NUMAResource] is a [CPUSet] and the free memory available on a NUMA node. [Topology.NUMARequest] is a request for a given number of vCPUs and memory in bytes. [Topology.NUMA] represents a host’s NUMA allocation state. [Topology.NUMA.candidates] are groups of nodes orderd by minimum average distance. The sequence is limited to [N+65520], where [N] is the number of NUMA nodes. This avoids exponential state space explosion on very large systems (>16 NUMA nodes). [Topology.NUMA.choose] will choose one NUMA node deterministically, while trying to keep overall NUMA node usage balanced. [Domain.numa_placement] builds a [NUMARequest] and uses the above [Topology] and [Softaffinity] functions to compute and apply a plan. We used to have a xenopsd.conf configuration option to enable numa placement, for backwards compatibility this is still supported, but only if the admin hasn’t set an explicit policy on the Host. It is best to remove the experimental xenopsd.conf entry though, a future version may completely drop it.
Tests are in [test_topology.ml] which checks balancing properties and whether the plan has improved best/worst/average-case access times in a simulated test based on 2 predefined NUMA distance matrixes (one from Intel and one from an AMD system).
Future work enable ‘best_effort’ mode by default once more testing has been done an API to query Xen where it has actually allocated the VM’s memory. Currently only an xl debug-keys interface exists which is not supported in production as it can result in killing the host via the watchdog, and is not a proper API, but a textual debug output with no stability guarantees. more host policies (e.g. strict). Requires the XAPI pool scheduler to be NUMA aware and consider it as part of chosing hosts. VM level policy that can set a NUMA affinity index, mapped to a NUMA node modulo NUMA nodes available on the system (this is needed so that after migration we don’t end up trying to allocate vCPUs to a non-existent NUMA node) VM level anti-affinity rules for NUMA placement (can be achieved by setting unique NUMA affinity indexes) Xen on NUMA Machines ↩︎
What is NUMA? ↩︎
created with lstopo-no-graphics --no-io --of svg --vert=L3 >hwloc.svg on a bare metal Linux ↩︎
Lepers, Baptiste. “Improving performance on NUMA systems.” PhD diss., Université de Grenoble, 2014. ↩︎
`,description:"",tags:null,title:"NUMA",uri:"/new-docs/toolstack/features/NUMA/index.html"},{content:`Let’s trace through interesting operations to see how the whole system works.
Starting a VM Migrating a VM Shutting down a VM and waiting for it to happen A VM wants to reboot itself A disk is hotplugged A disk refuses to hotunplug A VM is suspended `,description:"",tags:null,title:"Operation Walk-Throughs",uri:"/new-docs/xenopsd/walkthroughs/index.html"},{content:` Warning This was converted to markdown from squeezer.tex. It is not clear how much of this document is still relevant and/or already present in the other docs.
summary ballooning is a per-domain operation; not a per-VM operation. A VM may be represented by multiple domains (currently localhost migrate, in the future stubdomains)
most free host memory is divided up between running domains proportionally, so they all end up with the same value of ratio
where ratio(domain) = if domain.dynamic_max - domain.dynamic_min = 0 then 0 else (domain.target - domain.dynamic_min) / (domain.dynamic_max - domain.dynamic_min) Assumptions all memory values are stored and processed in units of KiB
the squeezing algorithm doesn’t know about host or VM overheads but this doesn’t matter because
the squeezer assumes that any free host memory can be allocated to running domains and this will be directly reflected in their memory_actual i.e. if x KiB is free on the host we can tell a guest to use x KiB and see the host memory goes to 0 and the guest’s memory_actual increase by x KiB. We assume that no-extra ’overhead’ is required in this operation (all overheads are functions of static_max only)
Definitions domain: an object representing a xen domain
domain.domid: unique identifier of the domain on the host
domaininfo(domain): a function which returns live per-domain information from xen (in real-life a hypercall)
a domain is said to “have never run” if never_been_run(domain)
where never_been_run(domain) = domaininfo(domain).paused and not domaininfo(domain).shutdown and domaininfo(domain).cpu_time = 0 xenstore-read(path): a function which returns the value associated with ’path’ in xenstore
domain.initial_reservation: used to associate freshly freed memory with a new domain which is being built or restored
domain.initial_reservation = xenstore-read(/local/domain/<domain.domid>/memory/initial-reservation) domain.target: represents what we think the balloon target currently is
domain.target = if never_been_run(domain) then xenstore-read(/local/domain/<domain.domid>/memory/target) else domain.initial_reservation domain.dynamic_min: represents what we think the dynamic_min currently is
domain.dynamic_min = if never_been_run(domain) then xenstore-read(/local/domain/<domain.domid>/memory/dynamic_min) else domain.initial_reservation domain.dynamic_max: represents what we think the dynamic_max currently is
domain.dynamic_max = if never_been_run(domain) then xenstore-read(/local/domain/<domain.domid>/memory/dynamic_max) else domain.initial_reservation domain.memory_actual: represents the memory we think the guest is using (doesn’t take overheads like shadow into account)
domain.memory_actual = if never_been_run(domain) max domaininfo(domain).total_memory_pages domain.initial_reservation else domaininfo(domain).total_memory_pages domain.memory_actual_last_update_time: time when we saw the last change in memory_actual
domain.unaccounted_for: a fresh domain has memory reserved for it but xen doesn’t know about it. We subtract this from the host memory xen thinks is free.
domain.unaccounted_for = if never_been_run(domain) then max 0 (domain.initial_reservation - domaininfo(domain).total_memory_pages) domain.max_mem: an upper-limit on the amount of memory a domain can allocate. Initially static_max.
domain.max_mem = domaininfo(domain).max_mem assume_balloon_driver_stuck_after: a constant number of seconds after which we conclude that the balloon driver has stopped working
assume_balloon_driver_stuck_after = 2 domain.active: a boolean value which is true when we think the balloon driver is functioning
domain.active = has_hit_target(domain) or (now - domain.memory_actual_last_update_time) > assume_balloon_driver_stuck_after a domain is said to “have hit its target” if has_hit_target(domain)
where has_hit_target(domain) = floor(memory_actual / 4) = floor(target / 4) NB this definition might have to be loosened if it turns out that some drivers are less accurate than this.
a domain is said to “be capable of ballooning” if can_balloon(domain) where can_balloon(domain) = not domaininfo(domain).paused
host: an object representing a XenServer host
host.domains: a list of domains present on the host
physinfo(host): a function which returns live per-host information from xen (in real-life a hypercall)
host.free_mem: amount of memory we consider to be free on the host
host.free_mem = physinfo(host).free_pages + physinfo(host).scrub_pages - \\sigma d\\in host.domains. d.unaccounted_for Squeezer APIs The squeezer has 2 APIs:
allocate-memory-for-domain(host, domain, amount): frees “amount” and “reserves” (as best it can) it for a particular domain
rebalance-memory: called after e.g. domain destruction to rebalance memory between the running domains
allocate-memory-for-domain keeps contains the main loop which performs the actual target and max_mem adjustments:
function allocate-memory-for-domain(host, domain, amount): \\forall d\\in host.domains. d.max_mem <- d.target while true do -- call change-host-free-memory with a "success condition" set to -- "when the host memory is >= amount" declared_active, declared_inactive, result = change-host-free-memory(host, amount, \\lambda m >= amount) if result == Success: domain.initial_reservation <- amount return Success elif result == DynamicMinsTooHigh: return DynamicMinsTooHigh elif result == DomainsRefusedToCooperate: return DomainsRefusedToCooperate elif result == AdjustTargets(adjustments): \\forall (domain, target)\\in adjustments: domain.max_mem <- target domain.target <- target \\forall d\\in declared_inactive: domain.max_mem <- min domain.target domain.memory_actual \\forall d\\in declared_active: domain.max_mem <- domain.target done The helper function change-host-free-memory(host, amount) does the “thinking”:
it keeps track of whether domains are active or inactive (only for the duration of the squeezer API call – when the next call comes in we assume that all domains are active and capable of ballooning… a kind of “innocent until proven guilty” approaxh)
it computes what the balloon targets should be
function change-host-free-memory(host, amount, success_condition): \\forall d\\in host.domains. recalculate domain.active active_domains <- d\\in host.domains where d.active = true inactive_domains <- d\\in host.domains where d.active = false -- since the last time we were called compute the lists of domains -- which have become active and inactive declared_active, declared_inactive <- ... -- compute how much memory we could free or allocate given only the -- active domains maximum_freeable_memory = sum(d\\in active_domains)(d.memory_actual - d.dynamic_min) maximum_allocatable_memory = sum(d\\in active_domains)(d.dynamic_max - d.memory_actual) -- hypothetically consider freeing the maximum memory possible. -- How much would we have to give back after we've taken as much as we want? give_back = max 0 (maximum_freeable_memory - amount) -- compute a list of target changes to 'give this memory back' to active_domains -- NB this code is careful to allocate *all* memory, not just most -- of it because of a rounding error. adjustments = ... -- decide whether every VM has reached its target (a good thing) all_targets_reached = true if \\forall d\\in active_domains.has_hit_target(d) -- If we're happy with the amount of free memory we've got and the active -- guests have finished ballooning if success_condition host.free_mem = true and all_targets_reached and adjustments = [] then return declared_active, declared_inactive, Success -- If we're happy with the amount of free memory and the running domains -- can't absorb any more of the surplus if host.free_mem >= amount and host.free_mem - maximum_allocatable_memory = 0 then return declared_active, declared_inactive, Success -- If the target is too aggressive because of some non-active domains if maximum_freeable_memory < amount and inactive_domains <> [] then return declared_active, declared_inactive, DomainsRefusedToCooperate inactive_domains -- If the target is too aggressive not because of the domains themselves -- but because of the dynamic_mins return declared_active, declared_inactive, DynamicMinsTooHigh The API rebalance-memory aims to use up as much host memory as possible EXCEPT it is necessary to keep some around for xen to use to create empty domains with.
Currently we have: -- 10 MiB target_host_free_mem = 10204 -- it's not always possible to allocate everything so a bit of slop has -- been added here: free_mem_tolerance = 1024 function rebalance-memory(host): change-host-free-memory(host, target_host_free_mem, \\lambda m. m - target_host_free_mem < free_mem_tolerance) -- and then wait for the xen page scrubber `,description:"",tags:null,title:"Overview of the memory squeezer",uri:"/new-docs/squeezed/squeezer/index.html"},{content:`Rule Design The Open vSwitch (OVS) daemon implements a programmable switch. XenServer uses it to re-direct traffic between three entities:
PVS server - identified by its IP address a local VM - identified by its MAC address a local Proxy - identified by its MAC address VM and PVS server are unaware of the Proxy; xapi configures OVS to redirect traffic between PVS and VM to pass through the proxy.
OVS uses rules that match packets. Rules are organised in sets called tables. A rule can be used to match a packet and to inject it into another rule set/table table such that a packet can be matched again.
Furthermore, a rule can set registers associated with a packet which that can be matched in subsequent rules. In that way, a packet can be tagged such that it will only match specific rules downstream that match the tag.
Xapi configures 3 rule sets:
Table 0 - Entry Rules Rules match UDP traffic between VM/PVS, Proxy/VM, and PVS/VM where the PVS server is identified by its IP and all other components by their MAC address. All packets are tagged with the direction they are going and re-submitted into Table 101 which handles ports.
Table 101 - Port Rules Rules match UDP traffic going to a specific port of the PVS server and re-submit it into Table 102.
Table 102 - Exit Rules These rules implement the redirection:
Rules matching packets coming from VM to PVS are directed to the Proxy. Rules matching packets coming from PVS to VM are directed to the Proxy. Rules matching packets coming from the Proxy are already addressed properly (to the VM) are handled normally. `,description:"",tags:null,title:"PVS Proxy OVS Rules",uri:"/new-docs/xenopsd/design/pvs-proxy-ovs/index.html"},{content:`We are currently (Dec 2013) undergoing a transition from the ‘classic’ xenopsd backend (built upon calls to libxc) to the ‘xenlight’ backend built on top of the officially supported libxl API.
During this work, we have come across an incompatibility between the suspend images created using the ‘classic’ backend and those created using the new libxl-based backend. This needed to be fixed to enable RPU to any new version of XenServer.
Historic ‘classic’ stack Prior to this work, xenopsd was involved in the construction of the suspend image and we ended up with an image with the following format:
+-----------------------------+ | "XenSavedDomain\\n" | <-- added by xenopsd-classic |-----------------------------| | Memory image dump | <-- libxc |-----------------------------| | "QemuDeviceModelRecord\\n" | | <size of following record> | <-- added by xenopsd-classic | (a 32-bit big-endian int) | |-----------------------------| | "QEVM" | <-- libxc/qemu | Qemu device record | +-----------------------------+ We have also been carrying a patch in the Xen patchqueue against xc_domain_restore. This patch (revert_qemu_tail.patch) stopped xc_domain_restore from attempting to read past the memory image dump. At which point xenopsd-classic would just take over and restore what it had put there.
Requirements for new stack For xenopsd-xenlight to work, we need to operate without the revert_qemu_tail.patch since libxl assumes it is operating on top of an upstream libxc.
We need the following relationship between suspend images created on one backend being able to be restored on another backend. Where the backends are old-classic (OC), new-classic (NC) and xenlight (XL). Obviously all suspend images created on any backend must be able to be restored on the same backend:
OC _______ NC _______ XL \\ >>>>> >>>>> / \\__________________/ >>>>>>>>>>>>>>>> It turns out this was not so simple. After removing the patch against xc_domain_restore and allowing libxc to restore the hvm_buffer_tail, we found that supsend images created with OC (detailed in the previous section) are not of a valid format for two reasons:
i. The "XenSavedDomain\\n" was extraneous; ii. The Qemu signature section (prior to the record) is not of valid form.
It turns out that the section with the Qemu signature can be one of the following:
a. "QemuDeviceModelRecord" (NB. no newline) followed by the record to EOF; b. "DeviceModelRecord0002" then a uint32_t length followed by record; c. "RemusDeviceModelState" then a uint32_t length followed by record; The old-classic (OC) backend not only uses an invalid signature (since it contains a trailing newline) but it also includes a length, and the length is in big-endian when the uint32_t is seen to be little-endian.
We considered creating a proxy for the fd in the incompatible cases but since this would need to be a 22-lookahead byte-by-byte proxy this was deemed impracticle. Instead we have made patched libxc with a much simpler patch to understand this legacy format.
Because peek-ahead is not possible on pipes, the patch for (ii) needed to be applied at a point where the hvm tail had been read completely. We piggy-backed on the point after (a) had been detected. At this point the remainder of the fd is buffered (only around 7k) and the magic “QEVM” is expected at the head of this buffer. So we simply added a patch to check if there was a pesky newline and the buffer[5:8] was “QEVM” and if it was we could discard the first 5 bytes:
0 1 2 3 4 5 6 7 8 Legacy format from OC: [...| \\n | \\x | \\x | \\x | \\x | Q | E | V | M |...] Required at this point: [...| Q | E | V | M |...] Changes made To make the above use-cases work, we have made the following changes:
1. Make new-classic (NC) not restore Qemu tail (let libxc do it) xenopsd.git:ef3bf4b 2. Make new-classic use valid signature (b) for future restore images xenopsd.git:9ccef3e 3. Make xc_domain_restore in libxc understand legacy xenopsd (OC) format xen-4.3.pq.hg:libxc-restore-legacy-image.patch 4. Remove revert-qemu-tail.patch from Xen patchqueue xen-4.3.pq.hg:3f0e16f2141e 5. Make xenlight (XL) use "XenSavedDomain\\n" start-of-image signature xenopsd.git:dcda545 This has made the required use-cases work as follows:
OC __134__ NC __245__ XL \\ >>>>> >>>>> / \\_______345________/ >>>>>>>>>>>>>>>> And the suspend-resume on same backends work by virtue of:
OC --> OC : Just works NC --> NC : By 1,2,4 XL --> XL : By 4 (5 is used but not required) New components The output of the changes above are:
A new xenops-xc binary for NC A new xenops-xl binary for XL A new libxenguest.4.3 for both of NC and XL Future considerations This should serve as a useful reference when considering making changes to the suspend image in any way.
`,description:"",tags:null,title:"Requirements for suspend image framing",uri:"/new-docs/xenopsd/design/suspend-image-considerations/index.html"},{content:`The xcp-rrdd daemon (hereafter simply called “rrdd”) is a component in the xapi toolstack that is responsible for collecting metrics, storing them as “Round-Robin Databases” (RRDs) and exposing these to clients.
The code is in ocaml/xcp-rrdd.
`,description:"",tags:null,title:"RRDD",uri:"/new-docs/xcp-rrdd/index.html"},{content:`Introduction Current problems with rrdd:
rrdd stores knowledge about whether it is running on a master or a slave This determines the host to which rrdd will archive a VM’s rrd when the VM’s domain disappears - rrdd will always try to archive to the master. However, when a host joins a pool as a slave rrdd is not restarted so this knowledge is out of date. When a VM shuts down on the slave rrdd will archive the rrd locally. When starting this VM again the master xapi will attempt to push any locally-existing rrd to the host on which the VM is being started, but since no rrd archive exists on the master the slave rrdd will end up creating a new rrd and the previous rrd will be lost.
rrdd handles rebooting VMs unpredictably When rebooting a VM, there is a chance rrdd will attempt to update that VM’s rrd during the brief period when there is no domain for that VM. If this happens, rrdd will archive the VM’s rrd to the master, and then create a new rrd for the VM when it sees the new domain. If rrdd doesn’t attempt to update that VM’s rrd during this period, rrdd will continue to add data for the new domain to the old rrd.
Proposal To solve these problems, we will remove some of the intelligence from rrdd and make it into more of a slave process of xapi. This will entail removing all knowledge from rrdd of whether it is running on a master or a slave, and also modifying rrdd to only start monitoring a VM when it is told to, and only archiving an rrd (to a specified address) when it is told to. This matches the way xenopsd only manages domains which it has been told to manage.
Design For most VM lifecycle operations, xapi and rrdd processes (sometimes across more than one host) cooperate to start or stop recording a VM’s metrics and/or to restore or backup the VM’s archived metrics. Below we will describe, for each relevant VM operation, how the VM’s rrd is currently handled, and how we propose it will be handled after the redesign.
VM.destroy The master xapi makes a remove_rrd call to the local rrdd, which causes rrdd to to delete the VM’s archived rrd from disk. This behaviour will remain unchanged.
VM.start(_on) and VM.resume(_on) The master xapi makes a push_rrd call to the local rrdd, which causes rrdd to send any locally-archived rrd for the VM in question to the rrdd of the host on which the VM is starting. This behaviour will remain unchanged.
VM.shutdown and VM.suspend Every update cycle rrdd compares its list of registered VMs to the list of domains actually running on the host. Any registered VMs which do not have a corresponding domain have their rrds archived to the rrdd running on the host believed to be the master. We will change this behaviour by stopping rrdd from doing the archiving itself; instead we will expose a new function in rrdd’s interface:
val archive_rrd : vm_uuid:string -> remote_address:string -> unitThis will cause rrdd to remove the specified rrd from its table of registered VMs, and archive the rrd to the specified host. When a VM has finished shutting down or suspending, the xapi process on the host on which the VM was running will call archive_rrd to ask the local rrdd to archive back to the master rrdd.
VM.reboot Removing rrdd’s ability to automatically archive the rrds for disappeared domains will have the bonus effect of fixing how the rrds of rebooting VMs are handled, as we don’t want the rrds of rebooting VMs to be archived at all.
VM.checkpoint This will be handled automatically, as internally VM.checkpoint carries out a VM.suspend followed by a VM.resume.
VM.pool_migrate and VM.migrate_send The source host’s xapi makes a migrate_rrd call to the local rrd, with a destination address and an optional session ID. The session ID is only required for cross-pool migration. The local rrdd sends the rrd for that VM to the destination host’s rrdd as an HTTP PUT. This behaviour will remain unchanged.
`,description:"",tags:null,title:"RRDD archival redesign",uri:"/new-docs/xcp-rrdd/futures/archival-redesign/index.html"},{content:`Motivation rrdd plugins currently report datasources via a shared-memory file, using the following format:
DATASOURCES 000001e4 dba4bf7a84b6d11d565d19ef91f7906e { "timestamp": 1339685573, "data_sources": { "cpu-temp-cpu0": { "description": "Temperature of CPU 0", "type": "absolute", "units": "degC", "value": "64.33" "value_type": "float", }, "cpu-temp-cpu1": { "description": "Temperature of CPU 1", "type": "absolute", "units": "degC", "value": "62.14" "value_type": "float", } } }This format contains four main components:
A constant header string DATASOURCES
This should always be present.
The JSON data length, encoded as hexadecimal 000001e4
The md5sum of the JSON data dba4bf7a84b6d11d565d19ef91f7906e
The JSON data itself, encoding the values and metadata associated with the reported datasources. { "timestamp": 1339685573, "data_sources": { "cpu-temp-cpu0": { "description": "Temperature of CPU 0", "type": "absolute", "units": "degC", "value": "64.33" "value_type": "float", }, "cpu-temp-cpu1": { "description": "Temperature of CPU 1", "type": "absolute", "units": "degC", "value": "62.14" "value_type": "float", } } }The disadvantage of this protocol is that rrdd has to parse the entire JSON structure each tick, even though most of the time only the values will change.
For this reason a new protocol is proposed.
Protocol V2 value bits format notes header string (string length)*8 string “Datasources” as in the V1 protocol data checksum 32 int32 binary-encoded crc32 of the concatenation of the encoded timestamp and datasource values metadata checksum 32 int32 binary-encoded crc32 of the metadata string (see below) number of datasources 32 int32 only needed if the metadata has changed - otherwise RRDD can use a cached value timestamp 64 int64 Unix epoch datasource values n * 64 int64 n is the number of datasources exported by the plugin metadata length 32 int32 metadata (string length)*8 string All integers are bigendian. The metadata will have the same JSON-based format as in the V1 protocol, minus the timestamp and value key-value pair for each datasource, for example:
{ "datasources": { "memory_reclaimed": { "description":"Host memory reclaimed by squeezed", "owner":"host", "value_type":"int64", "type":"absolute", "default":"true", "units":"B", "min":"-inf", "max":"inf" }, "memory_reclaimed_max": { "description":"Host memory that could be reclaimed by squeezed", "owner":"host", "value_type":"int64", "type":"absolute", "default":"true", "units":"B", "min":"-inf", "max":"inf" } } }The above formatting is not required, but added here for readability.
Reading algorithm if header != expected_header: raise InvalidHeader() if data_checksum == last_data_checksum: raise NoUpdate() if data_checksum != md5sum(encoded_timestamp_and_values): raise InvalidChecksum() if metadata_checksum == last_metadata_checksum: for datasource, value in cached_datasources, values: update(datasource, value) else: if metadata_checksum != md5sum(metadata): raise InvalidChecksum() cached_datasources = create_datasources(metadata) for datasource, value in cached_datasources, values: update(datasource, value)This means that for a normal update, RRDD will only have to read the header plus the first (16 + 16 + 4 + 8 + 8*n) bytes of data, where n is the number of datasources exported by the plugin. If the metadata changes RRDD will have to read all the data (and parse the metadata).
n.b. the timestamp reported by plugins is not currently used by RRDD - it uses its own global timestamp.
`,description:"",tags:null,title:"RRDD plugin protocol v2",uri:"/new-docs/xcp-rrdd/design/plugin-protocol-v2/index.html"},{content:`Snapshots represent the state of a VM, or a disk (VDI) at a point in time. They can be used for:
backups (hourly, daily, weekly etc) experiments (take snapshot, try something, revert back again) golden images (install OS, get it just right, clone it 1000s of times) Read more about the Snapshot APIs.
Disk snapshots Disks are represented in the XenAPI as VDI objects. Disk snapshots are represented as VDI objects with the flag is_a_snapshot set to true. Snapshots are always considered read-only, and should only be used for backup or cloning into new disks. Disk snapshots have a lifetime independent of the disk they are a snapshot of i.e. if someone deletes the original disk, the snapshots remain. This contrasts with some storage arrays in which snapshots are “second class” objects which are automatically deleted when the original disk is deleted.
Disks are implemented in Xapi via “Storage Manager” (SM) plugins. The SM plugins conform to an api (the SMAPI) which has operations including
vdi_create: make a fresh disk, full of zeroes vdi_snapshot: create a snapshot of a disk File-based vhd implementation The existing “EXT” and “NFS” file-based Xapi SM plugins store disk data in trees of .vhd files as in the following diagram:
From the XenAPI point of view, we have one current VDI and a set of snapshots, each taken at a different point in time. These VDIs correspond to leaf vhds in a tree stored on disk, where the non-leaf nodes contain all the shared blocks.
The vhd files are always thinly-provisioned which means they only allocate new blocks on an as-needed basis. The snapshot leaf vhd files only contain vhd metadata and therefore are very small (a few KiB). The parent nodes containing the shared blocks only contain the shared blocks. The current leaf initially contains only the vhd metadata and therefore is very small (a few KiB) and will only grow when the VM writes blocks.
File-based vhd implementations are a good choice if a “gold image” snapshot is going to be cloned lots of times.
Block-based vhd implementation The existing “LVM”, “LVMoISCSI” and “LVMoHBA” block-based Xapi SM plugins store disk data in trees of .vhd files contained within LVM logical volumes:
Non-snapshot VDIs are always stored full size (a.k.a. thickly-provisioned). When parent nodes are created they are automatically shrunk to the minimum size needed to store the shared blocks. The LVs corresponding with snapshot VDIs only contain vhd metadata and by default consume 8MiB. Note: this is different to VDI.clones which are stored full size.
Block-based vhd implementations are not a good choice if a “gold image” snapshot is going to be cloned lots of times, since each clone will be stored full size.
Hypothetical LUN implementation A hypothetical Xapi SM plugin could use LUNs on an iSCSI storage array as VDIs, and the array’s custom control interface to implement the “snapshot” operation:
From the XenAPI point of view, we have one current VDI and a set of snapshots, each taken at a different point in time. These VDIs correspond to LUNs on the same iSCSI target, and internally within the target these LUNs are comprised of blocks from a large shared copy-on-write pool with support for dedup.
Reverting disk snapshots There is no current way to revert in-place a disk to a snapshot, but it is possible to create a writable disk by “cloning” a snapshot.
VM snapshots Let’s say we have a VM, “VM1” that has 2 disks. Concentrating only on the VM, VBDs and VDIs, we have the following structure:
When we take a snapshot, we first ask the storage backends to snapshot all of the VDIs associated with the VM, producing new VDI objects. Then we copy all of the metadata, producing a new ‘snapshot’ VM object, complete with its own VBDs copied from the original, but now pointing at the snapshot VDIs. We also copy the VIFs and VGPUs but for now we will ignore those.
This process leads to a set of objects that look like this:
We have fields that help navigate the new objects: VM.snapshot_of, and VDI.snapshot_of. These, like you would expect, point to the relevant other objects.
Deleting VM snapshots When a snapshot is deleted Xapi calls the SM API vdi_delete. The Xapi SM plugins which use vhd format data do not reclaim space immediately; instead they mark the corresponding vhd leaf node as “hidden” and, at some point later, run a garbage collector process.
The garbage collector will first determine whether a “coalesce” should happen i.e. whether any parent nodes have only one child i.e. the “shared” blocks are only “shared” with one other node. In the following example the snapshot delete leaves such a parent node and the coalesce process copies blocks from the redundant parent’s only child into the parent:
Note that if the vhd data is being stored in LVM, then the parent node will have had to be expanded to full size to accommodate the writes. Unfortunately this means the act of reclaiming space actually consumes space itself, which means it is important to never completely run out of space in such an SR.
Once the blocks have been copied, we can now cut one of the parents out of the tree by relinking its children into their grandparent:
Finally the garbage collector can remove unused vhd files / LVM LVs:
Reverting VM snapshots The XenAPI call VM.revert overwrites the VM metadata with the snapshot VM metadata, deletes the current VDIs and replaces them with clones of the snapshot VDIs. Note there is no “vdi_revert” in the SMAPI.
Revert implementation details This is the process by which we revert a VM to a snapshot. The first thing to notice is that there is some logic that is called from message_forwarding.ml, which uses some low-level database magic to turn the current VM record into one that looks like the snapshot object. We then go to the rest of the implementation in xapi_vm_snapshot.ml. First, we shut down the VM if it is currently running. Then, we revert all of the VBDs, VIFs and VGPUs. To revert the VBDs, we need to deal with the VDIs underneath them. In order to create space, the first thing we do is delete all of the VDIs currently attached via VBDs to the VM. We then clone the disks from the snapshot. Note that there is no SMAPI operation ‘revert’ currently - we simply clone from the snapshot VDI. It’s important to note that cloning creates a new VDI object: this is not the one we started with gone.
`,description:"",tags:null,title:"Snapshots",uri:"/new-docs/toolstack/features/snapshots/index.html"},{content:`Introduction Xapi has RRDs to track VM- and host-level metrics. There is a desire to have SR-level RRDs as a new category, because SR stats are not specific to a certain VM or host. Examples are size and free space on the SR. While recording SR metrics is relatively straightforward within the current RRD system, the main question is where to archive them, which is what this design aims to address.
Stats Collection All SR types, including the existing ones, should be able to have RRDs defined for them. Some RRDs, such as a “free space” one, may make sense for multiple (if not all) SR types. However, the way to measure something like free space will be SR specific. Furthermore, it should be possible for each type of SR to have its own specialised RRDs.
It follows that each SR will need its own xcp-rrdd plugin, which runs on the SR master and defines and collects the stats. For the new thin-lvhd SR this could be xenvmd itself. The plugin registers itself with xcp-rrdd, so that the latter records the live stats from the plugin into RRDs.
Archiving SR-level RRDs will be archived in the SR itself, in a VDI, rather than in the local filesystem of the SR master. This way, we don’t need to worry about master failover.
The VDI will be 4MB in size. This is a little more space than we would need for the RRDs we have in mind at the moment, but will give us enough headroom for the foreseeable future. It will not have a filesystem on it for simplicity and performance. There will only be one RRD archive file for each SR (possibly containing data for multiple metrics), which is gzipped by xcp-rrdd, and can be copied onto the VDI.
There will be a simple framing format for the data on the VDI. This will be as follows:
Offset Type Name Comment 0 32 bit network-order int magic Magic number = 0x7ada7ada 4 32 bit network-order int version 1 8 32 bit network-order int length length of payload 12 gzipped data data Xapi will be in charge of the lifecycle of this VDI, not the plugin or xcp-rrdd, which will make it a little easier to manage them. Only xapi will attach/detach and read from/write to this VDI. We will keep xcp-rrdd as simple as possible, and have it archive to its standard path in the local file system. Xapi will then copy the RRDs in and out of the VDI.
A new value "rrd" in the vdi_type enum of the datamodel will be defined, and the VDI.type of the VDI will be set to that value. The storage backend will write the VDI type to the LVM metadata of the VDI, so that xapi can discover the VDI containing the SR-level RRDs when attaching an SR to a new pool. This means that SR-level RRDs are currently restricted to LVM SRs.
Because we will not write plugins for all SRs at once, and therefore do not need xapi to set up the VDI for all SRs, we will add an SR “capability” for the backends to be able to tell xapi whether it has the ability to record stats and will need storage for them. The capability name will be: SR_STATS.
Management of the SR-stats VDI The SR-stats VDI will be attached/detached on PBD.plug/unplug on the SR master.
On PBD.plug on the SR master, if the SR has the stats capability, xapi:
Creates a stats VDI if not already there (search for an existing one based on the VDI type). Attaches the stats VDI if it did already exist, and copies the RRDs to the local file system (standard location in the filesystem; asks xcp-rrdd where to put them). Informs xcp-rrdd about the RRDs so that it will load the RRDs and add newly recorded data to them (needs a function like push_rrd_local for VM-level RRDs). Detaches stats VDI. On PBD.unplug on the SR master, if the SR has the stats capability xapi:
Tells xcp-rrdd to archive the RRDs for the SR, which it will do to the local filesystem. Attaches the stats VDI, copies the RRDs into it, detaches VDI. Periodic Archiving Xapi’s periodic scheduler regularly triggers xcp-rrdd to archive the host and VM RRDs. It will need to do this for the SR ones as well. Furthermore, xapi will need to attach the stats VDI and copy the RRD archives into it (as on PBD.unplug).
Exporting There will be a new handler for downloading an SR RRD:
http://<server>/sr_rrd?session_id=<SESSION HANDLE>&uuid=<SR UUID> RRD updates are handled via a single handler for the host, VM and SR UUIDs RRD updates for the host, VMs and SRs are handled by a a single handler at /rrd_updates. Exactly what is returned will be determined by the parameters passed to this handler.
Whether the host RRD updates are returned is governed by the presence of host=true in the parameters. host=<anything else> or the absence of the host key will mean the host RRD is not returned.
Whether the VM RRD updates are returned is governed by the vm_uuid key in the URL parameters. vm_uuid=all will return RRD updates for all VM RRDs. vm_uuid=xxx will return the RRD updates for the VM with uuid xxx only. If vm_uuid is none (or any other string which is not a valid VM UUID) then the handler will return no VM RRD updates. If the vm_uuid key is absent, RRD updates for all VMs will be returned.
Whether the SR RRD updates are returned is governed by the sr_uuid key in the URL parameters. sr_uuid=all will return RRD updates for all SR RRDs. sr_uuid=xxx will return the RRD updates for the SR with uuid xxx only. If sr_uuid is none (or any other string which is not a valid SR UUID) then the handler will return no SR RRD updates. If the sr_uuid key is absent, no SR RRD updates will be returned.
It will be possible to mix and match these parameters; for example to return RRD updates for the host and all VMs, the URL to use would be:
http://<server>/rrd_updates?session_id=<SESSION HANDLE>&start=10258122541&host=true&vm_uuid=all&sr_uuid=none Or, to return RRD updates for all SRs but nothing else, the URL to use would be:
http://<server>/rrd_updates?session_id=<SESSION HANDLE>&start=10258122541&host=false&vm_uuid=none&sr_uuid=all While behaviour is defined if any of the keys host, vm_uuid and sr_uuid is missing, this is for backwards compatibility and it is recommended that clients specify each parameter explicitly.
Database updating. If the SR is presenting a data source called ‘physical_utilisation’, xapi will record this periodically in its database. In order to do this, xapi will fork a thread that, every n minutes (2 suggested, but open to suggestions here), will query the attached SRs, then query RRDD for the latest data source for these, and update the database.
The utilisation of VDIs will not be updated in this way until scalability worries for RRDs are addressed.
Xapi will cache whether it is SR master for every attached SR and only attempt to update if it is the SR master.
New APIs. xcp-rrdd: Get the filesystem location where sr rrds are archived: val sr_rrds_path : uid:string -> string
Archive the sr rrds to the filesystem: val archive_sr_rrd : sr_uuid:string -> unit
Load the sr rrds from the filesystem: val push_sr_rrd : sr_uuid:string -> unit
`,description:"",tags:null,title:"SR-Level RRDs",uri:"/new-docs/xcp-rrdd/futures/sr-level-rrds/index.html"},{content:`Overview sequenceDiagram participant local_tapdisk as local tapdisk participant local_smapiv2 as local SMAPIv2 participant xapi participant remote_xapi as remote xapi participant remote_smapiv2 as remote SMAPIv2 (might redirect) participant remote_tapdisk as remote tapdisk Note over xapi: Sort VDIs increasingly by size and then age loop VM's & snapshots' VDIs & suspend images xapi->>remote_xapi: plug dest SR to dest host and pool master alt VDI is not mirrored Note over xapi: We don't mirror RO VDIs & VDIs of snapshots xapi->>local_smapiv2: DATA.copy remote_sm_url activate local_smapiv2 local_smapiv2-->>local_smapiv2: SR.scan local_smapiv2-->>local_smapiv2: VDI.similar_content local_smapiv2-->>remote_smapiv2: SR.scan Note over local_smapiv2: Find nearest smaller remote VDI remote_base, if any alt remote_base local_smapiv2-->>remote_smapiv2: VDI.clone local_smapiv2-->>remote_smapiv2: VDI.resize else no remote_base local_smapiv2-->>remote_smapiv2: VDI.create end Note over local_smapiv2: call copy' activate local_smapiv2 local_smapiv2-->>remote_smapiv2: SR.list local_smapiv2-->>remote_smapiv2: SR.scan Note over local_smapiv2: create new datapaths remote_dp, base_dp, leaf_dp Note over local_smapiv2: find local base_vdi with same content_id as dest, if any local_smapiv2-->>remote_smapiv2: VDI.attach2 remote_dp dest local_smapiv2-->>remote_smapiv2: VDI.activate remote_dp dest opt base_vdi local_smapiv2-->>local_smapiv2: VDI.attach2 base_dp base_vdi local_smapiv2-->>local_smapiv2: VDI.activate base_dp base_vdi end local_smapiv2-->>local_smapiv2: VDI.attach2 leaf_dp vdi local_smapiv2-->>local_smapiv2: VDI.activate leaf_dp vdi local_smapiv2-->>remote_xapi: sparse_dd base_vdi vdi dest [NBD URI for dest & remote_dp] Note over remote_xapi: HTTP handler verifies credentials remote_xapi-->>remote_tapdisk: then passes connection to tapdisk's NBD server local_smapiv2-->>local_smapiv2: VDI.deactivate leaf_dp vdi local_smapiv2-->>local_smapiv2: VDI.detach leaf_dp vdi opt base_vdi local_smapiv2-->>local_smapiv2: VDI.deactivate base_dp base_vdi local_smapiv2-->>local_smapiv2: VDI.detach base_dp base_vdi end local_smapiv2-->>remote_smapiv2: DP.destroy remote_dp deactivate local_smapiv2 local_smapiv2-->>remote_smapiv2: VDI.snapshot remote_copy local_smapiv2-->>remote_smapiv2: VDI.destroy remote_copy local_smapiv2->>xapi: task(snapshot) deactivate local_smapiv2 else VDI is mirrored Note over xapi: We mirror RW VDIs of the VM Note over xapi: create new datapath dp xapi->>local_smapiv2: VDI.attach2 dp xapi->>local_smapiv2: VDI.activate dp xapi->>local_smapiv2: DATA.MIRROR.start dp remote_sm_url activate local_smapiv2 Note over local_smapiv2: copy disk data & mirror local writes local_smapiv2-->>local_smapiv2: SR.scan local_smapiv2-->>local_smapiv2: VDI.similar_content local_smapiv2-->>remote_smapiv2: DATA.MIRROR.receive_start similars activate remote_smapiv2 remote_smapiv2-->>local_smapiv2: mirror_vdi,mirror_dp,copy_diffs_from,copy_diffs_to,dummy_vdi deactivate remote_smapiv2 local_smapiv2-->>local_smapiv2: DP.attach_info dp local_smapiv2-->>remote_xapi: connect to [NBD URI for mirror_vdi & mirror_dp] Note over remote_xapi: HTTP handler verifies credentials remote_xapi-->>remote_tapdisk: then passes connection to tapdisk's NBD server local_smapiv2-->>local_tapdisk: pass socket & dp to tapdisk of dp local_smapiv2-->>local_smapiv2: VDI.snapshot local_vdi [mirror:dp] local_smapiv2-->>local_tapdisk: [Python] unpause disk, pass dp local_tapdisk-->>remote_tapdisk: mirror new writes via NBD to socket Note over local_smapiv2: call copy' snapshot copy_diffs_to local_smapiv2-->>remote_smapiv2: VDI.compose copy_diffs_to mirror_vdi local_smapiv2-->>remote_smapiv2: VDI.remove_from_sm_config mirror_vdi base_mirror local_smapiv2-->>remote_smapiv2: VDI.destroy dummy_vdi local_smapiv2-->>local_smapiv2: VDI.destroy snapshot local_smapiv2->>xapi: task(mirror ID) deactivate local_smapiv2 xapi->>local_smapiv2: DATA.MIRROR.stat activate local_smapiv2 local_smapiv2->>xapi: dest_vdi deactivate local_smapiv2 end loop until task finished xapi->>local_smapiv2: UPDATES.get xapi->>local_smapiv2: TASK.stat end xapi->>local_smapiv2: TASK.stat xapi->>local_smapiv2: TASK.destroy end opt for snapshot VDIs xapi->>local_smapiv2: SR.update_snapshot_info_src remote_sm_url activate local_smapiv2 local_smapiv2-->>remote_smapiv2: SR.update_snapshot_info_dest deactivate local_smapiv2 end Note over xapi: ... Note over xapi: reserve resources for the new VM in dest host loop all VDIs opt VDI is mirrored xapi->>local_smapiv2: DP.destroy dp end end opt post_detach_hook opt active local mirror local_smapiv2-->>remote_smapiv2: DATA.MIRROR.receive_finalize [mirror ID] Note over remote_smapiv2: destroy mirror dp end end Note over xapi: memory image migration by xenopsd Note over xapi: destroy the VM record Receiving SXM These are the remote calls in the above diagram sent from the remote host to the receiving end of storage motion:
Remote SMAPIv2 -> local SMAPIv2 RPC calls: SR.list SR.scan SR.update_snapshot_info_dest VDI.attach2 VDI.activate VDI.snapshot VDI.destroy For copying: For copying from base: VDI.clone VDI.resize For copying without base: VDI.create For mirroring: DATA.MIRROR.receive_start VDI.compose VDI.remove_from_sm_config DATA.MIRROR.receive_finalize HTTP requests to xapi: Connecting to NBD URI via xapi’s HTTP handler This is how xapi coordinates storage migration. We’ll do it as a code walkthrough through the two layers: xapi and storage-in-xapi (SMAPIv2).
Xapi code The entry point is in xapi_vm_migration.ml
The function takes several arguments:
a vm reference (vm) a dictionary of (string * string) key-value pairs about the destination (dest). This is the result of a previous call to the destination pool, Host.migrate_receive live, a boolean of whether we should live-migrate or suspend-resume, vdi_map, a mapping of VDI references to destination SR references, vif_map, a mapping of VIF references to destination network references, vgpu_map, similar for VGPUs options, another dictionary of options let migrate_send' ~__context ~vm ~dest ~live ~vdi_map ~vif_map ~vgpu_map ~options = SMPERF.debug "vm.migrate_send called vm:%s" (Db.VM.get_uuid ~__context ~self:vm); let open Xapi_xenops in let localhost = Helpers.get_localhost ~__context in let remote = remote_of_dest dest in (* Copy mode means we don't destroy the VM on the source host. We also don't copy over the RRDs/messages *) let copy = try bool_of_string (List.assoc "copy" options) with _ -> false inIt begins by getting the local host reference, deciding whether we’re copying or moving, and converting the input dest parameter from an untyped string association list to a typed record, remote, which is declared further up the file:
type remote = { rpc : Rpc.call -> Rpc.response; session : API.ref_session; sm_url : string; xenops_url : string; master_url : string; remote_ip : string; (* IP address *) remote_master_ip : string; (* IP address *) dest_host : API.ref_host; }this contains:
A function, rpc, for calling XenAPI RPCs on the destination A session valid on the destination A sm_url on which SMAPIv2 APIs can be called on the destination A master_url on which XenAPI commands can be called (not currently used) The IP address, remote_ip, of the destination host The IP address, remote_master_ip, of the master of the destination pool Next, we determine which VDIs to copy:
(* The first thing to do is to create mirrors of all the disks on the remote. We look through the VM's VBDs and all of those of the snapshots. We then compile a list of all of the associated VDIs, whether we mirror them or not (mirroring means we believe the VDI to be active and new writes should be mirrored to the destination - otherwise we just copy it) We look at the VDIs of the VM, the VDIs of all of the snapshots, and any suspend-image VDIs. *) let vm_uuid = Db.VM.get_uuid ~__context ~self:vm in let vbds = Db.VM.get_VBDs ~__context ~self:vm in let vifs = Db.VM.get_VIFs ~__context ~self:vm in let snapshots = Db.VM.get_snapshots ~__context ~self:vm in let vm_and_snapshots = vm :: snapshots in let snapshots_vbds = List.flatten (List.map (fun self -> Db.VM.get_VBDs ~__context ~self) snapshots) in let snapshot_vifs = List.flatten (List.map (fun self -> Db.VM.get_VIFs ~__context ~self) snapshots) inwe now decide whether we’re intra-pool or not, and if we’re intra-pool whether we’re migrating onto the same host (localhost migrate). Intra-pool is decided by trying to do a lookup of our current host uuid on the destination pool.
let is_intra_pool = try ignore(Db.Host.get_uuid ~__context ~self:remote.dest_host); true with _ -> false in let is_same_host = is_intra_pool && remote.dest_host == localhost in if copy && is_intra_pool then raise (Api_errors.Server_error(Api_errors.operation_not_allowed, [ "Copy mode is disallowed on intra pool storage migration, try efficient alternatives e.g. VM.copy/clone."]));Having got all of the VBDs of the VM, we now need to find the associated VDIs, filtering out empty CDs, and decide whether we’re going to copy them or mirror them - read-only VDIs can be copied but RW VDIs must be mirrored.
let vms_vdis = List.filter_map (vdi_filter __context true) vbds inwhere vdi_filter is defined earler:
(* We ignore empty or CD VBDs - nothing to do there. Possible redundancy here: I don't think any VBDs other than CD VBDs can be 'empty' *) let vdi_filter __context allow_mirror vbd = if Db.VBD.get_empty ~__context ~self:vbd || Db.VBD.get_type ~__context ~self:vbd = \`CD then None else let do_mirror = allow_mirror && (Db.VBD.get_mode ~__context ~self:vbd = \`RW) in let vm = Db.VBD.get_VM ~__context ~self:vbd in let vdi = Db.VBD.get_VDI ~__context ~self:vbd in Some (get_vdi_mirror __context vm vdi do_mirror)This in turn calls get_vdi_mirror which gathers together some important info:
let get_vdi_mirror __context vm vdi do_mirror = let snapshot_of = Db.VDI.get_snapshot_of ~__context ~self:vdi in let size = Db.VDI.get_virtual_size ~__context ~self:vdi in let xenops_locator = Xapi_xenops.xenops_vdi_locator ~__context ~self:vdi in let location = Db.VDI.get_location ~__context ~self:vdi in let dp = Storage_access.presentative_datapath_of_vbd ~__context ~vm ~vdi in let sr = Db.SR.get_uuid ~__context ~self:(Db.VDI.get_SR ~__context ~self:vdi) in {vdi; dp; location; sr; xenops_locator; size; snapshot_of; do_mirror}The record is helpfully commented above:
type vdi_mirror = { vdi : [ \`VDI ] API.Ref.t; (* The API reference of the local VDI *) dp : string; (* The datapath the VDI will be using if the VM is running *) location : string; (* The location of the VDI in the current SR *) sr : string; (* The VDI's current SR uuid *) xenops_locator : string; (* The 'locator' xenops uses to refer to the VDI on the current host *) size : Int64.t; (* Size of the VDI *) snapshot_of : [ \`VDI ] API.Ref.t; (* API's snapshot_of reference *) do_mirror : bool; (* Whether we should mirror or just copy the VDI *) }xenops_locator is <sr uuid>/<vdi uuid>, and dp is vbd/<domid>/<device> if the VM is running and vbd/<vm_uuid>/<vdi_uuid> if not.
So now we have a list of these records for all VDIs attached to the VM. For these we check explicitly that they’re all defined in the vdi_map, the mapping of VDI references to their destination SR references.
check_vdi_map ~__context vms_vdis vdi_map;We then figure out the VIF map:
let vif_map = if is_intra_pool then vif_map else infer_vif_map ~__context (vifs @ snapshot_vifs) vif_map inMore sanity checks: We can’t do a storage migration if any of the VDIs is a reset-on-boot one - since the state will be lost on the destination when it’s attached:
(* Block SXM when VM has a VDI with on_boot=reset *) List.(iter (fun vconf -> let vdi = vconf.vdi in if (Db.VDI.get_on_boot ~__context ~self:vdi ==\`reset) then raise (Api_errors.Server_error(Api_errors.vdi_on_boot_mode_incompatible_with_operation, [Ref.string_of vdi]))) vms_vdis) ;We now consider all of the VDIs associated with the snapshots. As for the VM’s VBDs above, we end up with a vdi_mirror list. Note we pass false to the allow_mirror parameter of the get_vdi_mirror function as none of these snapshot VDIs will ever require mirrorring.
let snapshots_vdis = List.filter_map (vdi_filter __context false)Finally we get all of the suspend-image VDIs from all snapshots as well as the actual VM, since it might be suspended itself:
snapshots_vbds in let suspends_vdis = List.fold_left (fun acc vm -> if Db.VM.get_power_state ~__context ~self:vm = \`Suspended then let vdi = Db.VM.get_suspend_VDI ~__context ~self:vm in let sr = Db.VDI.get_SR ~__context ~self:vdi in if is_intra_pool && Helpers.host_has_pbd_for_sr ~__context ~host:remote.dest_host ~sr then acc else (get_vdi_mirror __context vm vdi false):: acc else acc) [] vm_and_snapshots inSanity check that we can see all of the suspend-image VDIs on this host:
(* Double check that all of the suspend VDIs are all visible on the source *) List.iter (fun vdi_mirror -> let sr = Db.VDI.get_SR ~__context ~self:vdi_mirror.vdi in if not (Helpers.host_has_pbd_for_sr ~__context ~host:localhost ~sr) then raise (Api_errors.Server_error (Api_errors.suspend_image_not_accessible, [ Ref.string_of vdi_mirror.vdi ]))) suspends_vdis;Next is a fairly complex piece that determines the destination SR for all of these VDIs. We don’t require API uses to decide destinations for all of the VDIs on snapshots and hence we have to make some decisions here:
let dest_pool = List.hd (XenAPI.Pool.get_all remote.rpc remote.session) in let default_sr_ref = XenAPI.Pool.get_default_SR remote.rpc remote.session dest_pool in let suspend_sr_ref = let pool_suspend_SR = XenAPI.Pool.get_suspend_image_SR remote.rpc remote.session dest_pool and host_suspend_SR = XenAPI.Host.get_suspend_image_sr remote.rpc remote.session remote.dest_host in if pool_suspend_SR <> Ref.null then pool_suspend_SR else host_suspend_SR in (* Resolve placement of unspecified VDIs here - unspecified VDIs that are 'snapshot_of' a specified VDI go to the same place. suspend VDIs that are unspecified go to the suspend_sr_ref defined above *) let extra_vdis = suspends_vdis @ snapshots_vdis in let extra_vdi_map = List.map (fun vconf -> let dest_sr_ref = let is_mapped = List.mem_assoc vconf.vdi vdi_map and snapshot_of_is_mapped = List.mem_assoc vconf.snapshot_of vdi_map and is_suspend_vdi = List.mem vconf suspends_vdis and remote_has_suspend_sr = suspend_sr_ref <> Ref.null and remote_has_default_sr = default_sr_ref <> Ref.null in let log_prefix = Printf.sprintf "Resolving VDI->SR map for VDI %s:" (Db.VDI.get_uuid ~__context ~self:vconf.vdi) in if is_mapped then begin debug "%s VDI has been specified in the map" log_prefix; List.assoc vconf.vdi vdi_map end else if snapshot_of_is_mapped then begin debug "%s Snapshot VDI has entry in map for it's snapshot_of link" log_prefix; List.assoc vconf.snapshot_of vdi_map end else if is_suspend_vdi && remote_has_suspend_sr then begin debug "%s Mapping suspend VDI to remote suspend SR" log_prefix; suspend_sr_ref end else if is_suspend_vdi && remote_has_default_sr then begin debug "%s Remote suspend SR not set, mapping suspend VDI to remote default SR" log_prefix; default_sr_ref end else if remote_has_default_sr then begin debug "%s Mapping unspecified VDI to remote default SR" log_prefix; default_sr_ref end else begin error "%s VDI not in VDI->SR map and no remote default SR is set" log_prefix; raise (Api_errors.Server_error(Api_errors.vdi_not_in_map, [ Ref.string_of vconf.vdi ])) end in (vconf.vdi, dest_sr_ref)) extra_vdis inAt the end of this we’ve got all of the VDIs that need to be copied and destinations for all of them:
let vdi_map = vdi_map @ extra_vdi_map in let all_vdis = vms_vdis @ extra_vdis in (* The vdi_map should be complete at this point - it should include all the VDIs in the all_vdis list. *)Now we gather some final information together:
assert_no_cbt_enabled_vdi_migrated ~__context ~vdi_map; let dbg = Context.string_of_task __context in let open Xapi_xenops_queue in let queue_name = queue_of_vm ~__context ~self:vm in let module XenopsAPI = (val make_client queue_name : XENOPS) in let remote_vdis = ref [] in let ha_always_run_reset = not is_intra_pool && Db.VM.get_ha_always_run ~__context ~self:vm in let cd_vbds = find_cds_to_eject __context vdi_map vbds in eject_cds __context cd_vbds;check there’s no CBT (we can’t currently migrate the CBT metadata), make our client to talk to Xenopsd, make a mutable list of remote VDIs (which I think is redundant right now), decide whether we need to do anything for HA (we disable HA protection for this VM on the destination until it’s fully migrated) and eject any CDs from the VM.
Up until now this has mostly been gathering info (aside from the ejecting CDs bit), but now we’ll start to do some actions, so we begin a try-catch block:
trybut we’ve still got a bit of thinking to do: we sort the VDIs to copy based on age/size:
(* Sort VDIs by size in principle and then age secondly. This gives better chances that similar but smaller VDIs would arrive comparatively earlier, which can serve as base for incremental copying the larger ones. *) let compare_fun v1 v2 = let r = Int64.compare v1.size v2.size in if r = 0 then let t1 = Date.to_float (Db.VDI.get_snapshot_time ~__context ~self:v1.vdi) in let t2 = Date.to_float (Db.VDI.get_snapshot_time ~__context ~self:v2.vdi) in compare t1 t2 else r in let all_vdis = all_vdis |> List.sort compare_fun in let total_size = List.fold_left (fun acc vconf -> Int64.add acc vconf.size) 0L all_vdis in let so_far = ref 0L inOK, let’s copy/mirror:
with_many (vdi_copy_fun __context dbg vdi_map remote is_intra_pool remote_vdis so_far total_size copy) all_vdis @@ fun all_map ->The copy functions are written such that they take continuations. This it to make the error handling simpler - each individual component function can perform its setup and execute the continuation. In the event of an exception coming from the continuation it can then unroll its bit of state and rethrow the exception for the next layer to handle.
with_many is a simple helper function for nesting invocations of functions that take continuations. It has the delightful type:
('a -> ('b -> 'c) -> 'c) -> 'a list -> ('b list -> 'c) -> 'c(* Helper function to apply a 'with_x' function to a list *) let rec with_many withfn many fn = let rec inner l acc = match l with | [] -> fn acc | x::xs -> withfn x (fun y -> inner xs (y::acc)) in inner many []As an example of its operation, imagine our withfn is as follows:
let withfn x c = Printf.printf "Starting withfn: x=%d\\n" x; try c (string_of_int x) with e -> Printf.printf "Handling exception for x=%d\\n" x; raise e;;applying this gives the output:
utop # with_many withfn [1;2;3;4] (String.concat ",");; Starting with fn: x=1 Starting with fn: x=2 Starting with fn: x=3 Starting with fn: x=4 - : string = "4,3,2,1"whereas raising an exception in the continutation results in the following:
utop # with_many with_fn [1;2;3;4] (fun _ -> failwith "error");; Starting with fn: x=1 Starting with fn: x=2 Starting with fn: x=3 Starting with fn: x=4 Handling exception for x=4 Handling exception for x=3 Handling exception for x=2 Handling exception for x=1 Exception: Failure "error".All the real action is in vdi_copy_fun, which copies or mirrors a single VDI:
let vdi_copy_fun __context dbg vdi_map remote is_intra_pool remote_vdis so_far total_size copy vconf continuation = TaskHelper.exn_if_cancelling ~__context; let open Storage_access in let dest_sr_ref = List.assoc vconf.vdi vdi_map in let dest_sr_uuid = XenAPI.SR.get_uuid remote.rpc remote.session dest_sr_ref in (* Plug the destination shared SR into destination host and pool master if unplugged. Plug the local SR into destination host only if unplugged *) let dest_pool = List.hd (XenAPI.Pool.get_all remote.rpc remote.session) in let master_host = XenAPI.Pool.get_master remote.rpc remote.session dest_pool in let pbds = XenAPI.SR.get_PBDs remote.rpc remote.session dest_sr_ref in let pbd_host_pair = List.map (fun pbd -> (pbd, XenAPI.PBD.get_host remote.rpc remote.session pbd)) pbds in let hosts_to_be_attached = [master_host; remote.dest_host] in let pbds_to_be_plugged = List.filter (fun (_, host) -> (List.mem host hosts_to_be_attached) && (XenAPI.Host.get_enabled remote.rpc remote.session host)) pbd_host_pair in List.iter (fun (pbd, _) -> if not (XenAPI.PBD.get_currently_attached remote.rpc remote.session pbd) then XenAPI.PBD.plug remote.rpc remote.session pbd) pbds_to_be_plugged;It begins by attempting to ensure the SRs we require are definitely attached on the destination host and on the destination pool master.
There’s now a little logic to support the case where we have cross-pool SRs and the VDI is already visible to the destination pool. Since this is outside our normal support envelope there is a key in xapi_globs that has to be set (via xapi.conf) to enable this:
let rec dest_vdi_exists_on_sr vdi_uuid sr_ref retry = try let dest_vdi_ref = XenAPI.VDI.get_by_uuid remote.rpc remote.session vdi_uuid in let dest_vdi_sr_ref = XenAPI.VDI.get_SR remote.rpc remote.session dest_vdi_ref in if dest_vdi_sr_ref = sr_ref then true else false with _ -> if retry then begin XenAPI.SR.scan remote.rpc remote.session sr_ref; dest_vdi_exists_on_sr vdi_uuid sr_ref false end else false in (* CP-4498 added an unsupported mode to use cross-pool shared SRs - the initial use case is for a shared raw iSCSI SR (same uuid, same VDI uuid) *) let vdi_uuid = Db.VDI.get_uuid ~__context ~self:vconf.vdi in let mirror = if !Xapi_globs.relax_xsm_sr_check then if (dest_sr_uuid = vconf.sr) then begin (* Check if the VDI uuid already exists in the target SR *) if (dest_vdi_exists_on_sr vdi_uuid dest_sr_ref true) then false else failwith ("SR UUID matches on destination but VDI does not exist") end else true else (not is_intra_pool) || (dest_sr_uuid <> vconf.sr) inThe check also covers the case where we’re doing an intra-pool migration and not copying all of the disks, in which case we don’t need to do anything for that disk.
We now have a wrapper function that creates a new datapath and passes it to a continuation function. On error it handles the destruction of the datapath:
let with_new_dp cont = let dp = Printf.sprintf (if vconf.do_mirror then "mirror_%s" else "copy_%s") vconf.dp in try cont dp with e -> (try SMAPI.DP.destroy ~dbg ~dp ~allow_leak:false with _ -> info "Failed to cleanup datapath: %s" dp); raise e inand now a helper that, given a remote VDI uuid, looks up the reference on the remote host and gives it to a continuation function. On failure of the continuation it will destroy the remote VDI:
let with_remote_vdi remote_vdi cont = debug "Executing remote scan to ensure VDI is known to xapi"; XenAPI.SR.scan remote.rpc remote.session dest_sr_ref; let query = Printf.sprintf "(field \\"location\\"=\\"%s\\") and (field \\"SR\\"=\\"%s\\")" remote_vdi (Ref.string_of dest_sr_ref) in let vdis = XenAPI.VDI.get_all_records_where remote.rpc remote.session query in let remote_vdi_ref = match vdis with | [] -> raise (Api_errors.Server_error(Api_errors.vdi_location_missing, [Ref.string_of dest_sr_ref; remote_vdi])) | h :: [] -> debug "Found remote vdi reference: %s" (Ref.string_of (fst h)); fst h | _ -> raise (Api_errors.Server_error(Api_errors.location_not_unique, [Ref.string_of dest_sr_ref; remote_vdi])) in try cont remote_vdi_ref with e -> (try XenAPI.VDI.destroy remote.rpc remote.session remote_vdi_ref with _ -> error "Failed to destroy remote VDI"); raise e inanother helper to gather together info about a mirrored VDI:
let get_mirror_record ?new_dp remote_vdi remote_vdi_reference = { mr_dp = new_dp; mr_mirrored = mirror; mr_local_sr = vconf.sr; mr_local_vdi = vconf.location; mr_remote_sr = dest_sr_uuid; mr_remote_vdi = remote_vdi; mr_local_xenops_locator = vconf.xenops_locator; mr_remote_xenops_locator = Xapi_xenops.xenops_vdi_locator_of_strings dest_sr_uuid remote_vdi; mr_local_vdi_reference = vconf.vdi; mr_remote_vdi_reference = remote_vdi_reference } inand finally the really important function:
let mirror_to_remote new_dp = let task = if not vconf.do_mirror then SMAPI.DATA.copy ~dbg ~sr:vconf.sr ~vdi:vconf.location ~dp:new_dp ~url:remote.sm_url ~dest:dest_sr_uuid else begin (* Though we have no intention of "write", here we use the same mode as the associated VBD on a mirrored VDIs (i.e. always RW). This avoids problem when we need to start/stop the VM along the migration. *) let read_write = true in (* DP set up is only essential for MIRROR.start/stop due to their open ended pattern. It's not necessary for copy which will take care of that itself. *) ignore(SMAPI.VDI.attach ~dbg ~dp:new_dp ~sr:vconf.sr ~vdi:vconf.location ~read_write); SMAPI.VDI.activate ~dbg ~dp:new_dp ~sr:vconf.sr ~vdi:vconf.location; ignore(Storage_access.register_mirror __context vconf.location); SMAPI.DATA.MIRROR.start ~dbg ~sr:vconf.sr ~vdi:vconf.location ~dp:new_dp ~url:remote.sm_url ~dest:dest_sr_uuid end in let mapfn x = let total = Int64.to_float total_size in let done_ = Int64.to_float !so_far /. total in let remaining = Int64.to_float vconf.size /. total in done_ +. x *. remaining in let open Storage_access in let task_result = task |> register_task __context |> add_to_progress_map mapfn |> wait_for_task dbg |> remove_from_progress_map |> unregister_task __context |> success_task dbg in let mirror_id, remote_vdi = if not vconf.do_mirror then let vdi = task_result |> vdi_of_task dbg in remote_vdis := vdi.vdi :: !remote_vdis; None, vdi.vdi else let mirrorid = task_result |> mirror_of_task dbg in let m = SMAPI.DATA.MIRROR.stat ~dbg ~id:mirrorid in Some mirrorid, m.Mirror.dest_vdi in so_far := Int64.add !so_far vconf.size; debug "Local VDI %s %s to %s" vconf.location (if vconf.do_mirror then "mirrored" else "copied") remote_vdi; mirror_id, remote_vdi inThis is the bit that actually starts the mirroring or copying. Before the call to mirror we call VDI.attach and VDI.activate locally to ensure that if the VM is shutdown then the detach/deactivate there doesn’t kill the mirroring process.
Note the parameters to the SMAPI call are sr and vdi, locating the local VDI and SM backend, new_dp, the datapath we’re using for the mirroring, url, which is the remote url on which SMAPI calls work, and dest, the destination SR uuid. These are also the arguments to copy above too.
There’s a little function to calculate the overall progress of the task, and the function waits until the completion of the task before it continues. The function success_task will raise an exception if the task failed. For DATA.mirror, completion implies both that the disk data has been copied to the destination and that all local writes are being mirrored to the destination. Hence more cleanup must be done on cancellation. In contrast, if the DATA.copy path had been taken then the operation at this point has completely finished.
The result of this function is an optional mirror id and the remote VDI uuid.
Next, there is a post_mirror function:
let post_mirror mirror_id mirror_record = try let result = continuation mirror_record in (match mirror_id with | Some mid -> ignore(Storage_access.unregister_mirror mid); | None -> ()); if mirror && not (Xapi_fist.storage_motion_keep_vdi () || copy) then Helpers.call_api_functions ~__context (fun rpc session_id -> XenAPI.VDI.destroy rpc session_id vconf.vdi); result with e -> let mirror_failed = match mirror_id with | Some mid -> ignore(Storage_access.unregister_mirror mid); let m = SMAPI.DATA.MIRROR.stat ~dbg ~id:mid in (try SMAPI.DATA.MIRROR.stop ~dbg ~id:mid with _ -> ()); m.Mirror.failed | None -> false in if mirror_failed then raise (Api_errors.Server_error(Api_errors.mirror_failed,[Ref.string_of vconf.vdi])) else raise e inThis is poorly named - it is post mirror and copy. The aim of this function is to destroy the source VDIs on successful completion of the continuation function, which will have migrated the VM to the destination. In its exception handler it will stop the mirroring, but before doing so it will check to see if the mirroring process it was looking after has itself failed, and raise mirror_failed if so. This is because a failed mirror can result in a range of actual errors, and we decide here that the failed mirror was probably the root cause.
These functions are assembled together at the end of the vdi_copy_fun function:
if mirror then with_new_dp (fun new_dp -> let mirror_id, remote_vdi = mirror_to_remote new_dp in with_remote_vdi remote_vdi (fun remote_vdi_ref -> let mirror_record = get_mirror_record ~new_dp remote_vdi remote_vdi_ref in post_mirror mirror_id mirror_record)) else let mirror_record = get_mirror_record vconf.location (XenAPI.VDI.get_by_uuid remote.rpc remote.session vdi_uuid) in continuation mirror_recordagain, mirror here is poorly named, and means mirror or copy.
Once all of the disks have been mirrored or copied, we jump back to the body of migrate_send. We split apart the mirror records according to the source of the VDI:
let was_from vmap = List.exists (fun vconf -> vconf.vdi = vmap.mr_local_vdi_reference) in let suspends_map, snapshots_map, vdi_map = List.fold_left (fun (suspends, snapshots, vdis) vmap -> if was_from vmap suspends_vdis then vmap :: suspends, snapshots, vdis else if was_from vmap snapshots_vdis then suspends, vmap :: snapshots, vdis else suspends, snapshots, vmap :: vdis ) ([],[],[]) all_map inthen we reassemble all_map from this, for some reason:
let all_map = List.concat [suspends_map; snapshots_map; vdi_map] inNow we need to update the snapshot-of links:
(* All the disks and snapshots have been created in the remote SR(s), * so update the snapshot links if there are any snapshots. *) if snapshots_map <> [] then update_snapshot_info ~__context ~dbg ~url:remote.sm_url ~vdi_map ~snapshots_map;I’m not entirely sure why this is done in this layer as opposed to in the storage layer.
A little housekeeping:
let xenops_vdi_map = List.map (fun mirror_record -> (mirror_record.mr_local_xenops_locator, mirror_record.mr_remote_xenops_locator)) all_map in (* Wait for delay fist to disappear *) wait_for_fist __context Xapi_fist.pause_storage_migrate "pause_storage_migrate"; TaskHelper.exn_if_cancelling ~__context;the fist thing here simply allows tests to put in a delay at this specific point.
We also check the task to see if we’ve been cancelled and raise an exception if so.
The VM metadata is now imported into the remote pool, with all the XenAPI level objects remapped:
let new_vm = if is_intra_pool then vm else (* Make sure HA replaning cycle won't occur right during the import process or immediately after *) let () = if ha_always_run_reset then XenAPI.Pool.ha_prevent_restarts_for ~rpc:remote.rpc ~session_id:remote.session ~seconds:(Int64.of_float !Xapi_globs.ha_monitor_interval) in (* Move the xapi VM metadata to the remote pool. *) let vms = let vdi_map = List.map (fun mirror_record -> { local_vdi_reference = mirror_record.mr_local_vdi_reference; remote_vdi_reference = Some mirror_record.mr_remote_vdi_reference; }) all_map in let vif_map = List.map (fun (vif, network) -> { local_vif_reference = vif; remote_network_reference = network; }) vif_map in let vgpu_map = List.map (fun (vgpu, gpu_group) -> { local_vgpu_reference = vgpu; remote_gpu_group_reference = gpu_group; }) vgpu_map in inter_pool_metadata_transfer ~__context ~remote ~vm ~vdi_map ~vif_map ~vgpu_map ~dry_run:false ~live:true ~copy in let vm = List.hd vms in let () = if ha_always_run_reset then XenAPI.VM.set_ha_always_run ~rpc:remote.rpc ~session_id:remote.session ~self:vm ~value:false in (* Reserve resources for the new VM on the destination pool's host *) let () = XenAPI.Host.allocate_resources_for_vm remote.rpc remote.session remote.dest_host vm true in vm inMore waiting for fist points:
wait_for_fist __context Xapi_fist.pause_storage_migrate2 "pause_storage_migrate2"; (* Attach networks on remote *) XenAPI.Network.attach_for_vm ~rpc:remote.rpc ~session_id:remote.session ~host:remote.dest_host ~vm:new_vm;also make sure all the networks are plugged for the VM on the destination. Next we create the xenopsd-level vif map, equivalent to the vdi_map above:
(* Create the vif-map for xenops, linking VIF devices to bridge names on the remote *) let xenops_vif_map = let vifs = XenAPI.VM.get_VIFs ~rpc:remote.rpc ~session_id:remote.session ~self:new_vm in List.map (fun vif -> let vifr = XenAPI.VIF.get_record ~rpc:remote.rpc ~session_id:remote.session ~self:vif in let bridge = Xenops_interface.Network.Local (XenAPI.Network.get_bridge ~rpc:remote.rpc ~session_id:remote.session ~self:vifr.API.vIF_network) in vifr.API.vIF_device, bridge ) vifs inNow we destroy any extra mirror datapaths we set up previously:
(* Destroy the local datapaths - this allows the VDIs to properly detach, invoking the migrate_finalize calls *) List.iter (fun mirror_record -> if mirror_record.mr_mirrored then match mirror_record.mr_dp with | Some dp -> SMAPI.DP.destroy ~dbg ~dp ~allow_leak:false | None -> ()) all_map;More housekeeping:
SMPERF.debug "vm.migrate_send: migration initiated vm:%s" vm_uuid; (* In case when we do SXM on the same host (mostly likely a VDI migration), the VM's metadata in xenopsd will be in-place updated as soon as the domain migration starts. For these case, there will be no (clean) way back from this point. So we disable task cancellation for them here. *) if is_same_host then (TaskHelper.exn_if_cancelling ~__context; TaskHelper.set_not_cancellable ~__context);Finally we get to the memory-image part of the migration:
(* It's acceptable for the VM not to exist at this point; shutdown commutes with storage migrate *) begin try Xapi_xenops.Events_from_xenopsd.with_suppressed queue_name dbg vm_uuid (fun () -> let xenops_vgpu_map = (* can raise VGPU_mapping *) infer_vgpu_map ~__context ~remote new_vm in migrate_with_retry ~__context queue_name dbg vm_uuid xenops_vdi_map xenops_vif_map xenops_vgpu_map remote.xenops_url; Xapi_xenops.Xenopsd_metadata.delete ~__context vm_uuid) with | Xenops_interface.Does_not_exist ("VM",_) | Xenops_interface.Does_not_exist ("extra",_) -> info "%s: VM %s stopped being live during migration" "vm_migrate_send" vm_uuid | VGPU_mapping(msg) -> info "%s: VM %s - can't infer vGPU map: %s" "vm_migrate_send" vm_uuid msg; raise Api_errors. (Server_error (vm_migrate_failed, ([ vm_uuid ; Helpers.get_localhost_uuid () ; Db.Host.get_uuid ~__context ~self:remote.dest_host ; "The VM changed its power state during migration" ]))) end; debug "Migration complete"; SMPERF.debug "vm.migrate_send: migration complete vm:%s" vm_uuid;Now we tidy up after ourselves:
(* So far the main body of migration is completed, and the rests are updates, config or cleanup on the source and destination. There will be no (clean) way back from this point, due to these destructive changes, so we don't want user intervention e.g. task cancellation. *) TaskHelper.exn_if_cancelling ~__context; TaskHelper.set_not_cancellable ~__context; XenAPI.VM.pool_migrate_complete remote.rpc remote.session new_vm remote.dest_host; detach_local_network_for_vm ~__context ~vm ~destination:remote.dest_host; Xapi_xenops.refresh_vm ~__context ~self:vm;the function pool_migrate_complete is called on the destination host, and consists of a few things that ordinarily would be set up during VM.start or the like:
let pool_migrate_complete ~__context ~vm ~host = let id = Db.VM.get_uuid ~__context ~self:vm in debug "VM.pool_migrate_complete %s" id; let dbg = Context.string_of_task __context in let queue_name = Xapi_xenops_queue.queue_of_vm ~__context ~self:vm in if Xapi_xenops.vm_exists_in_xenopsd queue_name dbg id then begin Cpuid_helpers.update_cpu_flags ~__context ~vm ~host; Xapi_xenops.set_resident_on ~__context ~self:vm; Xapi_xenops.add_caches id; Xapi_xenops.refresh_vm ~__context ~self:vm; Monitor_dbcalls_cache.clear_cache_for_vm ~vm_uuid:id endMore tidying up, remapping some remaining VBDs and clearing state on the sender:
(* Those disks that were attached at the point the migration happened will have been remapped by the Events_from_xenopsd logic. We need to remap any other disks at this point here *) if is_intra_pool then List.iter (fun vm' -> intra_pool_vdi_remap ~__context vm' all_map; intra_pool_fix_suspend_sr ~__context remote.dest_host vm') vm_and_snapshots; (* If it's an inter-pool migrate, the VBDs will still be 'currently-attached=true' because we supressed the events coming from xenopsd. Destroy them, so that the VDIs can be destroyed *) if not is_intra_pool && not copy then List.iter (fun vbd -> Db.VBD.destroy ~__context ~self:vbd) (vbds @ snapshots_vbds); new_vm inThe remark about the Events_from_xenopsd is that we have a thread watching for events that are emitted by xenopsd, and we resynchronise xapi’s state according to xenopsd’s state for several fields for which xenopsd is considered the canonical source of truth. One of these is the exact VDI the VBD is associated with.
The suspend_SR field of the VM is set to the source’s value, so we reset that.
Now we move the RRDs:
if not copy then begin Rrdd_proxy.migrate_rrd ~__context ~remote_address:remote.remote_ip ~session_id:(Ref.string_of remote.session) ~vm_uuid:vm_uuid ~host_uuid:(Ref.string_of remote.dest_host) () end;This can be done for intra- and inter- pool migrates in the same way, simplifying the logic.
However, for messages and blobs we have to only migrate them for inter-pool migrations:
if not is_intra_pool && not copy then begin (* Replicate HA runtime flag if necessary *) if ha_always_run_reset then XenAPI.VM.set_ha_always_run ~rpc:remote.rpc ~session_id:remote.session ~self:new_vm ~value:true; (* Send non-database metadata *) Xapi_message.send_messages ~__context ~cls:\`VM ~obj_uuid:vm_uuid ~session_id:remote.session ~remote_address:remote.remote_master_ip; Xapi_blob.migrate_push ~__context ~rpc:remote.rpc ~remote_address:remote.remote_master_ip ~session_id:remote.session ~old_vm:vm ~new_vm ; (* Signal the remote pool that we're done *) end;Lastly, we destroy the VM record on the source:
Helpers.call_api_functions ~__context (fun rpc session_id -> if not is_intra_pool && not copy then begin info "Destroying VM ref=%s uuid=%s" (Ref.string_of vm) vm_uuid; Xapi_vm_lifecycle.force_state_reset ~__context ~self:vm ~value:\`Halted; List.iter (fun self -> Db.VM.destroy ~__context ~self) vm_and_snapshots end); SMPERF.debug "vm.migrate_send exiting vm:%s" vm_uuid; new_vmThe exception handler still has to clean some state, but mostly things are handled in the CPS functions declared above:
with e -> error "Caught %s: cleaning up" (Printexc.to_string e); (* We do our best to tidy up the state left behind *) Events_from_xenopsd.with_suppressed queue_name dbg vm_uuid (fun () -> try let _, state = XenopsAPI.VM.stat dbg vm_uuid in if Xenops_interface.(state.Vm.power_state = Suspended) then begin debug "xenops: %s: shutting down suspended VM" vm_uuid; Xapi_xenops.shutdown ~__context ~self:vm None; end; with _ -> ()); if not is_intra_pool && Db.is_valid_ref __context vm then begin List.map (fun self -> Db.VM.get_uuid ~__context ~self) vm_and_snapshots |> List.iter (fun self -> try let vm_ref = XenAPI.VM.get_by_uuid remote.rpc remote.session self in info "Destroying stale VM uuid=%s on destination host" self; XenAPI.VM.destroy remote.rpc remote.session vm_ref with e -> error "Caught %s while destroying VM uuid=%s on destination host" (Printexc.to_string e) self) end; let task = Context.get_task_id __context in let oc = Db.Task.get_other_config ~__context ~self:task in if List.mem_assoc "mirror_failed" oc then begin let failed_vdi = List.assoc "mirror_failed" oc in let vconf = List.find (fun vconf -> vconf.location=failed_vdi) vms_vdis in debug "Mirror failed for VDI: %s" failed_vdi; raise (Api_errors.Server_error(Api_errors.mirror_failed,[Ref.string_of vconf.vdi])) end; TaskHelper.exn_if_cancelling ~__context; begin match e with | Storage_interface.Backend_error(code, params) -> raise (Api_errors.Server_error(code, params)) | Storage_interface.Unimplemented(code) -> raise (Api_errors.Server_error(Api_errors.unimplemented_in_sm_backend, [code])) | Xenops_interface.Cancelled _ -> TaskHelper.raise_cancelled ~__context | _ -> raise e endFailures during the migration can result in the VM being in a suspended state. There’s no point leaving it like this since there’s nothing that can be done to resume it, so we force shut it down.
We also try to remove the VM record from the destination if we managed to send it there.
Finally we check for mirror failure in the task - this is set by the events thread watching for events from the storage layer, in storage_access.ml
Storage code The part of the code that is conceptually in the storage layer, but physically in xapi, is located in storage_migrate.ml. There are logically a few separate parts to this file:
A stateful module for persisting state across xapi restarts. Some general helper functions Some quite specific helper functions related to actions to be taken on deactivate/detach An NBD handler The implementations of the SMAPIv2 mirroring APIs Let’s start by considering the way the storage APIs are intended to be used.
Copying a VDI DATA.copy takes several parameters:
dbg - a debug string sr - the source SR (a uuid) vdi - the source VDI (a uuid) dp - unused url - a URL on which SMAPIv2 API calls can be made sr - the destination SR in which the VDI should be copied and returns a parameter of type Task.id. The API call is intended to be called in an asynchronous fashion - ie., the caller makes the call, receives the task ID back and polls or uses the event mechanism to wait until the task has completed. The task may be cancelled via the Task.cancel API call. The result of the operation is obtained by calling TASK.stat, which returns a record:
type t = { id: id; dbg: string; ctime: float; state: state; subtasks: (string * state) list; debug_info: (string * string) list; backtrace: string; }Where the state field contains the result once the task has completed:
type async_result_t = | Vdi_info of vdi_info | Mirror_id of Mirror.id type completion_t = { duration : float; result : async_result_t option } type state = | Pending of float | Completed of completion_t | Failed of Rpc.tOnce the result has been obtained from the task, the task should be destroyed via the TASK.destroy API call.
The implementation uses the url parameter to make SMAPIv2 calls to the destination SR. This is used, for example, to invoke a VDI.create call if necessary. The URL contains an authentication token within it (valid for the duration of the XenAPI call that caused this DATA.copy API call).
The implementation tries to minimize the amount of data copied by looking for related VDIs on the destination SR. See below for more details.
Mirroring a VDI DATA.MIRROR.start takes a similar set of parameters to that of copy:
dbg - a debug string sr - the source SR (a uuid) vdi - the source VDI (a uuid) dp - the datapath on which the VDI has been attached url - a URL on which SMAPIv2 API calls can be made sr - the destination SR in which the VDI should be copied Similar to copy above, this returns a task id. The task ‘completes’ once the mirror has been set up - that is, at any point afterwards we can detach the disk and the destination disk will be identical to the source. Unlike for copy the operation is ongoing after the API call completes, since new writes need to be mirrored to the destination. Therefore the completion type of the mirror operation is Mirror_id which contains a handle on which further API calls related to the mirror call can be made. For example MIRROR.stat whose signature is:
MIRROR.stat: dbg:debug_info -> id:Mirror.id -> Mirror.tThe return type of this call is a record containing information about the mirror:
type state = | Receiving | Sending | Copying type t = { source_vdi : vdi; dest_vdi : vdi; state : state list; failed : bool; }Note that state is a list since the initial phase of the operation requires both copying and mirroring.
Additionally the mirror can be cancelled using the MIRROR.stop API call.
Code walkthrough let’s go through the implementation of copy:
DATA.copy let copy ~task ~dbg ~sr ~vdi ~dp ~url ~dest = debug "copy sr:%s vdi:%s url:%s dest:%s" sr vdi url dest; let remote_url = Http.Url.of_string url in let module Remote = Client(struct let rpc = rpc ~srcstr:"smapiv2" ~dststr:"dst_smapiv2" remote_url end) inHere we are constructing a module Remote on which we can do SMAPIv2 calls directly on the destination.
tryWrap the whole function in an exception handler.
(* Find the local VDI *) let vdis = Local.SR.scan ~dbg ~sr in let local_vdi = try List.find (fun x -> x.vdi = vdi) vdis with Not_found -> failwith (Printf.sprintf "Local VDI %s not found" vdi) inWe first find the metadata for our source VDI by doing a local SMAPIv2 call SR.scan. This returns a list of VDI metadata, out of which we extract the VDI we’re interested in.
tryAnother exception handler. This looks redundant to me right now.
let similar_vdis = Local.VDI.similar_content ~dbg ~sr ~vdi in let similars = List.map (fun vdi -> vdi.content_id) similar_vdis in debug "Similar VDIs to %s = [ %s ]" vdi (String.concat "; " (List.map (fun x -> Printf.sprintf "(vdi=%s,content_id=%s)" x.vdi x.content_id) similar_vdis));Here we look for related VDIs locally using the VDI.similar_content SMAPIv2 API call. This searches for related VDIs and returns an ordered list where the most similar is first in the list. It returns both clones and snapshots, and hence is more general than simply following snapshot_of links.
let remote_vdis = Remote.SR.scan ~dbg ~sr:dest in (** We drop cbt_metadata VDIs that do not have any actual data *) let remote_vdis = List.filter (fun vdi -> vdi.ty <> "cbt_metadata") remote_vdis in let nearest = List.fold_left (fun acc content_id -> match acc with | Some x -> acc | None -> try Some (List.find (fun vdi -> vdi.content_id = content_id && vdi.virtual_size <= local_vdi.virtual_size) remote_vdis) with Not_found -> None) None similars in debug "Nearest VDI: content_id=%s vdi=%s" (Opt.default "None" (Opt.map (fun x -> x.content_id) nearest)) (Opt.default "None" (Opt.map (fun x -> x.vdi) nearest));Here we look for VDIs on the destination with the same content_id as one of the locally similar VDIs. We will use this as a base image and only copy deltas to the destination. This is done by cloning the VDI on the destination and then using sparse_dd to find the deltas from our local disk to our local copy of the content_id disk and streaming these to the destination. Note that we need to ensure the VDI is smaller than the one we want to copy since we can’t resize disks downwards in size.
let remote_base = match nearest with | Some vdi -> debug "Cloning VDI %s" vdi.vdi; let vdi_clone = Remote.VDI.clone ~dbg ~sr:dest ~vdi_info:vdi in if vdi_clone.virtual_size <> local_vdi.virtual_size then begin let new_size = Remote.VDI.resize ~dbg ~sr:dest ~vdi:vdi_clone.vdi ~new_size:local_vdi.virtual_size in debug "Resize remote VDI %s to %Ld: result %Ld" vdi_clone.vdi local_vdi.virtual_size new_size; end; vdi_clone | None -> debug "Creating a blank remote VDI"; Remote.VDI.create ~dbg ~sr:dest ~vdi_info:{ local_vdi with sm_config = [] } inIf we’ve found a base VDI we clone it and resize it immediately. If there’s nothing on the destination already we can use, we just create a new VDI. Note that the calls to create and clone may well fail if the destination host is not the SRmaster. This is handled purely in the rpc function:
let rec rpc ~srcstr ~dststr url call = let result = XMLRPC_protocol.rpc ~transport:(transport_of_url url) ~srcstr ~dststr ~http:(xmlrpc ~version:"1.0" ?auth:(Http.Url.auth_of url) ~query:(Http.Url.get_query_params url) (Http.Url.get_uri url)) call in if not result.Rpc.success then begin debug "Got failure: checking for redirect"; debug "Call was: %s" (Rpc.string_of_call call); debug "result.contents: %s" (Jsonrpc.to_string result.Rpc.contents); match Storage_interface.Exception.exnty_of_rpc result.Rpc.contents with | Storage_interface.Exception.Redirect (Some ip) -> let open Http.Url in let newurl = match url with | (Http h, d) -> (Http {h with host=ip}, d) | _ -> remote_url ip in debug "Redirecting to ip: %s" ip; let r = rpc ~srcstr ~dststr newurl call in debug "Successfully redirected. Returning"; r | _ -> debug "Not a redirect"; result end else resultBack to the copy function:
let remote_copy = copy' ~task ~dbg ~sr ~vdi ~url ~dest ~dest_vdi:remote_base.vdi |> vdi_info inThis calls the actual data copy part. See below for more on that.
let snapshot = Remote.VDI.snapshot ~dbg ~sr:dest ~vdi_info:remote_copy in Remote.VDI.destroy ~dbg ~sr:dest ~vdi:remote_copy.vdi; Some (Vdi_info snapshot)Finally we snapshot the remote VDI to ensure we’ve got a VDI of type ‘snapshot’ on the destination, and we delete the non-snapshot VDI.
with e -> error "Caught %s: copying snapshots vdi" (Printexc.to_string e); raise (Internal_error (Printexc.to_string e)) with | Backend_error(code, params) | Api_errors.Server_error(code, params) -> raise (Backend_error(code, params)) | e -> raise (Internal_error(Printexc.to_string e))The exception handler does nothing - so we leak remote VDIs if the exception happens after we’ve done our cloning :-(
DATA.copy_into Let’s now look at the data-copying part. This is common code shared between VDI.copy, VDI.copy_into and MIRROR.start and hence has some duplication of the calls made above.
let copy_into ~task ~dbg ~sr ~vdi ~url ~dest ~dest_vdi = copy' ~task ~dbg ~sr ~vdi ~url ~dest ~dest_vdicopy_into is a stub and just calls copy'
let copy' ~task ~dbg ~sr ~vdi ~url ~dest ~dest_vdi = let remote_url = Http.Url.of_string url in let module Remote = Client(struct let rpc = rpc ~srcstr:"smapiv2" ~dststr:"dst_smapiv2" remote_url end) in debug "copy local=%s/%s url=%s remote=%s/%s" sr vdi url dest dest_vdi;This call takes roughly the same parameters as the \`\`DATA.copy\` call above, except it specifies the destination VDI. Once again we construct a module to do remote SMAPIv2 calls
(* Check the remote SR exists *) let srs = Remote.SR.list ~dbg in if not(List.mem dest srs) then failwith (Printf.sprintf "Remote SR %s not found" dest);Sanity check.
let vdis = Remote.SR.scan ~dbg ~sr:dest in let remote_vdi = try List.find (fun x -> x.vdi = dest_vdi) vdis with Not_found -> failwith (Printf.sprintf "Remote VDI %s not found" dest_vdi) inFind the metadata of the destination VDI
let dest_content_id = remote_vdi.content_id inIf we’ve got a local VDI with the same content_id as the destination, we only need copy the deltas, so we make a note of the destination content ID here.
(* Find the local VDI *) let vdis = Local.SR.scan ~dbg ~sr in let local_vdi = try List.find (fun x -> x.vdi = vdi) vdis with Not_found -> failwith (Printf.sprintf "Local VDI %s not found" vdi) in debug "copy local=%s/%s content_id=%s" sr vdi local_vdi.content_id; debug "copy remote=%s/%s content_id=%s" dest dest_vdi remote_vdi.content_id;Find the source VDI metadata.
if local_vdi.virtual_size > remote_vdi.virtual_size then begin (* This should never happen provided the higher-level logic is working properly *) error "copy local=%s/%s virtual_size=%Ld > remote=%s/%s virtual_size = %Ld" sr vdi local_vdi.virtual_size dest dest_vdi remote_vdi.virtual_size; failwith "local VDI is larger than the remote VDI"; end;Sanity check - the remote VDI can’t be smaller than the source.
let on_fail : (unit -> unit) list ref = ref [] inWe do some ugly error handling here by keeping a mutable list of operations to perform in the event of a failure.
let base_vdi = try let x = (List.find (fun x -> x.content_id = dest_content_id) vdis).vdi in debug "local VDI %s has content_id = %s; we will perform an incremental copy" x dest_content_id; Some x with _ -> debug "no local VDI has content_id = %s; we will perform a full copy" dest_content_id; None inSee if we can identify a local VDI with the same content_id as the destination. If not, no problem.
try let remote_dp = Uuid.string_of_uuid (Uuid.make_uuid ()) in let base_dp = Uuid.string_of_uuid (Uuid.make_uuid ()) in let leaf_dp = Uuid.string_of_uuid (Uuid.make_uuid ()) inConstruct some datapaths - named reasons why the VDI is attached - that we will pass to VDI.attach/activate.
let dest_vdi_url = Http.Url.set_uri remote_url (Printf.sprintf "%s/nbd/%s/%s/%s" (Http.Url.get_uri remote_url) dest dest_vdi remote_dp) |> Http.Url.to_string in debug "copy remote=%s/%s NBD URL = %s" dest dest_vdi dest_vdi_url;Here we are constructing a URI that we use to connect to the destination xapi. The handler for this particular path will verify the credentials and then pass the connection on to tapdisk which will behave as a NBD server. The VDI has to be attached and activated for this to work, unlike the new NBD handler in xapi-nbd that is smarter. The handler for this URI is declared in this file
let id=State.copy_id_of (sr,vdi) in debug "Persisting state for copy (id=%s)" id; State.add id State.(Copy_op Copy_state.({ base_dp; leaf_dp; remote_dp; dest_sr=dest; copy_vdi=remote_vdi.vdi; remote_url=url}));Since we’re about to perform a long-running operation that is stateful, we persist the state here so that if xapi is restarted we can cancel the operation and not leak VDI attaches. Normally in xapi code we would be doing VBD.plug operations to persist the state in the xapi db, but this is storage code so we have to use a different mechanism.
SMPERF.debug "mirror.copy: copy initiated local_vdi:%s dest_vdi:%s" vdi dest_vdi; Pervasiveext.finally (fun () -> debug "activating RW datapath %s on remote=%s/%s" remote_dp dest dest_vdi; ignore(Remote.VDI.attach ~dbg ~sr:dest ~vdi:dest_vdi ~dp:remote_dp ~read_write:true); Remote.VDI.activate ~dbg ~dp:remote_dp ~sr:dest ~vdi:dest_vdi; with_activated_disk ~dbg ~sr ~vdi:base_vdi ~dp:base_dp (fun base_path -> with_activated_disk ~dbg ~sr ~vdi:(Some vdi) ~dp:leaf_dp (fun src -> let dd = Sparse_dd_wrapper.start ~progress_cb:(progress_callback 0.05 0.9 task) ?base:base_path true (Opt.unbox src) dest_vdi_url remote_vdi.virtual_size in Storage_task.with_cancel task (fun () -> Sparse_dd_wrapper.cancel dd) (fun () -> try Sparse_dd_wrapper.wait dd with Sparse_dd_wrapper.Cancelled -> Storage_task.raise_cancelled task) ) ); ) (fun () -> Remote.DP.destroy ~dbg ~dp:remote_dp ~allow_leak:false; State.remove_copy id );In this chunk of code we attach and activate the disk on the remote SR via the SMAPI, then locally attach and activate both the VDI we’re copying and the base image we’re copying deltas from (if we’ve got one). We then call sparse_dd to copy the data to the remote NBD URL. There is some logic to update progress indicators and to cancel the operation if the SMAPIv2 call TASK.cancel is called.
Once the operation has terminated (either on success, error or cancellation), we remove the local attach and activations in the with_activated_disk function and the remote attach and activation by destroying the datapath on the remote SR. We then remove the persistent state relating to the copy.
SMPERF.debug "mirror.copy: copy complete local_vdi:%s dest_vdi:%s" vdi dest_vdi; debug "setting remote=%s/%s content_id <- %s" dest dest_vdi local_vdi.content_id; Remote.VDI.set_content_id ~dbg ~sr:dest ~vdi:dest_vdi ~content_id:local_vdi.content_id; (* PR-1255: XXX: this is useful because we don't have content_ids by default *) debug "setting local=%s/%s content_id <- %s" sr local_vdi.vdi local_vdi.content_id; Local.VDI.set_content_id ~dbg ~sr ~vdi:local_vdi.vdi ~content_id:local_vdi.content_id; Some (Vdi_info remote_vdi)The last thing we do is to set the local and remote content_id. The local set_content_id is there because the content_id of the VDI is constructed from the location if it is unset in the storage_access.ml module of xapi (still part of the storage layer)
with e -> error "Caught %s: performing cleanup actions" (Printexc.to_string e); perform_cleanup_actions !on_fail; raise eHere we perform the list of cleanup operations. Theoretically. It seems we don’t ever actually set this to anything, so this is dead code.
DATA.MIRROR.start let start' ~task ~dbg ~sr ~vdi ~dp ~url ~dest = debug "Mirror.start sr:%s vdi:%s url:%s dest:%s" sr vdi url dest; SMPERF.debug "mirror.start called sr:%s vdi:%s url:%s dest:%s" sr vdi url dest; let remote_url = Http.Url.of_string url in let module Remote = Client(struct let rpc = rpc ~srcstr:"smapiv2" ~dststr:"dst_smapiv2" remote_url end) in (* Find the local VDI *) let vdis = Local.SR.scan ~dbg ~sr in let local_vdi = try List.find (fun x -> x.vdi = vdi) vdis with Not_found -> failwith (Printf.sprintf "Local VDI %s not found" vdi) inAs with the previous calls, we make a remote module for SMAPIv2 calls on the destination, and we find local VDI metadata via SR.scan
let id = State.mirror_id_of (sr,local_vdi.vdi) inMirror ids are deterministically constructed.
(* A list of cleanup actions to perform if the operation should fail. *) let on_fail : (unit -> unit) list ref = ref [] inThis on_fail list is actually used.
try let similar_vdis = Local.VDI.similar_content ~dbg ~sr ~vdi in let similars = List.filter (fun x -> x <> "") (List.map (fun vdi -> vdi.content_id) similar_vdis) in debug "Similar VDIs to %s = [ %s ]" vdi (String.concat "; " (List.map (fun x -> Printf.sprintf "(vdi=%s,content_id=%s)" x.vdi x.content_id) similar_vdis));As with copy we look locally for similar VDIs. However, rather than use that here we actually pass this information on to the destination SR via the receive_start internal SMAPIv2 call:
let result_ty = Remote.DATA.MIRROR.receive_start ~dbg ~sr:dest ~vdi_info:local_vdi ~id ~similar:similars in let result = match result_ty with Mirror.Vhd_mirror x -> x inThis gives the destination SR a chance to say what sort of migration it can support. We only support Vhd_mirror style migrations which require the destination to support the compose SMAPIv2 operation. The type of x is a record:
type mirror_receive_result_vhd_t = { mirror_vdi : vdi_info; mirror_datapath : dp; copy_diffs_from : content_id option; copy_diffs_to : vdi; dummy_vdi : vdi; }Field descriptions:
mirror_vdi is the VDI to which new writes should be mirrored. mirror_datapath is the remote datapath on which the VDI has been attached and activated. This is required to construct the remote NBD url copy_diffs_from represents the source base VDI to be used for the non-mirrored data copy. copy_diffs_to is the remote VDI to copy those diffs to dummy_vdi exists to prevent leaf-coalesce on the mirror_vdi (* Enable mirroring on the local machine *) let mirror_dp = result.Mirror.mirror_datapath in let uri = (Printf.sprintf "/services/SM/nbd/%s/%s/%s" dest result.Mirror.mirror_vdi.vdi mirror_dp) in let dest_url = Http.Url.set_uri remote_url uri in let request = Http.Request.make ~query:(Http.Url.get_query_params dest_url) ~version:"1.0" ~user_agent:"smapiv2" Http.Put uri in let transport = Xmlrpc_client.transport_of_url dest_url inThis is where we connect to the NBD server on the destination.
debug "Searching for data path: %s" dp; let attach_info = Local.DP.attach_info ~dbg:"nbd" ~sr ~vdi ~dp in debug "Got it!";we need the local attach_info to find the local tapdisk so we can send it the connected NBD socket.
on_fail := (fun () -> Remote.DATA.MIRROR.receive_cancel ~dbg ~id) :: !on_fail;This should probably be set directly after the call to receive_start
let tapdev = match tapdisk_of_attach_info attach_info with | Some tapdev -> debug "Got tapdev"; let pid = Tapctl.get_tapdisk_pid tapdev in let path = Printf.sprintf "/var/run/blktap-control/nbdclient%d" pid in with_transport transport (with_http request (fun (response, s) -> debug "Here inside the with_transport"; let control_fd = Unix.socket Unix.PF_UNIX Unix.SOCK_STREAM 0 in finally (fun () -> debug "Connecting to path: %s" path; Unix.connect control_fd (Unix.ADDR_UNIX path); let msg = dp in let len = String.length msg in let written = Unixext.send_fd control_fd msg 0 len [] s in debug "Sent fd"; if written <> len then begin error "Failed to transfer fd to %s" path; failwith "foo" end) (fun () -> Unix.close control_fd))); tapdev | None -> failwith "Not attached" inHere we connect to the remote NBD server, then pass that connected fd to the local tapdisk that is using the disk. This fd is passed with a name that is later used to tell tapdisk to start using it - we use the datapath name for this.
debug "Adding to active local mirrors: id=%s" id; let alm = State.Send_state.({ url; dest_sr=dest; remote_dp=mirror_dp; local_dp=dp; mirror_vdi=result.Mirror.mirror_vdi.vdi; remote_url=url; tapdev; failed=false; watchdog=None}) in State.add id (State.Send_op alm); debug "Added";As for copy we persist some state to disk to say that we’re doing a mirror so we can undo any state changes after a toolstack restart.
debug "About to snapshot VDI = %s" (string_of_vdi_info local_vdi); let local_vdi = add_to_sm_config local_vdi "mirror" ("nbd:" ^ dp) in let local_vdi = add_to_sm_config local_vdi "base_mirror" id in let snapshot = try Local.VDI.snapshot ~dbg ~sr ~vdi_info:local_vdi with | Storage_interface.Backend_error(code, _) when code = "SR_BACKEND_FAILURE_44" -> raise (Api_errors.Server_error(Api_errors.sr_source_space_insufficient, [ sr ])) | e -> raise e in debug "Done!"; SMPERF.debug "mirror.start: snapshot created, mirror initiated vdi:%s snapshot_of:%s" snapshot.vdi local_vdi.vdi ; on_fail := (fun () -> Local.VDI.destroy ~dbg ~sr ~vdi:snapshot.vdi) :: !on_fail;This bit inserts into sm_config the name of the fd we passed earlier to do mirroring. This is interpreted by the python SM backends and passed on the tap-ctl invocation to unpause the disk. This causes all new writes to be mirrored via NBD to the file descriptor passed earlier.
begin let rec inner () = debug "tapdisk watchdog"; let alm_opt = State.find_active_local_mirror id in match alm_opt with | Some alm -> let stats = Tapctl.stats (Tapctl.create ()) tapdev in if stats.Tapctl.Stats.nbd_mirror_failed = 1 then Updates.add (Dynamic.Mirror id) updates; alm.State.Send_state.watchdog <- Some (Scheduler.one_shot scheduler (Scheduler.Delta 5) "tapdisk_watchdog" inner) | None -> () in inner () end;This is the watchdog that runs tap-ctl stats every 5 seconds watching mirror_failed for evidence of a failure in the mirroring code. If it detects one the only thing it does is to notify that the state of the mirroring has changed. This will be picked up by the thread in xapi that is monitoring the state of the mirror. It will then issue a MIRROR.stat call which will return the state of the mirror including the information that it has failed.
on_fail := (fun () -> stop ~dbg ~id) :: !on_fail; (* Copy the snapshot to the remote *) let new_parent = Storage_task.with_subtask task "copy" (fun () -> copy' ~task ~dbg ~sr ~vdi:snapshot.vdi ~url ~dest ~dest_vdi:result.Mirror.copy_diffs_to) |> vdi_info in debug "Local VDI %s == remote VDI %s" snapshot.vdi new_parent.vdi;This is where we copy the VDI returned by the snapshot invocation to the remote VDI called copy_diffs_to. We only copy deltas, but we rely on copy' to figure out which disk the deltas should be taken from, which it does via the content_id field.
Remote.VDI.compose ~dbg ~sr:dest ~vdi1:result.Mirror.copy_diffs_to ~vdi2:result.Mirror.mirror_vdi.vdi; Remote.VDI.remove_from_sm_config ~dbg ~sr:dest ~vdi:result.Mirror.mirror_vdi.vdi ~key:"base_mirror"; debug "Local VDI %s now mirrored to remote VDI: %s" local_vdi.vdi result.Mirror.mirror_vdi.vdi;Once the copy has finished we invoke the compose SMAPIv2 call that composes the diffs from the mirror with the base image copied from the snapshot.
debug "Destroying dummy VDI %s on remote" result.Mirror.dummy_vdi; Remote.VDI.destroy ~dbg ~sr:dest ~vdi:result.Mirror.dummy_vdi; debug "Destroying snapshot %s on src" snapshot.vdi; Local.VDI.destroy ~dbg ~sr ~vdi:snapshot.vdi; Some (Mirror_id id)we can now destroy the dummy vdi on the remote (which will cause a leaf-coalesce in due course), and we destroy the local snapshot here (which will also cause a leaf-coalesce in due course, providing we don’t destroy it first). The return value from the function is the mirror_id that we can use to monitor the state or cancel the mirror.
with | Sr_not_attached(sr_uuid) -> error " Caught exception %s:%s. Performing cleanup." Api_errors.sr_not_attached sr_uuid; perform_cleanup_actions !on_fail; raise (Api_errors.Server_error(Api_errors.sr_not_attached,[sr_uuid])) | e -> error "Caught %s: performing cleanup actions" (Api_errors.to_string e); perform_cleanup_actions !on_fail; raise eThe exception handler just cleans up afterwards.
This is not the end of the story, since we need to detach the remote datapath being used for mirroring when we detach this end. The hook function is in storage_migrate.ml:
let post_detach_hook ~sr ~vdi ~dp = let open State.Send_state in let id = State.mirror_id_of (sr,vdi) in State.find_active_local_mirror id |> Opt.iter (fun r -> let remote_url = Http.Url.of_string r.url in let module Remote = Client(struct let rpc = rpc ~srcstr:"smapiv2" ~dststr:"dst_smapiv2" remote_url end) in let t = Thread.create (fun () -> debug "Calling receive_finalize"; log_and_ignore_exn (fun () -> Remote.DATA.MIRROR.receive_finalize ~dbg:"Mirror-cleanup" ~id); debug "Finished calling receive_finalize"; State.remove_local_mirror id; debug "Removed active local mirror: %s" id ) () in Opt.iter (fun id -> Scheduler.cancel scheduler id) r.watchdog; debug "Created thread %d to call receive finalize and dp destroy" (Thread.id t))This removes the persistent state and calls receive_finalize on the destination. The body of that functions is:
let receive_finalize ~dbg ~id = let recv_state = State.find_active_receive_mirror id in let open State.Receive_state in Opt.iter (fun r -> Local.DP.destroy ~dbg ~dp:r.leaf_dp ~allow_leak:false) recv_state; State.remove_receive_mirror idwhich removes the persistent state on the destination and destroys the datapath associated with the mirror.
Additionally, there is also a pre-deactivate hook. The rationale for this is that we want to detect any failures to write that occur right at the end of the SXM process. So if there is a mirror operation going on, before we deactivate we wait for tapdisk to flush its queue of outstanding requests, then we query whether there has been a mirror failure. The code is just above the detach hook in storage_migrate.ml:
let pre_deactivate_hook ~dbg ~dp ~sr ~vdi = let open State.Send_state in let id = State.mirror_id_of (sr,vdi) in let start = Mtime_clock.counter () in let get_delta () = Mtime_clock.count start |> Mtime.Span.to_s in State.find_active_local_mirror id |> Opt.iter (fun s -> try (* We used to pause here and then check the nbd_mirror_failed key. Now, we poll until the number of outstanding requests has gone to zero, then check the status. This avoids confusing the backend (CA-128460) *) let open Tapctl in let ctx = create () in let rec wait () = if get_delta () > reqs_outstanding_timeout then raise Timeout; let st = stats ctx s.tapdev in if st.Stats.reqs_outstanding > 0 then (Thread.delay 1.0; wait ()) else st in let st = wait () in debug "Got final stats after waiting %f seconds" (get_delta ()); if st.Stats.nbd_mirror_failed = 1 then begin error "tapdisk reports mirroring failed"; s.failed <- true end; with | Timeout -> error "Timeout out after %f seconds waiting for tapdisk to complete all outstanding requests" (get_delta ()); s.failed <- true | e -> error "Caught exception while finally checking mirror state: %s" (Printexc.to_string e); s.failed <- true )`,description:"",tags:null,title:"Storage migration",uri:"/new-docs/xapi/storage/sxm/index.html"},{content:`Example suspend image layout:
+----------------------------+ | 1. Suspend image signature | +============================+ | 2.0 Xenops header | | 2.1 Xenops record | +============================+ | 3.0 Libxc header | | 3.1 Libxc record | +============================+ | 4.0 Qemu header | | 4.1 Qemu save record | +============================+ | 5.0 End_of_image footer | +----------------------------+ A suspend image is now constucted as a series of header-record pairs. The initial signature (1.) is used to determine whether we are dealing with the unstructured, “legacy” suspend image or the new, structured format.
Each header is two 64-bit integers: the first identifies the header type and the second is the length of the record that follows in bytes. The following types have been defined (the ones marked with a (*) have yet to be implemented):
* Xenops : Metadata for the suspend image * Libxc : The result of a xc_domain_save * Libxl* : Not implemented * Libxc_legacy : Marked as a libxc record saved using pre-Xen-4.5 * Qemu_trad : The qemu save file for the Qemu used in XenServer * Qemu_xen* : Not implemented * Demu* : Not implemented * End_of_image : A footer marker to denote the end of the suspend image Some of the above types do not have the notion of a length since they cannot be known upfront before saving and also are delegated to other layers of the stack on restoring. Specifically these are the memory image sections, libxc and libxl.
`,description:"",tags:null,title:"Suspend image framing format",uri:"/new-docs/xenopsd/design/suspend-image-framing-format/index.html"},{content:"",description:"",tags:null,title:"Tags",uri:"/new-docs/tags/index.html"},{content:`Some operations performed by Xenopsd are blocking, for example:
suspend/resume/migration attaching disks (where the SMAPI VDI.attach/activate calls can perform network I/O) We want to be able to
present the user with an idea of progress (perhaps via a “progress bar”) allow the user to cancel a blocked operation that is taking too long associate logging with the user/client-initiated actions that spawned them Principles all operations which may block (the vast majority) should be written in an asynchronous style i.e. the operations should immediately return a Task id all operations should guarantee to respond to a cancellation request in a bounded amount of time (30s) when cancelled, the system should always be left in a valid state clients are responsible for destroying Tasks when they are finished with the results Types A task has a state, which may be Pending, Completed or failed:
type async_result = unit type completion_t = { duration : float; result : async_result option } type state = | Pending of float | Completed of completion_t | Failed of Rpc.tWhen a task is Failed, we assocate it with a marshalled exception (a value of type Rpc.t). This exception must be one from the set defined in the Xenops_interface. To see how they are marshalled, see Xenops_server.
From the point of view of a client, a Task has the immutable type (which can be queried with a Task.stat):
type t = { id: id; dbg: string; ctime: float; state: state; subtasks: (string * state) list; debug_info: (string * string) list; }where
id is a unique (integer) id generated by Xenopsd. This is how a Task is represented to clients dbg is a client-provided debug key which will be used in log lines, allowing lines from the same Task to be associated together ctime is the creation time state is the current state (Pending/Completed/Failed) subtasks lists logical internal sub-operations for debugging debug_info includes miscellaneous key/value pairs used for debugging Internally, Xenopsd uses a mutable record type to track Task state. This is broadly similar to the interface type except
the state is mutable: this allows Tasks to complete the task contains a “do this now” thunk there is a “cancelling” boolean which is toggled to request a cancellation. there is a list of cancel callbacks there are some fields related to “cancel points” Persistence The Tasks are intended to represent activities associated with in-memory queues and threads. Therefore the active Tasks are kept in memory in a map, and will be lost over a process restart. This is desirable since we will also lose the queued items and the threads, so there is no need to resync on start.
Note that every operation must ensure that the state of the system is recoverable on restart by not leaving it in an invalid state. It is not necessary to either guarantee to complete or roll-back a Task. Tasks are not expected to be transactional.
Lifecycle of a Task All Tasks returned by API functions are created as part of the enqueue functions: queue_operation_*. Even operations which are performed internally are normally wrapped in Tasks by the function immediate_operation.
A queued operation will be processed by one of the queue worker threads. It will
set the thread-local debug key to the Task.dbg call task.Xenops_task.run, taking care to catch exceptions and update the task.Xenops_task.state unset the thread-local debug key generate an event on the Task to provoke clients to query the current state. Task implementations must update their progress as they work. For the common case of a compound operation like VM_start which is decomposed into multiple “micro-ops” (e.g. VM_create VM_build) there is a useful helper function perform_atomics which divides the progress ‘bar’ into sections, where each “micro-op” can have a different size (weight). A progress callback function is passed into each Xenopsd backend function so it can be updated with fine granulatiry. For example note the arguments to B.VM.save
Clients are expected to destroy Tasks they are responsible for creating. Xenopsd cannot do this on their behalf because it does not know if they have successfully queried the Task status/result.
When Xenopsd is a client of itself, it will take care to destroy the Task properly, for example see immediate_operation.
Cancellation The goal of cancellation is to unstick a blocked operation and to return the system to some valid state, not any valid state in particular. Xenopsd does not treat operations as transactions; when an operation is cancelled it may
fully complete (e.g. if it was about to do this anyway) fully abort (e.g. if it had made no progress) enter some other valid state (e.g. if it had gotten half way through) Xenopsd will never leave the system in an invalid state after cancellation.
Every Xenopsd operation should unblock and return the system to a valid state within a reasonable amount of time after a cancel request. This should be as quick as possible but up to 30s may be acceptable. Bear in mind that a human is probably impatiently watching a UI say “please wait” and which doesn’t have any notion of progress itself. Keep it quick!
Cancellation is triggered by TASK.cancel which calls cancel. This
sets the cancelling boolean calls all registered cancel callbacks Implementations respond to cancellation by
if running: periodically call check_cancelling if about to block: register a suitable cancel callback safely with with_cancel. Xenopsd’s libxc backend can block in 2 different ways, and therefore has 2 different types of cancel callback:
cancellable Xenstore watches cancellable subprocesses Xenstore watches are used for device hotplug and unplug. Xenopsd has to wait for the backend or for a udev script to do something. If that blocks then we need a way to cancel the watch. The easiest way to cancel a watch is to watch an additional path (a “cancel path”) and delete it, see cancellable_watch. The “cancel paths” are placed within the VM’s Xenstore directory to ensure that cleanup code which does xenstore-rm will automatically “cancel” all outstanding watches. Note that we trigger a cancel by deleting rather than creating, to avoid racing with delete and creating orphaned Xenstore entries.
Subprocesses are used for suspend/resume/migrate. Xenopsd hands file descriptors to libxenguest by running a subprocess and passing the fds to it. Xenopsd therefore gets the process id and can send it a signal to cancel it. See Cancellable_subprocess.run.
Testing with cancel points Cancellation is difficult to test, as it is completely asynchronous. Therefore Xenopsd has some built-in cancellation testing infrastructure known as “cancel points”. A “cancel point” is a point in the code where a Cancelled exception could be thrown, either by checking the cancelling boolean or as a side-effect of a cancel callback. The check_cancelling function increments a counter every time it passes one of these points, and this value is returned to clients in the Task.debug_info.
A test harness runs a series of operations. Each operation is first run all the way through to completion to discover the total number of cancel points. The operation is then re-run with a request to cancel at a particular point. The test then waits for the system to stabilise and verifies that it appears to be in a valid state.
Preventing Tasks leaking The client who creates a Task must destroy it when the Task is finished, and they have processed the result. What if a client like xapi is restarted while a Task is running?
We assume that, if xapi is talking to a xenopsd, then xapi completely owns it. Therefore xapi should destroy any completed tasks that it doesn’t recognise.
If a user wishes to manage VMs with xenopsd in parallel with xapi, the user should run a separate xenopsd.
`,description:"",tags:null,title:"Tasks",uri:"/new-docs/xenopsd/design/Tasks/index.html"},{content:`XenServer has supported passthrough for GPU devices since XenServer 6.0. Since the advent of NVIDIA’s vGPU-capable GRID K1/K2 cards it has been possible to carve up a GPU into smaller pieces yielding a more scalable solution to boosting graphics performance within virtual machines.
The K1 has four GK104 GPUs and the K2 two GK107 GPUs. Each of these will be exposed through Xapi so a host with a single K1 card will have access to four independent PGPUs.
Each of the GPUs can then be subdivided into vGPUs. For each type of PGPU, there are a few options of vGPU type which consume different amounts of the PGPU. For example, K1 and K2 cards can currently be configured in the following ways:
Note, this diagram is not to scale, the PGPU resource required by each vGPU type is as follows:
vGPU type PGPU kind vGPUs / PGPU k100 GK104 8 k140Q GK104 4 k200 GK107 8 k240Q GK107 4 k260Q GK107 2 Currently each physical GPU (PGPU) only supports homogeneous vGPU configurations but different configurations are supported on different PGPUs across a single K1/K2 card. This means that, for example, a host with a K1 card can run 64 VMs with k100 vGPUs (8 per PGPU).
XenServer’s vGPU architecture A new display type has been added to the device model:
@@ -4519,6 +4522,7 @@ static const QEMUOption qemu_options[] = /* Xen tree options: */ { "std-vga", 0, QEMU_OPTION_std_vga }, + { "vgpu", 0, QEMU_OPTION_vgpu }, { "videoram", HAS_ARG, QEMU_OPTION_videoram }, { "d", HAS_ARG, QEMU_OPTION_domid }, /* deprecated; for xend compatibility */ { "domid", HAS_ARG, QEMU_OPTION_domid }, With this in place, qemu can now be started using a new option that will enable it to communicate with a new display emulator, vgpu to expose the graphics device to the guest. The vgpu binary is responsible for handling the VGX-capable GPU and, once it has been successfully passed through, the in-guest drivers can be installed in the same way as when it detects new hardware.
The diagram below shows the relevant parts of the architecture for this project.
Relevant code In Xenopsd: Xenops_server_xen is where Xenopsd gets the vGPU information from the values passed from Xapi; In Xenopsd: Device.__start is where the vgpu process is started, if necessary, before Qemu. Xapi’s API and data model A lot of work has gone into the toolstack to handle the creation and management of VMs with vGPUs. We revised our data model, introducing a semantic link between VGPU and PGPU objects to help with utilisation tracking; we maintained the GPU_group concept as a pool-wide abstraction of PGPUs available for VMs; and we added VGPU_types which are configurations for VGPU objects.
Aside: The VGPU type in Xapi’s data model predates this feature and was synonymous with GPU-passthrough. A VGPU is simply a display device assigned to a VM which may be a vGPU (this feature) or a whole GPU (a VGPU of type passthrough).
VGPU_types can be enabled/disabled on a per-PGPU basis allowing for reservation of particular PGPUs for certain workloads. VGPUs are allocated on PGPUs within their GPU group in either a depth-first or breadth-first manner, which is configurable on a per-group basis.
VGPU_types are created by xapi at startup depending on the available hardware and config files present in dom0. They exist in the pool database, and a primary key is used to avoid duplication. In XenServer 6.x the tuple of (vendor_name, model_name) was used as the primary key, however this was not ideal as these values are subject to change. XenServer 7.0 switched to a new primary key generated from static metadata, falling back to the old method for backwards compatibility.
A VGPU_type will be garbage collected when there is no VGPU of that type and there is no hardware which supports that type. On VM import, all VGPUs and VGPU_types will be created if necessary - if this results in the creation of a new VGPU_type then the VM will not be usable until the required hardware and drivers are installed.
Relevant code In Xapi: Xapi_vgpu_type contains the type definitions and parsing logic for vGPUs; In Xapi: Xapi_pgpu_helpers defines the functions used to allocate vGPUs on PGPUs. Xapi <-> Xenopsd interface In XenServer 6.x, all VGPU config was added to the VM’s platform field at startup, and this information was used by xenopsd to start the display emulator. See the relevant code here.
In XenServer 7.0, to facilitate support of VGPU on Intel hardware in parallel with the existing NVIDIA support, VGPUs were made first-class objects in the xapi-xenopsd interface. The interface is described here.
VM startup On the pool master:
Assuming no WLB, all VM.start tasks pass through Xapi_vm_helpers.choose_host_for_vm_no_wlb. If the VM has a vGPU, the list of all hosts in the pool is split into a list of lists, where the first list is the most optimal in terms of the GPU group’s allocation mode and the PGPU availability on each host. Each list of hosts in turn is passed to Xapi_vm_placement.select_host, which checks storage, network and memory availability, until a suitable host is found. Once a host has been chosen, allocate_vm_to_host will set the VM.scheduled_to_be_resident_on and VGPU.scheduled_to_be_resident_on fields. The task is then ready to be forwarded to the host on which the VM will start:
If the VM has a VGPU, the startup task is wrapped in Xapi_gpumon.with_gpumon_stopped. This makes sure that the NVIDIA driver is not in use so can be loaded or unloaded from physical GPUs as required. The VM metadata, including VGPU metadata, is passed to xenopsd. The creation of the VGPU metadata is done by vgpus_of_vm. Note that at this point passthrough VGPUs are represented by the PCI device type, and metadata is generated by pcis_of_vm. As part of starting up the VM, xenopsd should report a VGPU event or a PCI event, which xapi will use to indicate that the xapi VGPU object can be marked as currently_attached. Usage To create a VGPU of a given type you can use vgpu-create:
$ xe vgpu-create vm-uuid=... gpu-group-uuid=... vgpu-type-uuid=...To see a list of VGPU types available for use on your XenServer, run the following command. Note: these will only be populated if you have installed the relevant NVIDIA RPMs and if there is hardware installed on that host supported each type. Using params=all will display more information such as the maximum number of heads supported by that VGPU type and which PGPUs have this type enabled and supported.
$ xe vgpu-type-list [params=all]To access the new and relevant parameters on a PGPU (i.e. supported_VGPU_types, enabled_VGPU_types, resident_VGPUs) you can use pgpu-param-get with param-name=supported-vgpu-types param-name=enabled-vgpu-types and param-name=resident-vgpus respectively. Or, alternatively, you can use the following command to list all the parameters for the PGPU. You can get the types supported or enabled for a given PGPU:
$ xe pgpu-list uuid=... params=all`,description:"",tags:null,title:"vGPU",uri:"/new-docs/toolstack/features/VGPU/index.html"},{content:`A XenAPI client wishes to migrate a VM from one host to another within the same pool.
The client will issue a command to migrate the VM and it will be dispatched by the autogenerated dispatch_call function from xapi/server.ml. For more information about the generated functions you can have a look to XAPI IDL model.
The command will trigger the operation VM_migrate that has low level operations performed by the backend. These atomics operations that we will describe in the documentation are:
VM.restore VM.rename VBD.set_active VBD.plug VIF.set_active VGPU.set_active VM.create_device_model PCI.plug VM.set_domain_action_request The command have serveral parameters such as: should it be ran asynchronously, should it be forwared to another host, how arguments should be marshalled and so on. A new thread is created by xapi/server_helpers.ml to handle the command asynchronously. At this point the helper also check if the command should be passed to the message forwarding layer in order to be executed on another host (the destination) or locally if we are already at the right place.
It will finally reach xapi/api_server.ml that will take the action of posted a command to the message broker message switch. It is a JSON-RPC HTTP request sends on a Unix socket to communicate between some XAPI daemons. In the case of the migration this message sends by XAPI will be consumed by the xenopsd daemon that will do the job of migrating the VM.
The migration of the VM The migration is an asynchronous task and a thread is created to handle this task. The tasks’s reference is returned to the client, which can then check its status until completion.
As we see in the introduction the xenopsd daemon will pop the operation VM_migrate from the message broker.
Only one backend is know available that interacts with libxc, libxenguest and xenstore. It is the xc backend.
The entities that need to be migrated are: VDI, VIF, VGPU and PCI components.
During the migration process the destination domain will be built with the same uuid than the original VM but the last part of the UUID will be XXXXXXXX-XXXX-XXXX-XXXX-000000000001. The original domain will be removed using XXXXXXXX-XXXX-XXXX-XXXX-000000000000.
There are some points called hooks at which xenopsd can execute some script. Before starting a migration a command is send to the original domain to execute a pre migrate script if it exists.
Before starting the migration a command is sent to Qemu using the Qemu Machine Protocol (QMP) to check that the domain can be suspended (see xenopsd/xc/device_common.ml). After checking with Qemu that the VM is suspendable we can start the migration.
Importing metadata As for hooks, commands to source domain are sent using stunnel a daemon which is used as a wrapper to manage SSL encryption communication between two hosts on the same pool. To import metada an XML RPC command is sent to the original domain.
Once imported it will give us a reference id and will allow to build the new domain on the destination using the temporary VM uuid XXXXXXXX-XXXX-XXXX-XXXX-000000000001 where XXX... is the reference id of the original VM.
Setting memory One of the first thing to do is to setup the memory. The backend will check that there is no ballooning operation in progress. At this point the migration can fail if a ballooning operation is in progress and takes too much time.
Once memory checked the daemon will get the state of the VM (running, halted, …) and information about the VM are retrieve by the backend like the maximum memory the domain can consume but also information about quotas for example. Information are retrieve by the backend from xenstore.
Once this is complete, we can restore VIF and create the domain.
The synchronisation of the memory is the first point of synchronisation and everythin is ready for VM migration.
VM Migration After receiving memory we can set up the destination domain. If we have a vGPU we need to kick off its migration process. We will need to wait the acknowledge that indicates that the entry for the GPU has been well initialized. before starting the main VM migration.
Their is a mechanism of handshake for synchronizing between the source and the destination. Using the handshake protocol the receiver inform the sender of the request that everything has been setup and ready to save/restore.
VM restore VM restore is a low level atomic operation VM.restore. This operation is represented by a function call to backend. It uses Xenguest, a low-level utility from XAPI toolstack, to interact with the Xen hypervisor and libxc for sending a request of migration to the emu-manager.
After sending the request results coming from emu-manager are collected by the main thread. It blocks until results are received.
During the live migration, emu-manager helps in ensuring the correct state transitions for the devices and handling the message passing for the VM as it’s moved between hosts. This includes making sure that the state of the VM’s virtual devices, like disks or network interfaces, is correctly moved over.
VM renaming Once all operations are done we can rename the VM on the target from its temporary name to its real UUID. This operation is another low level atomic one VM.rename that will take care of updating the xenstore on the destination.
The next step is the restauration of devices and unpause the domain.
Restoring remaining devices Restoring devices starts by activating VBD using the low level atomic operation VBD.set_active. It is an update of Xenstore. VBDs that are read-write must be plugged before read-only ones. Once activated the low level atomic operation VBD.plug is called. VDI are attached and activate.
Next devices are VIFs that are set as active VIF.set_active and plug VIF.plug. If there are VGPUs we will set them as active now using the atomic VGPU.set_active.
We are almost done. The next step is to create the device model
create device model Create device model is done by using the atomic operation VM.create_device_model. This will configure qemu-dm and started. This allow to manage PCI devices.
PCI plug PCI.plug is executed by the backend. It plugs a PCI device and advertise it to QEMU if this option is set. It is the case for NVIDIA SR-IOV vGPUS.
At this point devices have been restored. The new domain is considered survivable. We can unpause the domain and performs last actions
Unpause and done Unpause is done by managing the state of the domain using bindings to xenctrl. Once hypervisor has unpaused the domain some actions can be requested using VM.set_domain_action_request. It is a path in xenstore. By default no action is done but a reboot can be for example initiated.
Previously we spoke about some points called hooks at which xenopsd can execute some script. There is also a hook to run a post migrate script. After the execution of the script if there is one the migration is almost done. The last step is a handskake to seal the success of the migration and the old VM can now be cleaned.
Links Some links are old but even if many changes occured they are relevant for a global understanding of the XAPI toolstack.
XAPI architecture XAPI dispatcher Xenopsd architecture `,description:"",tags:null,title:"Walkthrough: Migrating a VM",uri:"/new-docs/xenopsd/walkthroughs/VM.migrate/index.html"},{content:`A Xenopsd client wishes to start a VM. They must first tell Xenopsd the VM configuration to use. A VM configuration is broken down into objects:
VM: A device-less Virtual Machine VBD: A virtual block device for a VM VIF: A virtual network interface for a VM PCI: A virtual PCI device for a VM Treating devices as first-class objects is convenient because we wish to expose operations on the devices such as hotplug, unplug, eject (for removable media), carrier manipulation (for network interfaces) etc.
The “add” functions in the Xenopsd interface cause Xenopsd to create the objects:
VM.add VBD.add VIF.add PCI.add In the case of xapi, there are a set of functions which convert between the XenAPI objects and the Xenopsd objects. The two interfaces are slightly different because they have different expected users:
the XenAPI has many clients which are updated on long release cycles. The main property needed is backwards compatibility, so that new release of xapi remain compatible with these older clients. Quite often we will chose to “grandfather in” some poorly designed interface simply because we wish to avoid imposing churn on 3rd parties. the Xenopsd API clients are all open-source and are part of the xapi-project. These clients can be updated as the API is changed. The main property needed is to keep the interface clean, so that it properly hides the complexity of dealing with Xen from other components. The Xenopsd “VM.add” function has code like this:
let add' x = debug "VM.add %s" (Jsonrpc.to_string (rpc_of_t x)); DB.write x.id x; let module B = (val get_backend () : S) in B.VM.add x; x.idThis function does 2 things:
it stores the VM configuration in the “database” it tells the “backend” that the VM exists The Xenopsd database is really a set of config files in the filesystem. All objects belonging to a VM (recall we only have VMs, VBDs, VIFs, PCIs and not stand-alone entities like disks) and are placed into a subdirectory named after the VM e.g.:
# ls /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701 config	vbd.xvda vbd.xvdb # cat /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701/config {"id": "7b719ce6-0b17-9733-e8ee-dbc1e6e7b701", "name": "fedora", ... }Xenopsd doesn’t have as persistent a notion of a VM as xapi, it is expected that all objects are deleted when the host is rebooted. However the objects should be persisted over a simple Xenopsd restart, which is why the objects are stored in the filesystem.
Aside: it would probably be more appropriate to store the metadata in Xenstore since this has the exact object lifetime we need. This will require a more performant Xenstore to realise.
Every running Xenopsd process is linked with a single backend. Currently backends exist for:
Xen via libxc, libxenguest and xenstore Xen via libxl, libxc and xenstore Xen via libvirt KVM by direct invocation of qemu Simulation for testing From here we shall assume the use of the “Xen via libxc, libxenguest and xenstore” (a.k.a. “Xenopsd classic”) backend.
The backend VM.add function checks whether the VM we have to manage already exists – and if it does then it ensures the Xenstore configuration is intact. This Xenstore configuration is important because at any time a client can query the state of a VM with VM.stat and this relies on certain Xenstore keys being present.
Once the VM metadata has been registered with Xenopsd, the client can call VM.start. Like all potentially-blocking Xenopsd APIs, this function returns a Task id. Please refer to the Task handling design for a general overview of how tasks are handled.
Clients can poll the state of a task by calling TASK.stat but most clients will prefer to use the event system instead. Please refer to the Event handling design for a general overview of how events are handled.
The event model is similar to the XenAPI: clients call a blocking UPDATES.get passing in a token which represents the point in time when the last UPDATES.get returned. The call blocks until some objects have changed state, and these object ids are returned (NB in the XenAPI the current object states are returned) The client must then call the relevant “stat” function, in this case TASK.stat
The client will be able to see the task make progress and use this to – for example – populate a progress bar in a UI. If the client needs to cancel the task then it can call the TASK.cancel; again see the Task handling design to understand how this is implemented.
When the Task has completed successfully, then calls to *.stat will show:
the power state is Paused exactly one valid Xen domain id all VBDs have active = plugged = true all VIFs have active = plugged = true all PCI devices have plugged = true at least one active console a valid start time valid “targets” for memory and vCPU Note: before a Task completes, calls to *.stat will show partial updates e.g. the power state may be Paused but none of the disks may have become plugged. UI clients must choose whether they are happy displaying this in-between state or whether they wish to hide it and pretend the whole operation has happened transactionally. If a particular client wishes to perform side-effects in response to Xenopsd state changes – for example to clean up an external resource when a VIF becomes unplugged – then it must be very careful to avoid responding to these in-between states. Generally it is safest to passively report these values without driving things directly from them. Think of them as status lights on the front panel of a PC: fine to look at but it’s not a good idea to wire them up to actuators which actually do things.
Note: the Xenopsd implementation guarantees that, if it is restarted at any point during the start operation, on restart the VM state shall be “fixed” by either (i) shutting down the VM; or (ii) ensuring the VM is intact and running.
In the case of xapi every Xenopsd Task id bound one-to-one with a XenAPI task by the function sync_with_task. The function update_task is called when xapi receives a notification that a Xenopsd Task has changed state, and updates the corresponding XenAPI task. Xapi launches exactly one thread per Xenopsd instance (“queue”) to monitor for background events via the function events_watch while each thread performing a XenAPI call waits for its specific Task to complete via the function event_wait.
It is the responsibility of the client to call TASK.destroy when the Task is nolonger needed. Xenopsd won’t destroy the task because it contains the success/failure result of the operation which is needed by the client.
What happens when a Xenopsd receives a VM.start request?
When Xenopsd receives the request it adds it to the appropriate per-VM queue via the function queue_operation. To understand this and other internal details of Xenopsd, consult the architecture description. The queue_operation_int function looks like this:
let queue_operation_int dbg id op = let task = Xenops_task.add tasks dbg (fun t -> perform op t; None) in Redirector.push id (op, task); taskThe “task” is a record containing Task metadata plus a “do it now” function which will be executed by a thread from the thread pool. The module Redirector takes care of:
pushing operations to the right queue ensuring at most one worker thread is working on a VM’s operations reducing the queue size by coalescing items together providing a diagnostics interface Once a thread from the worker pool becomes free, it will execute the “do it now” function. In the example above this is perform op t where op is VM_start vm and t is the Task. The function perform has fragments like this:
| VM_start id -> debug "VM.start %s" id; perform_atomics (atomics_of_operation op) t; VM_DB.signal idEach “operation” (e.g. VM_start vm) is decomposed into “micro-ops” by the function atomics_of_operation where the micro-ops are small building-block actions common to the higher-level operations. Each operation corresponds to a list of “micro-ops”, where there is no if/then/else. Some of the “micro-ops” may be a no-op depending on the VM configuration (for example a PV domain may not need a qemu). In the case of VM_start vm this decomposes into the sequence:
1. run the “VM_pre_start” scripts The VM_hook_script micro-op runs the corresponding “hook” scripts. The code is all in the Xenops_hooks module and looks for scripts in the hardcoded path /etc/xapi.d.
2. create a Xen domain The VM_create micro-op calls the VM.create function in the backend. In the classic Xenopsd backend the VM.create_exn function must
check if we’re creating a domain for a fresh VM or resuming an existing one: if it’s a resume then the domain configuration stored in the VmExtra database table must be used ask squeezed to create a memory “reservation” big enough to hold the VM memory. Unfortunately the domain cannot be created until the memory is free because domain create often fails in low-memory conditions. This means the “reservation” is associated with our “session” with squeezed; if Xenopsd crashes and restarts the reservation will be freed automatically. create the Domain via the libxc hypercall “transfer” the squeezed reservation to the domain such that squeezed will free the memory if the domain is destroyed later compute and set an initial balloon target depending on the amount of memory reserved (recall we ask for a range between dynamic_min and dynamic_max) apply the “suppress spurious page faults” workaround if requested set the “machine address size” “hotplug” the vCPUs. This operates a lot like memory ballooning – Xen creates lots of vCPUs and then the guest is asked to only use some of them. Every VM therefore starts with the “VCPUs_max” setting and co-operative hotplug is used to reduce the number. Note there is no enforcement mechanism: a VM which cheats and uses too many vCPUs would have to be caught by looking at the performance statistics. 3. build the domain On a Xen system a domain is created empty, and memory is actually allocated from the host in the “build” phase via functions in libxenguest. The VM.build_domain_exn function must
run pygrub (or eliloader) to extract the kernel and initrd, if necessary invoke the xenguest binary to interact with libxenguest. apply the cpuid configuration store the current domain configuration on disk – it’s important to know the difference between the configuration you started with and the configuration you would use after a reboot because some properties (such as maximum memory and vCPUs) as fixed on create. The xenguest binary was originally a separate binary for two reasons: (i) the libxenguest functions weren’t threadsafe since they used lots of global variables; and (ii) the libxenguest functions used to have a different, incompatible license, which prevent us linking. Both these problems have been resolved but we still shell out to the xenguest binary.
The xenguest binary has also evolved to configure more of the initial domain state. It also reads Xenstore and configures
the vCPU affinity the vCPU credit2 weight/cap parameters whether the NX bit is exposed whether the viridian CPUID leaf is exposed whether the system has PAE or not whether the system has ACPI or not whether the system has nested HVM or not whether the system has an HPET or not 4. mark each VBD as “active” VBDs and VIFs are said to be “active” when they are intended to be used by a particular VM, even if the backend/frontend connection hasn’t been established, or has been closed. If someone calls VBD.stat or VIF.stat then the result includes both “active” and “plugged”, where “plugged” is true if the frontend/backend connection is established. For example xapi will set VBD.currently_attached to “active || plugged”. The “active” flag is conceptually very similar to the traditional “online” flag (which is not documented in the upstream Xen tree as of Oct/2014 but really should be) except that on unplug, one would set the “online” key to “0” (false) first before initiating the hotunplug. By contrast the “active” flag is set to false after the unplug i.e. “set_active” calls bracket plug/unplug. If the “active” flag was set before the unplug attempt then as soon as the frontend/backend connection is removed clients would see the VBD as completely dissociated from the VM – this would be misleading because Xenopsd will not have had time to use the storage API to release locks on the disks. By doing all the cleanup before setting “active” to false, clients can be assured that the disks are now free to be reassigned.
5. handle non-persistent disks A non-persistent disk is one which is reset to a known-good state on every VM start. The VBD_epoch_begin is the signal to perform any necessary reset.
6. plug VBDs The VBD_plug micro-op will plug the VBD into the VM. Every VBD is plugged in a carefully-chosen order. Generally, plug order is important for all types of devices. For VBDs, we must work around the deficiency in the storage interface where a VDI, once attached read/only, cannot be attached read/write. Since it is legal to attach the same VDI with multiple VBDs, we must plug them in such that the read/write VBDs come first. From the guest’s point of view the order we plug them doesn’t matter because they are indexed by the Xenstore device id (e.g. 51712 = xvda).
The function VBD.plug will
call VDI.attach and VDI.activate in the storage API to make the devices ready (start the tapdisk processes etc) add the Xenstore frontend/backend directories containing the block device info add the extra xenstore keys returned by the VDI.attach call that are needed for SCSIid passthrough which is needed to support VSS write the VBD information to the Xenopsd database so that future calls to VBD.stat can be told about the associated disk (this is needed so clients like xapi can cope with CD insert/eject etc) if the qemu is going to be in a different domain to the storage, a frontend device in the qemu domain is created. The Xenstore keys are written by the functions Device.Vbd.add_async and Device.Vbd.add_wait. In a Linux domain (such as dom0) when the backend directory is created, the kernel creates a “backend device”. Creating any device will cause a kernel UEVENT to fire which is picked up by udev. The udev rules run a script whose only job is to stat(2) the device (from the “params” key in the backend) and write the major and minor number to Xenstore for blkback to pick up. (Aside: FreeBSD doesn’t do any of this, instead the FreeBSD kernel module simply opens the device in the “params” key). The script also writes the backend key “hotplug-status=connected”. We currently wait for this key to be written so that later calls to VBD.stat will return with “plugged=true”. If the call returns before this key is written then sometimes we receive an event, call VBD.stat and conclude erroneously that a spontaneous VBD unplug occurred.
7. mark each VIF as “active” This is for the same reason as VBDs are marked “active”.
8. plug VIFs Again, the order matters. Unlike VBDs, there is no read/write read/only constraint and the devices have unique indices (0, 1, 2, …) but Linux kernels have often (always?) ignored the actual index and instead relied on the order of results from the xenstore-ls listing. The order that xenstored returns the items happens to be the order the nodes were created so this means that (i) xenstored must continue to store directories as ordered lists rather than maps (which would be more efficient); and (ii) Xenopsd must make sure to plug the vifs in the same order. Note that relying on ethX device numbering has always been a bad idea but is still common. I bet if you change this lots of tests will suddenly start to fail!
The function VIF.plug_exn will
compute the port locking configuration required and write this to a well-known location in the filesystem where it can be read from the udev scripts. This really should be written to Xenstore instead, since this scheme doesn’t work with driver domains. add the Xenstore frontend/backend directories containing the network device info write the VIF information to the Xenopsd database so that future calls to VIF.stat can be told about the associated network if the qemu is going to be in a different domain to the storage, a frontend device in the qemu domain is created. Similarly to the VBD case, the function Device.Vif.add will write the Xenstore keys and wait for the “hotplug-status=connected” key. We do this because we cannot apply the port locking rules until the backend device has been created, and we cannot know the rules have been applied until after the udev script has written the key. If we didn’t wait for it then the VM might execute without all the port locking properly configured.
9. create the device model The VM_create_device_model micro-op will create a qemu device model if
the VM is HVM; or the VM uses a PV keyboard or mouse (since only qemu currently has backend support for these devices). The function VM.create_device_model_exn will
(if using a qemu stubdom) it will create and build the qemu domain compute the necessary qemu arguments and launch it. Note that qemu (aka the “device model”) is created after the VIFs and VBDs have been plugged but before the PCI devices have been plugged. Unfortunately qemu traditional infers the needed emulated hardware by inspecting the Xenstore VBD and VIF configuration and assuming that we want one emulated device per PV device, up to the natural limits of the emulated buses (i.e. there can be at most 4 IDE devices: {primary,secondary}{master,slave}). Not only does this create an ordering dependency that needn’t exist – and which impacts migration downtime – but it also completely ignores the plain fact that, on a Xen system, qemu can be in a different domain than the backend disk and network devices. This hack only works because we currently run everything in the same domain. There is an option (off by default) to list the emulated devices explicitly on the qemu command-line. If we switch to this by default then we ought to be able to start up qemu early, as soon as the domain has been created (qemu will need to know the domain id so it can map the I/O request ring).
10. plug PCI devices PCI devices are treated differently to VBDs and VIFs. If we are attaching the device to an HVM guest then instead of relying on the traditional Xenstore frontend/backend state machine we instead send RPCs to qemu requesting they be hotplugged. Note the domain is paused at this point, but qemu still supports PCI hotplug/unplug. The reasons why this doesn’t follow the standard Xenstore model are known only to the people who contributed this support to qemu. Again the order matters because it determines the position of the virtual device in the VM.
Note that Xenopsd doesn’t know anything about the PCI devices; concepts such as “GPU groups” belong to higher layers, such as xapi.
11. mark the domain as alive A design principle of Xenopsd is that it should tolerate failures such as being suddenly restarted. It guarantees to always leave the system in a valid state, in particular there should never be any “half-created VMs”. We achieve this for VM start by exploiting the mechanism which is necessary for reboot. When a VM wishes to reboot it causes the domain to exit (via SCHEDOP_shutdown) with a “reason code” of “reboot”. When Xenopsd sees this event VM_check_state operation is queued. This operation calls VM.get_domain_action_request to ask the question, “what needs to be done to make this VM happy now?”. The implementation checks the domain state for shutdown codes and also checks a special Xenopsd Xenstore key. When Xenopsd creates a Xen domain it sets this key to “reboot” (meaning “please reboot me if you see me”) and when Xenopsd finishes starting the VM it clears this key. This means that if Xenopsd crashes while starting a VM, the new Xenopsd will conclude that the VM needs to be rebooted and will clean up the current domain and create a fresh one.
12. unpause the domain A Xenopsd VM.start will always leave the domain paused, so strictly speaking this is a separate “operation” queued by the client (such as xapi) after the VM.start has completed. The function VM.unpause is reassuringly simple:
if di.Xenctrl.total_memory_pages = 0n then raise (Domain_not_built); Domain.unpause ~xc di.Xenctrl.domid; Opt.iter (fun stubdom_domid -> Domain.unpause ~xc stubdom_domid ) (get_stubdom ~xs di.Xenctrl.domid)`,description:"",tags:null,title:"Walkthrough: Starting a VM",uri:"/new-docs/xenopsd/walkthroughs/VM.start/index.html"},{content:`Let’s detail the handling process of an XML request within XAPI. The first document uses the migration as an example of such request.
How the migration request goes through Xen API? `,description:"",tags:null,title:"XAPI requests walk-throughs",uri:"/new-docs/xapi/walkthroughs/index.html"},{content:`The Xapi Storage Migration (XSM) also known as “Storage Motion” allows
a running VM to be migrated within a pool, between different hosts and different storage simultaneously; a running VM to be migrated to another pool; a disk attached to a running VM to be moved to another SR. The following diagram shows how XSM works at a high level:
The slowest part of a storage migration is migrating the storage, since virtual disks can be very large. Xapi starts by taking a snapshot and copying that to the destination as a background task. Before the datapath connecting the VM to the disk is re-established, xapi tells tapdisk to start mirroring all writes to a remote tapdisk over NBD. From this point on all VM disk writes are written to both the old and the new disk. When the background snapshot copy is complete, xapi can migrate the VM memory across. Once the VM memory image has been received, the destination VM is complete and the original can be safely destroyed.
`,description:"",tags:null,title:"Xapi Storage Migration",uri:"/new-docs/toolstack/features/XSM/index.html"},{content:`The XAPI Toolstack:
Forms the control plane of both XenServer as well as xcp-ng, manages clusters of Xen hosts with shared storage and networking, has a full-featured API, used by clients such as XenCenter and Xen Orchestra. The XAPI Toolstack is an open-source project developed by the xapi project, a sub-project of the Linux Foundation Xen Project.
The source code is available on Github under the xapi-project. the main repository is xen-api.
This developer guide documents the internals of the Toolstack to help developers understand the code, fix bugs and add new features. It is a work-in-progress, with new documents added when ready and updated whenever needed.
`,description:"",tags:null,title:"XAPI Toolstack Developer Guide",uri:"/new-docs/index.html"},{content:` Info The links in this page point to the source files of xapi v1.127.0, and xcp-idl v1.62.0, not to the latest source code.
In the beginning of 2023, significant changes have been made in the layering. In particular, the wrapper code from storage_impl.ml has been pushed down the stack, below the mux, such that it only covers the SMAPIv1 backend and not SMAPIv3. Also, all of the code (from xcp-idl etc) is now present in this repo (xen-api).
Xapi directly communicates only with the SMAPIv2 layer. There are no plugins directly implementing the SMAPIv2 interface, but the plugins in other layers are accessed through it:
graph TD A[xapi] --> B[SMAPIv2 interface] B --> C[SMAPIv2 <-> SMAPIv1 translation: storage_access.ml] B --> D[SMAPIv2 <-> SMAPIv3 translation: xapi-storage-script] C --> E[SMAPIv1 plugins] D --> F[SMAPIv3 plugins] SMAPIv1 These are the files related to SMAPIv1 in xen-api/ocaml/xapi/:
sm.ml: OCaml “bindings” for the SMAPIv1 Python “drivers” (SM) sm_exec.ml: support for implementing the above “bindings”. The parameters are converted to XML-RPC, passed to the relevant python script (“driver”), and then the standard output of the program is parsed as an XML-RPC response (we use xen-api-libs-transitional/http-svr/xMLRPC.ml for parsing XML-RPC). When adding new functionality, we can modify type call to add parameters, but when we don’t add any common ones, we should just pass the new parameters in the args record. smint.ml: Contains types, exceptions, … for the SMAPIv1 OCaml interface SMAPIv2 These are the files related to SMAPIv2, which need to be modified to implement new calls:
xcp-idl/storage/storage_interface.ml: Contains the SMAPIv2 interface xcp-idl/storage/storage_skeleton.ml: A stub SMAPIv2 storage server implementation that matches the SMAPIv2 storage server interface (this is verified by storage_skeleton_test.ml), each of its function just raise a Storage_interface.Unimplemented error. This skeleton is used to automatically fill the unimplemented methods of the below storage servers to satisfy the interface. xen-api/ocaml/xapi/storage_access.ml: module SMAPIv1: a SMAPIv2 server that does SMAPIv2 -> SMAPIv1 translation. It passes the XML-RPC requests as the first command-line argument to the corresponding Python script, which returns an XML-RPC response on standard output. xen-api/ocaml/xapi/storage_impl.ml: The Wrapper module wraps a SMAPIv2 server (Server_impl) and takes care of locking and datapaths (in case of multiple connections (=datapaths) from VMs to the same VDI, it will use the superstate computed by the Vdi_automaton in xcp-idl). It also implements some functionality, like the DP module, that is not implemented in lower layers. xen-api/ocaml/xapi/storage_mux.ml: A SMAPIv2 server, which multiplexes between other servers. A different SMAPIv2 server can be registered for each SR. Then it forwards the calls for each SR to the “storage plugin” registered for that SR. How SMAPIv2 works: We use message-switch under the hood for RPC communication between xcp-idl components. The main Storage_mux.Server (basically Storage_impl.Wrapper(Mux)) is registered to listen on the “org.xen.xapi.storage” queue during xapi’s startup, and this is the main entry point for incoming SMAPIv2 function calls. Storage_mux does not really multiplex between different plugins right now: earlier during xapi’s startup, the same SMAPIv1 storage server module is registered on the various “org.xen.xapi.storage.<sr type>” queues for each supported SR type. (This will change with SMAPIv3, which is accessed via a SMAPIv2 plugin outside of xapi that translates between SMAPIv2 and SMAPIv3.) Then, in Storage_access.create_sr, which is called during SR.create, and also during PBD.plug, the relevant “org.xen.xapi.storage.<sr type>” queue needed for that PBD is registered with Storage_mux in Storage_access.bind for the SR of that PBD.
So basically what happens is that xapi registers itself as a SMAPIv2 server, and forwards incoming function calls to itself through message-switch, using its Storage_mux module. These calls are forwarded to xapi’s SMAPIv1 module doing SMAPIv2 -> SMAPIv1 translation.
Registration of the various storage servers sequenceDiagram participant q as message-switch participant v1 as Storage_access.SMAPIv1 participant svr as Storage_mux.Server Note over q, svr: xapi startup, "Starting SMAPIv1 proxies" q ->> v1:org.xen.xapi.storage.sr_type_1 q ->> v1:org.xen.xapi.storage.sr_type_2 q ->> v1:org.xen.xapi.storage.sr_type_3 Note over q, svr: xapi startup, "Starting SM service" q ->> svr:org.xen.xapi.storage Note over q, svr: SR.create, PBD.plug svr ->> q:org.xapi.storage.sr_type_2 What happens when a SMAPIv2 “function” is called graph TD call[SMAPIv2 call] --VDI.attach2--> org.xen.xapi.storage subgraph message-switch org.xen.xapi.storage org.xen.xapi.storage.SR_type_x end org.xen.xapi.storage --VDI.attach2--> Storage_impl.Wrapper subgraph xapi subgraph Storage_mux.server Storage_impl.Wrapper --> Storage_mux.mux end Storage_access.SMAPIv1 end Storage_mux.mux --VDI.attach2--> org.xen.xapi.storage.SR_type_x org.xen.xapi.storage.SR_type_x --VDI.attach2--> Storage_access.SMAPIv1 subgraph SMAPIv1 driver_x[SMAPIv1 driver for SR_type_x] end Storage_access.SMAPIv1 --vdi_attach--> driver_x Interface Changes, Backward Compatibility, & SXM During SXM, xapi calls SMAPIv2 functions on a remote xapi. Therefore it is important to keep all those SMAPIv2 functions backward-compatible that we call remotely (e.g. Remote.VDI.attach), otherwise SXM from an older to a newer xapi will break.
Functionality implemented in SMAPIv2 layers The layer between SMAPIv2 and SMAPIv1 is much fatter than the one between SMAPIv2 and SMAPIv3. The latter does not do much, apart from simple translation. However, the former has large portions of code in its intermediate layers, in addition to the basic SMAPIv2 <-> SMAPIv1 translation in storage_access.ml.
These are the three files in xapi that implement the SMAPIv2 storage interface, from higher to lower level:
xen-api/ocaml/xapi/storage_impl.ml: xen-api/ocaml/xapi/storage_mux.ml: xen-api/ocaml/xapi/storage_access.ml: Functionality implemented by higher layers is not implemented by the layers below it.
Extra functionality in storage_impl.ml In addition to its usual functions, Storage_impl.Wrapper also implements the UPDATES and TASK SMAPIv2 APIs, without calling the wrapped module.
These are backed by the Updates, Task_server, and Scheduler modules from xcp-idl, instantiated in xapi’s Storage_task module. Migration code in Storage_mux will interact with these to update task progress. There is also an event loop in xapi that keeps calling UPDATES.get to keep the tasks in xapi’s database in sync with the storage manager’s tasks.
Storage_impl.Wrapper also implements the legacy VDI.attach call by simply calling the newer VDI.attach2 call in the same module. In general, this is a good place to implement a compatibility layer for deprecated functionality removed from other layers, because this is the first module that intercepts a SMAPIv2 call.
Extra functionality in storage_mux.ml Storage_mux implements storage motion (SXM): it implements the DATA and DATA.MIRROR modules. Migration code will use the Storage_task module to run the operations and update the task’s progress.
It also implements the Policy module from the SMAPIv2 interface.
SMAPIv3 SMAPIv3 has a slightly different interface from SMAPIv2.The xapi-storage-script daemon is a SMAPIv2 plugin separate from xapi that is doing the SMAPIv2 ↔ SMAPIv3 translation. It keeps the plugins registered with xcp-idl (their message-switch queues) up to date as their files appear or disappear from the relevant directory.
SMAPIv3 Interface The SMAPIv3 interface is defined using an OCaml-based IDL from the ocaml-rpc library, and is in this repo: https://github.com/xapi-project/xapi-storage
From this interface we generate
OCaml RPC client bindings used in xapi-storage-script The SMAPIv3 API reference Python bindings, used by the SM scripts that implement the SMAPIv3 interface. These bindings are built by running “make” in the root xapi-storage, and appear in the _build/default/python/xapi/storage/api/v5 directory. On a XenServer host, they are stored in the /usr/lib/python2.7/site-packages/xapi/storage/api/v5/ directory SMAPIv3 Plugins For SMAPIv3 we have volume plugins to manipulate SRs and volumes (=VDIs) in them, and datapath plugins for connecting to the volumes. Volume plugins tell us which datapath plugins we can use with each volume, and what to pass to the plugin. Both volume and datapath plugins implement some common functionality: the SMAPIv3 plugin interface.
How SMAPIv3 works: The xapi-storage-script daemon detects volume and datapath plugins stored in subdirectories of the /usr/libexec/xapi-storage-script/volume/ and /usr/libexec/xapi-storage-script/datapath/ directories, respectively. When it finds a new datapath plugin, it adds the plugin to a lookup table and uses it the next time that datapath is required. When it finds a new volume plugin, it binds a new message-switch queue named after the plugin’s subdirectory to a new server instance that uses these volume scripts.
To invoke a SMAPIv3 method, it executes a program named <Interface name>.<function name> in the plugin’s directory, for example /usr/libexec/xapi-storage-script/volume/org.xen.xapi.storage.gfs2/SR.ls. The inputs to each script can be passed as command-line arguments and are type-checked using the generated Python bindings, and so are the outputs. The URIs of the SRs that xapi-storage-script knows about are stored in the /var/run/nonpersistent/xapi-storage-script/state.db file, these URIs can be used on the command line when an sr argument is expected. Registration of the various SMAPIv3 plugins sequenceDiagram participant q as message-switch participant v1 as (Storage_access.SMAPIv1) participant svr as Storage_mux.Server participant vol_dir as /../volume/ participant dp_dir as /../datapath/ participant script as xapi-storage-script Note over script, vol_dir: xapi-storage-script startup script ->> vol_dir: new subdir org.xen.xapi.storage.sr_type_4 q ->> script: org.xen.xapi.storage.sr_type_4 script ->> dp_dir: new subdir sr_type_4_dp Note over q, svr: xapi startup, "Starting SMAPIv1 proxies" q -->> v1:org.xen.xapi.storage.sr_type_1 q -->> v1:org.xen.xapi.storage.sr_type_2 q -->> v1:org.xen.xapi.storage.sr_type_3 Note over q, svr: xapi startup, "Starting SM service" q ->> svr:org.xen.xapi.storage Note over q, svr: SR.create, PBD.plug svr ->> q:org.xapi.storage.sr_type_4 What happens when a SMAPIv3 “function” is called graph TD call[SMAPIv2 call] --VDI.attach2--> org.xen.xapi.storage subgraph message-switch org.xen.xapi.storage org.xen.xapi.storage.SR_type_x end org.xen.xapi.storage --VDI.attach2--> Storage_impl.Wrapper subgraph xapi subgraph Storage_mux.server Storage_impl.Wrapper --> Storage_mux.mux end Storage_access.SMAPIv1 end Storage_mux.mux --VDI.attach2--> org.xen.xapi.storage.SR_type_x org.xen.xapi.storage.SR_type_x -."VDI.attach2".-> Storage_access.SMAPIv1 subgraph SMAPIv1 driver_x[SMAPIv1 driver for SR_type_x] end Storage_access.SMAPIv1 -.vdi_attach.-> driver_x subgraph SMAPIv3 xapi-storage-script --Datapath.attach--> v3_dp_plugin_x subgraph SMAPIv3 plugins v3_vol_plugin_x[volume plugin for SR_type_x] v3_dp_plugin_x[datapath plugin for SR_type_x] end end org.xen.xapi.storage.SR_type_x --VDI.attach2-->xapi-storage-script Error reporting In our SMAPIv1 OCaml “bindings” in xapi (xen-api/ocaml/xapi/sm_exec.ml), when we inspect the error codes returned from a call to SM, we translate some of the SMAPIv1/SM error codes to XenAPI errors, and for others, we just construct an error code of the form SR_BACKEND_FAILURE_<SM error number>.
The file xcp-idl/storage/storage_interface.ml defines a number of SMAPIv2 errors, ultimately all errors from the various SMAPIv2 storage servers in xapi will be returned as one of these. Most of the errors aren’t converted into a specific exception in Storage_interface, but are simply wrapped with Storage_interface.Backend_error.
The Storage_access.transform_storage_exn function is used by the client code in xapi to translate the SMAPIv2 errors into XenAPI errors again, this unwraps the errors wrapped with Storage_interface.Backend_error.
Message Forwarding In the message forwarding layer, first we check the validity of VDI operations using mark_vdi and mark_sr. These first check that the operation is valid operations, using Xapi_vdi.check_operation_error, for mark_vdi, which also inspects the current operations of the VDI, and then, if the operation is valid, it is added to the VDI’s current operations, and update_allowed_operations is called. Then we forward the VDI operation to a suitable host that has a PBD plugged for the VDI’s SR.
Checking that the SR is attached For the VDI operations, we check at two different places whether the SR is attached: first, at the Xapi level, in Xapi_vdi.check_operation_error, for the resize operation, and then, at the SMAPIv1 level, in Sm.assert_pbd_is_plugged. Sm.assert_pbd_is_plugged performs the same checks, plus it checks that the PBD is attached to the localhost, unlike Xapi_vdi.check_operation_error. This behaviour is correct, because Xapi_vdi.check_operation_error is called from the message forwarding layer, which forwards the call to a host that has the SR attached.
VDI Identifiers and Storage Motion VDI “location”: this is the VDI identifier used by the SM backend. It is usually the UUID of the VDI, but for ISO SRs it is the name of the ISO. VDI “content_id”: this is used for storage motion, to reduce the amount of data copied. When we copy over a VDI, the content_id will initially be the same. However, when we attach a VDI as read-write, and then detach it, then we will blank its content_id (set it to a random UUID), because we may have written to it, so the content could be different. . `,description:"",tags:null,title:"XAPI's Storage Layers",uri:"/new-docs/xapi/storage/index.html"},{content:` Info The links in this page point to the source files of xapi v1.132.0, not to the latest source code. Meanwhile, the CLI server code in xapi has been moved to a library separate from the main xapi binary, and has its own subdirectory ocaml/xapi-cli-server.
Architecture The actual CLI is a very lightweight binary in ocaml/xe-cli
It is just a dumb client, that does everything that xapi tells it to do This is a security issue We must trust the xenserver that we connect to, because it can tell xe to read local files, download files, … When it is first called, it takes the few command-line arguments it needs, and then passes the rest to xapi in a HTTP PUT request Each argument is in a separate line Then it loops doing what xapi tells it to do, in a loop, until xapi tells it to exit or an exception happens The protocol description is in ocaml/xapi-cli-protocol/cli_protocol.ml
The CLI has such a protocol that one binary can talk to multiple versions of xapi as long as their CLI protocol versions are compatible and the CLI can be changed without updating the xe binary and also for performance reasons, it is more efficient this way than by having a CLI that makes XenAPI calls Xapi
The HTTP POST request is sent to the /cli URL In Xapi.server_init, xapi registers the appropriate function to handle these requests, defined in common_http_handlers in the same file: Xapi_cli.handler The relevant code is in ocaml/xapi/records.ml, ocaml/xapi/cli_*.ml CLI object definitions are in records.ml, command definitions in cli_frontend.ml (in cmdtable_data), implementations of commands in cli_operations.ml When a command is received, it is parsed into a command name and a parameter list of key-value pairs and the command table is populated lazily from the commands defined in cmdtable_data in cli_frontend.ml, and automatically generated low-level parameter commands (the ones defined in section A.3.2 of the XenServer Administrator’s Guide) are also added for a list of standard classes the command table maps command names to records that contain the implementation of the command, among other things Then the command name is looked up in the command table, and the corresponding operation is executed with the parsed key-value parameter list passed to it Walk-through: CLI handler in xapi (external calls) Definitions for the HTTP handler Constants.cli_uri = "/cli" Datamodel.http_actions = [...; ("post_cli", (Post, Constants.cli_uri, false, [], _R_READ_ONLY, [])); ...] (* these public http actions will NOT be checked by RBAC *) (* they are meant to be used in exceptional cases where RBAC is already *) (* checked inside them, such as in the XMLRPC (API) calls *) Datamodel.public_http_actions_with_no_rbac_check\` = [... "post_cli"; (* CLI commands -> calls XMLRPC *) ...] Xapi.common_http_handlers = [...; ("post_cli", (Http_svr.BufIO Xapi_cli.handler)); ...] Xapi.server_init () = ... "Registering http handlers", [], (fun () -> List.iter Xapi_http.add_handler common_http_handlers); ... Due to there definitions, Xapi_http.add_handler does not perform RBAC checks for post_cli. This means that the CLI handler does not use Xapi_http.assert_credentials_ok when a request comes in, as most other handlers do. The reason is that RBAC checking is delegated to the actual XenAPI calls that are being done by the commands in Cli_operations.
This means that the Xapi_http.add_handler call so resolves to simply:
Http_svr.Server.add_handler server Http.Post "/cli" (Http_svr.BufIO Xapi_cli.handler)) …which means that the function Xapi_cli.handler is called directly when an HTTP POST request with path /cli comes in.
High-level request processing Xapi_cli.handler:
Reads the body of the HTTP request, limitted to Xapi_globs.http_limit_max_cli_size = 200 * 1024 characters. Sends a protocol version string to the client: "XenSource thin CLI protocol" plus binary encoded major (0) and (2) minor numbers. Reads the protocol version from the client and exits with an error if it does not match the above. Calls Xapi_cli.parse_session_and_args with the request’s body to extract the session ref, if there. Calls Cli_frontend.parse_commandline to parse the rest of the command line from the body. Calls Xapi_cli.exec_command to execute the command. On error, calls exception_handler. Xapi_cli.parse_session_and_args:
Is passed the request body and reads it line by line. Each line is considered an argument. Removes any CR chars from the end of each argument. If the first arg starts with session_id=, the the bit after this prefix is considered to be a session reference. Returns the session ref (if there) and (remaining) list of args. Cli_frontend.parse_commandline:
Returns the command name and assoc list of param names and values. It handles --name and -flag arguments by turning them into key/value string pairs. Xapi_cli.exec_command:
Finds username/password params. Get the rpc function: this is the so-called “fake_rpc callback”, which does not use the network or HTTP at all, but goes straight to Api_server.callback1 (the XenAPI RPC entry point). This function is used by the CLI handler to do loopback XenAPI calls. Logs the parsed xe command, omitting sensitive data. Continues as Xapi_cli.do_rpcs Looks up the command name in the command table from Cli_frontend (raises an error if not found). Checks if all required params have been supplied (raises an error if not). Checks that the host is a pool master (raises an error if not). Depending on the command, a session.login_with_password or session.slave_local_login_with_password XenAPI call is made with the supplied username and password. If the authentication passes, then a session reference is returned for the RBAC role that belongs to the user. This session is used to do further XenAPI calls. Next, the implementation of the command in Cli_operations is executed. Command implementations The various commands are implemented in cli_operations.ml. These functions are only called after user authentication has passed (see above). However, RBAC restrictions are only enforced inside any XenAPI calls that are made, and not on any of the other code in cli_operations.ml.
The type of each command implementation function is as follows (see cli_cmdtable.ml):
type op = Cli_printer.print_fn -> (Rpc.call -> Rpc.response) -> API.ref_session -> ((string*string) list) -> unit So each function receives a printer for sending text output to the xe client, and rpc function and session reference for doing XenAPI calls, and a key/value pair param list. Here is a typical example:
let bond_create printer rpc session_id params = let network = List.assoc "network-uuid" params in let mac = List.assoc_default "mac" params "" in let network = Client.Network.get_by_uuid rpc session_id network in let pifs = List.assoc "pif-uuids" params in let uuids = String.split ',' pifs in let pifs = List.map (fun uuid -> Client.PIF.get_by_uuid rpc session_id uuid) uuids in let mode = Record_util.bond_mode_of_string (List.assoc_default "mode" params "") in let properties = read_map_params "properties" params in let bond = Client.Bond.create rpc session_id network pifs mac mode properties in let uuid = Client.Bond.get_uuid rpc session_id bond in printer (Cli_printer.PList [ uuid]) The necessary parameters are looked up in params using List.assoc or similar. UUIDs are translated into reference by get_by_uuid XenAPI calls (note that the Client module is the XenAPI client, and functions in there require the rpc function and session reference). Then the main API call is made (Client.Bond.create in this case). Further API calls may be made to output data for the client, and passed to the printer. This is the common case for CLI operations: they do API calls based on the parameters that were passed in.
However, other commands are more complicated, for example vm_import/export and vm_migrate. These contain a lot more logic in the CLI commands, and also send commands to the client to instruct it to read or write files and/or do HTTP calls.
Yet other commands do not actually do any XenAPI calls, but instead get “helpful” information from other places. Example: diagnostic_gc_stats, which displays statistics from xapi’s OCaml GC.
Tutorials The following tutorials show how to extend the CLI (and XenAPI):
Adding a field Adding an operation `,description:"",tags:null,title:"XE CLI architecture",uri:"/new-docs/xapi/cli/index.html"}]