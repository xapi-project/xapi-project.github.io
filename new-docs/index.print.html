<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.127.0"><meta name=generator content="Relearn 5.20.0+tip"><meta name=description content><title>XAPI Toolstack Developer Guide :: XAPI Toolstack Developer Documentation</title>
<link href=https://xapi-project.github.io/new-docs/index.html rel=canonical type=text/html title="XAPI Toolstack Developer Guide :: XAPI Toolstack Developer Documentation"><link href=/new-docs/index.xml rel=alternate type=application/rss+xml title="XAPI Toolstack Developer Guide :: XAPI Toolstack Developer Documentation"><link href=/new-docs/index.search.js rel=alternate type=text/javascript title="XAPI Toolstack Developer Guide :: XAPI Toolstack Developer Documentation"><link href=/new-docs/images/favicon.png?1724316354 rel=icon type=image/png><link href=/new-docs/css/fontawesome-all.min.css?1724316359 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/fontawesome-all.min.css?1724316359 rel=stylesheet></noscript><link href=/new-docs/css/nucleus.css?1724316359 rel=stylesheet><link href=/new-docs/css/auto-complete.css?1724316359 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/auto-complete.css?1724316359 rel=stylesheet></noscript><link href=/new-docs/css/perfect-scrollbar.min.css?1724316359 rel=stylesheet><link href=/new-docs/css/fonts.css?1724316359 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/fonts.css?1724316359 rel=stylesheet></noscript><link href=/new-docs/css/theme.css?1724316359 rel=stylesheet><link href=/new-docs/css/theme-auto.css?1724316359 rel=stylesheet id=variant-style><link href=/new-docs/css/variant.css?1724316359 rel=stylesheet><link href=/new-docs/css/print.css?1724316359 rel=stylesheet media=print><link href=/new-docs/css/format-print.css?1724316359 rel=stylesheet><link href=/new-docs/css/ie.css?1724316359 rel=stylesheet><script src=/new-docs/js/url.js?1724316359></script><script src=/new-docs/js/variant.js?1724316359></script><script>window.index_js_url="/new-docs/index.search.js";var baseUriFull,root_url="/",baseUri=root_url.replace(/\/$/,"");window.T_Copy_to_clipboard="Copy to clipboard",window.T_Copied_to_clipboard="Copied to clipboard!",window.T_Copy_link_to_clipboard="Copy link to clipboard",window.T_Link_copied_to_clipboard="Copied link to clipboard!",window.T_No_results_found='No results found for "{0}"',window.T_N_results_found='{1} results found for "{0}"',baseUriFull="https://xapi-project.github.io/new-docs/",window.variants&&variants.init(["auto","zen-light","zen-dark","red","blue","green","learn","neon","relearn-light","relearn-bright","relearn-dark"])</script><link rel=stylesheet href=https://xapi-project.github.io/new-docs/css/misc.css></head><body class="mobile-support print" data-url=/new-docs/index.html><div id=body class=default-animation><div id=sidebar-overlay></div><div id=toc-overlay></div><nav id=topbar class=highlightable><div><div id=breadcrumbs><span id=sidebar-toggle-span><a href=# id=sidebar-toggle class=topbar-link title='Menu (CTRL+ALT+n)'><i class="fas fa-bars fa-fw"></i></a></span><ol class=links itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>XAPI Toolstack Developer Guide</span><meta itemprop=position content="1"></li></ol></div></div></nav><main id=body-inner class="highlightable home" tabindex=-1><div class=flex-block-wrapper><article class=home><header class=headline></header><h1 id=xapi-toolstack-developer-guide>XAPI Toolstack Developer Guide</h1><p>The <strong>XAPI Toolstack</strong>:</p><ul><li>Forms the control plane of both <a href=http://xenserver.com target=_blank>XenServer</a> as well as
<a href=http://xcp-ng.org target=_blank>xcp-ng</a>,</li><li>manages clusters of Xen hosts with shared storage and networking,</li><li>has a full-featured <a href=http://xapi-project-github.io/xen-api target=_blank>API</a>, used by clients such as
<a href=https://github.com/xenserver/xenadmin target=_blank>XenCenter</a> and <a href=https://xen-orchestra.com target=_blank>Xen Orchestra</a>.</li></ul><p>The XAPI Toolstack is an open-source project developed by the <a href=http://www.xenproject.org/developers/teams/xapi.html target=_blank>xapi
project</a>, a sub-project of the Linux
Foundation Xen Project.</p><p>The source code is available on <a href=https://github.com/xapi-project/ target=_blank>Github under the xapi-project</a>. the main repository is <a href=https://github.com/xapi-project/xen-api target=_blank>xen-api</a>.</p><p>This developer guide documents the internals of the Toolstack to help developers understand the code, fix bugs and add new features. It is a work-in-progress, with new documents added when ready and updated whenever needed.</p><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of XAPI Toolstack Developer Guide</h1><article class=default><header class=headline></header><h1 id=the-xapi-toolstack>The XAPI Toolstack</h1><ul class="children children-li children-sort-"><li><a href=/new-docs/toolstack/responsibilities/index.html>Responsibilities</a><ul></ul></li><li><a href=/new-docs/toolstack/high-level/index.html>High-level architecture</a><ul><li><a href=/new-docs/toolstack/high-level/environment/index.html>Environment</a></li><li><a href=/new-docs/toolstack/high-level/daemons/index.html>Daemons</a></li><li><a href=/new-docs/toolstack/high-level/interfaces/index.html>Interfaces</a></li></ul></li><li><a href=/new-docs/toolstack/features/index.html>Features</a><ul><li><a href=/new-docs/toolstack/features/DR/index.html>Disaster Recovery</a></li><li><a href=/new-docs/toolstack/features/events/index.html>Event handling in the Control Plane - Xapi, Xenopsd and Xenstore</a></li><li><a href=/new-docs/toolstack/features/HA/index.html>High-Availability</a></li><li><a href=/new-docs/toolstack/features/NUMA/index.html>NUMA</a></li><li><a href=/new-docs/toolstack/features/snapshots/index.html>Snapshots</a></li><li><a href=/new-docs/toolstack/features/VGPU/index.html>vGPU</a></li><li><a href=/new-docs/toolstack/features/XSM/index.html>Xapi Storage Migration</a></li></ul></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of The XAPI Toolstack</h1><article class=default><header class=headline></header><h1 id=responsibilities>Responsibilities</h1><p>The XAPI Toolstack forms the main control plane of a pool of XenServer hosts. It allow the administrator to:</p><ul><li>Configure the hardware resources of XenServer hosts: storage, networking, graphics, memory.</li><li>Create, configure and destroy VMs and their virtual resources.</li><li>Control the lifecycle of VMs.</li><li>Monitor the status of hosts, VMs and related resources.</li></ul><p>To this, the Toolstack:</p><ul><li>Exposes an API that can be accessed by external clients over HTTP(s).</li><li>Exposes a CLI.</li><li>Ensures that physical resources are configured when needed, and VMs receive the resources they require.</li><li>Implements various features to help the administrator manage their systems.</li><li>Monitors running VMs.</li><li>Records metrics about physical and virtual resources.</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=high-level-architecture>High-level architecture</h1><p>The XAPI Toolstack manages a cluster of hosts, network switches and storage on
behalf of clients such as <a href=https://github.com/xenserver/xenadmin target=_blank>XenCenter</a>
and <a href=https://xen-orchestra.com target=_blank>Xen Orchestra</a>.</p><p>The most fundamental concept is of a <em>Resource pool</em>: the whole cluster managed
as a single entity. The following diagram shows a cluster of hosts running
xapi, all sharing some storage:</p><p><a href=#image-e633ec14d44af6027a4042ea35d0f255 class=lightbox-link><img src=/new-docs/toolstack/high-level/pool.png alt="A Resource Pool" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-e633ec14d44af6027a4042ea35d0f255><img src=/new-docs/toolstack/high-level/pool.png alt="A Resource Pool" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>At any time, at most one host is known as the <em>pool coordinator</em> (formerly
known as &ldquo;master&rdquo;) and is responsible for coordination and locking resources
within the pool. When a pool is first created a coordinator host is chosen. The
coordinator role can be transferred</p><ul><li>on user request in an orderly fashion (<code>xe pool-designate-new-master</code>)</li><li>on user request in an emergency (<code>xe pool-emergency-transition-to-master</code>)</li><li>automatically if HA is enabled on the cluster.</li></ul><p>All hosts expose an HTTP, XML-RPC and JSON-RPC interface running on port 80 and
with TLS on port 443, but control operations will only be processed on the
coordinator host. Attempts to send a control operation to another host will
result in a XenAPI redirect error message. For efficiency the following
operations are permitted on non-coordinator hosts:</p><ul><li>querying performance counters (and their history)</li><li>connecting to VNC consoles</li><li>import/export (particularly when disks are on local storage)</li></ul><p>Since the coordinator host acts as coordinator and lock manager, the other
hosts will often talk to the coordinator. Non-coordinator hosts will also talk
to each other (over the same HTTP and RPC channels) to</p><ul><li>transfer VM memory images (<em>VM migration</em>)</li><li>mirror disks (<em>storage migration</em>)</li></ul><p>Note that some types of shared storage (in particular all those using vhd)
require coordination for disk GC and coalesce. This coordination is currently
done by xapi and hence it is not possible to share this kind of storage between
resource pools.</p><p>The following diagram shows the software running on a single host. Note that
all hosts run the same software (although not necessarily the same version, if
we are in the middle of a rolling update).</p><p><a href=#image-021e283895e499cf326805d1e552ce4f class=lightbox-link><img src=/new-docs/toolstack/high-level/host.png alt="A Host" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-021e283895e499cf326805d1e552ce4f><img src=/new-docs/toolstack/high-level/host.png alt="A Host" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The XAPI Toolstack expects the host to be running Xen on x86. The Xen
hypervisor partitions the host into <em>Domains</em>, some of which can have
privileged hardware access, and the rest are unprivileged guests. The XAPI
Toolstack normally runs all of its components in the privileged initial domain,
Domain 0, also known as &ldquo;the control domain&rdquo;. However there is experimental
code which supports &ldquo;driver domains&rdquo; allowing storage and networking drivers to
be isolated in their own domains.</p><ul class="children children-li children-sort-"><li><a href=/new-docs/toolstack/high-level/environment/index.html>Environment</a><ul></ul></li><li><a href=/new-docs/toolstack/high-level/daemons/index.html>Daemons</a><ul></ul></li><li><a href=/new-docs/toolstack/high-level/interfaces/index.html>Interfaces</a><ul></ul></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of High-level architecture</h1><article class=default><header class=headline></header><h1 id=environment>Environment</h1><p>The Toolstack runs in an environment on a server (host) that has:</p><ul><li>Physical hardware.</li><li>The Xen hypervisor.</li><li>The control domain (domain 0): the priviledged domain that the Toolstack runs in.</li><li>Other, mostly unpriviledged domains, usually for guests (VMs).</li></ul><p>The Toolstack relies on various bits of software inside the control domain, and directly communicates with most of these:</p><ul><li>Linux kernel including drivers for hardware and Xen paravirtualised devices (e.g. <code>netback</code> and <code>blkback</code>).<ul><li>Interacts through <code>/sys</code> and <code>/proc</code>, udev scripts, xenstore, &mldr;</li></ul></li><li>CentOS distibution including userspace tools and libraries.<ul><li>systemd, networking tools, &mldr;</li></ul></li><li>Xen-specific libraries, especially <code>libxenctrl</code> (a.k.a. <code>libxc</code>)</li><li><code>xenstored</code>: a key-value pair configuration database<ul><li>Accessible from all domains on a host, which makes it useful for inter-domain communication.</li><li>The control domain has access to the entire xenstore database, while other domains only see sub-trees that are specific to that domain.</li><li>Used for connecting VM disks and network interfaces, and other VM configuration options.</li><li>Used for VM status reporting, e.g. the capabilities of the PV drivers (if installed), the IP address, etc.</li></ul></li><li><a href=https://github.com/xapi-project/sm target=_blank>SM</a>: Storage Manager
plugins which connect xapi&rsquo;s internal storage interfaces to the control
APIs of external storage systems.</li><li><code>stunnel</code>: a daemon which decodes TLS and forwards traffic to xapi (and the other way around).</li><li>Open vSwitch (OVS): a virtual network switch, used to connect VMs to network interfaces. The OVS offers several networking features that xapi takes advantage of.</li><li>QEMU: emulation of various bits of hardware</li><li>DEMU: emulation of Nvidia vGPUs</li><li><code>xenguest</code></li><li><code>emu-manager</code></li><li><code>pvsproxy</code></li><li><code>xenconsoled</code>: allows access to guest consoles. This is common to all Xen
hosts.</li></ul><p>The Toolstack also interacts with software that runs inside the guests:</p><ul><li>PV drivers</li><li>The guest agent</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=daemons>Daemons</h1><p>The Toolstack consists of a set of co-operating daemons:</p><dl><dt>xapi</dt><dd>manages clusters of hosts, co-ordinating access to shared storage and networking.</dd><dt>xenopsd</dt><dd>a low-level &ldquo;domain manager&rdquo; which takes care of creating, suspending,
resuming, migrating, rebooting domains by interacting with Xen via libxc and
libxl.</dd><dt>xcp-rrdd</dt><dd>a performance counter monitoring daemon which aggregates &ldquo;datasources&rdquo; defined
via a plugin API and records history for each. There are various rrdd-plugin daemons:<ul><li>xcp-rrdd-gpumon</li><li>xcp-rrdd-iostat</li><li>xcp-rrdd-squeezed</li><li>xcp-rrdd-xenpm</li></ul></dd><dt>xcp-networkd</dt><dd>a host network manager which takes care of configuring interfaces, bridges
and OpenVSwitch instances</dd><dt>squeezed</dt><dd>a daemon in charge of VM memory management</dd><dt>xapi-storage-script</dt><dd>for storage manipulation over SMAPIv3</dd><dt>message-switch</dt><dd>exchanges messages between the daemons on a host</dd><dt>xapi-guard</dt><dd>forwards uefi and vtpm persistence calls from domains to xapi</dd><dt>v6d</dt><dd>controls which features are enabled.</dd><dt>forkexecd</dt><dd>a helper daemon that assists the above daemons with executing binaries and scripts</dd><dt>xhad</dt><dd>The High-Availability daemon</dd><dt>perfmon</dt><dd>a daemon which monitors performance counters and sends &ldquo;alerts&rdquo;
if values exceed some pre-defined threshold</dd><dt>mpathalert</dt><dd>a daemon which monitors &ldquo;storage paths&rdquo; and sends &ldquo;alerts&rdquo;
if paths fail and need repair</dd><dt>wsproxy</dt><dd>handles access to VM consoles</dd></dl><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=interfaces>Interfaces</h1><p>Communication between the Toolstack daemon is built upon libraries from a
component called
<a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xapi-idl target=_blank>xapi-idl</a>.</p><ul><li>Abstracts communication between daemons over the message-switch using JSON/RPC.</li><li>Contains the definition of the interfaces exposed by the daemons (except xapi).</li></ul><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=features>Features</h1><ul class="children children-li children-sort-"><li><a href=/new-docs/toolstack/features/DR/index.html>Disaster Recovery</a></li><li><a href=/new-docs/toolstack/features/events/index.html>Event handling in the Control Plane - Xapi, Xenopsd and Xenstore</a></li><li><a href=/new-docs/toolstack/features/HA/index.html>High-Availability</a></li><li><a href=/new-docs/toolstack/features/NUMA/index.html>NUMA</a></li><li><a href=/new-docs/toolstack/features/snapshots/index.html>Snapshots</a></li><li><a href=/new-docs/toolstack/features/VGPU/index.html>vGPU</a></li><li><a href=/new-docs/toolstack/features/XSM/index.html>Xapi Storage Migration</a></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Features</h1><article class=default><header class=headline></header><h1 id=disaster-recovery>Disaster Recovery</h1><p>The <a href=/new-docs/toolstack/features/DR/../HA/HA.html>HA</a> feature will restart VMs after hosts have failed, but what
happens if a whole site (e.g. datacenter) is lost? A disaster recovery
configuration is shown in the following diagram:</p><p><a href=#image-a3af5549e5b280ec974db631e0539b46 class=lightbox-link><img src=/new-docs/toolstack/features/DR/dr.png alt="Disaster recovery maintaining a secondary site" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-a3af5549e5b280ec974db631e0539b46><img src=/new-docs/toolstack/features/DR/dr.png alt="Disaster recovery maintaining a secondary site" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>We rely on the storage array&rsquo;s built-in mirroring to replicate (synchronously
or asynchronously: the admin&rsquo;s choice) between the primary and the secondary
site. When DR is enabled the VM disk data and VM metadata are written to the
storage server and mirrored. The secondary site contains the other side
of the data mirror and a set of hosts, which may be powered off.</p><p>In normal operation, the DR feature allows a &ldquo;dry-run&rdquo; recovery where a host
on the secondary site checks that it can indeed see all the VM disk data
and metadata. This should be done regularly, so that admins are familiar with
the process.</p><p>After a disaster, the admin breaks the mirror on the secondary site and triggers
a remote power-on of the offline hosts (either using an out-of-band tool or
the built-in host power-on feature of xapi). The pool master on the secondary
site can connect to the storage and extract all the VM metadata. Finally the
VMs can all be restarted.</p><p>When the primary site is fully recovered, the mirror can be re-synchronised
and the VMs can be moved back.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=event-handling-in-the-control-plane---xapi-xenopsd-and-xenstore>Event handling in the Control Plane - Xapi, Xenopsd and Xenstore</h1><h2 id=introduction>Introduction</h2><p>Xapi, xenopsd and xenstore use a number of different events to obtain
indications that some state changed in dom0 or in the guests. The events
are used as an efficient alternative to polling all these states
periodically.</p><ul><li><strong>xenstore</strong> provides a very configurable approach in which each and
any key can be watched individually by a xenstore client. Once the
value of a watched key changes, xenstore will indicate to the client
that the value for that key has changed. An ocaml xenstore client
library provides a way for ocaml programs such as xenopsd,
message-cli and rrdd to provide high-level ocaml callback functions
to watch specific key. It&rsquo;s very common, for instance, for xenopsd
to watch specific keys in the xenstore keyspace of a guest and then
after receiving events for some or all of them, read other keys or
subkeys in xenstored to update its internal state mirroring the
state of guests and its devices (for instance, if the guest has pv
drivers and specific frontend devices have established connections
with the backend devices in dom0).</li><li><strong>xapi</strong> also provides a very configurable event mechanism in which
the xenapi can be used to provide events whenever a xapi object (for
instance, a VM, a VBD etc) changes state. This event mechanism is
very reliable and is extensively used by XenCenter to provide
real-time update on the XenCenter GUI.</li><li><strong>xenopsd</strong> provides a somewhat less configurable event mechanism,
where it always provides signals for all objects (VBDs, VMs
etc) whose state changed (so it&rsquo;s not possible to select a subset of
objects to watch for as in xenstore or in xapi). It&rsquo;s up to the
xenopsd client (eg. xapi) to receive these events and then filter
out or act on each received signal by calling back xenopsd and
asking it information for the specific signalled object.  The main
use in xapi for the xenopsd signals is to update xapi&rsquo;s database of
the current state of each object controlled by xenopsd (VBDs,
VMs etc).</li></ul><p>Given a choice between polling states and receiving events when the
state change, we should in general opt for receiving events in the code
in order to avoid adding bottlenecks in dom0 that will prevent the
scalability of XenServer to many VMs and virtual devices.</p><p><a href=#image-5d0253f93318b2c5510a6849a92a0526 class=lightbox-link><img src=/new-docs/toolstack/features/events/xapi-xenopsd-events.png alt="Connection of events between XAPI, xenopsd and xenstore, with main functions and data structures responsible for receiving and sending them" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-5d0253f93318b2c5510a6849a92a0526><img src=/new-docs/toolstack/features/events/xapi-xenopsd-events.png alt="Connection of events between XAPI, xenopsd and xenstore, with main functions and data structures responsible for receiving and sending them" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><h2 id=xapi>Xapi</h2><h3 id=sending-events-from-the-xenapi>Sending events from the xenapi</h3><p>A xenapi user client, such as XenCenter, the xe-cli or a python script,
can register to receive events from XAPI for specific objects in the
XAPI DB. XAPI will generate events for those registered clients whenever
the corresponding XAPI DB object changes.</p><p><a href=#image-4c98de4b2c4c6bea0ce47eea830ae28f class=lightbox-link><img src=/new-docs/toolstack/features/events/sending-events-from-xapi.png alt="Sending events from the xenapi" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-4c98de4b2c4c6bea0ce47eea830ae28f><img src=/new-docs/toolstack/features/events/sending-events-from-xapi.png alt="Sending events from the xenapi" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>This small python scripts shows how to register a simple event watch
loop for XAPI:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> XenAPI
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> XenAPI<span style=color:#f92672>.</span>Session(<span style=color:#e6db74>&#34;http://xshost&#34;</span>)
</span></span><span style=display:flex><span>session<span style=color:#f92672>.</span>login_with_password(<span style=color:#e6db74>&#34;username&#34;</span>,<span style=color:#e6db74>&#34;password&#34;</span>)
</span></span><span style=display:flex><span>session<span style=color:#f92672>.</span>xenapi<span style=color:#f92672>.</span>event<span style=color:#f92672>.</span>register([<span style=color:#e6db74>&#34;VM&#34;</span>,<span style=color:#e6db74>&#34;pool&#34;</span>]) <span style=color:#75715e># register for events in the pool and VM objects                                                </span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    events <span style=color:#f92672>=</span> session<span style=color:#f92672>.</span>xenapi<span style=color:#f92672>.</span>event<span style=color:#f92672>.</span>next() <span style=color:#75715e># block until a xapi event on a xapi DB object is available</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> event <span style=color:#f92672>in</span> events:
</span></span><span style=display:flex><span>      print <span style=color:#e6db74>&#34;received event op=</span><span style=color:#e6db74>%s</span><span style=color:#e6db74> class=</span><span style=color:#e6db74>%s</span><span style=color:#e6db74> ref=</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> (event[<span style=color:#e6db74>&#39;operation&#39;</span>], event[<span style=color:#e6db74>&#39;class&#39;</span>], event[<span style=color:#e6db74>&#39;ref&#39;</span>])                                      
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> event[<span style=color:#e6db74>&#39;class&#39;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;vm&#39;</span> <span style=color:#f92672>and</span> event[<span style=color:#e6db74>&#39;operatoin&#39;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;mod&#39;</span>:
</span></span><span style=display:flex><span>        vm <span style=color:#f92672>=</span> event[<span style=color:#e6db74>&#39;snapshot&#39;</span>]
</span></span><span style=display:flex><span>        print <span style=color:#e6db74>&#34;xapi-event on vm: vm_uuid=</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, power_state=</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>, current_operation=</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> (vm[<span style=color:#e6db74>&#39;uuid&#39;</span>],vm[<span style=color:#e6db74>&#39;name_label&#39;</span>],vm[<span style=color:#e6db74>&#39;power_state&#39;</span>],vm[<span style=color:#e6db74>&#39;current_operations&#39;</span>]<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>except</span> XenAPI<span style=color:#f92672>.</span>Failure, e:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(e<span style=color:#f92672>.</span>details) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> e<span style=color:#f92672>.</span>details[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;EVENTS_LOST&#39;</span>:
</span></span><span style=display:flex><span>      session<span style=color:#f92672>.</span>xenapi<span style=color:#f92672>.</span>event<span style=color:#f92672>.</span>unregister([<span style=color:#e6db74>&#34;VM&#34;</span>,<span style=color:#e6db74>&#34;pool&#34;</span>])
</span></span><span style=display:flex><span>      session<span style=color:#f92672>.</span>xenapi<span style=color:#f92672>.</span>event<span style=color:#f92672>.</span>register([<span style=color:#e6db74>&#34;VM&#34;</span>,<span style=color:#e6db74>&#34;pool&#34;</span>])</span></span></code></pre></div><p> </p><h3 id=receiving-events-from-xenopsd>Receiving events from xenopsd</h3><p>Xapi receives all events from xenopsd via the function
xapi_xenops.events_watch() in its own independent thread. This is a
single-threaded function that is responsible for handling all of the
signals sent by xenopsd. In some situations with lots of VMs and virtual
devices such as VBDs, this loop may saturate a single dom0 vcpu, which
will slow down handling all of the xenopsd events and may cause the
xenopsd signals to accumulate unboundedly in the worst case in the
updates queue in xenopsd (see Figure 1).</p><p>The function xapi_xenops.events_watch() calls
xenops_client.UPDATES.get() to obtain a list of (barrier, 
barrier_events), and then it process each one of the barrier_event,
which can be one of the following events:</p><ul><li><strong>Vm id:</strong> something changed in this VM,
run xapi_xenops.update_vm() to query xenopsd about its state. The
function update_vm() will update power_state, allowed_operations,
console and guest_agent state in the xapi DB.</li><li><strong>Vbd id:</strong> something changed in this VM,
run xapi_xenops.update_vbd() to query xenopsd about its state. The
function update_vbd() will update currently_attached and connected
in the xapi DB.</li><li><strong>Vif id:</strong> something changed in this VM,
run xapi_xenops.update_vif() to query xenopsd about its state. The
function update_vif() will update activate and plugged state of in
the xapi DB.</li><li><strong>Pci id:</strong> something changed in this VM,
run xapi_xenops.update_pci() to query xenopsd about its state.</li><li><strong>Vgpu id:</strong> something changed in this VM,
run xapi_xenops.update_vgpu() to query xenopsd about its state.</li><li><strong>Task id:</strong> something changed in this VM,
run xapi_xenops.update_task() to query xenopsd about its state.
The function update_task() will update the progress of the task in
the xapi DB using the information of the task in xenopsd.</li></ul><p><a href=#image-32998a105e2a376a6f85df4e6875700f class=lightbox-link><img src=/new-docs/toolstack/features/events/receiving-events-from-xenopsd.png alt="Receiving events from xenopsd" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-32998a105e2a376a6f85df4e6875700f><img src=/new-docs/toolstack/features/events/receiving-events-from-xenopsd.png alt="Receiving events from xenopsd" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>All the xapi_xenops.update_X() functions above will call
Xenopsd_client.X.stat() functions to obtain the current state of X from
xenopsd:</p><p><a href=#image-321f49af102ec81e5630eae3b771e8db class=lightbox-link><img src=/new-docs/toolstack/features/events/obtaining-current-state.png alt="Obtaining current state" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-321f49af102ec81e5630eae3b771e8db><img src=/new-docs/toolstack/features/events/obtaining-current-state.png alt="Obtaining current state" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>There are a couple of optimisations while processing the events in
xapi_xenops.events_watch():</p><ul><li>if an event X=(vm_id,dev_id) (eg. Vbd dev_id) has already been
processed in a barrier_events, it&rsquo;s not processed again. A typical
value for X is eg. &ldquo;&lt;vm_uuid>.xvda&rdquo; for a VBD.</li><li>if Events_from_xenopsd.are_supressed X, then this event
is ignored. Events are supressed if VM X.vm_id is migrating away
from the host</li></ul><h4 id=barriers>Barriers</h4><p>When xapi needs to execute (and to wait for events indicating completion
of) a xapi operation (such as VM.start and VM.shutdown) containing many
xenopsd sub-operations (such as VM.start – to force xenopsd to change
the VM power_state, and VM.stat, VBD.stat, VIF.stat etc – to force the
xapi DB to catch up with the xenopsd new state for these objects), xapi
sends to the xenopsd input queue a barrier, indicating that xapi will
then block and only continue execution of the barred operation when
xenopsd returns the barrier. The barrier should only be returned when
xenopsd has finished the execution of all the operations requested by
xapi (such as VBD.stat and VM.stat in order to update the state of the
VM in the xapi database after a VM.start has been issued to xenopsd). </p><p>A recent problem has been detected in the xapi_xenops.events_watch() 
function: when it needs to process many VM_check_state events, this
may push for later the processing of barriers associated with a
VM.start, delaying xapi in reporting (via a xapi event) that the VM
state in the xapi DB has reached the running power_state. This needs
further debugging, and is probably one of the reasons in CA-87377 why in
some conditions a xapi event reporting that the VM power_state is
running (causing it to go from yellow to green state in XenCenter) is
taking so long to be returned, way after the VM is already running.</p><h2 id=xenopsd>Xenopsd</h2><p>Xenopsd has a few queues that are used by xapi to store commands to be
executed (eg. VBD.stat) and update events to be picked up by xapi. The
main ones, easily seen at runtime by running the following command in
dom0, are:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># xenops-cli diagnostics --queue=org.xen.xapi.xenops.classic</span>
</span></span><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>   queues: <span style=color:#f92672>[</span>  <span style=color:#75715e># XENOPSD INPUT QUEUE</span>
</span></span><span style=display:flex><span>            ... stuff that still needs to be processed by xenopsd
</span></span><span style=display:flex><span>            VM.stat
</span></span><span style=display:flex><span>            VBD.stat
</span></span><span style=display:flex><span>            VM.start
</span></span><span style=display:flex><span>            VM.shutdown
</span></span><span style=display:flex><span>            VIF.plug
</span></span><span style=display:flex><span>            etc
</span></span><span style=display:flex><span>           <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>   workers: <span style=color:#f92672>[</span> <span style=color:#75715e># XENOPSD WORKER THREADS</span>
</span></span><span style=display:flex><span>            ... which stuff each worker thread is processing
</span></span><span style=display:flex><span>   <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>   updates: <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>     updates: <span style=color:#f92672>[</span> <span style=color:#75715e># XENOPSD OUTPUT QUEUE</span>
</span></span><span style=display:flex><span>            ... signals from xenopsd that need to be picked up by xapi
</span></span><span style=display:flex><span>               VM_check_state
</span></span><span style=display:flex><span>               VBD_check_state
</span></span><span style=display:flex><span>               etc
</span></span><span style=display:flex><span>        <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>}</span> tasks: <span style=color:#f92672>[</span> <span style=color:#75715e># XENOPSD TASKS</span>
</span></span><span style=display:flex><span>               ... state of each known task, before they are manually deleted after completion of the task
</span></span><span style=display:flex><span>               <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div><h3 id=sending-events-to-xapi>Sending events to xapi</h3><p>Whenever xenopsd changes the state of a XenServer object such as a VBD
or VM, or when it receives an event from xenstore indicating that the
states of these objects have changed (perhaps because either a guest or
the dom0 backend changed the state of a virtual device), it creates a
signal for the corresponding object (VM_check_state, VBD_check_state
etc) and send it up to xapi. Xapi will then process this event in its
xapi_xenops.events_watch() function.</p><p><a href=#image-301f145a9d690e7693acfb98a183522a class=lightbox-link><img src=/new-docs/toolstack/features/events/sending-events-to-xapi.png alt="Sending events to xapi" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-301f145a9d690e7693acfb98a183522a><img src=/new-docs/toolstack/features/events/sending-events-to-xapi.png alt="Sending events to xapi" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>These signals may need to wait a long time to be processed if the
single-threaded xapi_xenops.events_watch() function is having
difficulties (ie taking a long time) to process previous signals in the
UPDATES queue from xenopsd.  </p><h3 id=receiving-events-from-xenstore>Receiving events from xenstore</h3><p>Xenopsd watches a number of keys in xenstore, both in dom0 and in each
guest. Xenstore is responsible to send watch events to xenopsd whenever
the watched keys change state. Xenopsd uses a xenstore client library to
make it easier to create a callback function that is called whenever
xenstore sends these events.</p><p><a href=#image-1d4b5a22ea985825667ddd865bdaca28 class=lightbox-link><img src=/new-docs/toolstack/features/events/receiving-events-from-xenstore.png alt="Receiving events from xenstore" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-1d4b5a22ea985825667ddd865bdaca28><img src=/new-docs/toolstack/features/events/receiving-events-from-xenstore.png alt="Receiving events from xenstore" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Xenopsd also needs to complement sometimes these watch events with
polling of some values. An example is the @introduceDomain event in
xenstore (handled in xenopsd/xc/xenstore_watch.ml), which indicates
that a new VM has been created. This event unfortunately does not
indicate the domid of the VM, and xenopsd needs to query Xen (via libxc)
which domains are now available in the host and compare with the
previous list of known domains, in order to figure out the domid of the
newly introduced domain.</p><p> It is not good practice to poll xenstore for changes of values. This
will add a large overhead to both xenstore and xenopsd, and decrease the
scalability of XenServer in terms of number of VMs/host and virtual
devices per VM. A much better approach is to rely on the watch events of
xenstore to indicate when a specific value has changed in xenstore.</p><h2 id=xenstore>Xenstore</h2><h3 id=sending-events-to-xenstore-clients>Sending events to xenstore clients</h3><p>If a xenstore client has created watch events for a key, then xenstore
will send events to this client whenever this key changes state.</p><h3 id=receiving-events-from-xenstore-clients>Receiving events from xenstore clients</h3><p>Xenstore clients indicate to xenstore that something state changed by
writing to some xenstore key. This may or may not cause xenstore to
create watch events for the corresponding key, depending on if other
xenstore clients have watches on this key.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=high-availability>High-Availability</h1><p>High-Availability (HA) tries to keep VMs running, even when there are hardware
failures in the resource pool, when the admin is not present. Without HA
the following may happen:</p><ul><li>during the night someone spills a cup of coffee over an FC switch; then</li><li>VMs running on the affected hosts will lose access to their storage; then</li><li>business-critical services will go down; then</li><li>monitoring software will send a text message to an off-duty admin; then</li><li>the admin will travel to the office and fix the problem by restarting
the VMs elsewhere.</li></ul><p>With HA the following will happen:</p><ul><li>during the night someone spills a cup of coffee over an FC switch; then</li><li>VMs running on the affected hosts will lose access to their storage; then</li><li>business-critical services will go down; then</li><li>the HA software will determine which hosts are affected and shut them down; then</li><li>the HA software will restart the VMs on unaffected hosts; then</li><li>services are restored; then <em>on the next working day</em></li><li>the admin can arrange for the faulty switch to be replaced.</li></ul><p>HA is designed to handle an emergency and allow the admin time to fix
failures properly.</p><h1 id=example>Example</h1><p>The following diagram shows an HA-enabled pool, before and after a network
link between two hosts fails.</p><p><a href=#image-ac5cff8270de9a5f099e22e7aeb2fa7f class=lightbox-link><img src=/new-docs/toolstack/features/HA/ha.png alt="High-Availability in action" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-ac5cff8270de9a5f099e22e7aeb2fa7f><img src=/new-docs/toolstack/features/HA/ha.png alt="High-Availability in action" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>When HA is enabled, all hosts in the pool</p><ul><li>exchange periodic heartbeat messages over the network</li><li>send heartbeats to a shared storage device.</li><li>attempt to acquire a &ldquo;master lock&rdquo; on the shared storage.</li></ul><p>HA is designed to recover as much as possible of the pool after a single failure
i.e. it removes single points of failure. When some subset of the pool suffers
a failure then the remaining pool members</p><ul><li>figure out whether they are in the largest fully-connected set (the
&ldquo;liveset&rdquo;);<ul><li>if they are not in the largest set then they &ldquo;fence&rdquo; themselves (i.e.
force reboot via the hypervisor watchdog)</li></ul></li><li>elect a master using the &ldquo;master lock&rdquo;</li><li>restart all lost VMs.</li></ul><p>After HA has recovered a pool, it is important that the original failure is
addressed because the remaining pool members may not be able to cope with
any more failures.</p><h1 id=design>Design</h1><p>HA must never violate the following safety rules:</p><ol><li>there must be at most one master at all times. This is because the master
holds the VM and disk locks.</li><li>there must be at most one instance of a particular VM at all times. This
is because starting the same VM twice will result in severe filesystem
corruption.</li></ol><p>However to be useful HA must:</p><ul><li>detect failures quickly;</li><li>minimise the number of false-positives in the failure detector; and</li><li>make the failure handling logic as robust as possible.</li></ul><p>The implementation difficulty arises when trying to be both useful and safe
at the same time.</p><h2 id=terminology>Terminology</h2><p>We use the following terminology:</p><ul><li><em>fencing</em>: also known as I/O fencing, refers to the act of isolating a
host from network and storage. Once a host has been fenced, any VMs running
there cannot generate side-effects observable to a third party. This means
it is safe to restart the running VMs on another node without violating the
safety-rule and running the same VM simultaneously in two locations.</li><li><em>heartbeating</em>: exchanging status updates with other hosts at regular
pre-arranged intervals. Heartbeat messages reveal that hosts are alive
and that I/O paths are working.</li><li><em>statefile</em>: a shared disk (also known as a &ldquo;quorum disk&rdquo;) on the &ldquo;Heartbeat&rdquo;
SR which is mapped as a block device into every host&rsquo;s domain 0. The shared
disk acts both as a channel for heartbeat messages and also as a building
block of a Pool master lock, to prevent multiple hosts becoming masters in
violation of the safety-rule (a dangerous situation also known as
&ldquo;split-brain&rdquo;).</li><li><em>management network</em>: the network over which the XenAPI XML/RPC requests
flow and also used to send heartbeat messages.</li><li><em>liveset</em>: a per-Host view containing a subset of the Hosts in the Pool
which are considered by that Host to be alive i.e. responding to XenAPI
commands and running the VMs marked as <code>resident_on</code> there. When a Host <code>b</code>
leaves the liveset as seen by Host <code>a</code> it is safe for Host <code>a</code> to assume
that Host <code>b</code> has been fenced and to take recovery actions (e.g. restarting
VMs), without violating either of the safety-rules.</li><li><em>properly shared SR</em>: an SR which has field <code>shared=true</code>; and which has a
<code>PBD</code> connecting it to every <code>enabled</code> Host in the Pool; and where each of
these <code>PBD</code>s has field <code>currently_attached</code> set to true. A VM whose disks
are in a properly shared SR could be restarted on any <code>enabled</code> Host,
memory and network permitting.</li><li><em>properly shared Network</em>: a Network which has a <code>PIF</code> connecting it to
every <code>enabled</code> Host in the Pool; and where each of these <code>PIF</code>s has
field <code>currently_attached</code> set to true. A VM whose VIFs connect to
properly shared Networks could be restarted on any <code>enabled</code> Host,
memory and storage permitting.</li><li><em>agile</em>: a VM is said to be agile if all disks are in properly shared SRs
and all network interfaces connect to properly shared Networks.</li><li><em>unprotected</em>: an unprotected VM has field <code>ha_always_run</code> set to false
and will never be restarted automatically on failure
or have reconfiguration actions blocked by the HA overcommit protection.</li><li><em>best-effort</em>: a best-effort VM has fields <code>ha_always_run</code> set to true and
<code>ha_restart_priority</code> set to best-effort.
A best-effort VM will only be restarted if (i) the failure is directly
observed; and (ii) capacity exists for an immediate restart.
No more than one restart attempt will ever be made.</li><li><em>protected</em>: a VM is said to be protected if it will be restarted by HA
i.e. has field <code>ha_always_run</code> set to true and
field <code>ha_restart_priority</code> not set to `best-effort.</li><li><em>survival rule 1</em>: describes the situation where hosts survive
because they are in the largest network partition with statefile access.
This is the normal state of the <code>xhad</code> daemon.</li><li><em>survival rule 2</em>: describes the situation where <em>all</em> hosts have lost
access to the statefile but remain alive
while they can all see each-other on the network. In this state any further
failure will cause all nodes to self-fence.
This state is intended to cope with the system-wide temporary loss of the
storage service underlying the statefile.</li></ul><h2 id=assumptions>Assumptions</h2><p>We assume:</p><ul><li>All I/O used for monitoring the health of hosts (i.e. both storage and
network-based heartbeating) is along redundant paths, so that it survives
a single hardware failure (e.g. a broken switch or an accidentally-unplugged
cable). It is up to the admin to ensure their environment is setup correctly.</li><li>The hypervisor watchdog mechanism will be able to guarantee the isolation
of nodes, once communication has been lost, within a pre-arranged time
period. Therefore no active power fencing equipment is required.</li><li>VMs may only be marked as <em>protected</em> if they are fully <em>agile</em> i.e. able
to run on any host, memory permitting. No additional constraints of any kind
may be specified e.g. it is not possible to make &ldquo;CPU reservations&rdquo;.</li><li>Pools are assumed to be homogenous with respect to CPU type and presence of
VT/SVM support (also known as &ldquo;HVM&rdquo;). If a Pool is created with
non-homogenous hosts using the <code>--force</code> flag then the additional
constraints will not be noticed by the VM failover planner resulting in
runtime failures while trying to execute the failover plans.</li><li>No attempt will ever be made to shutdown or suspend &ldquo;lower&rdquo; priority VMs
to guarantee the survival of &ldquo;higher&rdquo; priority VMs.</li><li>Once HA is enabled it is not possible to reconfigure the management network
or the SR used for storage heartbeating.</li><li>VMs marked as <em>protected</em> are considered to have failed if they are offline
i.e. the VM failure handling code is level-sensitive rather than
edge-sensitive.</li><li>VMs marked as <em>best-effort</em> are considered to have failed only when the host
where they are resident is declared offline
i.e. the best-effort VM failure handling code is edge-sensitive rather than
level-sensitive.
A single restart attempt is attempted and if this fails no further start is
attempted.</li><li>HA can only be enabled if all Pool hosts are online and actively responding
to requests.</li><li>when HA is enabled the database is configured to write all updates to
the &ldquo;Heartbeat&rdquo; SR, guaranteeing that VM configuration changes are not lost
when a host fails.</li></ul><h2 id=components>Components</h2><p>The implementation is split across the following components:</p><ul><li><a href=https://github.com/xenserver/xha target=_blank>xhad</a>: the cluster membership daemon
maintains a quorum of hosts through network and storage heartbeats</li><li><a href=https://github.com/xapi-project/xen-api target=_blank>xapi</a>: used to configure the
HA policy i.e. which network and storage to use for heartbeating and which
VMs to restart after a failure.</li><li><a href=http://xenproject.org/ target=_blank>xen</a>: the Xen watchdog is used to reliably
fence the host when the host has been (partially or totally) isolated
from the cluster</li></ul><p>To avoid a &ldquo;split-brain&rdquo;, the cluster membership daemon must &ldquo;fence&rdquo; (i.e.
isolate) nodes when they are not part of the cluster. In general there are
2 approaches:</p><ul><li>cut the power of remote hosts which you can&rsquo;t talk to on the network
any more. This is the approach taken by most open-source clustering
software since it is simpler. However it has the downside of requiring
the customer buy more hardware and set it up correctly.</li><li>rely on the remote hosts using a watchdog to cut their own power (i.e.
halt or reboot) after a timeout. This relies on the watchdog being
reliable. Most other people <a href=https://www.suse.com/documentation/sle_ha/singlehtml/book_sleha/book_sleha.html target=_blank>don&rsquo;t trust the Linux watchdog</a>;
after all the Linux kernel is highly threaded, performs a lot of (useful)
functions and kernel bugs which result in deadlocks do happen.
We use the Xen watchdog because we believe that the Xen hypervisor is
simple enough to reliably fence the host (via triggering a reboot of
domain 0 which then triggers a host reboot).</li></ul><h1 id=xhad>xhad</h1><p><a href=https://github.com/xenserver/xha target=_blank>xhad</a> is the cluster membership daemon:
it exchanges heartbeats with the other nodes to determine which nodes are
still in the cluster (the &ldquo;live set&rdquo;) and which nodes have <em>definitely</em>
failed (through watchdog fencing). When a host has definitely failed, xapi
will unlock all the disks and restart the VMs according to the HA policy.</p><p>Since Xapi is a critical part of the system, the xhad also acts as a
Xapi watchdog. It polls Xapi every few seconds and checks if Xapi can
respond. If Xapi seems to have failed then xhad will restart it. If restarts
continue to fail then xhad will consider the host to have failed and
self-fence.</p><p>xhad is configured via a simple config file written on each host in
<code>/etc/xensource/xhad.conf</code>. The file must be identical on each host
in the cluster. To make changes to the file, HA must be disabled and then
re-enabled afterwards. Note it may not be possible to re-enable HA depending
on the configuration change (e.g. if a host has been added but that host has
a broken network configuration then this will block HA enable).</p><p>The xhad.conf file is written in XML and contains</p><ul><li>pool-wide configuration: this includes a list of all hosts which should
be in the liveset and global timeout information</li><li>local host configuration: this identifies the local host and described
which local network interface and block device to use for heartbeating.</li></ul><p>The following is an example xhad.conf file:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#75715e>&lt;?xml version=&#34;1.0&#34; encoding=&#34;utf-8&#34;?&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;xhad-config</span> <span style=color:#a6e22e>version=</span><span style=color:#e6db74>&#34;1.0&#34;</span><span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>&lt;!--pool-wide configuration--&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;common-config&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;GenerationUUID&gt;</span>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx<span style=color:#f92672>&lt;/GenerationUUID&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;UDPport&gt;</span>694<span style=color:#f92672>&lt;/UDPport&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>&lt;!--for each host, specify host UUID, and IP address--&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;host&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HostID&gt;</span>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx<span style=color:#f92672>&lt;/HostID&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;IPaddress&gt;</span>xxx.xxx.xxx.xx1<span style=color:#f92672>&lt;/IPaddress&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/host&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;host&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HostID&gt;</span>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx<span style=color:#f92672>&lt;/HostID&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;IPaddress&gt;</span>xxx.xxx.xxx.xx2<span style=color:#f92672>&lt;/IPaddress&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/host&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;host&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HostID&gt;</span>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx<span style=color:#f92672>&lt;/HostID&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;IPaddress&gt;</span>xxx.xxx.xxx.xx3<span style=color:#f92672>&lt;/IPaddress&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/host&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>&lt;!--optional parameters [sec] --&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;parameters&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HeartbeatInterval&gt;</span>4<span style=color:#f92672>&lt;/HeartbeatInterval&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HeartbeatTimeout&gt;</span>30<span style=color:#f92672>&lt;/HeartbeatTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;StateFileInterval&gt;</span>4<span style=color:#f92672>&lt;/StateFileInterval&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;StateFileTimeout&gt;</span>30<span style=color:#f92672>&lt;/StateFileTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HeartbeatWatchdogTimeout&gt;</span>30<span style=color:#f92672>&lt;/HeartbeatWatchdogTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;StateFileWatchdogTimeout&gt;</span>45<span style=color:#f92672>&lt;/StateFileWatchdogTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;BootJoinTimeout&gt;</span>90<span style=color:#f92672>&lt;/BootJoinTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;EnableJoinTimeout&gt;</span>90<span style=color:#f92672>&lt;/EnableJoinTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;XapiHealthCheckInterval&gt;</span>60<span style=color:#f92672>&lt;/XapiHealthCheckInterval&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;XapiHealthCheckTimeout&gt;</span>10<span style=color:#f92672>&lt;/XapiHealthCheckTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;XapiRestartAttempts&gt;</span>1<span style=color:#f92672>&lt;/XapiRestartAttempts&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;XapiRestartTimeout&gt;</span>30<span style=color:#f92672>&lt;/XapiRestartTimeout&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;XapiLicenseCheckTimeout&gt;</span>30<span style=color:#f92672>&lt;/XapiLicenseCheckTimeout&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/parameters&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;/common-config&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>&lt;!--local host configuration--&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;local-config&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;localhost&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HostID&gt;</span>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx2<span style=color:#f92672>&lt;/HostID&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HeartbeatInterface&gt;</span> xapi1<span style=color:#f92672>&lt;/HeartbeatInterface&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;HeartbeatPhysicalInterface&gt;</span>bond0<span style=color:#f92672>&lt;/HeartbeatPhysicalInterface&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;StateFile&gt;</span>/dev/statefiledevicename<span style=color:#f92672>&lt;/StateFile&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/localhost&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;/local-config&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;/xhad-config&gt;</span></span></span></code></pre></div><p>The fields have the following meaning:</p><ul><li>GenerationUUID: a UUID generated each time HA is reconfigured. This allows
xhad to tell an old host which failed; had been removed from the
configuration; repaired and then restarted that the world has changed
while it was away.</li><li>UDPport: the port number to use for network heartbeats. It&rsquo;s important
to allow this traffic through the firewall and to make sure the same
port number is free on all hosts (beware of portmap services occasionally
binding to it).</li><li>HostID: a UUID identifying a host in the pool. We would normally use
xapi&rsquo;s notion of a host uuid.</li><li>IPaddress: any IP address on the remote host. We would normally use
xapi&rsquo;s notion of a management network.</li><li>HeartbeatTimeout: if a heartbeat packet is not received for this many
seconds, then xhad considers the heartbeat to have failed. This is
the user-supplied &ldquo;HA timeout&rdquo; value, represented below as <code>T</code>.
<code>T</code> must be bigger than 10; we would normally use 60s.</li><li>StateFileTimeout: if a storage update is not seen for a host for this
many seconds, then xhad considers the storage heartbeat to have failed.
We would normally use the same value as the HeartbeatTimeout <code>T</code>.</li><li>HeartbeatInterval: interval between heartbeat packets sent. We would
normally use a value <code>2 &lt;= t &lt;= 6</code>, derived from the user-supplied
HA timeout via <code>t = (T + 10) / 10</code></li><li>StateFileInterval: interval betwen storage updates (also known as
&ldquo;statefile updates&rdquo;). This would normally be set to the same value as
HeartbeatInterval.</li><li>HeartbeatWatchdogTimeout: If the host does not send a heartbeat for this
amount of time then the host self-fences via the Xen watchdog. We normally
set this to <code>T</code>.</li><li>StateFileWatchdogTimeout: If the host does not update the statefile for
this amount of time then the host self-fences via the Xen watchdog. We
normally set this to <code>T+15</code>.</li><li>BootJoinTimeout: When the host is booting and joining the liveset (i.e.
the cluster), consider the join a failure if it takes longer than this
amount of time. We would normally set this to <code>T+60</code>.</li><li>EnableJoinTimeout: When the host is enabling HA for the first time,
consider the enable a failure if it takes longer than this amount of time.
We would normally set this to <code>T+60</code>.</li><li>XapiHealthCheckInterval: Interval between &ldquo;health checks&rdquo; where we run
a script to check whether Xapi is responding or not.</li><li>XapiHealthCheckTimeout: Number of seconds to wait before assuming that
Xapi has deadlocked during a &ldquo;health check&rdquo;.</li><li>XapiRestartAttempts: Number of Xapi restarts to attempt before concluding
Xapi has permanently failed.</li><li>XapiRestartTimeout: Number of seconds to wait for a Xapi restart to
complete before concluding it has failed.</li><li>XapiLicenseCheckTimeout: Number of seconds to wait for a Xapi license
check to complete before concluding that xhad should terminate.</li></ul><p>In addition to the config file, Xhad exposes a simple control API which
is exposed as scripts:</p><ul><li><code>ha_set_pool_state (Init | Invalid)</code>: sets the global pool state to &ldquo;Init&rdquo; (before starting
HA) or &ldquo;Invalid&rdquo; (causing all other daemons who can see the statefile to
shutdown)</li><li><code>ha_start_daemon</code>: if the pool state is &ldquo;Init&rdquo; then the daemon will
attempt to contact other daemons and enable HA. If the pool state is
&ldquo;Active&rdquo; then the host will attempt to join the existing liveset.</li><li><code>ha_query_liveset</code>: returns the current state of the cluster.</li><li><code>ha_propose_master</code>: returns whether the current node has been
elected pool master.</li><li><code>ha_stop_daemon</code>: shuts down the xhad on the local host. Note this
will not disarm the Xen watchdog by itself.</li><li><code>ha_disarm_fencing</code>: disables fencing on the local host.</li><li><code>ha_set_excluded</code>: when a host is being shutdown cleanly, record the
fact that the VMs have all been shutdown so that this host can be ignored
in future cluster membership calculations.</li></ul><h2 id=fencing>Fencing</h2><p>Xhad continuously monitors whether the host should remain alive, or if
it should self-fence. There are two &ldquo;survival rules&rdquo; which will keep a host
alive; if neither rule applies (or if xhad crashes or deadlocks) then the
host will fence. The rules are:</p><ol><li>Xapi is running; the storage heartbeats are visible; this host is a
member of the &ldquo;best&rdquo; partition (as seen through the storage heartbeats)</li><li>Xapi is running; the storage is inaccessible; all hosts which should
be running (i.e. not those &ldquo;excluded&rdquo; by being cleanly shutdown) are
online and have also lost storage access (as seen through the network
heartbeats).</li></ol><p>where the &ldquo;best&rdquo; partition is the largest one if that is unique, or if there
are multiple partitions of the same size then the one containing the lowest
host uuid is considered best.</p><p>The first survival rule is the &ldquo;normal&rdquo; case. The second rule exists only
to prevent the storage from becoming a single point of failure: all hosts
can remain alive until the storage is repaired. Note that if a host has
failed and has not yet been repaired, then the storage becomes a single
point of failure for the degraded pool. HA removes single point of failures,
but multiple failures can still cause problems. It is important to fix
failures properly after HA has worked around them.</p><h1 id=xapi>xapi</h1><p><a href=https://github.com/xapi-project/xen-api target=_blank>Xapi</a> is responsible for</p><ul><li>exposing an interface for setting HA policy</li><li>creating VDIs (disks) on shared storage for heartbeating and storing
the pool database</li><li>arranging for these disks to be attached on host boot, before the &ldquo;SRmaster&rdquo;
is online</li><li>configuring and managing the <code>xhad</code> heartbeating daemon</li></ul><p>The HA policy APIs include</p><ul><li>methods to determine whether a VM is <em>agile</em> i.e. can be restarted in
principle on any host after a failure</li><li>planning for a user-specified number of host failures and enforcing
access control</li><li>restarting failed <em>protected</em> VMs in policy order</li></ul><p>The HA policy settings are stored in the Pool database which is written
(synchronously)
to a VDI in the same SR that&rsquo;s being used for heartbeating. This ensures
that the database can be recovered after a host fails and the VMs are
recovered.</p><p>Xapi stores 2 settings in its local database:</p><ul><li><em>ha_disable_failover_actions</em>: this is set to false when we want nodes
to be able to recover VMs &ndash; this is the normal case. It is set to true
during the HA disable process to prevent a split-brain forming while
HA is only partially enabled.</li><li><em>ha_armed</em>: this is set to true to tell Xapi to start <code>Xhad</code> during
host startup and wait to join the liveset.</li></ul><h2 id=disks-on-shared-storage>Disks on shared storage</h2><p>The regular disk APIs for creating, destroying, attaching, detaching (etc)
disks need the <code>SRmaster</code> (usually but not always the Pool master) to be
online to allow the disks to be locked. The <code>SRmaster</code> cannot be brought
online until the host has joined the liveset. Therefore we have a
cyclic dependency: joining the liveset needs the statefile disk to be attached
but attaching a disk requires being a member of the liveset already.</p><p>The dependency is broken by adding an explicit &ldquo;unlocked&rdquo; attach storage
API called <code>VDI_ATTACH_FROM_CONFIG</code>. Xapi uses the <code>VDI_GENERATE_CONFIG</code> API
during the HA enable operation and stores away the result. When the system
boots the <code>VDI_ATTACH_FROM_CONFIG</code> is able to attach the disk without the
SRmaster.</p><h2 id=the-role-of-hostenabled>The role of Host.enabled</h2><p>The <code>Host.enabled</code> flag is used to mean, &ldquo;this host is ready to start VMs and
should be included in failure planning&rdquo;.
The VM restart planner assumes for simplicity that all <em>protected</em> VMs can
be started anywhere; therefore all involved networks and storage must be
<em>properly shared</em>.
If a host with an unplugged <code>PBD</code> were to become enabled then the corresponding
<code>SR</code> would cease to be <em>properly shared</em>, all the VMs would cease to be
<em>agile</em> and the VM restart logic would fail.</p><p>To ensure the VM restart logic always works, great care is taken to make
sure that Hosts may only become enabled when their networks and storage are
properly configured. This is achieved by:</p><ul><li>when the master boots and initialises its database it sets all Hosts to
dead and disabled and then signals the HA background thread
(<a href=https://github.com/xapi-project/xen-api/blob/0bbd4f5ac5fe46f9e982e5d5587ac56ed8427295/ocaml/xapi/xapi_ha.ml#L627 target=_blank>signal_database_state_valid</a>)
to wake up from sleep and
start processing liveset information (and potentially setting hosts to live)</li><li>when a slave calls
<a href=https://github.com/xapi-project/xen-api/blob/0bbd4f5ac5fe46f9e982e5d5587ac56ed8427295/ocaml/xapi/xapi_pool.ml#L1019 target=_blank>Pool.hello</a>
(i.e. after the slave has rebooted),
the master sets it to disabled, allowing it a grace period to plug in its
storage;</li><li>when a host (master or slave) successfully plugs in its networking and
storage it calls
<a href=https://github.com/xapi-project/xen-api/blob/0bbd4f5ac5fe46f9e982e5d5587ac56ed8427295/ocaml/xapi/xapi_host_helpers.ml#L193 target=_blank>consider_enabling_host</a>
which checks that the
preconditions are met and then sets the host to enabled; and</li><li>when a slave notices its database connection to the master restart
(i.e. after the master <code>xapi</code> has just restarted) it calls
<code>consider_enabling_host}</code></li></ul><h2 id=the-steady-state>The steady-state</h2><p>When HA is enabled and all hosts are running normally then each calls
<code>ha_query_liveset</code> every 10s.</p><p>Slaves check to see if the host they believe is the master is alive and has
the master lock. If another node has become master then the slave will
rewrite its <code>pool.conf</code> and restart. If no node is the master then the
slave will call
<a href=https://github.com/xapi-project/xen-api/blob/0bbd4f5ac5fe46f9e982e5d5587ac56ed8427295/ocaml/xapi/xapi_ha.ml#L129 target=_blank>on_master_failure</a>,
proposing itself and, if it is rejected,
checking the liveset to see which node acquired the lock.</p><p>The master monitors the liveset and updates the <code>Host_metrics.live</code> flag
of every host to reflect the liveset value. For every host which is not in
the liveset (i.e. has fenced) it enumerates all resident VMs and marks them
as <code>Halted</code>. For each protected VM which is not running, the master computes
a VM restart plan and attempts to execute it. If the plan fails then a
best-effort <code>VM.start</code> call is attempted. Finally an alert is generated if
the VM could not be restarted.</p><p>Note that XenAPI heartbeats are still sent when HA is enabled, even though
they are not used to drive the values of the <code>Host_metrics.live</code> field.
Note further that, when a host is being shutdown, the host is immediately
marked as dead and its host reference is added to a list used to prevent the
<code>Host_metrics.live</code> being accidentally reset back to live again by the
asynchronous liveset query. The Host reference is removed from the list when
the host restarts and calls <code>Pool.hello</code>.</p><h2 id=planning-and-overcommit>Planning and overcommit</h2><p>The VM failover planning code is sub-divided into two pieces, stored in
separate files:</p><ul><li><a href=https://github.com/xapi-project/xen-api/blob/0bbd4f5ac5fe46f9e982e5d5587ac56ed8427295/ocaml/xapi/binpack.ml target=_blank>binpack.ml</a>: contains two algorithms for packing items of different sizes
(i.e. VMs) into bins of different sizes (i.e. Hosts); and</li><li><a href=https://github.com/xapi-project/xen-api/blob/0bbd4f5ac5fe46f9e982e5d5587ac56ed8427295/ocaml/xapi/xapi_ha_vm_failover.ml target=_blank>xapi_ha_vm_failover.ml</a>: interfaces between the Pool database and the
binpacker; also performs counterfactual reasoning for overcommit protection.</li></ul><p>The input to the binpacking algorithms are configuration values which
represent an abstract view of the Pool:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>type</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>&#39;</span>a<span style=color:#f92672>,</span> <span style=color:#66d9ef>&#39;</span>b<span style=color:#f92672>)</span> configuration <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  hosts<span style=color:#f92672>:</span>        <span style=color:#f92672>(</span><span style=color:#66d9ef>&#39;</span>a <span style=color:#f92672>*</span> int64<span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span> <span style=color:#75715e>(** a list of live hosts and free memory *)</span>
</span></span><span style=display:flex><span>  vms<span style=color:#f92672>:</span>          <span style=color:#f92672>(</span><span style=color:#66d9ef>&#39;</span>b <span style=color:#f92672>*</span> int64<span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span> <span style=color:#75715e>(** a list of VMs and their memory requirements *)</span>
</span></span><span style=display:flex><span>  placement<span style=color:#f92672>:</span>    <span style=color:#f92672>(</span><span style=color:#66d9ef>&#39;</span>b <span style=color:#f92672>*</span> <span style=color:#66d9ef>&#39;</span>a<span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>    <span style=color:#75715e>(** current VM locations *)</span>
</span></span><span style=display:flex><span>  total_hosts<span style=color:#f92672>:</span>  <span style=color:#66d9ef>int</span><span style=color:#f92672>;</span>               <span style=color:#75715e>(** total number of hosts in the pool &#39;n&#39; *)</span>
</span></span><span style=display:flex><span>  num_failures<span style=color:#f92672>:</span> <span style=color:#66d9ef>int</span><span style=color:#f92672>;</span>               <span style=color:#75715e>(** number of failures to tolerate &#39;r&#39; *)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div><p>Note that:</p><ul><li>the memory required by the VMs listed in <code>placement</code> has already been
substracted from the total memory of the hosts; it doesn&rsquo;t need to be
subtracted again.</li><li>the free memory of each host has already had per-host miscellaneous
overheads subtracted from it, including that used by unprotected VMs,
which do not appear in the VM list.</li><li>the total number of hosts in the pool (<code>total_hosts</code>) is a constant for
any particular invocation of HA.</li><li>the number of failures to tolerate (<code>num_failures</code>) is the user-settable
value from the XenAPI <code>Pool.ha_host_failures_to_tolerate</code>.</li></ul><p>There are two algorithms which satisfy the interface:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>sig</span>
</span></span><span style=display:flex><span>  plan_always_possible<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>&#39;</span>a<span style=color:#f92672>,</span> <span style=color:#66d9ef>&#39;</span>b<span style=color:#f92672>)</span> configuration <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>bool</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  get_specific_plan<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>&#39;</span>a<span style=color:#f92672>,</span> <span style=color:#66d9ef>&#39;</span>b<span style=color:#f92672>)</span> configuration <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>&#39;</span>b <span style=color:#66d9ef>list</span> <span style=color:#f92672>-&gt;</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>&#39;</span>b <span style=color:#f92672>*</span> <span style=color:#66d9ef>&#39;</span>a<span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span></span></span></code></pre></div><p>The function <code>get_specific_plan</code> takes a configuration and a list of Hosts
which have failed. It returns a VM restart plan represented as a VM to Host
association list. This is the function called by the
background HA VM restart thread on the master.</p><p>The function <code>plan_always_possible</code> returns true if every sequence of Host
failures of length
<code>num_failures</code> (irrespective of whether all hosts failed at once, or in
multiple separate episodes)
would result in calls to <code>get_specific_plan</code> which would allow all protected
VMs to be restarted.
This function is heavily used by the overcommit protection logic as well as code in XenCenter which aims to
maximise failover capacity using the counterfactual reasoning APIs:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>Pool.ha_compute_max_host_failures_to_tolerate
</span></span><span style=display:flex><span>Pool.ha_compute_hypothetical_max_host_failures_to_tolerate</span></span></code></pre></div><p>There are two binpacking algorithms: the more detailed but expensive
algorithmm is used for smaller/less
complicated pool configurations while the less detailed, cheaper algorithm
is used for the rest. The
choice between algorithms is based only on <code>total_hosts</code> (<code>n</code>) and
<code>num_failures</code> (<code>r</code>).
Note that the choice of algorithm will only change if the number of Pool
hosts is varied (requiring HA to be disabled and then enabled) or if the
user requests a new <code>num_failures</code> target to plan for.</p><p>The expensive algorithm uses an exchaustive search with a
&ldquo;biggest-fit-decreasing&rdquo; strategy that
takes the biggest VMs first and allocates them to the biggest remaining Host.
The implementation keeps the VMs and Hosts as sorted lists throughout.
There are a number of transformations to the input configuration which are
guaranteed to preserve the existence of a VM to host allocation (even if
the actual allocation is different). These transformations which are safe
are:</p><ul><li>VMs may be removed from the list</li><li>VMs may have their memory requirements reduced</li><li>Hosts may be added</li><li>Hosts may have additional memory added.</li></ul><p>The cheaper algorithm is used for larger Pools where the state space to
search is too large. It uses the same &ldquo;biggest-fit-decreasing&rdquo; strategy
with the following simplifying approximations:</p><ul><li>every VM that fails is as big as the biggest</li><li>the number of VMs which fail due to a single Host failure is always the
maximum possible (even if these are all very small VMs)</li><li>the largest and most capable Hosts fail</li></ul><p>An informal argument that these approximations are safe is as follows:
if the maximum <em>number</em> of VMs fail, each of which is size of the largest
and we can find a restart plan using only the smaller hosts then any real
failure:</p><ul><li>can never result in the failure of more VMs;</li><li>can never result in the failure of bigger VMs; and</li><li>can never result in less host capacity remaining.</li></ul><p>Therefore we can take this <em>almost-certainly-worse-than-worst-case</em> failure
plan and:</p><ul><li>replace the remaining hosts in the worst case plan with the real remaining
hosts, which will be the same size or larger; and</li><li>replace the failed VMs in the worst case plan with the real failed VMs,
which will be fewer or the same in number and smaller or the same in size.</li></ul><p>Note that this strategy will perform best when each host has the same number
of VMs on it and when all VMs are approximately the same size. If one very big
VM exists and a lot of smaller VMs then it will probably fail to find a plan.
It is more tolerant of differing amounts of free host memory.</p><h2 id=overcommit-protection>Overcommit protection</h2><p>Overcommit protection blocks operations which would prevent the Pool being
able to restart <em>protected</em> VMs after host failure.
The Pool may become unable to restart protected VMs in two general ways:
(i) by running out of resource i.e. host memory; and (ii) by altering host
configuration in such a way that VMs cannot be started (or the planner
thinks that VMs cannot be started).</p><p>API calls which would change the amount of host memory currently in use
(<code>VM.start</code>, <code>VM.resume</code>, <code>VM.migrate</code> etc)
have been modified to call the planning functions supplying special
&ldquo;configuration change&rdquo; parameters.
Configuration change values represent the proposed operation and have type</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>type</span> configuration_change <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(** existing VMs which are leaving *)</span>
</span></span><span style=display:flex><span>  old_vms_leaving<span style=color:#f92672>:</span> <span style=color:#f92672>(</span>API.ref_host <span style=color:#f92672>*</span> <span style=color:#f92672>(</span>API.ref_VM <span style=color:#f92672>*</span> API.vM_t<span style=color:#f92672>))</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(** existing VMs which are arriving *)</span>
</span></span><span style=display:flex><span>  old_vms_arriving<span style=color:#f92672>:</span> <span style=color:#f92672>(</span>API.ref_host <span style=color:#f92672>*</span> <span style=color:#f92672>(</span>API.ref_VM <span style=color:#f92672>*</span> API.vM_t<span style=color:#f92672>))</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>  
</span></span><span style=display:flex><span>  <span style=color:#75715e>(** hosts to pretend to disable *)</span>
</span></span><span style=display:flex><span>  hosts_to_disable<span style=color:#f92672>:</span> API.ref_host <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(** new number of failures to consider *)</span>
</span></span><span style=display:flex><span>  num_failures<span style=color:#f92672>:</span> <span style=color:#66d9ef>int</span> option<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(** new VMs to restart *)</span>  
</span></span><span style=display:flex><span>  new_vms_to_protect<span style=color:#f92672>:</span> API.ref_VM <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div><p>A VM migration will be represented by saying the VM is &ldquo;leaving&rdquo; one host and
&ldquo;arriving&rdquo; at another. A VM start or resume will be represented by saying the
VM is &ldquo;arriving&rdquo; on a host.</p><p>Note that no attempt is made to integrate the overcommit protection with the
general <code>VM.start</code> host chooser as this would be quite expensive.</p><p>Note that the overcommit protection calls are written as <code>asserts</code> called
within the message forwarder in the master, holding the main forwarding lock.</p><p>API calls which would change the system configuration in such a way as to
prevent the HA restart planner being able to guarantee to restart protected
VMs are also blocked. These calls include:</p><ul><li><code>VBD.create</code>: where the disk is not in a <em>properly shared</em> SR</li><li><code>VBD.insert</code>: where the CDROM is local to a host</li><li><code>VIF.create</code>: where the network is not <em>properly shared</em></li><li><code>PIF.unplug</code>: when the network would cease to be <em>properly shared</em></li><li><code>PBD.unplug</code>: when the storage would cease to be <em>properly shared</em></li><li><code>Host.enable</code>: when some network or storage would cease to be
<em>properly shared</em> (e.g. if this host had a broken storage configuration)</li></ul><h1 id=xen>xen</h1><p>The Xen hypervisor has per-domain watchdog counters which, when enabled,
decrement as time passes and can be reset from a hypercall from the domain.
If the domain fails to make the hypercall and the timer reaches zero then
the domain is immediately shutdown with reason reboot. We configure Xen
to reboot the host when domain 0 enters this state.</p><h1 id=high-level-operations>High-level operations</h1><h2 id=enabling-ha>Enabling HA</h2><p>Before HA can be enabled the admin must take care to configure the
environment properly. In particular:</p><ul><li>NIC bonds should be available for network heartbeats;</li><li>multipath should be configured for the storage heartbeats;</li><li>all hosts should be online and fully-booted.</li></ul><p>The XenAPI client can request a specific shared SR to be used for
storage heartbeats, otherwise Xapi will use the Pool&rsquo;s default SR.
Xapi will use <code>VDI_GENERATE_CONFIG</code> to ensure the disk will be attached
automatically on system boot before the liveset has been joined.</p><p>Note that extra effort is made to re-use any existing heartbeat VDIS
so that</p><ul><li>if HA is disabled with some hosts offline, when they are rebooted they
stand a higher chance of seeing a well-formed statefile with an explicit
<em>invalid</em> state. If the VDIs were destroyed on HA disable then hosts which
boot up later would fail to attach the disk and it would be harder to
distinguish between a temporary storage failure and a permanent HA disable.</li><li>the heartbeat SR can be created on expensive low-latency high-reliability
storage and made as small as possible (to minimise infrastructure cost),
safe in the knowledge that if HA enables successfully once, it won&rsquo;t run
out of space and fail to enable in the future.</li></ul><p>The Xapi-to-Xapi communication looks as follows:</p><p><a href=#image-ecdfb5da39f8d95c1a6a74e20192d103 class=lightbox-link><img src=/new-docs/toolstack/features/HA/HA.configure.svg alt="Configuring HA around the Pool" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-ecdfb5da39f8d95c1a6a74e20192d103><img src=/new-docs/toolstack/features/HA/HA.configure.svg alt="Configuring HA around the Pool" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The Xapi Pool master calls <code>Host.ha_join_liveset</code> on all hosts in the
pool simultaneously. Each host
runs the <code>ha_start_daemon</code> script
which starts Xhad. Each Xhad starts exchanging heartbeats over the network
and storage defined in the <code>xhad.conf</code>.</p><h2 id=joining-a-liveset>Joining a liveset</h2><p><a href=#image-554dc4670d4a6d7790cbae180704f983 class=lightbox-link><img src=/new-docs/toolstack/features/HA/HA.start.svg alt="Starting up a host" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-554dc4670d4a6d7790cbae180704f983><img src=/new-docs/toolstack/features/HA/HA.start.svg alt="Starting up a host" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The Xhad instances exchange heartbeats and decide which hosts are in
the &ldquo;liveset&rdquo; and which have been fenced.</p><p>After joining the liveset, each host clears
the &ldquo;excluded&rdquo; flag which would have
been set if the host had been shutdown cleanly before &ndash; this is only
needed when a host is shutdown cleanly and then restarted.</p><p>Xapi periodically queries the state of xhad via the <code>ha_query_liveset</code>
command. The state will be <code>Starting</code> until the liveset is fully
formed at which point the state will be <code>Online</code>.</p><p>When the <code>ha_start_daemon</code> script returns then Xapi will decide
whether to stand for master election or not. Initially when HA is being
enabled and there is a master already, this node will be expected to
stand unopposed. Later when HA notices that the master host has been
fenced, all remaining hosts will stand for election and one of them will
be chosen.</p><h2 id=shutting-down-a-host>Shutting down a host</h2><p><a href=#image-a22743c5800366b2f530419b5ba63c48 class=lightbox-link><img src=/new-docs/toolstack/features/HA/HA.shutdown.svg alt="Shutting down a host" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-a22743c5800366b2f530419b5ba63c48><img src=/new-docs/toolstack/features/HA/HA.shutdown.svg alt="Shutting down a host" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>When a host is to be shutdown cleanly, it can be safely &ldquo;excluded&rdquo;
from the pool such that a future failure of the storage heartbeat will
not cause all pool hosts to self-fence (see survival rule 2 above).
When a host is &ldquo;excluded&rdquo; all other hosts know that the host does not
consider itself a master and has no resources locked i.e. no VMs are
running on it. An excluded host will never allow itself to form part
of a &ldquo;split brain&rdquo;.</p><p>Once a host has given up its master role and shutdown any VMs, it is safe
to disable fencing with <code>ha_disarm_fencing</code> and stop xhad with
<code>ha_stop_daemon</code>. Once the daemon has been stopped the &ldquo;excluded&rdquo;
bit can be set in the statefile via <code>ha_set_excluded</code> and the
host safely rebooted.</p><h2 id=restarting-a-host>Restarting a host</h2><p>When a host restarts after a failure Xapi notices that <em>ha_armed</em> is
set in the local database. Xapi</p><ul><li>runs the <code>attach-static-vdis</code> script to attach the statefile and
database VDIs. This can fail if the storage is inaccessible; Xapi will
retry until it succeeds.</li><li>runs the ha_start_daemon to join the liveset, or determine that HA
has been cleanly disabled (via setting the state to <em>Invalid</em>).</li></ul><p>In the special case where Xhad fails to access the statefile and the
host used to be a slave then Xapi will try to contact the previous master
and find out</p><ul><li>who the new master is;</li><li>whether HA is enabled on the Pool or not.</li></ul><p>If Xapi can confirm that HA was disabled then it will disarm itself and
join the new master. Otherwise it will keep waiting for the statefile
to recover.</p><p>In the special case where the statefile has been destroyed and cannot
be recovered, there is an emergency HA disable API the admin can use to
assert that HA really has been disabled, and it&rsquo;s not simply a connectivity
problem. Obviously this API should only be used if the admin is totally
sure that HA has been disabled.</p><h2 id=disabling-ha>Disabling HA</h2><p>There are 2 methods of disabling HA: one for the &ldquo;normal&rdquo; case when the
statefile is available; and the other for the &ldquo;emergency&rdquo; case when the
statefile has failed and can&rsquo;t be recovered.</p><h2 id=disabling-ha-cleanly>Disabling HA cleanly</h2><p><a href=#image-6a3639aa8fcd95dfd9e64dccb4b93666 class=lightbox-link><img src=/new-docs/toolstack/features/HA/HA.disable.clean.svg alt="Disabling HA cleanly" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-6a3639aa8fcd95dfd9e64dccb4b93666><img src=/new-docs/toolstack/features/HA/HA.disable.clean.svg alt="Disabling HA cleanly" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>HA can be shutdown cleanly when the statefile is working i.e. when hosts
are alive because of survival rule 1. First the master Xapi tells the local
Xhad to mark the pool state as &ldquo;invalid&rdquo; using <code>ha_set_pool_state</code>.
Every xhad instance will notice this state change the next time it performs
a storage heartbeat. The Xhad instances will shutdown and Xapi will notice
that HA has been disabled the next time it attempts to query the liveset.</p><p>If a host loses access to the statefile (or if none of the hosts have
access to the statefile) then HA can be disabled uncleanly.</p><h2 id=disabling-ha-uncleanly>Disabling HA uncleanly</h2><p>The Xapi master first calls <code>Host.ha_disable_failover_actions</code> on each host
which sets <code>ha_disable_failover_decisions</code> in the lcoal database. This
prevents the node rebooting, gaining statefile access, acquiring the
master lock and restarting VMs when other hosts have disabled their
fencing (i.e. a &ldquo;split brain&rdquo;).</p><p><a href=#image-4f445089db174e89f3a2bcbfcebc45a3 class=lightbox-link><img src=/new-docs/toolstack/features/HA/HA.disable.unclean.svg alt="Disabling HA uncleanly" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-4f445089db174e89f3a2bcbfcebc45a3><img src=/new-docs/toolstack/features/HA/HA.disable.unclean.svg alt="Disabling HA uncleanly" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Once the master is sure that no host will suddenly start recovering VMs
it is safe to call <code>Host.ha_disarm_fencing</code> which runs the script
<code>ha_disarm_fencing</code> and then shuts down the Xhad with <code>ha_stop_daemon</code>.</p><h2 id=add-a-host-to-the-pool>Add a host to the pool</h2><p>We assume that adding a host to the pool is an operation the admin will
perform manually, so it is acceptable to disable HA for the duration
and to re-enable it afterwards. If a failure happens during this operation
then the admin will take care of it by hand.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=numa>NUMA</h1><h2 id=numa-in-a-nutshell>NUMA in a nutshell</h2><p>Systems that contain more than one CPU socket are typically built on a Non-Uniform Memory Architecture (NUMA) <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.
In a NUMA system each node has fast, lower latency access to local memory.</p><p><a href=#image-30db8969deb31740679a057e523332fc class=lightbox-link><img src=/new-docs/toolstack/features/NUMA/hwloc.svg alt=hwloc class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-30db8969deb31740679a057e523332fc><img src=/new-docs/toolstack/features/NUMA/hwloc.svg alt=hwloc class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>In the diagram <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> above we have 4 NUMA nodes:</p><ul><li>2 of those are due to 2 separate physical packages (sockets)</li><li>a further 2 is due to Sub-NUMA-Clustering (aka Nodes Per Socket for AMD) where the L3 cache is split</li></ul><p>The L3 cache is shared among multiple cores, but cores <code>0-5</code> have lower latency access to one part of it, than cores <code>6-11</code>, and this is also reflected by splitting memory addresses into 4 31GiB ranges in total.</p><p>In the diagram the closer the memory is to the core, the lower the access latency:</p><ul><li>per-core caches: L1, L2</li><li>per-package shared cache: L3 (local part), L3 (remote part)</li><li>local NUMA node (to a group of cores, e.g. <code>L#0 P#0</code>), node 0</li><li>remote NUMA node in same package (<code>L#1 P#2</code>), node 1</li><li>remote NUMA node in other packages (<code>L#2 P#1</code> and &lsquo;L#3P#3&rsquo;), node 2 and 3</li></ul><h3 id=the-numa-distance-matrix>The NUMA distance matrix</h3><p>Accessing remote NUMA node in the other package has to go through a shared interconnect, which has lower bandwidth than the direct connections, and also a bottleneck if both cores have to access remote memory: the bandwidth for a single core is effectively at most half.</p><p>This is reflected in the NUMA distance/latency matrix.
The units are arbitrary, and by convention access latency to the local NUMA node is given distance &lsquo;10&rsquo;.</p><p>Relative latency matrix by logical indexes:</p><table><thead><tr><th>index</th><th>0</th><th>2</th><th>1</th><th>3</th></tr></thead><tbody><tr><td>0</td><td>10</td><td>21</td><td>11</td><td>21</td></tr><tr><td>2</td><td>21</td><td>10</td><td>21</td><td>11</td></tr><tr><td>1</td><td>11</td><td>21</td><td>10</td><td>21</td></tr><tr><td>3</td><td>21</td><td>11</td><td>21</td><td>10</td></tr></tbody></table><p>This follows the latencies described previously:</p><ul><li>fast access to local NUMA node memory (by definition), node 0, cost 10</li><li>slightly slower access latency to the other NUMA node in same package, node 1, cost 11</li><li>twice as slow access latency to remote NUMA memory in the other physical package (socket): nodes 2 and 3, cost 21</li></ul><p>There is also I/O NUMA where a cost is similarly associated to where a PCIe is plugged in, but exploring that is future work (it requires exposing NUMA topology to the Dom0 kernel to benefit from it), and for simplicity the diagram above does not show it.</p><h2 id=advantages-of-numa>Advantages of NUMA</h2><p>NUMA does have advantages though: if each node accesses only its local memory, then each node can independently achieve maximum throughput.</p><p>For best performance we should:</p><ul><li>minimize the amount of interconnect bandwidth we are using</li><li>run code that accesses memory allocated on the closest NUMA node</li><li>maximize the number of NUMA nodes that we use in the system as a whole</li></ul><p>If a VM&rsquo;s memory and vCPUs can entirely fit within a single NUMA node then we should tell Xen to prefer to allocate memory from and run the vCPUs on a single NUMA node.</p><h2 id=xen-vcpu-soft-affinity>Xen vCPU soft-affinity</h2><p>The Xen scheduler supports 2 kinds of constraints:</p><ul><li>hard pinning: a vCPU may only run on the specified set of pCPUs and nowhere else</li><li>soft pinning: a vCPU is <em>preferably</em> run on the specified set of pCPUs, but if they are all busy then it may run elsewhere</li></ul><p>The former is useful if you want strict separation, but it can potentially leave part of the system idle while another part is bottlenecked with lots of vCPUs all competing for the same limited set of pCPUs.</p><p>Xen does not migrate workloads between NUMA nodes on its own (the Linux kernel does), although it is possible to achieve a similar effect with explicit migration.
However migration introduces additional delays and is best avoided for entire VMs.</p><p>The latter (soft pinning) is preferred: running a workload now, even on a potentially suboptimal pCPU (higher NUMA latency) is still better than not running it at all and waiting until a pCPU is freed up.</p><p>Xen will also allocate memory for the VM according to the vCPU (soft) pinning: if the vCPUs are pinned only to NUMA nodes A and B, then it will allocate the VM&rsquo;s memory from NUMA nodes A and B (in a round-robin way, resulting in interleaving).</p><p>By default (no pinning) it will interleave memory from all NUMA nodes, which provides average performance, but individual tasks&rsquo; performance may be significantly higher or lower depending on which NUMA node the application may have &ldquo;landed&rdquo; on.
Furthermore restarting processes will speed them up or slow them down as address space randomization picks different memory regions inside a VM.</p><p>Note that this is not the worst case: the worst case would be for memory to be allocated on one NUMA node, but the vCPU always running on the furthest away NUMA node.</p><h2 id=best-effort-numa-aware-memory-allocation-for-vms>Best effort NUMA-aware memory allocation for VMs</h2><p>By default Xen stripes the VM&rsquo;s memory accross all NUMA nodes of the host, which means that every VM has to go through all the interconnects.
The goal here is to find a better allocation than the default, not necessarily an optimal allocation.
An optimal allocation would require knowing what VMs you would start/create in the future, and planning across hosts too.</p><p>Overall we want to balance the VMs across NUMA nodes, such that we use all NUMA nodes to take advantage of the maximum memory bandwidth available on the system.
For now this proposed balancing will be done only by balancing memory usage: always heuristically allocating VMs on the NUMA node that has the most available memory.
Note that this allocation has a race condition for now when multiple VMs are booted in parallel, because we don&rsquo;t wait until Xen has constructed the domain for each one (that&rsquo;d serialize domain construction, which is currently parallel).
This may be improved in the future by having an API to query Xen where it has allocated the memory, and to explicitly ask it to place memory on a given NUMA node (instead of best_effort).</p><p>If a VM doesn&rsquo;t fit into a single node then it is not so clear what the best approach is.
One criteria to consider is minimizing the NUMA distance between the nodes chosen for the VM.
Large NUMA systems may not be fully connected in a mesh requiring multiple hops to each a node, or even have assymetric links, or links with different bitwidth.
These tradeoff should be approximatively reflected in the ACPI SLIT tables, as a matrix of distances between nodes.
It is possible that 3 NUMA nodes have a smaller average/maximum distance than 2, so we need to consider all possibilities.</p><p>For N nodes there would be 2^N possibilities, so [Topology.NUMA.candidates] limits the number of choices to 65520+N (full set of 2^N possibilities for 16 NUMA nodes, and a reduced set of choices for larger systems).</p><p>[Topology.NUMA.candidates] is a sorted sequence of node sets, in ascending order of maximum/average distances.
Once we&rsquo;ve eliminated the candidates not suitable for this VM (that do not have enough total memory/pCPUs) we are left with a monotonically increasing sequence of nodes.
There are still multiple possibilities with same average distance.
This is where we consider our second criteria - balancing - and pick the node with most available free memory.</p><p>Once a suitable set of NUMA nodes are picked we compute the CPU soft affinity as the union of the CPUs from all these NUMA nodes.
If we didn&rsquo;t find a solution then we let Xen use its default allocation.</p><p>The &ldquo;distances&rdquo; between NUMA nodes may not all be equal, e.g. some nodes may have shorter links to some remote NUMA nodes, while others may have to go through multiple hops to reach it.
See page 13 in <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> for a diagram of an AMD Opteron 6272 system.</p><h2 id=limitations-and-tradeoffs>Limitations and tradeoffs</h2><ul><li>Booting multiple VMs in parallel will result in potentially allocating both on the same NUMA node (race condition)</li><li>When we&rsquo;re about to run out of host memory we&rsquo;ll fall back to striping memory again, but the soft affinity mask won&rsquo;t reflect that (this needs an API to query Xen on where it has actually placed the VM, so we can fix up the mask accordingly)</li><li>XAPI is not aware of NUMA balancing across a pool, and choses hosts purely based on total amount of free memory, even if a better NUMA placement could be found on another host</li><li>Very large (>16 NUMA nodes) systems may only explore a limited number of choices (fit into a single node vs fallback to full interleaving)</li><li>The exact VM placement is not yet controllable</li><li>Microbenchmarks with a single VM on a host show both performance improvements and regressions on memory bandwidth usage: previously a single VM may have been able to take advantage of the bandwidth of both NUMA nodes if it happened to allocate memory from the right places, whereas now it&rsquo;ll be forced to use just a single node.
As soon as you have more than 1 VM that is busy on a system enabling NUMA balancing should almost always be an improvement though.</li><li>it is not supported to combine hard vCPU masks with soft affinity: if hard affinities are used then no NUMA scheduling is done by the toolstack and we obey exactly what the user has asked for with hard affinities.
This shouldn&rsquo;t affect other VMs since the memory used by hard-pinned VMs will still be reflected in overall less memory available on individual NUMA nodes.</li><li>Corner case: the ACPI standard allows certain NUMA nodes to be unreachable (distance <code>0xFF</code> = <code>-1</code> in the Xen bindings).
This is not supported and will cause an exception to be raised.
If this is an issue in practice the NUMA matrix could be pre-filtered to contain only reachable nodes.
NUMA nodes with 0 CPUs <em>are</em> accepted (it can result from hard affinity pinnings)</li><li>NUMA balancing is not considered during HA planning</li><li>Dom0 is a single VM that needs to communicate with all other VMs, so NUMA balancing is not applied to it (we&rsquo;d need to expose NUMA topology to the Dom0 kernel so it can better allocate processes)</li><li>IO NUMA is out of scope for now</li></ul><h2 id=xapi-datamodel-design>XAPI datamodel design</h2><ul><li>New API field: <code>Host.numa_affinity_policy</code>.</li><li>Choices: <code>default_policy</code>, <code>any</code>, <code>best_effort</code>.</li><li>On upgrade the field is set to <code>default_policy</code></li><li>Changes in the field only affect newly (re)booted VMs, for changes to take effect on existing VMs a host evacuation or reboot is needed</li></ul><p>There may be more choices in the future (e.g. <code>strict</code>, which requires both Xen and toolstack changes).</p><p>Meaning of the policy:</p><ul><li><p><code>any</code>: the Xen default where it allocated memory by striping across NUMA nodes</p></li><li><p><code>best_effort</code>: the algorithm described in this document, where soft pinning is used to achieve better balancing and lower latency</p></li><li><p><code>default_policy</code>: when the admin hasn&rsquo;t expressed a preference</p></li><li><p>Currently <code>default_policy</code> is treated as <code>any</code>, but the admin can change it, and then the system will remember that change across upgrades.
If we didn&rsquo;t have a <code>default_policy</code> then changing the &ldquo;default&rdquo; policy on an upgrade would be tricky: we either risk overriding an explicit choice of the admin, or existing installs cannot take advantage of the improved performance from <code>best_effort</code></p></li><li><p>Future XAPI versions may change <code>default_policy</code> to mean <code>best_effort</code>.
Admins can still override it to <code>any</code> if they wish on a host by host basis.</p></li></ul><p>It is not expected that users would have to change <code>best_effort</code>, unless they run very specific workloads, so a pool level control is not provided at this moment.</p><p>There is also no separate feature flag: this host flag acts as a feature flag that can be set through the API without restarting the toolstack.
Although obviously only new VMs will benefit.</p><p>Debugging the allocator is done by running <code>xl vcpu-list</code> and investigating the soft pinning masks, and by analyzing xensource.log.</p><h3 id=xenopsd-implementation>Xenopsd implementation</h3><p>See the documentation in [softaffinity.mli] and [topology.mli].</p><ul><li>[Softaffinity.plan] returns a [CPUSet] given a host&rsquo;s NUMA allocation state and a VM&rsquo;s NUMA allocation request.</li><li>[Topology.CPUSet] provides helpers for operating on a set of CPU indexes.</li><li>[Topology.NUMAResource] is a [CPUSet] and the free memory available on a NUMA node.</li><li>[Topology.NUMARequest] is a request for a given number of vCPUs and memory in bytes.</li><li>[Topology.NUMA] represents a host&rsquo;s NUMA allocation state.</li><li>[Topology.NUMA.candidates] are groups of nodes orderd by minimum average distance.
The sequence is limited to [N+65520], where [N] is the number of NUMA nodes.
This avoids exponential state space explosion on very large systems (>16 NUMA nodes).</li><li>[Topology.NUMA.choose] will choose one NUMA node deterministically, while trying to keep overall NUMA node usage balanced.</li><li>[Domain.numa_placement] builds a [NUMARequest] and uses the above [Topology] and [Softaffinity] functions to compute and apply a plan.</li></ul><p>We used to have a <code>xenopsd.conf</code> configuration option to enable numa placement, for backwards compatibility this is still supported, but only if the admin hasn&rsquo;t set an explicit policy on the Host.
It is best to remove the experimental <code>xenopsd.conf</code> entry though, a future version may completely drop it.</p><p>Tests are in [test_topology.ml] which checks balancing properties and whether the plan has improved best/worst/average-case access times in a simulated test based on 2 predefined NUMA distance matrixes (one from Intel and one from an AMD system).</p><h2 id=future-work>Future work</h2><ul><li>enable &lsquo;best_effort&rsquo; mode by default once more testing has been done</li><li>an API to query Xen where it has actually allocated the VM&rsquo;s memory.
Currently only an <code>xl debug-keys</code> interface exists which is not supported in production as it can result in killing the host via the watchdog, and is not a proper API, but a textual debug output with no stability guarantees.</li><li>more host policies (e.g. <code>strict</code>).
Requires the XAPI pool scheduler to be NUMA aware and consider it as part of chosing hosts.</li><li>VM level policy that can set a NUMA affinity index, mapped to a NUMA node modulo NUMA nodes available on the system (this is needed so that after migration we don&rsquo;t end up trying to allocate vCPUs to a non-existent NUMA node)</li><li>VM level anti-affinity rules for NUMA placement (can be achieved by setting unique NUMA affinity indexes)</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://wiki.xenproject.org/wiki/Xen_on_NUMA_Machines target=_blank>Xen on NUMA Machines</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.kernel.org/doc/html/v6.6/mm/numa.html target=_blank>What is NUMA?</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>created with <code>lstopo-no-graphics --no-io --of svg --vert=L3 >hwloc.svg</code> on a bare metal Linux&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Lepers, Baptiste. <a href=https://theses.hal.science/tel-01549294/document target=_blank>&ldquo;Improving performance on NUMA systems.&rdquo;</a> PhD diss., Université de Grenoble, 2014.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=snapshots>Snapshots</h1><p>Snapshots represent the state of a VM, or a disk (VDI) at a point in time. They can be used for:</p><ul><li>backups (hourly, daily, weekly etc)</li><li>experiments (take snapshot, try something, revert back again)</li><li>golden images (install OS, get it just right, clone it 1000s of times)</li></ul><p>Read more about <a href=/new-docs/toolstack/features/snapshots/../../xen-api/snapshots.html>the Snapshot APIs</a>.</p><h1 id=disk-snapshots>Disk snapshots</h1><p>Disks are represented in the XenAPI as VDI objects. Disk snapshots are represented
as VDI objects with the flag <code>is_a_snapshot</code> set to true. Snapshots are always
considered read-only, and should only be used for backup or cloning into new
disks. Disk snapshots have a lifetime independent of the disk they are a snapshot
of i.e. if someone deletes the original disk, the snapshots remain. This contrasts
with some storage arrays in which snapshots are &ldquo;second class&rdquo; objects which are
automatically deleted when the original disk is deleted.</p><p>Disks are implemented in Xapi via &ldquo;Storage Manager&rdquo; (SM) plugins. The SM plugins
conform to an api (the SMAPI) which has operations including</p><ul><li>vdi_create: make a fresh disk, full of zeroes</li><li>vdi_snapshot: create a snapshot of a disk</li></ul><h1 id=file-based-vhd-implementation>File-based vhd implementation</h1><p>The existing &ldquo;EXT&rdquo; and &ldquo;NFS&rdquo; file-based Xapi SM plugins store disk data in
trees of .vhd files as in the following diagram:</p><p><a href=#image-0492e4b09b4dd3b67f34caafc395839b class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/vhd-trees.png alt="Relationship between VDIs and vhd files" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-0492e4b09b4dd3b67f34caafc395839b><img src=/new-docs/toolstack/features/snapshots/vhd-trees.png alt="Relationship between VDIs and vhd files" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>From the XenAPI point of view, we have one current VDI and a set of snapshots,
each taken at a different point in time. These VDIs correspond to leaf vhds in
a tree stored on disk, where the non-leaf nodes contain all the shared blocks.</p><p>The vhd files are always thinly-provisioned which means they only allocate new
blocks on an as-needed basis. The snapshot leaf vhd files only contain vhd
metadata and therefore are very small (a few KiB). The parent nodes containing
the shared blocks only contain the shared blocks. The current leaf initially
contains only the vhd metadata and therefore is very small (a few KiB) and will
only grow when the VM writes blocks.</p><p>File-based vhd implementations are a good choice if a &ldquo;gold image&rdquo; snapshot
is going to be cloned lots of times.</p><h1 id=block-based-vhd-implementation>Block-based vhd implementation</h1><p>The existing &ldquo;LVM&rdquo;, &ldquo;LVMoISCSI&rdquo; and &ldquo;LVMoHBA&rdquo; block-based Xapi SM plugins store
disk data in trees of .vhd files contained within LVM logical volumes:</p><p><a href=#image-31f3742d6613b102b256c7de3c309e39 class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/lun-trees.png alt="Relationship between VDIs and LVs containing vhd data" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-31f3742d6613b102b256c7de3c309e39><img src=/new-docs/toolstack/features/snapshots/lun-trees.png alt="Relationship between VDIs and LVs containing vhd data" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Non-snapshot VDIs are always stored full size (a.k.a. thickly-provisioned).
When parent nodes are created they are automatically shrunk to the minimum size
needed to store the shared blocks. The LVs corresponding with snapshot VDIs
only contain vhd metadata and by default consume 8MiB. Note: this is different
to VDI.clones which are stored full size.</p><p>Block-based vhd implementations are not a good choice if a &ldquo;gold image&rdquo; snapshot
is going to be cloned lots of times, since each clone will be stored full size.</p><h1 id=hypothetical-lun-implementation>Hypothetical LUN implementation</h1><p>A hypothetical Xapi SM plugin could use LUNs on an iSCSI storage array
as VDIs, and the array&rsquo;s custom control interface to implement the &ldquo;snapshot&rdquo;
operation:</p><p><a href=#image-f1a60ee2dbb20dafa8ee69d3eab24cbb class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/luns.png alt="Relationship between VDIs and LUNs on a hypothetical storage target" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-f1a60ee2dbb20dafa8ee69d3eab24cbb><img src=/new-docs/toolstack/features/snapshots/luns.png alt="Relationship between VDIs and LUNs on a hypothetical storage target" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>From the XenAPI point of view, we have one current VDI and a set of snapshots,
each taken at a different point in time. These VDIs correspond to LUNs on the
same iSCSI target, and internally within the target these LUNs are comprised of
blocks from a large shared copy-on-write pool with support for dedup.</p><h1 id=reverting-disk-snapshots>Reverting disk snapshots</h1><p>There is no current way to revert in-place a disk to a snapshot, but it is
possible to create a writable disk by &ldquo;cloning&rdquo; a snapshot.</p><h1 id=vm-snapshots>VM snapshots</h1><p>Let&rsquo;s say we have a VM, &ldquo;VM1&rdquo; that has 2 disks. Concentrating only
on the VM, VBDs and VDIs, we have the following structure:</p><p><a href=#image-b188e4fb7854643c6a4c2a286f6dadb7 class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/vm.png alt="VM objects" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-b188e4fb7854643c6a4c2a286f6dadb7><img src=/new-docs/toolstack/features/snapshots/vm.png alt="VM objects" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>When we take a snapshot, we first ask the storage backends to snapshot
all of the VDIs associated with the VM, producing new VDI objects.
Then we copy all of the metadata, producing a new &lsquo;snapshot&rsquo; VM
object, complete with its own VBDs copied from the original, but now
pointing at the snapshot VDIs. We also copy the VIFs and VGPUs
but for now we will ignore those.</p><p>This process leads to a set of objects that look like this:</p><p><a href=#image-2734fe8b0b8f53bdabdfbe74e1bbf5ca class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/vm-snapshot.png alt="VM and snapshot objects" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-2734fe8b0b8f53bdabdfbe74e1bbf5ca><img src=/new-docs/toolstack/features/snapshots/vm-snapshot.png alt="VM and snapshot objects" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>We have fields that help navigate the new objects: <code>VM.snapshot_of</code>,
and <code>VDI.snapshot_of</code>. These, like you would expect, point to the
relevant other objects.</p><h1 id=deleting-vm-snapshots>Deleting VM snapshots</h1><p>When a snapshot is deleted Xapi calls the SM API <code>vdi_delete</code>. The Xapi SM
plugins which use vhd format data do not reclaim space immediately; instead
they mark the corresponding vhd leaf node as &ldquo;hidden&rdquo; and, at some point later,
run a garbage collector process.</p><p>The garbage collector will first determine whether a &ldquo;coalesce&rdquo; should happen i.e.
whether any parent nodes have only one child i.e. the &ldquo;shared&rdquo; blocks are only
&ldquo;shared&rdquo; with one other node. In the following example the snapshot delete leaves
such a parent node and the coalesce process copies blocks from the redundant
parent&rsquo;s only child into the parent:</p><p><a href=#image-e42b28980ca93777df08b4e3fea16776 class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/coalesce1.png alt="We coalesce parent blocks into grand parent nodes" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-e42b28980ca93777df08b4e3fea16776><img src=/new-docs/toolstack/features/snapshots/coalesce1.png alt="We coalesce parent blocks into grand parent nodes" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Note that if the vhd data is being stored in LVM, then the parent node will
have had to be expanded to full size to accommodate the writes. Unfortunately
this means the act of reclaiming space actually consumes space itself, which
means it is important to never completely run out of space in such an SR.</p><p>Once the blocks have been copied, we can now cut one of the parents out of the
tree by relinking its children into their grandparent:</p><p><a href=#image-de581996004f7778d0263732fbf4f862 class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/coalesce2.png alt="Relink children into grand parent" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-de581996004f7778d0263732fbf4f862><img src=/new-docs/toolstack/features/snapshots/coalesce2.png alt="Relink children into grand parent" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Finally the garbage collector can remove unused vhd files / LVM LVs:</p><p><a href=#image-c152331a044c60c022091c957d15d15e class=lightbox-link><img src=/new-docs/toolstack/features/snapshots/coalesce3.png alt="Clean up" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-c152331a044c60c022091c957d15d15e><img src=/new-docs/toolstack/features/snapshots/coalesce3.png alt="Clean up" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><h1 id=reverting-vm-snapshots>Reverting VM snapshots</h1><p>The XenAPI call <code>VM.revert</code> overwrites the VM metadata with the snapshot VM
metadata, deletes the current VDIs and replaces them with clones of the
snapshot VDIs. Note there is no &ldquo;vdi_revert&rdquo; in the SMAPI.</p><h2 id=revert-implementation-details>Revert implementation details</h2><p>This is the process by which we revert a VM to a snapshot. The
first thing to notice is that there is some logic that is called
from <a href=https://github.com/xapi-project/xen-api/blob/ce6d3f276f0a56ef57ebcf10f45b0f478fd70322/ocaml/xapi/message_forwarding.ml#L1528 target=_blank>message_forwarding.ml</a>,
which uses some low-level database magic to turn the current VM
record into one that looks like the snapshot object. We then go
to the rest of the implementation in <a href=https://github.com/xapi-project/xen-api/blob/ce6d3f276f0a56ef57ebcf10f45b0f478fd70322/ocaml/xapi/xapi_vm_snapshot.ml#L403 target=_blank>xapi_vm_snapshot.ml</a>.
First,
we shut down the VM if it is currently running. Then, we revert
all of the <a href=https://github.com/xapi-project/xen-api/blob/ce6d3f276f0a56ef57ebcf10f45b0f478fd70322/ocaml/xapi/xapi_vm_snapshot.ml#L270 target=_blank>VBDs, VIFs and VGPUs</a>.
To revert the VBDs, we need to deal with the VDIs underneath them.
In order to create space, the first thing we do is <a href=https://github.com/xapi-project/xen-api/blob/ce6d3f276f0a56ef57ebcf10f45b0f478fd70322/ocaml/xapi/xapi_vm_snapshot.ml#L287 target=_blank>delete all of
the VDIs</a> currently attached via VBDs to the VM.
We then <em>clone</em> the disks from the snapshot. Note that there is
no SMAPI operation &lsquo;revert&rsquo; currently - we simply clone from
the snapshot VDI. It&rsquo;s important to note that cloning
creates a <em>new</em> VDI object: this is not the one we started with gone.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vgpu>vGPU</h1><p>XenServer has supported passthrough for GPU devices since XenServer 6.0. Since
the advent of NVIDIA&rsquo;s vGPU-capable GRID K1/K2 cards it has been possible to
carve up a GPU into smaller pieces yielding a more scalable solution to
boosting graphics performance within virtual machines.</p><p>The K1 has four GK104 GPUs and the K2 two GK107 GPUs. Each of these will be exposed through Xapi so a host with a single K1 card will have access to four independent PGPUs.</p><p>Each of the GPUs can then be subdivided into vGPUs. For each type of PGPU,
there are a few options of vGPU type which consume different amounts of the
PGPU. For example, K1 and K2 cards can currently be configured in the following
ways:</p><p><a href=#image-fffbd4a72494247d8f8a42533a28e4f6 class=lightbox-link><img src=/new-docs/toolstack/features/VGPU/vgx-configs.png alt="Possible VGX configurations" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-fffbd4a72494247d8f8a42533a28e4f6><img src=/new-docs/toolstack/features/VGPU/vgx-configs.png alt="Possible VGX configurations" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Note, this diagram is not to scale, the PGPU resource required by each
vGPU type is as follows:</p><table><thead><tr><th>vGPU type</th><th>PGPU kind</th><th>vGPUs / PGPU</th></tr></thead><tbody><tr><td>k100</td><td>GK104</td><td>8</td></tr><tr><td>k140Q</td><td>GK104</td><td>4</td></tr><tr><td>k200</td><td>GK107</td><td>8</td></tr><tr><td>k240Q</td><td>GK107</td><td>4</td></tr><tr><td>k260Q</td><td>GK107</td><td>2</td></tr></tbody></table><p>Currently each physical GPU (PGPU) only supports <em>homogeneous vGPU
configurations</em> but different configurations are supported on different PGPUs
across a single K1/K2 card. This means that, for example, a host with a K1 card
can run 64 VMs with k100 vGPUs (8 per PGPU).</p><h2 id=xenservers-vgpu-architecture>XenServer&rsquo;s vGPU architecture</h2><p>A new display type has been added to the device model:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-udiff data-lang=udiff><span style=display:flex><span><span style=color:#75715e>@@ -4519,6 +4522,7 @@ static const QEMUOption qemu_options[] =
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>     /* Xen tree options: */
</span></span><span style=display:flex><span>     { &#34;std-vga&#34;, 0, QEMU_OPTION_std_vga },
</span></span><span style=display:flex><span><span style=color:#a6e22e>+    { &#34;vgpu&#34;, 0, QEMU_OPTION_vgpu },
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span>     { &#34;videoram&#34;, HAS_ARG, QEMU_OPTION_videoram },
</span></span><span style=display:flex><span>     { &#34;d&#34;, HAS_ARG, QEMU_OPTION_domid }, /* deprecated; for xend compatibility */
</span></span><span style=display:flex><span>     { &#34;domid&#34;, HAS_ARG, QEMU_OPTION_domid },
</span></span></code></pre></div><p>With this in place, <code>qemu</code> can now be started using a new option that will
enable it to communicate with a new display emulator, <code>vgpu</code> to expose the
graphics device to the guest. The <code>vgpu</code> binary is responsible for handling the
VGX-capable GPU and, once it has been successfully passed through, the in-guest
drivers can be installed in the same way as when it detects new hardware.</p><p>The diagram below shows the relevant parts of the architecture for this
project.</p><p><a href=#image-3a419088335016f3b6c36f948bad6694 class=lightbox-link><img src=/new-docs/toolstack/features/VGPU/vgpu-arch.png alt="XenServer&amp;rsquo;s vGPU architecture" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-3a419088335016f3b6c36f948bad6694><img src=/new-docs/toolstack/features/VGPU/vgpu-arch.png alt="XenServer&amp;rsquo;s vGPU architecture" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><h3 id=relevant-code>Relevant code</h3><ul><li>In Xenopsd: <a href=https://github.com/xapi-project/xenopsd/blob/8d06778db2/xc/xenops_server_xen.ml#L1107-L1113 target=_blank>Xenops_server_xen</a> is where
Xenopsd gets the vGPU information from the values passed from Xapi;</li><li>In Xenopsd: <a href=https://github.com/xapi-project/xenopsd/blob/8d06778db2/xc/device.ml#L1696-L1708 target=_blank>Device.__start</a> is where the <code>vgpu</code> process is started, if
necessary, before Qemu.</li></ul><h2 id=xapis-api-and-data-model>Xapi&rsquo;s API and data model</h2><p>A lot of work has gone into the toolstack to handle the creation and management
of VMs with vGPUs. We revised our data model, introducing a semantic link
between <code>VGPU</code> and <code>PGPU</code> objects to help with utilisation tracking; we
maintained the <code>GPU_group</code> concept as a pool-wide abstraction of PGPUs
available for VMs; and we added <strong><code>VGPU_types</code></strong> which are configurations for
<code>VGPU</code> objects.</p><p><a href=#image-64deb11bf0cedbf1755053d6fecf7fc7 class=lightbox-link><img src=/new-docs/toolstack/features/VGPU/vgpu-datamodel.png alt="Xapi&amp;rsquo;s vGPU datamodel" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-64deb11bf0cedbf1755053d6fecf7fc7><img src=/new-docs/toolstack/features/VGPU/vgpu-datamodel.png alt="Xapi&amp;rsquo;s vGPU datamodel" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p><strong>Aside:</strong> The VGPU type in Xapi&rsquo;s data model predates this feature and was
synonymous with GPU-passthrough. A VGPU is simply a display device assigned to
a VM which may be a vGPU (this feature) or a whole GPU (a VGPU of type
<em>passthrough</em>).</p><p><strong><code>VGPU_types</code></strong> can be enabled/disabled on a <strong>per-PGPU basis</strong> allowing for
reservation of particular PGPUs for certain workloads. VGPUs are allocated on
PGPUs within their GPU group in either a <em>depth-first</em> or <em>breadth-first</em>
manner, which is configurable on a per-group basis.</p><p><strong><code>VGPU_types</code></strong> are created by xapi at startup depending on the available
hardware and config files present in dom0. They exist in the pool database, and
a primary key is used to avoid duplication. In XenServer 6.x the tuple of
<code>(vendor_name, model_name)</code> was used as the primary key, however this was not
ideal as these values are subject to change. XenServer 7.0 switched to a
<a href=%7b%7bsite.baseurl%7d%7d/xapi/futures/vgpu-type-identifiers.html>new primary key</a>
generated from static metadata, falling back to the old method for backwards
compatibility.</p><p>A <strong><code>VGPU_type</code></strong> will be garbage collected when there is no VGPU of that type
and there is no hardware which supports that type. On VM import, all VGPUs and
VGPU_types will be created if necessary - if this results in the creation of a
new VGPU_type then the VM will not be usable until the required hardware and
drivers are installed.</p><h3 id=relevant-code-1>Relevant code</h3><ul><li>In Xapi: <a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_vgpu_type.ml target=_blank>Xapi_vgpu_type</a> contains the type definitions and parsing logic
for vGPUs;</li><li>In Xapi: <a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_pgpu_helpers.mli target=_blank>Xapi_pgpu_helpers</a> defines the functions used to allocate vGPUs
on PGPUs.</li></ul><h2 id=xapi---xenopsd-interface>Xapi &lt;-> Xenopsd interface</h2><p>In XenServer 6.x, all VGPU config was added to the VM&rsquo;s <code>platform</code> field at
startup, and this information was used by xenopsd to start the display emulator.
See the relevant code <a href=https://github.com/xenserver/xen-api/blob/50bce20546/ocaml/xapi/vgpuops.ml#L149-L165 target=_blank>here</a>.</p><p>In XenServer 7.0, to facilitate support of VGPU on Intel hardware in parallel
with the existing NVIDIA support, VGPUs were made first-class objects in the
xapi-xenopsd interface. The interface is described
<a href=%7b%7bsite.baseurl%7d%7d/features/futures/gpu-support-evolution.html>here</a>.</p><h2 id=vm-startup>VM startup</h2><p>On the pool master:</p><ul><li>Assuming no WLB, all VM.start tasks pass through
<a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_vm_helpers.ml#L618-L651 target=_blank>Xapi_vm_helpers.choose_host_for_vm_no_wlb</a>. If the VM has a vGPU, the list
of all hosts in the pool is split into a list of lists, where the first list
is the most optimal in terms of the GPU group&rsquo;s allocation mode and the PGPU
availability on each host.</li><li>Each list of hosts in turn is passed to <a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_vm_placement.ml#L81-L97 target=_blank>Xapi_vm_placement.select_host</a>,
which checks storage, network and memory availability, until a suitable host
is found.</li><li>Once a host has been chosen, <a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/message_forwarding.ml#L811-L828 target=_blank>allocate_vm_to_host</a> will set the
<code>VM.scheduled_to_be_resident_on</code> and <code>VGPU.scheduled_to_be_resident_on</code>
fields.</li></ul><p>The task is then ready to be forwarded to the host on which the VM will start:</p><ul><li>If the VM has a VGPU, the startup task is wrapped in
<a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_vm.ml#L214-L220 target=_blank>Xapi_gpumon.with_gpumon_stopped</a>. This makes sure that the NVIDIA driver
is not in use so can be loaded or unloaded from physical GPUs as required.</li><li>The VM metadata, including VGPU metadata, is passed to xenopsd. The creation
of the VGPU metadata is done by <a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_xenops.ml#L698-L733 target=_blank>vgpus_of_vm</a>. Note that at this point
passthrough VGPUs are represented by the PCI device type, and metadata is
generated by <a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_xenops.ml#L598-618 target=_blank>pcis_of_vm</a>.</li><li>As part of starting up the VM, xenopsd should report a <a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_xenops.ml#L1841-L1854 target=_blank>VGPU event</a> or a
<a href=https://github.com/xapi-project/xen-api/blob/8a71a4aaaa/ocaml/xapi/xapi_xenops.ml#L1777-L1801 target=_blank>PCI event</a>, which xapi will use to indicate that the xapi VGPU object can
be marked as <code>currently_attached</code>.</li></ul><h2 id=usage>Usage</h2><p>To create a VGPU of a given type you can use <code>vgpu-create</code>:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ xe vgpu-create vm-uuid<span style=color:#f92672>=</span>... gpu-group-uuid<span style=color:#f92672>=</span>... vgpu-type-uuid<span style=color:#f92672>=</span>...</span></span></code></pre></div><p>To see a list of VGPU types available for use on your XenServer, run the
following command. Note: these will only be populated if you have installed the
relevant NVIDIA RPMs and if there is hardware installed on that host supported
each type. Using <code>params=all</code> will display more information such as the maximum
number of heads supported by that VGPU type and which PGPUs have this type
enabled and supported.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ xe vgpu-type-list <span style=color:#f92672>[</span>params<span style=color:#f92672>=</span>all<span style=color:#f92672>]</span></span></span></code></pre></div><p>To access the new and relevant parameters on a PGPU (i.e.
<code>supported_VGPU_types</code>, <code>enabled_VGPU_types</code>, <code>resident_VGPUs</code>) you can use
<code>pgpu-param-get</code> with <code>param-name=supported-vgpu-types</code>
<code>param-name=enabled-vgpu-types</code> and <code>param-name=resident-vgpus</code> respectively.
Or, alternatively, you can use the following command to list all the parameters
for the PGPU. You can get the types supported or enabled for a given PGPU:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ xe pgpu-list uuid<span style=color:#f92672>=</span>... params<span style=color:#f92672>=</span>all</span></span></code></pre></div><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-storage-migration>Xapi Storage Migration</h1><p>The Xapi Storage Migration (XSM) also known as &ldquo;Storage Motion&rdquo; allows</p><ul><li>a running VM to be migrated within a pool, between different hosts
and different storage simultaneously;</li><li>a running VM to be migrated to another pool;</li><li>a disk attached to a running VM to be moved to another SR.</li></ul><p>The following diagram shows how XSM works at a high level:</p><p><a href=#image-2bc23a1fd3430269d3f3721d070e26b8 class=lightbox-link><img src=/new-docs/toolstack/features/XSM/xsm.png alt="Xapi Storage Migration" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-2bc23a1fd3430269d3f3721d070e26b8><img src=/new-docs/toolstack/features/XSM/xsm.png alt="Xapi Storage Migration" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The slowest part of a storage migration is migrating the storage, since virtual
disks can be very large. Xapi starts by taking a snapshot and copying that to
the destination as a background task. Before the datapath connecting the VM
to the disk is re-established, xapi tells <code>tapdisk</code> to start mirroring all
writes to a remote <code>tapdisk</code> over NBD. From this point on all VM disk writes
are written to both the old and the new disk.
When the background snapshot copy is complete, xapi can migrate the VM memory
across. Once the VM memory image has been received, the destination VM is
complete and the original can be safely destroyed.</p><footer class=footline></footer></article></section></section><article class=default><header class=headline></header><h1 id=xapi>Xapi</h1><p>Xapi is the <a href=http://github.com/xapi-project target=_blank>xapi-project</a> host and cluster manager.</p><p>Xapi is responsible for:</p><ul><li>providing a stable interface (the XenAPI)</li><li>allowing one client to manage multiple hosts</li><li>hosting the &ldquo;xe&rdquo; CLI</li><li>authenticating users and applying role-based access control</li><li>locking resources (in particular disks)</li><li>allowing storage to be managed through plugins</li><li>planning and coping with host failures (&ldquo;High Availability&rdquo;)</li><li>storing VM and host configuration</li><li>generating alerts</li><li>managing software patching</li></ul><h2 id=principles>Principles</h2><ol><li>The XenAPI interface must remain backwards compatible, allowing older
clients to continue working</li><li>Xapi delegates all Xenstore/libxc/libxl access to Xenopsd, so Xapi could
be run in an unprivileged helper domain</li><li>Xapi delegates the low-level storage manipulation to SM plugins.</li><li>Xapi delegates setting up host networking to xcp-networkd.</li><li>Xapi delegates monitoring performance counters to xcp-rrdd.</li></ol><h2 id=overview>Overview</h2><p>The following diagram shows the internals of Xapi:</p><p><a href=#image-4c4c49fa3236e6f52ed8efdb8413480a class=lightbox-link><img src=/new-docs/xapi/xapi.png alt="Internals of xapi" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-4c4c49fa3236e6f52ed8efdb8413480a><img src=/new-docs/xapi/xapi.png alt="Internals of xapi" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The top of the diagram shows the XenAPI clients: XenCenter, XenOrchestra,
OpenStack and CloudStack using XenAPI and HTTP GET/PUT over ports 80 and 443 to
talk to xapi. These XenAPI (JSON-RPC or XML-RPC over HTTP POST) and HTTP
GET/PUT are always authenticated using either PAM (by default using the local
passwd and group files) or through Active Directory.</p><p>The APIs are classified into categories:</p><ul><li>coordinator-only: these are the majority of current APIs. The coordinator
should be called and relied upon to forward the call to the right place with
the right locks held.</li><li>normally-local: these are performance special cases
such as disk import/export and console connection which are sent directly to
hosts which have the most efficient access to the data.</li><li>emergency: these deal with scenarios where the coordinator is offline</li></ul><p>If the incoming API call should be resent to the coordinator than a XenAPI
<code>HOST_IS_SLAVE</code> error message containing the coordinator&rsquo;s IP is sent to the
client.</p><p>Once past the initial checks, API calls enter the &ldquo;message forwarding&rdquo; layer which</p><ul><li>locks resources (via the <code>current_operations</code> mechanism)</li><li>decides which host should execute the request.</li></ul><p>If the request should run locally then a direct function call is used;
otherwise the message forwarding code makes a synchronous API call to a
specific other host. Note: Xapi currently employs a &ldquo;thread per request&rdquo; model
which causes one full POSIX thread to be created for every request. Even when a
request is forwarded the full thread persists, blocking for the result to
become available.</p><p>If the XenAPI call is a VM lifecycle operation then it is converted into a
Xenopsd API call and forwarded over a Unix domain socket. Xapi and Xenopsd have
similar notions of cancellable asynchronous &ldquo;tasks&rdquo;, so the current Xapi task
(all operations run in the context of a task) is bound to the Xenopsd task, so
cancellation is passed through and progress updates are received.</p><p>If the XenAPI call is a storage operation then the &ldquo;storage access&rdquo; layer</p><ul><li>verifies that the storage objects are in the correct state (SR
attached/detached; VDI attached/activated read-only/read-write)</li><li>invokes the relevant operation in the Storage Manager API (SMAPI) v2
interface;</li><li>depending on the type of SR:<ul><li>uses the SMAPIv2 to SMAPIv1 converter to generate the necessary command-line
to talk to the SMAPIv1 plugin (EXT, NFS, LVM etc) and to execute it</li><li>uses the SMAPIv2 to SMAPIv3 converter daemon xapi-storage-script to
exectute the necessary SMAPIv3 command (GFS2)</li></ul></li><li>persists the state of the storage objects (including the result of a
<code>VDI.attach</code> call) to persistent storage</li></ul><p>Internally the SMAPIv1 plugins use privileged access to the Xapi database to
directly set fields (e.g. VDI.virtual_size) that would be considered read/only
to other clients. The SMAPIv1 plugins also rely on Xapi for</p><ul><li>knowledge of all hosts which may access the storage</li><li>locking of disks within the resource pool</li><li>safely executing code on other hosts via the &ldquo;Xapi plugin&rdquo; mechanism</li></ul><p>The Xapi database contains Host and VM metadata and is shared pool-wide. The
coordinator keeps a copy in memory, and all other nodes remote queries to the
coordinator. The database associates each object with a generation count which
is used to implement the XenAPI <code>event.next</code> and <code>event.from</code> APIs. The
database is routinely asynchronously flushed to disk in XML format. If the
&ldquo;redo-log&rdquo; is enabled then all database writes are made synchronously as deltas
to a shared block device. Without the redo-log, recent updates may be lost if
Xapi is killed before a flush.</p><p>High-Availability refers to planning for host failure, monitoring host liveness
and then following-through on the plans. Xapi defers to an external host
liveness monitor called <code>xhad</code>. When <code>xhad</code> confirms that a host has failed &ndash;
and has been isolated from the storage &ndash; then Xapi will restart any VMs which
have failed and which have been marked as &ldquo;protected&rdquo; by HA. Xapi can also
impose admission control to prevent the pool becoming too overloaded to cope
with <code>n</code> arbitrary host failures.</p><p>The <code>xe</code> CLI is implemented in terms of the XenAPI, but for efficiency the
implementation is linked directly into Xapi. The <code>xe</code> program remotes its
command-line to Xapi, and Xapi sends back a series of simple commands (prompt
for input; print line; fetch file; exit etc).</p><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Xapi</h1><article class=default><header class=headline></header><h1 id=guides>Guides</h1><p>Helpful guides for xapi developers.</p><ul class="children children-li children-sort-weight"><li><a href=/new-docs/xapi/guides/howtos/index.html>How to add....</a><ul><li><a href=/new-docs/xapi/guides/howtos/add-class/index.html>Adding a Class to the API</a><ul></ul></li><li><a href=/new-docs/xapi/guides/howtos/add-field/index.html>Adding a field to the API</a><ul></ul></li><li><a href=/new-docs/xapi/guides/howtos/add-function/index.html>Adding a function to the API</a><ul></ul></li><li><a href=/new-docs/xapi/guides/howtos/add-api-extension/index.html>Adding a XenAPI extension</a><ul></ul></li></ul></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Guides</h1><article class=default><header class=headline></header><h1 id=how-to-add>How to add....</h1><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of How to add....</h1><article class=default><header class=headline></header><h1 id=adding-a-class-to-the-api>Adding a Class to the API</h1><p>This document describes how to add a new class to the data model that
defines the Xen Server API. It complements two other documents that
describe how to extend an existing class:</p><ul><li><a href=https://xapi-project.github.io/new-docs/xapi/guides/howtos/add-field/>Adding a Field</a></li><li><a href=https://xapi-project.github.io/new-docs/xapi/guides/howtos/add-function/>Adding a Function</a></li></ul><p>As a running example, we will use the addition of a class that is part
of the design for the PVS Direct feature. PVS Direct introduces
proxies that serve VMs with disk images. This class was added via commit
<a href=https://github.com/xenserver/xen-api/commit/78fe558dad19458a89519fe196069317d57eac58 target=_blank>CP-16939</a> to Xen API.</p><h2 id=example-pvs_server>Example: PVS_server</h2><p>In the world of Xen Server, each important concept like a virtual
machine, interface, or users is represented by a class in the data model.
A class defines methods and instance variables. At runtime, all class
instances are held in an in-memory database. For example, part of [PVS
Direct] is a class <code>PVS_server</code>, representing a resource that provides
block-level data for virtual machines. The design document defines it to
have the following important properties:</p><h3 id=fields>Fields</h3><ul><li><p><code>(string set) addresses</code> (RO/constructor) IPv4 addresses of the
server.</p></li><li><p><code>(int) first_port</code> (RO/constructor) First UDP port accepted by the
server.</p></li><li><p><code>(int) last_port</code> (RO/constructor) Last UDP port accepted by the
server.</p></li><li><p><code>(PVS_farm ref) farm</code> (RO/constructor) Link to the farm that this
server is included in. A PVS_server object must always have a valid
farm reference; the PVS_server will be automatically GC’ed by xapi
if the associated PVS_farm object is removed.</p></li><li><p><code>(string) uuid (R0/runtime)</code> Unique identifier/object reference.
Allocated by the server.</p></li></ul><h3 id=methods-or-functions>Methods (or Functions)</h3><ul><li><p><code>(PVS_server ref) introduce (string set addresses, int first_port, int last_port, PVS_farm ref farm)</code> Introduce a new PVS server into
the farm. Allowed at any time, even when proxies are in use. The
proxies will be updated automatically.</p></li><li><p><code>(void) forget (PVS_server ref self)</code> Remove a PVS server from the
farm. Allowed at any time, even when proxies are in use. The
proxies will be updated automatically.</p></li></ul><h3 id=implementation-overview>Implementation Overview</h3><p>The implementation of a class is distributed over several files:</p><ul><li><code>ocaml/idl/datamodel.ml</code> &ndash; central class definition</li><li><code>ocaml/idl/datamodel_types.ml</code> &ndash; definition of releases</li><li><code>ocaml/xapi/cli_frontend.ml</code> &ndash; declaration of CLI operations</li><li><code>ocaml/xapi/cli_operations.ml</code> &ndash; implementation of CLI operations</li><li><code>ocaml/xapi/records.ml</code> &ndash; getters and setters</li><li><code>ocaml/xapi/OMakefile</code> &ndash; refers to <code>xapi_pvs_farm.ml</code></li><li><code>ocaml/xapi/api_server.ml</code> &ndash; refers to <code>xapi_pvs_farm.ml</code></li><li><code>ocaml/xapi/message_forwarding.ml</code></li><li><code>ocaml/xapi/xapi_pvs_farm.ml</code> &ndash; implementation of methods, new file</li></ul><h3 id=data-model>Data Model</h3><p>The data model <code>ocaml/idl/datamodel.ml</code> defines the class. To keep the
name space tidy, most helper functions are grouped into an internal
module:</p><pre><code>(* datamodel.ml *)

let schema_minor_vsn = 103 (* line 21 -- increment this *)
let _pvs_farm = &quot;PVS_farm&quot; (* line 153 *)

module PVS_farm = struct (* line 8658 *)
  let lifecycle = [Prototyped, rel_dundee_plus, &quot;&quot;]

  let introduce = call
    ~name:&quot;introduce&quot;
    ~doc:&quot;Introduce new PVS farm&quot;
    ~result:(Ref _pvs_farm, &quot;the new PVS farm&quot;)
    ~params:
    [ String,&quot;name&quot;,&quot;name of the PVS farm&quot;
    ]
    ~lifecycle
    ~allowed_roles:_R_POOL_OP
    ()

  let forget = call
    ~name:&quot;forget&quot;
    ~doc:&quot;Remove a farm's meta data&quot;
    ~params:
    [ Ref _pvs_farm, &quot;self&quot;, &quot;this PVS farm&quot;
    ]
    ~errs:[
      Api_errors.pvs_farm_contains_running_proxies;
      Api_errors.pvs_farm_contains_servers;
    ]
    ~lifecycle
    ~allowed_roles:_R_POOL_OP
    ()


  let set_name = call
    ~name:&quot;set_name&quot;
    ~doc:&quot;Update the name of the PVS farm&quot;
    ~params:
    [ Ref _pvs_farm, &quot;self&quot;, &quot;this PVS farm&quot;
    ; String, &quot;value&quot;, &quot;name to be used&quot;
    ]
    ~lifecycle
    ~allowed_roles:_R_POOL_OP
    ()

  let add_cache_storage = call
    ~name:&quot;add_cache_storage&quot;
    ~doc:&quot;Add a cache SR for the proxies on the farm&quot;
    ~params:
    [ Ref _pvs_farm, &quot;self&quot;, &quot;this PVS farm&quot;
    ; Ref _sr, &quot;value&quot;, &quot;SR to be used&quot;
    ]
    ~lifecycle
    ~allowed_roles:_R_POOL_OP
    ()

  let remove_cache_storage = call
    ~name:&quot;remove_cache_storage&quot;
    ~doc:&quot;Remove a cache SR for the proxies on the farm&quot;
    ~params:
    [ Ref _pvs_farm, &quot;self&quot;, &quot;this PVS farm&quot;
    ; Ref _sr, &quot;value&quot;, &quot;SR to be removed&quot;
    ]
    ~lifecycle
    ~allowed_roles:_R_POOL_OP
    ()

  let obj =
    let null_str = Some (VString &quot;&quot;) in
    let null_set = Some (VSet []) in
    create_obj (* &lt;---- creates class *)
    ~name: _pvs_farm
    ~descr:&quot;machines serving blocks of data for provisioning VMs&quot;
    ~doccomments:[]
    ~gen_constructor_destructor:false
    ~gen_events:true
    ~in_db:true
    ~lifecycle
    ~persist:PersistEverything
    ~in_oss_since:None
    ~messages_default_allowed_roles:_R_POOL_OP
    ~contents:
    [ uid     _pvs_farm ~lifecycle

    ; field   ~qualifier:StaticRO ~lifecycle
              ~ty:String &quot;name&quot; ~default_value:null_str
              &quot;Name of the PVS farm. Must match name configured in PVS&quot;

    ; field   ~qualifier:DynamicRO ~lifecycle
              ~ty:(Set (Ref _sr)) &quot;cache_storage&quot; ~default_value:null_set
              ~ignore_foreign_key:true
              &quot;The SR used by PVS proxy for the cache&quot;

    ; field   ~qualifier:DynamicRO ~lifecycle
              ~ty:(Set (Ref _pvs_server)) &quot;servers&quot;
              &quot;The set of PVS servers in the farm&quot;


    ; field   ~qualifier:DynamicRO ~lifecycle
              ~ty:(Set (Ref _pvs_proxy)) &quot;proxies&quot;
              &quot;The set of proxies associated with the farm&quot;
    ]
    ~messages:
    [ introduce
    ; forget
    ; set_name
    ; add_cache_storage
    ; remove_cache_storage
    ]
    ()
end
let pvs_farm = PVS_farm.obj
</code></pre><p>The class is defined by a call to <code>create_obj</code> and it defines the
fields and messages (methods) belonging to the class. Each field has a
name, a type, and some meta information. Likewise, each message
(or method) is created by <code>call</code> that describes its parameters.</p><p>The <code>PVS_farm</code> has additional getter and setter methods for accessing
its fields. These are not declared here as part of the messages
but are automatically generated.</p><p>To make sure the new class is actually used, it is important to enter it
into two lists:</p><pre><code>(* datamodel.ml *)
let all_system = (* line 8917 *)
  [
    ...
    vgpu_type;
    pvs_farm;
    ...
  ]

let expose_get_all_messages_for = [ (* line 9097 *)
  ...
  _pvs_farm;
  _pvs_server;
  _pvs_proxy;
</code></pre><p>When a field refers to another object that itself refers back to it,
these two need to be entered into the <code>all_relations</code> list. For example,
<code>_pvs_server</code> refers to a <code>_pvs_farm</code> value via <code>"farm"</code>, which, in
turn, refers to the <code>_pvs_server</code> value via its <code>"servers"</code> field.</p><pre><code>let all_relations =
  [
    (* ... *)
    (_sr, &quot;introduced_by&quot;), (_dr_task, &quot;introduced_SRs&quot;);
    (_pvs_server, &quot;farm&quot;), (_pvs_farm, &quot;servers&quot;);
    (_pvs_proxy,  &quot;farm&quot;), (_pvs_farm, &quot;proxies&quot;);
  ]
</code></pre><h2 id=cli-conventions>CLI Conventions</h2><p>The CLI provides access to objects from the command line. The following
conventions exist for naming fields:</p><ul><li><p>A field in the data model uses an underscore (<code>_</code>) but a hyphen (<code>-</code>)
in the CLI: what is <code>cache_storage</code> in the data model becomes
<code>cache-storage</code> in the CLI.</p></li><li><p>When a field contains a reference or multiple, like <code>proxies</code>, it
becomes <code>proxy-uuids</code> in the CLI because references are always
referred to by their UUID.</p></li></ul><h2 id=cli-getters-and-setters>CLI Getters and Setters</h2><p>All fields can be read from the CLI and some fields can also be set via
the CLI. These getters and setters are mostly generated automatically
and need to be connected to the CLI through a function in
<code>ocaml/xapi/records.ml</code>. Note that field names here use the
naming convention for the CLI:</p><pre><code>(* ocaml/xapi/records.ml *)
let pvs_farm_record rpc session_id pvs_farm =
  let _ref = ref pvs_farm in
  let empty_record =
    ToGet (fun () -&gt; Client.PVS_farm.get_record rpc session_id !_ref) in
  let record = ref empty_record in
  let x () = lzy_get record in
    { setref    = (fun r -&gt; _ref := r ; record := empty_record)
    ; setrefrec = (fun (a,b) -&gt; _ref := a; record := Got b)
    ; record    = x
    ; getref    = (fun () -&gt; !_ref)
    ; fields=
      [ make_field ~name:&quot;uuid&quot;
        ~get:(fun () -&gt; (x ()).API.pVS_farm_uuid) ()
      ; make_field ~name:&quot;name&quot;
        ~get:(fun () -&gt; (x ()).API.pVS_farm_name)
        ~set:(fun name -&gt;
          Client.PVS_farm.set_name rpc session_id !_ref name) ()
      ; make_field ~name:&quot;cache-storage&quot;
        ~get:(fun () -&gt; (x ()).API.pVS_farm_cache_storage
          |&gt; List.map get_uuid_from_ref |&gt; String.concat &quot;; &quot;)
        ~add_to_set:(fun sr_uuid -&gt;
          let sr = Client.SR.get_by_uuid rpc session_id sr_uuid in
          Client.PVS_farm.add_cache_storage rpc session_id !_ref sr)
        ~remove_from_set:(fun sr_uuid -&gt;
          let sr = Client.SR.get_by_uuid rpc session_id sr_uuid in
          Client.PVS_farm.remove_cache_storage rpc session_id !_ref sr)
        ()
      ; make_field ~name:&quot;server-uuids&quot;
        ~get:(fun () -&gt; (x ()).API.pVS_farm_servers
          |&gt; List.map get_uuid_from_ref |&gt; String.concat &quot;; &quot;)
        ~get_set:(fun () -&gt; (x ()).API.pVS_farm_servers
          |&gt; List.map get_uuid_from_ref)
        ()
      ; make_field ~name:&quot;proxy-uuids&quot;
        ~get:(fun () -&gt; (x ()).API.pVS_farm_proxies
          |&gt; List.map get_uuid_from_ref |&gt; String.concat &quot;; &quot;)
        ~get_set:(fun () -&gt; (x ()).API.pVS_farm_proxies
          |&gt; List.map get_uuid_from_ref)
        ()
      ]
    }
</code></pre><h2 id=cli-interface-to-methods>CLI Interface to Methods</h2><p>Methods accessible from the CLI are declared in
<code>ocaml/xapi/cli_frontend.ml</code>. Each declaration refers to the real
implementation of the method, like <code>Cli_operations.PVS_far.introduce</code>:</p><pre><code>(* cli_frontend.ml *)
let rec cmdtable_data : (string*cmd_spec) list =
  (* ... *)
  &quot;pvs-farm-introduce&quot;,
  {
    reqd=[&quot;name&quot;];
    optn=[];
    help=&quot;Introduce new PVS farm&quot;;
    implementation=No_fd Cli_operations.PVS_farm.introduce;
    flags=[];
  };
  &quot;pvs-farm-forget&quot;,
  {
    reqd=[&quot;uuid&quot;];
    optn=[];
    help=&quot;Forget a PVS farm&quot;;
    implementation=No_fd Cli_operations.PVS_farm.forget;
    flags=[];
  };
</code></pre><h2 id=cli-implementation-of-methods>CLI Implementation of Methods</h2><p>Each CLI operation that is not a getter or setter has an implementation
in <code>cli_operations.ml</code> which is implemented in terms of the real
implementation:</p><pre><code>(* cli_operations.ml *)
module PVS_farm = struct
  let introduce printer rpc session_id params =
    let name  = List.assoc &quot;name&quot; params in
    let ref   = Client.PVS_farm.introduce ~rpc ~session_id ~name in
    let uuid  = Client.PVS_farm.get_uuid rpc session_id ref in
    printer (Cli_printer.PList [uuid])

  let forget printer rpc session_id params =
    let uuid  = List.assoc &quot;uuid&quot; params in
    let ref   = Client.PVS_farm.get_by_uuid ~rpc ~session_id ~uuid in
    Client.PVS_farm.forget rpc session_id ref
end
</code></pre><p>Fields that should show up in the CLI interface by default are declared
in the <code>gen_cmds</code> value:</p><pre><code>(* cli_operations.ml *)
let gen_cmds rpc session_id =
  let mk = make_param_funs in
  List.concat
  [ (*...*)
  ; Client.Pool.(mk get_all get_all_records_where
    get_by_uuid pool_record &quot;pool&quot; []
    [&quot;uuid&quot;;&quot;name-label&quot;;&quot;name-description&quot;;&quot;master&quot;
    ;&quot;default-SR&quot;] rpc session_id)
  ; Client.PVS_farm.(mk get_all get_all_records_where
    get_by_uuid pvs_farm_record &quot;pvs-farm&quot; []
    [&quot;uuid&quot;;&quot;name&quot;;&quot;cache-storage&quot;;&quot;server-uuids&quot;] rpc session_id)
</code></pre><h2 id=error-messages>Error messages</h2><p>Error messages used by an implementation are introduced in two files:</p><pre><code>(* ocaml/xapi-consts/api_errors.ml *)
let pvs_farm_contains_running_proxies = &quot;PVS_FARM_CONTAINS_RUNNING_PROXIES&quot;
let pvs_farm_contains_servers = &quot;PVS_FARM_CONTAINS_SERVERS&quot;
let pvs_farm_sr_already_added = &quot;PVS_FARM_SR_ALREADY_ADDED&quot;
let pvs_farm_sr_is_in_use = &quot;PVS_FARM_SR_IS_IN_USE&quot;
let sr_not_in_pvs_farm = &quot;SR_NOT_IN_PVS_FARM&quot;
let pvs_farm_cant_set_name = &quot;PVS_FARM_CANT_SET_NAME&quot;

(* ocaml/idl/datamodel.ml *)
  (* PVS errors *)
  error Api_errors.pvs_farm_contains_running_proxies [&quot;proxies&quot;]
    ~doc:&quot;The PVS farm contains running proxies and cannot be forgotten.&quot; ();

  error Api_errors.pvs_farm_contains_servers [&quot;servers&quot;]
    ~doc:&quot;The PVS farm contains servers and cannot be forgotten.&quot;
    ();

  error Api_errors.pvs_farm_sr_already_added [&quot;farm&quot;; &quot;SR&quot;]
    ~doc:&quot;Trying to add a cache SR that is already associated with the farm&quot;
    ();

  error Api_errors.sr_not_in_pvs_farm [&quot;farm&quot;; &quot;SR&quot;]
    ~doc:&quot;The SR is not associated with the farm.&quot;
    ();

  error Api_errors.pvs_farm_sr_is_in_use [&quot;farm&quot;; &quot;SR&quot;]
    ~doc:&quot;The SR is in use by the farm and cannot be removed.&quot;
    ();

  error Api_errors.pvs_farm_cant_set_name [&quot;farm&quot;]
    ~doc:&quot;The name of the farm can't be set while proxies are active.&quot;
    ()
</code></pre><h2 id=method-implementation>Method Implementation</h2><p>The implementation of methods lives in a module in <code>ocaml/xapi</code>:</p><pre><code>(* ocaml/xapi/api_server.ml *)
  module PVS_farm = Xapi_pvs_farm
</code></pre><p>The file below is typically a new file and needs to be added to
<code>ocaml/xapi/OMakefile</code>.</p><pre><code>(* ocaml/xapi/xapi_pvs_farm.ml *)
module D = Debug.Make(struct let name = &quot;xapi_pvs_farm&quot; end)
module E = Api_errors

let api_error msg xs = raise (E.Server_error (msg, xs))

let introduce ~__context ~name =
  let pvs_farm = Ref.make () in
  let uuid = Uuid.to_string (Uuid.make_uuid ()) in
  Db.PVS_farm.create ~__context
    ~ref:pvs_farm ~uuid ~name ~cache_storage:[];
  pvs_farm

(* ... *)
</code></pre><p>Messages received on a slave host may or may not be executed there. In
the simple case, each methods executes locally:</p><pre><code>(* ocaml/xapi/message_forwarding.ml *)
module PVS_farm = struct
  let introduce ~__context ~name =
    info &quot;PVS_farm.introduce %s&quot; name;
    Local.PVS_farm.introduce ~__context ~name

  let forget ~__context ~self =
    info &quot;PVS_farm.forget&quot;;
    Local.PVS_farm.forget ~__context ~self

  let set_name ~__context ~self ~value =
    info &quot;PVS_farm.set_name %s&quot; value;
    Local.PVS_farm.set_name ~__context ~self ~value

  let add_cache_storage ~__context ~self ~value =
    info &quot;PVS_farm.add_cache_storage&quot;;
    Local.PVS_farm.add_cache_storage ~__context ~self ~value

  let remove_cache_storage ~__context ~self ~value =
    info &quot;PVS_farm.remove_cache_storage&quot;;
    Local.PVS_farm.remove_cache_storage ~__context ~self ~value
end
</code></pre><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=adding-a-field-to-the-api>Adding a field to the API</h1><p>This page describes how to add a field to XenAPI. A field is a parameter of a class that can be used in functions and read from the API.</p><h2 id=bumping-the-database-schema-version>Bumping the database schema version</h2><p>Whenever a field is added to or removed from the API, its schema version needs
to be increased. XAPI needs this fundamental procedure in order to be able to
detect that an automatic database upgrade is necessary or to find out that the
new schema is incompatible with the existing database. If the schema version is
not bumped, XAPI will start failing in unpredictable ways. Note that bumping
the version is not necessary when adding functions, only when adding fields.</p><p>The current version number is kept at the top of the file
<code>ocaml/idl/datamodel_common.ml</code> in the variables <code>schema_major_vsn</code> and
<code>schema_minor_vsn</code>, of which only the latter should be incremented (the major
version only exists for historical reasons). When moving to a new XenServer
release, also update the variable <code>last_release_schema_minor_vsn</code> to the schema
version of the last release. To keep track of the schema versions of recent
XenServer releases, the file contains variables for these, such as
<code>miami_release_schema_minor_vsn</code>. After starting a new version of Xapi on an
existing server, the database is automatically upgraded if the schema version
of the existing database matches the value of <code>last_release_schema_*_vsn</code> in the
new Xapi.</p><p>As an example, the patch below shows how the schema version was bumped when the
new API fields used for ActiveDirectory integration were added:</p><pre><code>--- a/ocaml/idl/datamodel.ml  Tue Nov 11 16:17:48 2008 +0000
+++ b/ocaml/idl/datamodel.ml  Tue Nov 11 15:53:29 2008 +0000
@@ -15,17 +15,20 @@ open Datamodel_types
  open Datamodel_types

  (* IMPORTANT: Please bump schema vsn if you change/add/remove a _field_.
     You do not have to dump vsn if you change/add/remove a message *)

  let schema_major_vsn = 5
 -let schema_minor_vsn = 55
 +let schema_minor_vsn = 56

  (* Historical schema versions just in case this is useful later *)
  let rio_schema_major_vsn = 5
  let rio_schema_minor_vsn = 19

 +let miami_release_schema_major_vsn = 5
 +let miami_release_schema_minor_vsn = 35
 +
  (* the schema vsn of the last release: used to determine whether we can
     upgrade or not.. *)
  let last_release_schema_major_vsn = 5
 -let last_release_schema_minor_vsn = 35
 +let last_release_schema_minor_vsn = 55
</code></pre><h3 id=setting-the-schema-hash>Setting the schema hash</h3><p>In the <code>ocaml/idl/schematest.ml</code> there is the <code>last_known_schema_hash</code> This needs to be updated to be the next hash after the schema version was bumped. Get the new hash by running <code>make test</code> and you will receive the correct hash in the error message.</p><h2 id=adding-the-new-field-to-some-existing-class>Adding the new field to some existing class</h2><h3 id=ocamlidldatamodelml>ocaml/idl/datamodel.ml</h3><p>Add a new &ldquo;field&rdquo; line to the class in the file <code>ocaml/idl/datamodel.ml</code> or <code>ocaml/idl/datamodel_[class].ml</code>. The new field might require
a suitable default value. This default value is used in case the user does not
provide a value for the field.</p><p>A field has a number of parameters:</p><ul><li>The lifecycle parameter, which shows how the field has evolved over time.</li><li>The qualifier parameter, which controls access to the field. The following
values are possible:</li></ul><table><thead><tr><th>Value</th><th>Meaning</th></tr></thead><tbody><tr><td>StaticRO</td><td>Field is set statically at install-time.</td></tr><tr><td>DynamicRO</td><td>Field is computed dynamically at run time.</td></tr><tr><td>RW</td><td>Field is read/write.</td></tr></tbody></table><ul><li>The ty parameter for the type of the field.</li><li>The default_value parameter.</li><li>The name of the field.</li><li>A documentation string.</li></ul><p>Example of a field in the pool class:</p><pre><code>field ~lifecycle:[Published, rel_orlando, &quot;Controls whether HA is enabled&quot;]
      ~qualifier:DynamicRO ~ty:Bool
      ~default_value:(Some (VBool false)) &quot;ha_enabled&quot; &quot;true if HA is enabled on the pool, false otherwise&quot;;
</code></pre><p>See datamodel_types.ml for information about other parameters.</p><h2 id=changing-constructors>Changing Constructors</h2><p>Adding a field would change the constructors for the class – functions
Db.*.create – and therefore, any references to these in the code need to be
updated. In the example, the argument ~ha_enabled:false should be added to any
call to Db.Pool.create.</p><p>Examples of where these calls can be found is in <code>ocaml/tests/common/test_common.ml</code> and <code>ocaml/xapi/xapi_[class].ml</code>.</p><h3 id=cli-records>CLI Records</h3><p>If you want this field to show up in the CLI (which you probably do), you will
also need to modify the Records module, in the file
<code>ocaml/xapi-cli-server/records.ml</code>. Find the record function for the class which
you have modified, add a new entry to the fields list using make_field. This type can be found in the same file.</p><p>The only required parameters are name and get (and unit, of course ).
If your field is a map or set, then you will need to pass in get_{map,set}, and
optionally set_{map,set}, if it is a RW field. The hidden parameter is useful
if you don&rsquo;t want this field to show up in a *_params_list call. As an example,
here is a field that we&rsquo;ve just added to the SM class:</p><pre><code>make_field ~name:&quot;versioned-capabilities&quot;
           ~get:(fun () -&gt; get_from_map (x ()).API.sM_versioned_capabilities)
           ~get_map:(fun () -&gt; (x ()).API.sM_versioned_capabilities)
           ~hidden:true ();
</code></pre><h2 id=testing>Testing</h2><p>The new fields can be tested by copying the newly compiled xapi binary to a
test box. After the new xapi service is started, the file
<em>/var/log/xensource.log</em> in the test box should contain a few lines reporting the
successful upgrade of the metadata schema in the test box:</p><pre><code>[...|xapi] Db has schema major_vsn=5, minor_vsn=57 (current is 5 58) (last is 5 57)
[...|xapi] Database schema version is that of last release: attempting upgrade
[...|sql] attempting to restore database from /var/xapi/state.db
[...|sql] finished parsing xml
[...|sql] writing db as xml to file '/var/xapi/state.db'.
[...|xapi] Database upgrade complete, restarting to use new db
</code></pre><h2 id=making-this-field-accessible-as-a-cli-attribute>Making this field accessible as a CLI attribute</h2><p>XenAPI functions to get and set the value of the new field are generated
automatically. It requires some extra work, however, to enable such operations
in the CLI.</p><p>The CLI has commands such as host-param-list and host-param-get. To make a new
field accessible by these commands, the file <code>xapi-cli-server/records.ml</code> needs to
be edited. For the pool.ha-enabled field, the pool_record function in this file
contains the following (note the convention to replace underscores by hyphens
in the CLI):</p><pre><code>let pool_record rpc session_id pool =
  ...
[
  ...
  make_field ~name:&quot;ha-enabled&quot; ~get:(fun () -&gt; string_of_bool (x ()).API.pool_ha_enabled) ();
  ...
]}
</code></pre><p>NB: the ~get parameter must return a string so include a relevant function to convert the type of the field into a string i.e. <code>string_of_bool</code></p><p>See <code>xapi-cli-server/records.ml</code> for examples of handling field types other than Bool.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=adding-a-function-to-the-api>Adding a function to the API</h1><p>This page describes how to add a function to XenAPI.</p><h2 id=add-message-to-api>Add message to API</h2><p>The file <code>idl/datamodel.ml</code> is a description of the API, from which the
marshalling and handler code is generated.</p><p>In this file, the <code>create_obj</code> function is used to define a class which may
contain fields and support operations (known as &ldquo;messages&rdquo;). For example, the
identifier host is defined using create_obj to encapsulate the operations which
can be performed on a host.</p><p>In order to add a function to the API, we need to add a message to an existing
class. This entails adding a function in <code>idl/datamodel.ml</code> or one of the other datamodel files to describe the new
message and adding it to the class&rsquo;s list of messages. In this example, we are adding to <code>idl/datamodel_host.ml</code>.</p><p>The function to describe the new message will look something like the following:</p><pre><code>let host_price_of = call ~flags:[`Session]
    ~name:&quot;price_of&quot;
    ~in_oss_since:None
    ~in_product_since:rel_orlando
    ~params:[(Ref _host, &quot;host&quot;, &quot;The host containing the price information&quot;);
             (String, &quot;item&quot;, &quot;The item whose price is queried&quot;)]
    ~result:(Float, &quot;The price of the item&quot;)
    ~doc:&quot;Returns the price of a named item.&quot;
    ~allowed_roles:_R_POOL_OP
    ()
</code></pre><p>By convention, the name of the function is formed from the name of the class
and the name of the message: host and price_of, in the example. An entry for
host_price_of is added to the messages of the host class:</p><pre><code>let host =
    create_obj ...
        ~messages: [...
                    host_price_of;
                   ]
...
</code></pre><p>The parameters passed to call are all optional (except ~name and ~in_product_since).</p><ul><li><p>The ~flags parameter is used to set conditions for the use of the message.
For example, `Session is used to indicate that the call must be made in the
presence of an existing session.</p></li><li><p>The value of the ~in_product_since parameter is a string taken from
<code>idl/datamodel_types.ml</code> indicates the XenServer release in which this
message was first introduced.</p></li><li><p>The ~params parameter describes a list of the formal parameters of the message.
Each parameter is described by a triple. The first component of the triple is
the type (from type ty in <code>idl/datamodel_types.ml</code>); the second is the name
of the parameter, and the third is a human-readable description of the parameter.
The first triple in the list is conventionally the instance of the class on
which the message will operate. In the example, this is a reference to the host.</p></li><li><p>Similarly, the ~result describes the message&rsquo;s return type, although this is
permitted to merely be a single value rather than a list of values. If no
~result is specified, the default is unit.</p></li><li><p>The ~doc parameter describes what the message is doing.</p></li><li><p>The bool ~hide_from_docs parameter prevents the message from being included in the documentation when generated.</p></li><li><p>The bool ~pool_internal parameter is used to indicate if the message should be callable by external systems or only internal hosts.</p></li><li><p>The ~errs parameter is a list of possible exceptions that the message can raise.</p></li><li><p>The parameter ~lifecycle takes in an array of (Status, version, doc) to indicate the lifecycle of the message type. This takes over from ~in_oss_since which indicated the release that the message type was introduced. NOTE: Leave this parameter empty, it will be populated on build.</p></li><li><p>The ~allowed_roles parameter is used for access control (see below).</p></li></ul><p>Compiling <code>xen-api.(hg|git)</code> will cause the code corresponding to this message
to be generated and output in <code>ocaml/xapi/server.ml</code>. In the example above, a
section handling an incoming call host.price_of appeared in <code>ocaml/xapi/server.ml</code>.
However, after this was generated, the rest of the build failed because this
call expects a price_of function in the Host object.</p><h2 id=expected-values-in-parameter-in_product_since>Expected values in parameter ~in_product_since</h2><p>In the example above, the value of the parameter ~in_product_since informs that
the message host_price_of was added during the rel_orlando release cycle. If a
new release cycle is required, then it needs to be added in the file
<code>idl/datamodel_types.ml</code>. The patch below shows how the new rel_george release
identifier was added. Any class, message, etc. added during the rel_george
release cycle should contain ~in_product_since:rel_george entries.
(obs: the release and upgrade infrastructure can handle only one new
<code>rel_*</code> identifier &ndash; in this case, rel_george &ndash; in each release)</p><pre><code>--- a/ocaml/idl/datamodel_types.ml Tue Nov 11 15:17:48 2008 +0000
+++ b/ocaml/idl/datamodel_types.ml Tue Nov 11 15:53:29 2008 +0000
@@ -27,14 +27,13 @@
 (* useful constants for product vsn tracking *)
 let oss_since_303 = Some &quot;3.0.3&quot;
+let rel_george = &quot;george&quot;
 let rel_orlando = &quot;orlando&quot;
 let rel_orlando_update_1 = &quot;orlando-update-1&quot;
 let rel_symc = &quot;symc&quot;
 let rel_miami = &quot;miami&quot;
 let rel_rio = &quot;rio&quot;
-let release_order = [engp:rel_rio; rel_miami; rel_symc; rel_orlando; rel_orlando_update_1]
+let release_order = [engp:rel_rio; rel_miami; rel_symc; rel_orlando; rel_orlando_update_1; rel_george] 
</code></pre><h2 id=update-expose_get_all_messages_for-list>Update expose_get_all_messages_for list</h2><p>If you are adding a new class, do not forget to add your new class _name to
the expose_get_all_messages_for list, at the bottom of datamodel.ml, in
order to have automatically generated get_all and get_all_records functions
attached to it.</p><h2 id=update-the-rbac-field-containing-the-roles-expected-to-use-the-new-api-call>Update the RBAC field containing the roles expected to use the new API call</h2><p>After the RBAC integration, Xapi provides by default a set of static roles
associated to the most common subject tasks.</p><p>The api calls associated with each role are defined by a new <code>~allowed_roles</code>
parameter in each api call, which specifies the list of static roles that
should be able to execute the call. The possible roles for this list is one of
the following names, defined in <code>datamodel.ml</code>:</p><ul><li>role_pool_admin</li><li>role_pool_operator</li><li>role_vm_power_admin</li><li>role_vm_admin</li><li>role_vm_operator</li><li>role_read_only</li></ul><p>So, for instance,</p><pre><code>~allowed_roles:[role_pool_admin,role_pool_operator] (* this is not the recommended usage, see example below *)
</code></pre><p>would be a valid list (though it is not the recommended way of using
allowed_roles, see below), meaning that subjects belonging to either
role_pool_admin or role_pool_operator can execute the api call.</p><p>The RBAC requirements define a policy where the roles in the list above are
supposed to be totally-ordered by the set of api-calls associated with each of
them. That means that any api-call allowed to role_pool_operator should also be
in role_pool_admin; any api-call allowed to role_vm_power_admin should also be
in role_pool_operator and also in role_pool_admin; and so on. Datamodel.ml
provides shortcuts for expressing these totally-ordered set of roles policy
associated with each api-call:</p><ul><li>_R_POOL_ADMIN, equivalent to [role_pool_admin]</li><li>_R_POOL_OP, equivalent to [role_pool_admin,role_pool_operator]</li><li>_R_VM_POWER_ADMIN, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin]</li><li>_R_VM_ADMIN, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin]</li><li>_R_VM_OP, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin,role_vm_op]</li><li>_R_READ_ONLY, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin,role_vm_op,role_read_only]</li></ul><p>The <code>~allowed_roles</code> parameter should use one of the shortcuts in the list above,
instead of directly using a list of roles, because the shortcuts above make sure
that the roles in the list are in a total order regarding the api-calls
permission sets. Creating an api-call with e.g.
allowed_roles:[role_pool_admin,role_vm_admin] would be wrong, because that
would mean that a pool_operator cannot execute the api-call that a vm_admin can,
breaking the total-order policy expected in the RBAC 1.0 implementation.
In the future, this requirement might be relaxed.</p><p>So, the example above should instead be used as:</p><pre><code>~allowed_roles:_R_POOL_OP  (* recommended usage via pre-defined totally-ordered role lists *)
</code></pre><p>and so on.</p><h2 id=how-to-determine-the-correct-role-of-a-new-api-call>How to determine the correct role of a new api-call:</h2><ul><li>if only xapi should execute the api-call, ie. it is an internal call: _R_POOL_ADMIN</li><li>if it is related to subject, role, external-authentication: _R_POOL_ADMIN</li><li>if it is related to accessing Dom0 (via console, ssh, whatever): _R_POOL_ADMIN</li><li>if it is related to the pool object: R_POOL_OP</li><li>if it is related to the host object, licenses, backups, physical devices: _R_POOL_OP</li><li>if it is related to managing VM memory, snapshot/checkpoint, migration: _R_VM_POWER_ADMIN</li><li>if it is related to creating, destroying, cloning, importing/exporting VMs: _R_VM_ADMIN</li><li>if it is related to starting, stopping, pausing etc VMs or otherwise accessing/manipulating VMs: _R_VM_OP</li><li>if it is related to being able to login, manipulate own tasks and read values only: _R_READ_ONLY</li></ul><h2 id=update-message-forwarding>Update message forwarding</h2><p>The &ldquo;message forwarding&rdquo; layer describes the policy of whether an incoming API
call should be forwarded to another host (such as another member of the pool)
or processed on the host which receives the call. This policy may be
non-trivial to describe and so cannot be auto-generated from the data model.</p><p>In <code>xapi/message_forwarding.ml</code>, add a function to the relevant module to
describe this policy. In the running example, we add the following function to
the Host module:</p><pre><code>let price_of ~__context ~host ~item =
    info &quot;Host.price_of for item %s&quot; item;
    let local_fn = Local.Host.price_of ~host ~item in
    do_op_on ~local_fn ~__context ~host
      (fun session_id rpc -&gt; Client.Host.price_of ~rpc ~session_id ~host ~item)
</code></pre><p>After the ~__context parameter, the parameters of this new function should
match the parameters we specified for the message. In this case, that is the
host and the item to query the price of.</p><p>The do_op_on function takes a function to execute locally and a function to
execute remotely and performs one of these operations depending on whether the
given host is the local host.</p><p>The local function references Local.Host.price_of, which is a function we will
write in the next step.</p><h2 id=implement-the-function>Implement the function</h2><p>Now we write the function to perform the logic behind the new API call.
For a host-based call, this will reside in <code>xapi/xapi_host.ml</code>. For other
classes, other files with similar names are used.</p><p>We add the following function to <code>xapi/xapi_host.ml</code>:</p><pre><code>let price_of ~__context ~host ~item =
    if item = &quot;fish&quot; then 3.14 else 0.00
</code></pre><p>We also need to add the function to the interface <code>xapi/xapi_host.mli</code>:</p><pre><code>val price_of :
    __context:Context.t -&gt; host:API.ref_host -&gt; item:string -&gt; float
</code></pre><p>Congratulations, you&rsquo;ve added a function to the API!</p><h2 id=add-the-operation-to-the-cli>Add the operation to the CLI</h2><p>Edit <code>xapi-cli-server/cli_frontend.ml</code>. Add a block to the definition of cmdtable_data as
in the following example:</p><pre><code>&quot;host-price-of&quot;,
{
  reqd=[&quot;host-uuid&quot;; &quot;item&quot;];
  optn=[];
  help=&quot;Find out the price of an item on a certain host.&quot;;
  implementation= No_fd Cli_operations.host_price_of;
  flags=[];
};
</code></pre><p>Include here the following:</p><ul><li><p>The names of required (<em>reqd</em>) and optional (<em>optn</em>) parameters.</p></li><li><p>A description to be displayed when calling <em>xe help &lt;cmd></em> in the help field.</p></li><li><p>The <em>implementation</em> should use <em>With_fd</em> if any communication with the
client is necessary (for example, showing the user a warning, sending the
contents of a file, etc.) Otherwise, <em>No_fd</em> can be used as above.</p></li><li><p>The <em>flags</em> field can be used to set special options:</p><ul><li><em>Vm_selectors</em>: adds a &ldquo;vm&rdquo; parameter for the name of a VM (rather than a UUID)</li><li><em>Host_selectors</em>: adds a &ldquo;host&rdquo; parameter for the name of a host (rather than a UUID)</li><li><em>Standard</em>: includes the command in the list of common commands displayed by <em>xe help</em></li><li><em>Neverforward:</em></li><li><em>Hidden:</em></li><li><em>Deprecated of string list:</em></li></ul></li></ul><p>Now we must implement <code>Cli_operations.host_price_of</code>. This is done in
<code>xapi-cli-server/cli_operations.ml</code>. This function typically extracts the parameters and
forwards them to the internal implementation of the function. Other arbitrary
code is permitted. For example:</p><pre><code>let host_price_of printer rpc session_id params =
  let host = Client.Host.get_by_uuid rpc session_id (List.assoc &quot;host-uuid&quot; params) in
  let item = List.assoc &quot;item&quot; params in
  let price = string_of_float (Client.Host.price_of ~rpc ~session_id ~host ~item) in
  printer (Cli_printer.PList [price])
</code></pre><h2 id=tab-completion-in-the-cli>Tab Completion in the CLI</h2><p>The CLI features tab completion for many of its commands&rsquo; parameters.
Tab completion is implemented in the file <code>ocaml/xe-cli/bash-completion</code>, which
is installed on the host as <code>/etc/bash_completion.d/cli</code>, and is done on a
parameter-name rather than on a command-name basis. The main portion of the
bash-completion file is a case statement that contains a section for each of
the parameters that benefit from completion. There is also an entry that
catches all parameter names ending at -uuid, and performs an automatic lookup
of suitable UUIDs. The host-uuid parameter of our new host-price-of command
therefore automatically gains completion capabilities.</p><h2 id=executing-the-cli-operation>Executing the CLI operation</h2><p>Recompile <code>xapi</code> with the changes described above and install it on a test machine.</p><p>Execute the following command to see if the function exists:</p><pre><code>xe help host-price-of
</code></pre><p>Invoke the function itself with the following command:</p><pre><code>xe host-price-of host-uuid=&lt;tab&gt; item=fish
</code></pre><p>and you should find out the price of fish.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=adding-a-xenapi-extension>Adding a XenAPI extension</h1><p>A XenAPI extension is a new RPC which is implemented as a separate executable
(i.e. it is not part of <code>xapi</code>)
but which still benefits from <code>xapi</code> parameter type-checking, multi-language
stub generation, documentation generation, authentication etc.
An extension can be backported to previous versions by simply adding the
implementation, without having to recompile <code>xapi</code> itself.</p><p>A XenAPI extension is in two parts:</p><ol><li>a declaration in the <a href=https://github.com/xapi-project/xen-api/blob/07056d661bbf58b652e1da59d9adf67a778a5626/ocaml/idl/datamodel.ml#L5608 target=_blank>xapi datamodel</a>.
This must use the <code>~forward_to:(Extension "filename")</code> parameter. The filename must be unique, and
should be the same as the XenAPI call name.</li><li>an implementation executable in the dom0 filesystem with path <code>/etc/xapi.d/extensions/filename</code></li></ol><h2 id=to-define-an-extension>To define an extension</h2><p>First write the declaration in the datamodel. The act of specifying the
types and writing the documentation will help clarify the intended meaning
of the call.</p><p>Second create a prototype of your implementation and put an executable file
in <code>/etc/xapi.d/extensions/filename</code>. The calling convention is:</p><ul><li>the file must be executable</li><li><code>xapi</code> will parse the XMLRPC call arguments received over the network and check the <code>session_id</code> is
valid</li><li><code>xapi</code> will execute the named executable</li><li>the XMLRPC call arguments will be sent to the executable on <code>stdin</code> and
<code>stdin</code> will be closed afterwards</li><li>the executable will run and print an XMLRPC response on <code>stdout</code></li><li><code>xapi</code> will read the response and return it to the client.</li></ul><p>See the <a href=https://github.com/xapi-project/xen-api/blob/07056d661bbf58b652e1da59d9adf67a778a5626/scripts/extensions/Test.test target=_blank>basic example</a>.</p><p>Second make a <a href=https://github.com/xapi-project/xen-api/pulls target=_blank>pull request</a>
containing only the datamodel definitions (it is not necessary to include the
prototype too).
This will attract review comments which will help you improve your API further.
Once the pull request is merged, then the API call name and extension are officially
yours and you may use them on any xapi version which supports the extension mechanism.</p><h2 id=packaging-your-extension>Packaging your extension</h2><p>Your extension <code>/etc/xapi.d/extensions/filename</code> (and dependencies) should be
packaged for your target distribution (for XenServer dom0 this would be a CentOS
RPM). Once the package is unpacked on the target machine, the extension should
be immediately callable via the XenAPI, provided the <code>xapi</code> version supports
the extension mechanism. Note the <code>xapi</code> version does not need to know about
the specific extension in advance: it will always look in <code>/etc/xapi.d/extensions/</code> for
all RPC calls whose name it does not recognise.</p><h2 id=limitations>Limitations</h2><p>On type-checking</p><ul><li>if the <code>xapi</code> version is new enough to know about your specific extension:
<code>xapi</code> will type-check the call arguments for you</li><li>if the <code>xapi</code> version is too old to know about your specific extension:
the extension will still be callable but the arguments will not be type-checked.</li></ul><p>On access control</p><ul><li>if the <code>xapi</code> version is new enough to know about your specific extension:
you can declare that a user must have a particular role (e.g. &lsquo;VM admin&rsquo;)</li><li>if the <code>xapi</code> version is too old to know about your specific extension:
the extension will still be callable but the client must have the &lsquo;Pool admin&rsquo; role.</li></ul><p>Since a <code>xapi</code> which knows about your specific extension is stricter than an older
<code>xapi</code>, it&rsquo;s a good idea to develop against the new <code>xapi</code> and then test older
<code>xapi</code> versions later.</p><footer class=footline></footer></article></section></section><article class=default><header class=headline></header><h1 id=database>Database</h1><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Database</h1><article class=default><header class=headline></header><h1 id=metadata-on-lun>Metadata-on-LUN</h1><p>In the present version of XenServer, metadata changes resulting in
writes to the database are not persisted in non-volatile storage. Hence,
in case of failure, up to five minutes’ worth of metadata changes could
be lost. The Metadata-on-LUN feature addresses the issue by
ensuring that all database writes are retained. This will be used to
improve recovery from failure by storing incremental <em>deltas</em> which can
be re-applied to an old version of the database to bring it more
up-to-date. An implication of this is that clients will no longer be
required to perform a ‘pool-sync-database’ to protect critical writes,
because all writes will be implicitly protected.</p><p>This is implemented by saving descriptions of all persistent database
writes to a LUN when HA is active. Upon xapi restart after failure, such
as on master fail-over, these descriptions are read and parsed to
restore the latest version of the database.</p><h1 id=layout-on-block-device>Layout on block device</h1><p>It is useful to store the database on the block device as well as the
deltas, so that it is unambiguous on recovery which version of the
database the deltas apply to.</p><p>The content of the block device will be structured as shown in
the table below. It consists of a header; the rest of the
device is split into two halves.</p><table><thead><tr><th></th><th style=text-align:right>Length (bytes)</th><th>Description</th></tr></thead><tbody><tr><td>Header</td><td style=text-align:right>16</td><td>Magic identifier</td></tr><tr><td></td><td style=text-align:right>1</td><td>ASCII NUL</td></tr><tr><td></td><td style=text-align:right>1</td><td>Validity byte</td></tr><tr><td>First half database</td><td style=text-align:right>36</td><td>UUID as ASCII string</td></tr><tr><td></td><td style=text-align:right>16</td><td>Length of database as decimal ASCII</td></tr><tr><td></td><td style=text-align:right><em>(as specified)</em></td><td>Database (binary data)</td></tr><tr><td></td><td style=text-align:right>16</td><td>Generation count as decimal ASCII</td></tr><tr><td></td><td style=text-align:right>36</td><td>UUID as ASCII string</td></tr><tr><td>First half deltas</td><td style=text-align:right>16</td><td>Length of database delta as decimal ASCII</td></tr><tr><td></td><td style=text-align:right><em>(as specified)</em></td><td>Database delta (binary data)</td></tr><tr><td></td><td style=text-align:right>16</td><td>Generation count as decimal ASCII</td></tr><tr><td></td><td style=text-align:right>36</td><td>UUID as ASCII string</td></tr><tr><td>Second half database</td><td style=text-align:right>36</td><td>UUID as ASCII string</td></tr><tr><td></td><td style=text-align:right>16</td><td>Length of database as decimal ASCII</td></tr><tr><td></td><td style=text-align:right><em>(as specified)</em></td><td>Database (binary data)</td></tr><tr><td></td><td style=text-align:right>16</td><td>Generation count as decimal ASCII</td></tr><tr><td></td><td style=text-align:right>36</td><td>UUID as ASCII string</td></tr><tr><td>Second half deltas</td><td style=text-align:right>16</td><td>Length of database delta as decimal ASCII</td></tr><tr><td></td><td style=text-align:right><em>(as specified)</em></td><td>Database delta (binary data)</td></tr><tr><td></td><td style=text-align:right>16</td><td>Generation count as decimal ASCII</td></tr><tr><td></td><td style=text-align:right>36</td><td>UUID as ASCII string</td></tr></tbody></table><p>After the header, one or both halves may be devoid of content. In a half
which contains a database, there may be zero or more deltas (repetitions
of the last three entries in each half).</p><p>The structure of the device is split into two halves to provide
double-buffering. In case of failure during write to one half, the other
half remains intact.</p><p>The magic identifier at the start of the file protect against attempting
to treat a different device as a redo log.</p><p>The validity byte is a single `ascii character indicating the
state of the two halves. It can take the following values:</p><table><thead><tr><th>Byte</th><th>Description</th></tr></thead><tbody><tr><td><code>0</code></td><td>Neither half is valid</td></tr><tr><td><code>1</code></td><td>First half is valid</td></tr><tr><td><code>2</code></td><td>Second half is valid</td></tr></tbody></table><p>The use of lengths preceding data sections permit convenient reading.
The constant repetitions of the UUIDs act as nonces to protect
against reading in invalid data in the case of an incomplete or corrupt
write.</p><h1 id=architecture>Architecture</h1><p>The I/O to and from the block device may involve long delays. For
example, if there is a network problem, or the iSCSI device disappears,
the I/O calls may block indefinitely. It is important to isolate this
from xapi. Hence, I/O with the block device will occur in a separate
process.</p><p>Xapi will communicate with the I/O process via a UNIX domain socket using a
simple text-based protocol described below. The I/O process will use to
ensure that it can always accept xapi’s requests with a guaranteed upper
limit on the delay. Xapi can therefore communicate with the process
using blocking I/O.</p><p>Xapi will interact with the I/O process in a best-effort fashion. If it
cannot communicate with the process, or the process indicates that it
has not carried out the requested command, xapi will continue execution
regardless. Redo-log entries are idempotent (modulo the raising of
exceptions in some cases) so it is of little consequence if a particular
entry cannot be written but others can. If xapi notices that the process
has died, it will attempt to restart it.</p><p>The I/O process keeps track of a pointer for each half indicating the
position at which the next delta will be written in that half.</p><h2 id=protocol>Protocol</h2><p>Upon connection to the control socket, the I/O process will attempt to
connect to the block device. Depending on whether this is successful or
unsuccessful, one of two responses will be sent to the client.</p><ul><li><p><code>connect|ack_</code> if it is successful; or</p></li><li><p><code>connect|nack|&lt;length>|&lt;message></code> if it is unsuccessful, perhaps
because the block device does not exist or cannot be read from. The
<code>&lt;message></code> is a description of the error; the <code>&lt;length></code> of the message
is expressed using 16 digits of decimal ascii.</p></li></ul><p>The former message indicates that the I/O process is ready to receive
commands. The latter message indicates that commands can not be sent to
the I/O process.</p><p>There are three commands which xapi can send to the I/O
process. These are described below, with a high level description of the
operational semantics of the I/O process’ actions, and the corresponding
responses. For ease of parsing, each command is ten bytes in length.</p><h3 id=write-database>Write database</h3><p>Xapi requests that a new database is written to the block device, and
sends its content using the data socket.</p><h5 id=command>Command:</h5><dl><dt>: <code>writedb___|&lt;uuid>|&lt;generation-count>|&lt;length></code></dt><dd>The UUID is expressed as 36 ASCII
characters. The <em>length</em> of the data and the <em>generation-count</em> are
expressed using 16 digits of decimal ASCII.</dd></dl><h5 id=semantics>Semantics:</h5><ol><li>Read the validity byte.</li><li>If one half is valid, we will use the other half. If no halves
are valid, we will use the first half.</li><li>Read the data from the data socket and write it into the
chosen half.</li><li>Set the pointer for the chosen half to point to the position
after the data.</li><li>Set the validity byte to indicate the chosen half is valid.</li></ol><h5 id=response>Response:</h5><dl><dt>: <code>writedb|ack_</code> in case of successful write; or</dt><dd><code>writedb|nack|&lt;length>|&lt;message></code> otherwise.</dd><dd>For error messages, the <em>length</em> of the message is expressed using
16 digits of decimal ascii. In particular, the
error message for timeouts is the string <code>Timeout</code>.</dd></dl><h3 id=write-database-delta>Write database delta</h3><p>Xapi sends a description of a database delta to append to the block
device.</p><h5 id=command-1>Command:</h5><dl><dt>: <code>writedelta|&lt;uuid>|&lt;generation-count>|&lt;length>|&lt;data></code></dt><dd>The UUID is expressed as 36 ASCII
characters. The <em>length</em> of the data and the <em>generation-count</em> are
expressed using 16 digits of decimal ASCII.</dd></dl><h5 id=semantics-1>Semantics:</h5><ol><li>Read the validity byte to establish which half is valid. If
neither half is valid, return with a <code>nack</code>.</li><li>If the half’s pointer is set, seek to that position. Otherwise,
scan through the half and stop at the position after the
last write.</li><li>Write the entry.</li><li>Update the half’s pointer to point to the position after
the entry.</li></ol><h5 id=response-1>Response:</h5><dl><dt>: <code>writedelta|ack_</code> in case of successful append; or</dt><dd><code>writedelta|nack|&lt;length>|&lt;message></code> otherwise.</dd><dd>For error messages, the <em>length</em> of the message is expressed using
16 digits of decimal ASCII. In particular, the
error message for timeouts is the string <code>Timeout</code>.</dd></dl><h3 id=read-log>Read log</h3><p>Xapi requests the contents of the log.</p><h5 id=command-2>Command:</h5><p>: <code>read______</code></p><h5 id=semantics-2>Semantics:</h5><ol><li>Read the validity byte to establish which half is valid. If
neither half is valid, return with an <code>end</code>.</li><li>Attempt to read the database from the current half.</li><li>If this is successful, continue in that half reading entries up
to the position of the half’s pointer. If the pointer is not
set, read until a record of length zero is found or the end of
the half is reached. Otherwise—if the attempt to the read the
database was not successful—switch to using the other half and
try again from step 2.</li><li>Finally output an <code>end</code>.</li></ol><h5 id=response-2>Response:</h5><dl><dt>: <code>read|nack_|&lt;length>|&lt;message></code> in case of error; or</dt><dd><code>read|db___|&lt;generation-count>|&lt;length>|&lt;data></code> for a database record, then a
sequence of zero or more</dd><dd><code>read|delta|&lt;generation-count>|&lt;length>|&lt;data></code> for each delta record, then</dd><dd><code>read|end__</code></dd><dd>For each record, and for error messages, the <em>length</em> of the data or
message is expressed using 16 digits of decimal ascii. In particular, the
error message for timeouts is the string <code>Timeout</code>.</dd></dl><h3 id=re-initialise-log>Re-initialise log</h3><p>Xapi requests that the block device is re-initialised with a fresh
redo-log.</p><h5 id=command-3>Command:</h5><p>: <code>empty_____</code>\</p><h5 id=semantics-3>Semantics:</h5><p>: 1. Set the validity byte to indicate that neither half is valid.</p><h5 id=response-3>Response:</h5><dl><dt>: <code>empty|ack_</code> in case of successful re-initialisation; or</dt><dt><code>empty|nack|&lt;length>|&lt;message></code> otherwise.</dt><dd>For error messages, the <em>length</em> of the message is expressed using
16 digits of decimal ASCII. In particular, the
error message for timeouts is the string <code>Timeout</code>.</dd></dl><h1 id=impact-on-xapi-performance>Impact on xapi performance</h1><p>The implementation of the feature causes a slow-down in xapi of around
6% in the general case. However, if the LUN becomes inaccessible this
can cause a slow-down of up to 25% in the worst case.</p><p>The figure below shows the result of testing four configurations,
counting the number of database writes effected through a command-line
‘xe pool-param-set’ call.</p><ul><li><p>The first and second configurations are xapi <em>without</em> the
Metadata-on-LUN feature, with HA disabled and
enabled respectively.</p></li><li><p>The third configuration shows xapi <em>with</em> the
Metadata-on-LUN feature using a healthy LUN to which
all database writes can be successfully flushed.</p></li><li><p>The fourth configuration shows xapi <em>with</em> the
Metadata-on-LUN feature using an inaccessible LUN for
which all database writes fail.</p></li></ul><p><a href=#image-6d55f54ccf775d0d0ce445a993664faa class=lightbox-link><img src=/new-docs/xapi/database/redo-log/performance.svg alt="Impact of feature on xapi database-writing performance. (Green points
represent individual samples; red bars are the arithmetic means of
samples.)" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-6d55f54ccf775d0d0ce445a993664faa><img src=/new-docs/xapi/database/redo-log/performance.svg alt="Impact of feature on xapi database-writing performance. (Green points
represent individual samples; red bars are the arithmetic means of
samples.)" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><h1 id=testing-strategy>Testing strategy</h1><p>The section above shows how xapi performance is affected by this feature. The
sections below describe the dev-testing which has already been undertaken, and
propose how this feature will impact on regression testing.</p><h2 id=dev-testing-performed>Dev-testing performed</h2><p>A variety of informal tests have been performed as part of the
development process:</p><dl><dt>Enable HA.</dt><dd><p>Confirm LUN starts being used to persist database writes.</p></dd><dt>Enable HA, disable HA.</dt><dd><p>Confirm LUN stops being used.</p></dd><dt>Enable HA, kill xapi on master, restart xapi on master.</dt><dd><p>Confirm that last database write before kill is successfully
restored on restart.</p></dd><dt>Repeatedly enable and disable HA.</dt><dd><p>Confirm that no file descriptors are leaked (verified by counting
the number of descriptors in /proc/<em>pid</em>/fd/).</p></dd><dt>Enable HA, reboot the master.</dt><dd><p>Due to HA, a slave becomes the master (or this can be forced using
‘xe pool-emergency-transition-to-master’). Confirm that the new
master starts is able to restore the database from the LUN from the
point the old master left off, and begins to write new changes to
the LUN.</p></dd><dt>Enable HA, disable the iSCSI volume.</dt><dd><p>Confirm that xapi continues to make progress, although database
writes are not persisted.</p></dd><dt>Enable HA, disable and enable the iSCSI volume.</dt><dd><p>Confirm that xapi begins to use the LUN when the iSCSI volume is
re-enabled and subsequent writes are persisted.</p></dd></dl><p>These tests have been undertaken using an iSCSI target VM and a real
iSCSI volume on lannik. In these scenarios, disabling the iSCSI volume
consists of stopping the VM and unmapping the LUN, respectively.</p><h2 id=proposed-new-regression-test>Proposed new regression test</h2><p>A new regression test is proposed to confirm that all database writes
are persisted across failure.</p><p>There are three types of database modification to test: row creation,
field-write and row deletion. Although these three kinds of write could
be tested in separate tests, the means of setting up the pre-conditions
for a field-write and a row deletion require a row creation, so it is
convenient to test them all in a single test.</p><ol><li><p>Start a pool containing three hosts.</p></li><li><p>Issue a CLI command on the master to create a row in the
database, e.g.</p><p><code>xe network-create name-label=a</code>.</p></li><li><p>Forcefully power-cycle the master.</p></li><li><p>On fail-over, issue a CLI command on the new master to check that
the row creation persisted:</p><p><code>xe network-list name-label=a</code>,</p><p>confirming that the returned string is non-empty.</p></li><li><p>Issue a CLI command on the master to modify a field in the new row
in the database:</p><p><code>xe network-param-set uuid=&lt;uuid> name-description=abcd</code>,</p><p>where <code>&lt;uuid></code> is the UUID returned from step 2.</p></li><li><p>Forcefully power-cycle the master.</p></li><li><p>On fail-over, issue a CLI command on the new master to check that
the field-write persisted:</p><p><code>xe network-param-get uuid=&lt;uuid> param-name=name-description</code>,</p><p>where <code>&lt;uuid></code> is the UUID returned from step 2. The returned string
should contain</p><p><code>abcd</code>.</p></li><li><p>Issue a CLI command on the master to delete the row from the
database:</p><p><code>xe network-destroy uuid=&lt;uuid></code>,</p><p>where <code>&lt;uuid></code> is the UUID returned from step 2.</p></li><li><p>Forcefully power-cycle the master.</p></li><li><p>On fail-over, issue a CLI command on the new master to check that
the row does not exist:</p><p><code>xe network-list name-label=a</code>,</p><p>confirming that the returned string is empty.</p></li></ol><h2 id=impact-on-existing-regression-tests>Impact on existing regression tests</h2><p>The Metadata-on-LUN feature should mean that there is no
need to perform an ‘xe pool-sync-database’ operation in existing HA
regression tests to ensure that database state persists on xapi failure.</p><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=host-memory-accounting>Host memory accounting</h1><p>Memory is used for many things:</p><ul><li>the hypervisor code: this is the Xen executable itself</li><li>the hypervisor heap: this is needed for per-domain structures and per-vCPU
structures</li><li>the crash kernel: this is needed to collect information after a host crash</li><li>domain RAM: this is the memory the VM believes it has</li><li>shadow memory: for HVM guests running on hosts without hardware assisted
paging (HAP) Xen uses shadow to optimise page table updates. For all guests
shadow is used during live migration for tracking the memory transfer.</li><li>video RAM for the virtual graphics card</li></ul><p>Some of these are constants (e.g. hypervisor code) while some depend on the VM
configuration (e.g. domain RAM). Xapi calls the constants &ldquo;host overhead&rdquo; and
the variables due to VM configuration as &ldquo;VM overhead&rdquo;. There is no low-level
API to query this information, therefore xapi will sample the host overheads
at system boot time and model the per-VM overheads.</p><h2 id=host-overhead>Host overhead</h2><p>The host overhead is not managed by xapi, instead it is sampled. After the host
boots and before any VMs start, xapi asks Xen how much memory the host has in
total, and how much memory is currently free. Xapi subtracts the free from the
total and stores this as the host overhead.</p><h2 id=vm-overhead>VM overhead</h2><p>The inputs to the model are</p><ul><li><code>VM.memory_static_max</code>: the maximum amount of RAM the domain will be able to use</li><li><code>VM.HVM_shadow_multiplier</code>: allows the shadow memory to be increased</li><li><code>VM.VCPUs_max</code>: the maximum number of vCPUs the domain will be able to use</li></ul><p>First the shadow memory is calculated, in MiB</p><p><a href=#image-b31ef44c04c20d0ebab0ba13c705c1ed class=lightbox-link><img src=/new-docs/xapi/memory/shadow.svg alt="Shadow memory in MiB" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-b31ef44c04c20d0ebab0ba13c705c1ed><img src=/new-docs/xapi/memory/shadow.svg alt="Shadow memory in MiB" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Second the VM overhead is calculated, in MiB</p><p><a href=#image-7850d280588bbd342142728b051f94d8 class=lightbox-link><img src=/new-docs/xapi/memory/overhead.svg alt="Memory overhead in MiB" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-7850d280588bbd342142728b051f94d8><img src=/new-docs/xapi/memory/overhead.svg alt="Memory overhead in MiB" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><h2 id=memory-required-to-start-a-vm>Memory required to start a VM</h2><p>If ballooning is disabled, the memory required to start a VM is the same as the VM
overhead above.</p><p>If ballooning is enabled then the memory calculation above is modified to use the
<code>VM.memory_dynamic_max</code> rather than the <code>VM.memory_static_max</code>.</p><h2 id=memory-required-to-migrate-a-vm>Memory required to migrate a VM</h2><p>If ballooning is disabled, the memory required to receive a migrating VM is the same
as the VM overhead above.</p><p>If ballooning is enabled, then the VM will first be ballooned down to <code>VM.memory_dynamic_min</code>
and then it will be migrated across. If the VM fails to balloon all the way down, then
correspondingly more memory will be required on the receiving side.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-requests-walk-throughs>XAPI requests walk-throughs</h1><p>Let&rsquo;s detail the handling process of an XML request within XAPI.
The first document uses the migration as an example of such request.</p><ul><li><a href=/new-docs/xapi/walkthroughs/migration_overview.md>How the migration request goes through Xen API?</a></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of XAPI requests walk-throughs</h1><article class=default><header class=headline></header><h1 id=from-rpc-migration-request-to-xapi-internals>From RPC migration request to xapi internals</h1><h2 id=overview>Overview</h2><p>In this document we will use the <code>VM.pool_migrate</code> request to illustrate
the interaction between various components within the XAPI toolstack during
migration. However this schema can be applied to other requests as well.</p><p>Not all parts of the Xapi toolstack are shown here as not all are involved in
the migration process. For instance you won&rsquo;t see the <em><strong>squeezed</strong></em>
nor <em><strong>mpathalert</strong></em> two daemons that belong to the toolstack but don&rsquo;t
participate in the migration of a VM.</p><h2 id=anatomy-of-a-vm-migration>Anatomy of a VM migration</h2><ul><li>Migration is initiated by a Xapi client that sends <code>VM.pool_migrate</code>, an RPC
XML request.</li><li>The Xen API server handles this request and dispatches it to the server.</li><li>The server is generated using <code>XAPI IDL</code> and requests are wrapped whithin a
context, either to be forwarded to a host or executed locally. Broadly, the
context follows RBAC rules. The executed function is related to the message of
the request (refer to <a href=https://xapi-project.github.io/xen-api/ target=_blank>XenAPI Reference</a>).</li><li>In the case of the migration you can refer to <em>ocaml/idl/datamodel_vm.ml</em>.</li><li>The server will dispatch the operation to server helpers, executing the
operation synchronously or asynchronously and returning the RPC answer.</li><li><em>Message forwarding</em> decides if operation must be executed by another host
of the pool and then forward the call or if is executed locally.</li><li>When executed locally the high-level migration operation is send to the
<em>Xenopsd daemon</em> by posting a message on a known queue on the <em>message switch</em>.</li><li><em>Xenopsd</em> will get the command and will split it into several <em>atomic</em>
operations that will be run by the <em>xenopsd backend</em>.</li><li><em>Xenopsd</em> with its <em>backend</em> can then access xenstore or execute hypercall to
interact with xen a server the micro operation.</li></ul><h2 id=a-diagram-is-worth-a-thousand-words>A diagram is worth a thousand words</h2><div class="mermaid align-center">flowchart TD
%% First we are starting by a XAPI client that is sending an XML-RPC request
client((Xapi client)) -. sends RPC XML request .->
xapi_server{"`Dispatch RPC
**api_server.ml**`"}
style client stroke:#CAFEEE,stroke-width:4px
%% XAPI Toolstack internals
subgraph "Xapi Toolstack (master of the pool)"
style server stroke:#BAFA00,stroke-width:4px,stroke-dasharray: 5 5
xapi_server --dispatch call (ie VM.pool_migrate)--> server("`Auto generated using *IDL*
**server.ml**`")
server --do_dispatch (ie VM.pool_migrate)--> server_helpers["`server helpers
**server_helpers.ml**`"]
server_helpers -- call management (ie xapi_vm_migrate.ml)--> message_forwarding["`check where to run the call **message_forwarding.ml**`"]
message_forwarding -- execute locally --> vm_management["`VM Mgmt
like **xapi_vm_migrate.ml**`"]
vm_management -- Call --> xapi_xenops["`Transform xenops
see (**xapi_xenops.ml**)`"]
xapi_xenops &lt;-- Post following IDL model (see xenops_interface.ml) --> msg_switch
subgraph "Message Switch Daemon"
msg_switch[["Queues"]]
end
subgraph "Xenopsd Daemon"
msg_switch &lt;-- Push/Pop on org.xen.xapi.xenopsd.classic --> xenopsd_server
xenopsd_server["`Xenposd *frontend*
get & split high level opertion into atomics`"] o-- linked at compile time --o xenopsd_backend
end
end
%% Xenopsd backend is accessing xen and xenstore
xenopsd_backend["`Xenopsd *backend*
Backend XC (libxenctrl)`"] -. access to .-> xen_hypervisor["Xen hypervisor & xenstore"]
style xen_hypervisor stroke:#BEEF00,stroke-width:2px
%% Can send request to the host where call must be executed
message_forwarding -.forward call to .-> elected_host["Host where call must be executed"]
style elected_host stroke:#B0A,stroke-width:4px</div><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=xapis-storage-layers>XAPI's Storage Layers</h1><div class="box notices cstyle info"><div class=box-label><i class="fa-fw fas fa-info-circle"></i> Info</div><div class=box-content><p>The links in this page point to the source files of xapi
<a href=https://github.com/xapi-project/xen-api/tree/v1.127.0 target=_blank>v1.127.0</a>, and xcp-idl
<a href=https://github.com/xapi-project/xcp-idl/tree/v1.62.0 target=_blank>v1.62.0</a>, not to the
latest source code.</p><p>In the beginning of 2023, significant changes have been made in the layering.
In particular, the wrapper code from <code>storage_impl.ml</code> has been pushed down the
stack, below the mux, such that it only covers the SMAPIv1 backend and not
SMAPIv3. Also, all of the code (from xcp-idl etc) is now present in this repo
(xen-api).</p></div></div><p>Xapi directly communicates only with the SMAPIv2 layer. There are no
plugins directly implementing the SMAPIv2 interface, but the plugins in
other layers are accessed through it:</p><div class="mermaid align-center">graph TD
A[xapi] --> B[SMAPIv2 interface]
B --> C[SMAPIv2 &lt;-> SMAPIv1 translation: storage_access.ml]
B --> D[SMAPIv2 &lt;-> SMAPIv3 translation: xapi-storage-script]
C --> E[SMAPIv1 plugins]
D --> F[SMAPIv3 plugins]</div><h2 id=smapiv1>SMAPIv1</h2><p>These are the files related to SMAPIv1 in <code>xen-api/ocaml/xapi/</code>:</p><ul><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/sm.ml target=_blank>sm.ml</a>:
OCaml &ldquo;bindings&rdquo; for the SMAPIv1 Python &ldquo;drivers&rdquo; (SM)</li><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/sm_exec.ml target=_blank>sm_exec.ml</a>:
support for implementing the above &ldquo;bindings&rdquo;. The
parameters are converted to XML-RPC, passed to the relevant python
script (&ldquo;driver&rdquo;), and then the standard output of the program is
parsed as an XML-RPC response (we use
<code>xen-api-libs-transitional/http-svr/xMLRPC.ml</code> for parsing XML-RPC).
When adding new functionality, we can modify <code>type call</code> to add parameters,
but when we don&rsquo;t add any common ones, we should just pass the new
parameters in the args record.</li><li><code>smint.ml</code>: Contains types, exceptions, &mldr; for the SMAPIv1 OCaml
interface</li></ul><h2 id=smapiv2>SMAPIv2</h2><p>These are the files related to SMAPIv2, which need to be modified to
implement new calls:</p><ul><li><a href=https://github.com/xapi-project/xcp-idl/blob/v1.62.0/storage/storage_interface.ml target=_blank>xcp-idl/storage/storage_interface.ml</a>:
Contains the SMAPIv2 interface</li><li><a href=https://github.com/xapi-project/xcp-idl/blob/v1.62.0/storage/storage_skeleton.ml target=_blank>xcp-idl/storage/storage_skeleton.ml</a>:
A stub SMAPIv2 storage server implementation that matches the
SMAPIv2 storage server interface (this is verified by
<a href=https://github.com/xapi-project/xcp-idl/blob/v1.62.0/storage/storage_skeleton_test.ml target=_blank>storage_skeleton_test.ml</a>),
each of its function just raise a <code>Storage_interface.Unimplemented</code>
error. This skeleton is used to automatically fill the unimplemented
methods of the below storage servers to satisfy the interface.</li><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml target=_blank>xen-api/ocaml/xapi/storage_access.ml</a>:
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml#L104 target=_blank>module SMAPIv1</a>:
a SMAPIv2 server that does SMAPIv2 -> SMAPIv1 translation.
It passes the XML-RPC requests as the first command-line argument to the
corresponding Python script, which returns an XML-RPC response on standard
output.</li><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_impl.ml target=_blank>xen-api/ocaml/xapi/storage_impl.ml</a>:
The
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_impl.ml#L302 target=_blank>Wrapper</a>
module wraps a SMAPIv2 server (Server_impl) and takes care of
locking and datapaths (in case of multiple connections (=datapaths)
from VMs to the same VDI, it will use the superstate computed by the
<a href=https://github.com/xapi-project/xcp-idl/blob/v1.62.0/storage/vdi_automaton.ml target=_blank>Vdi_automaton</a>
in xcp-idl). It also implements some functionality, like the <code>DP</code>
module, that is not implemented in lower layers.</li><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_mux.ml target=_blank>xen-api/ocaml/xapi/storage_mux.ml</a>:
A SMAPIv2 server, which multiplexes between other servers. A
different SMAPIv2 server can be registered for each SR. Then it
forwards the calls for each SR to the &ldquo;storage plugin&rdquo; registered
for that SR.</li></ul><h3 id=how-smapiv2-works>How SMAPIv2 works:</h3><p>We use <a href=https://github.com/xapi-project/message-switch target=_blank>message-switch</a> under the hood for RPC communication between
<a href=https://github.com/xapi-project/xcp-idl target=_blank>xcp-idl</a> components. The
main <code>Storage_mux.Server</code> (basically <code>Storage_impl.Wrapper(Mux)</code>) is
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml#L1279 target=_blank>registered to
listen</a>
on the &ldquo;<code>org.xen.xapi.storage</code>&rdquo; queue <a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/xapi.ml#L801 target=_blank>during xapi&rsquo;s
startup</a>,
and this is the main entry point for incoming SMAPIv2 function calls.
<code>Storage_mux</code> does not really multiplex between different plugins right
now: <a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/xapi.ml#L799 target=_blank>earlier during xapi&rsquo;s
startup</a>,
the same SMAPIv1 storage server module <a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml#L934 target=_blank>is
registered</a>
on the various &ldquo;<code>org.xen.xapi.storage.&lt;sr type></code>&rdquo; queues for each
supported SR type. (This will change with SMAPIv3, which is accessed via
a SMAPIv2 plugin outside of xapi that translates between SMAPIv2 and
SMAPIv3.) Then, in
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml#L1531 target=_blank>Storage_access.create_sr</a>,
which is called
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/xapi_sr.ml#L326 target=_blank>during SR.create</a>,
and also
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/xapi_pbd.ml#L121 target=_blank>during PBD.plug</a>,
the relevant &ldquo;<code>org.xen.xapi.storage.&lt;sr type></code>&rdquo; queue needed for that
PBD is <a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml#L1107 target=_blank>registered with Storage_mux in
Storage_access.bind</a>
for the SR of that PBD.<br>So basically what happens is that xapi registers itself as a SMAPIv2
server, and forwards incoming function calls to itself through
<code>message-switch</code>, using its <code>Storage_mux</code> module. These calls are
forwarded to xapi&rsquo;s <code>SMAPIv1</code> module doing SMAPIv2 -> SMAPIv1
translation.</p><h4 id=registration-of-the-various-storage-servers>Registration of the various storage servers</h4><div class="mermaid align-center">sequenceDiagram
participant q as message-switch
participant v1 as Storage_access.SMAPIv1
participant svr as Storage_mux.Server
Note over q, svr: xapi startup, "Starting SMAPIv1 proxies"
q ->> v1:org.xen.xapi.storage.sr_type_1
q ->> v1:org.xen.xapi.storage.sr_type_2
q ->> v1:org.xen.xapi.storage.sr_type_3
Note over q, svr: xapi startup, "Starting SM service"
q ->> svr:org.xen.xapi.storage
Note over q, svr: SR.create, PBD.plug
svr ->> q:org.xapi.storage.sr_type_2</div><h4 id=what-happens-when-a-smapiv2-function-is-called>What happens when a SMAPIv2 &ldquo;function&rdquo; is called</h4><div class="mermaid align-center">graph TD
call[SMAPIv2 call] --VDI.attach2--> org.xen.xapi.storage
subgraph message-switch
org.xen.xapi.storage
org.xen.xapi.storage.SR_type_x
end
org.xen.xapi.storage --VDI.attach2--> Storage_impl.Wrapper
subgraph xapi
subgraph Storage_mux.server
Storage_impl.Wrapper --> Storage_mux.mux
end
Storage_access.SMAPIv1
end
Storage_mux.mux --VDI.attach2--> org.xen.xapi.storage.SR_type_x
org.xen.xapi.storage.SR_type_x --VDI.attach2--> Storage_access.SMAPIv1
subgraph SMAPIv1
driver_x[SMAPIv1 driver for SR_type_x]
end
Storage_access.SMAPIv1 --vdi_attach--> driver_x</div><h3 id=interface-changes-backward-compatibility--sxm>Interface Changes, Backward Compatibility, & SXM</h3><p>During SXM, xapi calls SMAPIv2 functions on a remote xapi. Therefore it
is important to keep all those SMAPIv2 functions backward-compatible
that we call remotely (e.g. Remote.VDI.attach), otherwise SXM from an
older to a newer xapi will break.</p><h3 id=functionality-implemented-in-smapiv2-layers>Functionality implemented in SMAPIv2 layers</h3><p>The layer between SMAPIv2 and SMAPIv1 is much fatter than the one between
SMAPIv2 and SMAPIv3. The latter does not do much, apart from simple
translation. However, the former has large portions of code in its intermediate
layers, in addition to the basic SMAPIv2 &lt;-> SMAPIv1 translation in
<code>storage_access.ml</code>.</p><p>These are the three files in xapi that implement the SMAPIv2 storage interface,
from higher to lower level:</p><ul><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_impl.ml target=_blank>xen-api/ocaml/xapi/storage_impl.ml</a>:</li><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_mux.ml target=_blank>xen-api/ocaml/xapi/storage_mux.ml</a>:</li><li><a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml target=_blank>xen-api/ocaml/xapi/storage_access.ml</a>:</li></ul><p>Functionality implemented by higher layers is not implemented by the layers below it.</p><h4 id=extra-functionality-in-storage_implml>Extra functionality in <code>storage_impl.ml</code></h4><p>In addition to its usual functions, <code>Storage_impl.Wrapper</code> also implements the
<code>UPDATES</code> and <code>TASK</code> SMAPIv2 APIs, without calling the wrapped module.</p><p>These are backed by the <code>Updates</code>, <code>Task_server</code>, and <code>Scheduler</code> modules from
xcp-idl, instantiated in xapi&rsquo;s <code>Storage_task</code> module. Migration code in
<code>Storage_mux</code> will interact with these to update task progress. There is also
an event loop in xapi that keeps calling <code>UPDATES.get</code> to keep the tasks in
xapi&rsquo;s database in sync with the storage manager&rsquo;s tasks.</p><p><code>Storage_impl.Wrapper</code> also implements the legacy <code>VDI.attach</code> call by simply
calling the newer <code>VDI.attach2</code> call in the same module. In general, this is a
good place to implement a compatibility layer for deprecated functionality
removed from other layers, because this is the first module that intercepts a
SMAPIv2 call.</p><h4 id=extra-functionality-in-storage_muxml>Extra functionality in <code>storage_mux.ml</code></h4><p><code>Storage_mux</code> implements storage motion (SXM): it implements the <code>DATA</code> and
<code>DATA.MIRROR</code> modules. Migration code will use the <code>Storage_task</code> module to run
the operations and update the task&rsquo;s progress.</p><p>It also implements the <code>Policy</code> module from the SMAPIv2 interface.</p><h2 id=smapiv3>SMAPIv3</h2><p><a href=https://xapi-project.github.io/xapi-storage/ target=_blank>SMAPIv3</a> has a slightly
different interface from SMAPIv2.The
<a href=https://github.com/xapi-project/xapi-storage-script target=_blank>xapi-storage-script</a>
daemon is a SMAPIv2 plugin separate from xapi that is doing the SMAPIv2
↔ SMAPIv3 translation. It keeps the plugins registered with xcp-idl
(their message-switch queues) up to date as their files appear or
disappear from the relevant directory.</p><h3 id=smapiv3-interface>SMAPIv3 Interface</h3><p>The SMAPIv3 interface is defined using an OCaml-based IDL from the
<a href=https://github.com/mirage/ocaml-rpc target=_blank>ocaml-rpc</a> library, and is in this
repo: <a href=https://github.com/xapi-project/xapi-storage target=_blank>https://github.com/xapi-project/xapi-storage</a></p><p>From this interface we generate</p><ul><li>OCaml RPC client bindings used in
<a href=https://github.com/xapi-project/xapi-storage-script target=_blank>xapi-storage-script</a></li><li>The <a href=https://xapi-project.github.io/xapi-storage target=_blank>SMAPIv3 API
reference</a></li><li>Python bindings, used by the SM scripts that implement the SMAPIv3
interface.<ul><li>These bindings are built by running &ldquo;<code>make</code>&rdquo; in the root
<a href=https://github.com/xapi-project/xapi-storage target=_blank>xapi-storage</a>,
and appear in the<code> _build/default/python/xapi/storage/api/v5</code>
directory.</li><li>On a XenServer host, they are stored in the
<code>/usr/lib/python2.7/site-packages/xapi/storage/api/v5/</code>
directory</li></ul></li></ul><h3 id=smapiv3-plugins>SMAPIv3 Plugins</h3><p>For <a href=https://xapi-project.github.io/xapi-storage/ target=_blank>SMAPIv3</a> we have
volume plugins to manipulate SRs and volumes (=VDIs) in them, and
datapath plugins for connecting to the volumes. Volume plugins tell us
which datapath plugins we can use with each volume, and what to pass to
the plugin. Both volume and datapath plugins implement some common
functionality: the SMAPIv3 <a href=https://xapi-project.github.io/xapi-storage/#plugin target=_blank>plugin
interface</a>.</p><h3 id=how-smapiv3-works>How SMAPIv3 works:</h3><p>The <code>xapi-storage-script</code> daemon detects volume and datapath plugins
stored in subdirectories of the
<code>/usr/libexec/xapi-storage-script/volume/</code> and
<code>/usr/libexec/xapi-storage-script/datapath/</code> directories, respectively.
When it finds a new datapath plugin, it adds the plugin to a lookup table and
uses it the next time that datapath is required. When it finds a new volume
plugin, it binds a new <a href=https://github.com/xapi-project/message-switch target=_blank>message-switch</a> queue named after the plugin&rsquo;s
subdirectory to a new server instance that uses these volume scripts.</p><p>To invoke a SMAPIv3 method, it executes a program named
<code>&lt;Interface name>.&lt;function name></code> in the plugin&rsquo;s directory, for
example
<code>/usr/libexec/xapi-storage-script/volume/org.xen.xapi.storage.gfs2/SR.ls</code>.
The inputs to each script can be passed as command-line arguments and
are type-checked using the generated Python bindings, and so are the
outputs. The URIs of the SRs that xapi-storage-script knows about are
stored in the <code>/var/run/nonpersistent/xapi-storage-script/state.db</code>
file, these URIs can be used on the command line when an sr argument is
expected.<code></code></p><h4 id=registration-of-the-various-smapiv3-plugins>Registration of the various SMAPIv3 plugins</h4><div class="mermaid align-center">sequenceDiagram
participant q as message-switch
participant v1 as (Storage_access.SMAPIv1)
participant svr as Storage_mux.Server
participant vol_dir as /../volume/
participant dp_dir as /../datapath/
participant script as xapi-storage-script
Note over script, vol_dir: xapi-storage-script startup
script ->> vol_dir: new subdir org.xen.xapi.storage.sr_type_4
q ->> script: org.xen.xapi.storage.sr_type_4
script ->> dp_dir: new subdir sr_type_4_dp
Note over q, svr: xapi startup, "Starting SMAPIv1 proxies"
q -->> v1:org.xen.xapi.storage.sr_type_1
q -->> v1:org.xen.xapi.storage.sr_type_2
q -->> v1:org.xen.xapi.storage.sr_type_3
Note over q, svr: xapi startup, "Starting SM service"
q ->> svr:org.xen.xapi.storage
Note over q, svr: SR.create, PBD.plug
svr ->> q:org.xapi.storage.sr_type_4</div><h4 id=what-happens-when-a-smapiv3-function-is-called>What happens when a SMAPIv3 &ldquo;function&rdquo; is called</h4><div class="mermaid align-center">graph TD
call[SMAPIv2 call] --VDI.attach2--> org.xen.xapi.storage
subgraph message-switch
org.xen.xapi.storage
org.xen.xapi.storage.SR_type_x
end
org.xen.xapi.storage --VDI.attach2--> Storage_impl.Wrapper
subgraph xapi
subgraph Storage_mux.server
Storage_impl.Wrapper --> Storage_mux.mux
end
Storage_access.SMAPIv1
end
Storage_mux.mux --VDI.attach2--> org.xen.xapi.storage.SR_type_x
org.xen.xapi.storage.SR_type_x -."VDI.attach2".-> Storage_access.SMAPIv1
subgraph SMAPIv1
driver_x[SMAPIv1 driver for SR_type_x]
end
Storage_access.SMAPIv1 -.vdi_attach.-> driver_x
subgraph SMAPIv3
xapi-storage-script --Datapath.attach--> v3_dp_plugin_x
subgraph SMAPIv3 plugins
v3_vol_plugin_x[volume plugin for SR_type_x]
v3_dp_plugin_x[datapath plugin for SR_type_x]
end
end
org.xen.xapi.storage.SR_type_x --VDI.attach2-->xapi-storage-script</div><h2 id=error-reporting>Error reporting</h2><p>In our SMAPIv1 OCaml &ldquo;bindings&rdquo; in xapi
(<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/sm_exec.ml target=_blank>xen-api/ocaml/xapi/sm_exec.ml</a>),
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/sm_exec.ml#L199 target=_blank>when we inspect the error codes returned from a call to
SM</a>,
we translate some of the SMAPIv1/SM error codes to XenAPI errors, and
for others, we just <a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/sm_exec.ml#L214 target=_blank>construct an error
code</a>
of the form <code>SR_BACKEND_FAILURE_&lt;SM error number></code>.</p><p>The file
<a href=https://github.com/xapi-project/xcp-idl/blob/v1.62.0/storage/storage_interface.ml#L362 target=_blank>xcp-idl/storage/storage_interface.ml</a>
defines a number of SMAPIv2 errors, ultimately all errors from the various
SMAPIv2 storage servers in xapi will be returned as one of these. Most of the
errors aren&rsquo;t converted into a specific exception in <code>Storage_interface</code>, but
are simply wrapped with <code>Storage_interface.Backend_error</code>.</p><p>The
<a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/storage_access.ml#L29 target=_blank>Storage_access.transform_storage_exn</a>
function is used by the client code in xapi to translate the SMAPIv2
errors into XenAPI errors again, this unwraps the errors wrapped with
<code>Storage_interface.Backend_error</code>.</p><h2 id=message-forwarding>Message Forwarding</h2><p>In the message forwarding layer, first we check the validity of VDI
operations using <code>mark_vdi</code> and <code>mark_sr</code>. These first check that the
operation is valid operations,
using <a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/xapi_vdi.ml#L57 target=_blank>Xapi_vdi.check_operation_error</a>,
for <code>mark_vdi</code>, which also inspects the current operations of the VDI,
and then, if the operation is valid, it is added to the VDI&rsquo;s current
operations, and update_allowed_operations is called. Then we forward
the VDI operation to a suitable host that has a PBD plugged for the
VDI&rsquo;s SR.</p><h3 id=checking-that-the-sr-is-attached>Checking that the SR is attached</h3><p>For the VDI operations, we check at two different places whether the SR
is attached: first, at the Xapi level, <a href=https://github.com/xapi-project/xen-api/blob/v1.127.0/ocaml/xapi/xapi_vdi.ml#L98 target=_blank>in
Xapi_vdi.check_operation_error</a>,
for the resize operation, and then, at the SMAPIv1 level, in
<code>Sm.assert_pbd_is_plugged</code>. <code>Sm.assert_pbd_is_plugged</code> performs the
same checks, plus it checks that the PBD is attached to the localhost,
unlike Xapi_vdi.check_operation_error. This behaviour is correct,
because <code>Xapi_vdi.check_operation_error</code> is called from the message
forwarding layer, which forwards the call to a host that has the SR
attached.</p><h2 id=vdi-identifiers-and-storage-motion>VDI Identifiers and Storage Motion</h2><ul><li>VDI &ldquo;location&rdquo;: this is the VDI identifier used by the SM backend.
It is usually the UUID of the VDI, but for ISO SRs it is the name of
the ISO.</li><li>VDI &ldquo;content_id&rdquo;: this is used for storage motion, to reduce the
amount of data copied. When we copy over a VDI, the content_id will
initially be the same. However, when we attach a VDI as read-write,
and then detach it, then we will blank its content_id (set it to a
random UUID), because we may have written to it, so the content
could be different. .</li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of XAPI's Storage Layers</h1><article class=default><header class=headline></header><h1 id=storage-migration>Storage migration</h1><h2 id=overview>Overview</h2><div class="mermaid align-left">sequenceDiagram
participant local_tapdisk as local tapdisk
participant local_smapiv2 as local SMAPIv2
participant xapi
participant remote_xapi as remote xapi
participant remote_smapiv2 as remote SMAPIv2 (might redirect)
participant remote_tapdisk as remote tapdisk
Note over xapi: Sort VDIs increasingly by size and then age
loop VM's & snapshots' VDIs & suspend images
xapi->>remote_xapi: plug dest SR to dest host and pool master
alt VDI is not mirrored
Note over xapi: We don't mirror RO VDIs & VDIs of snapshots
xapi->>local_smapiv2: DATA.copy remote_sm_url
activate local_smapiv2
local_smapiv2-->>local_smapiv2: SR.scan
local_smapiv2-->>local_smapiv2: VDI.similar_content
local_smapiv2-->>remote_smapiv2: SR.scan
Note over local_smapiv2: Find nearest smaller remote VDI remote_base, if any
alt remote_base
local_smapiv2-->>remote_smapiv2: VDI.clone
local_smapiv2-->>remote_smapiv2: VDI.resize
else no remote_base
local_smapiv2-->>remote_smapiv2: VDI.create
end
Note over local_smapiv2: call copy'
activate local_smapiv2
local_smapiv2-->>remote_smapiv2: SR.list
local_smapiv2-->>remote_smapiv2: SR.scan
Note over local_smapiv2: create new datapaths remote_dp, base_dp, leaf_dp
Note over local_smapiv2: find local base_vdi with same content_id as dest, if any
local_smapiv2-->>remote_smapiv2: VDI.attach2 remote_dp dest
local_smapiv2-->>remote_smapiv2: VDI.activate remote_dp dest
opt base_vdi
local_smapiv2-->>local_smapiv2: VDI.attach2 base_dp base_vdi
local_smapiv2-->>local_smapiv2: VDI.activate base_dp base_vdi
end
local_smapiv2-->>local_smapiv2: VDI.attach2 leaf_dp vdi
local_smapiv2-->>local_smapiv2: VDI.activate leaf_dp vdi
local_smapiv2-->>remote_xapi: sparse_dd base_vdi vdi dest [NBD URI for dest & remote_dp]
Note over remote_xapi: HTTP handler verifies credentials
remote_xapi-->>remote_tapdisk: then passes connection to tapdisk's NBD server
local_smapiv2-->>local_smapiv2: VDI.deactivate leaf_dp vdi
local_smapiv2-->>local_smapiv2: VDI.detach leaf_dp vdi
opt base_vdi
local_smapiv2-->>local_smapiv2: VDI.deactivate base_dp base_vdi
local_smapiv2-->>local_smapiv2: VDI.detach base_dp base_vdi
end
local_smapiv2-->>remote_smapiv2: DP.destroy remote_dp
deactivate local_smapiv2
local_smapiv2-->>remote_smapiv2: VDI.snapshot remote_copy
local_smapiv2-->>remote_smapiv2: VDI.destroy remote_copy
local_smapiv2->>xapi: task(snapshot)
deactivate local_smapiv2
else VDI is mirrored
Note over xapi: We mirror RW VDIs of the VM
Note over xapi: create new datapath dp
xapi->>local_smapiv2: VDI.attach2 dp
xapi->>local_smapiv2: VDI.activate dp
xapi->>local_smapiv2: DATA.MIRROR.start dp remote_sm_url
activate local_smapiv2
Note over local_smapiv2: copy disk data & mirror local writes
local_smapiv2-->>local_smapiv2: SR.scan
local_smapiv2-->>local_smapiv2: VDI.similar_content
local_smapiv2-->>remote_smapiv2: DATA.MIRROR.receive_start similars
activate remote_smapiv2
remote_smapiv2-->>local_smapiv2: mirror_vdi,mirror_dp,copy_diffs_from,copy_diffs_to,dummy_vdi
deactivate remote_smapiv2
local_smapiv2-->>local_smapiv2: DP.attach_info dp
local_smapiv2-->>remote_xapi: connect to [NBD URI for mirror_vdi & mirror_dp]
Note over remote_xapi: HTTP handler verifies credentials
remote_xapi-->>remote_tapdisk: then passes connection to tapdisk's NBD server
local_smapiv2-->>local_tapdisk: pass socket & dp to tapdisk of dp
local_smapiv2-->>local_smapiv2: VDI.snapshot local_vdi [mirror:dp]
local_smapiv2-->>local_tapdisk: [Python] unpause disk, pass dp
local_tapdisk-->>remote_tapdisk: mirror new writes via NBD to socket
Note over local_smapiv2: call copy' snapshot copy_diffs_to
local_smapiv2-->>remote_smapiv2: VDI.compose copy_diffs_to mirror_vdi
local_smapiv2-->>remote_smapiv2: VDI.remove_from_sm_config mirror_vdi base_mirror
local_smapiv2-->>remote_smapiv2: VDI.destroy dummy_vdi
local_smapiv2-->>local_smapiv2: VDI.destroy snapshot
local_smapiv2->>xapi: task(mirror ID)
deactivate local_smapiv2
xapi->>local_smapiv2: DATA.MIRROR.stat
activate local_smapiv2
local_smapiv2->>xapi: dest_vdi
deactivate local_smapiv2
end
loop until task finished
xapi->>local_smapiv2: UPDATES.get
xapi->>local_smapiv2: TASK.stat
end
xapi->>local_smapiv2: TASK.stat
xapi->>local_smapiv2: TASK.destroy
end
opt for snapshot VDIs
xapi->>local_smapiv2: SR.update_snapshot_info_src remote_sm_url
activate local_smapiv2
local_smapiv2-->>remote_smapiv2: SR.update_snapshot_info_dest
deactivate local_smapiv2
end
Note over xapi: ...
Note over xapi: reserve resources for the new VM in dest host
loop all VDIs
opt VDI is mirrored
xapi->>local_smapiv2: DP.destroy dp
end
end
opt post_detach_hook
opt active local mirror
local_smapiv2-->>remote_smapiv2: DATA.MIRROR.receive_finalize [mirror ID]
Note over remote_smapiv2: destroy mirror dp
end
end
Note over xapi: memory image migration by xenopsd
Note over xapi: destroy the VM record</div><h3 id=receiving-sxm>Receiving SXM</h3><p>These are the remote calls in the above diagram sent from the remote host to
the receiving end of storage motion:</p><ul><li>Remote SMAPIv2 -> local SMAPIv2 RPC calls:<ul><li><code>SR.list</code></li><li><code>SR.scan</code></li><li><code>SR.update_snapshot_info_dest</code></li><li><code>VDI.attach2</code></li><li><code>VDI.activate</code></li><li><code>VDI.snapshot</code></li><li><code>VDI.destroy</code></li><li>For copying:<ul><li>For copying from base:<ul><li><code>VDI.clone</code></li><li><code>VDI.resize</code></li></ul></li><li>For copying without base:<ul><li><code>VDI.create</code></li></ul></li></ul></li><li>For mirroring:<ul><li><code>DATA.MIRROR.receive_start</code></li><li><code>VDI.compose</code></li><li><code>VDI.remove_from_sm_config</code></li><li><code>DATA.MIRROR.receive_finalize</code></li></ul></li></ul></li><li>HTTP requests to xapi:<ul><li>Connecting to NBD URI via xapi&rsquo;s HTTP handler</li></ul></li></ul><hr><p>This is how xapi coordinates storage migration. We&rsquo;ll do it as a code walkthrough through the two layers: xapi and storage-in-xapi (SMAPIv2).</p><h2 id=xapi-code>Xapi code</h2><p>The entry point is in <a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/xapi_vm_migrate.ml#L786 target=_blank>xapi_vm_migration.ml</a></p><p>The function takes several arguments:</p><ul><li>a vm reference (<code>vm</code>)</li><li>a dictionary of <code>(string * string)</code> key-value pairs about the destination (<code>dest)</code>. This is the result of a previous call to the destination pool, <code>Host.migrate_receive</code></li><li><code>live</code>, a boolean of whether we should live-migrate or suspend-resume,</li><li><code>vdi_map</code>, a mapping of VDI references to destination SR references,</li><li><code>vif_map</code>, a mapping of VIF references to destination network references,</li><li><code>vgpu_map</code>, similar for VGPUs</li><li><code>options</code>, another dictionary of options</li></ul><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> migrate_send&#39;  <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>vm <span style=color:#f92672>~</span>dest <span style=color:#f92672>~</span>live <span style=color:#f92672>~</span>vdi_map <span style=color:#f92672>~</span>vif_map <span style=color:#f92672>~</span>vgpu_map <span style=color:#f92672>~</span>options <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  SMPERF.debug <span style=color:#e6db74>&#34;vm.migrate_send called vm:%s&#34;</span> <span style=color:#f92672>(</span>Db.VM.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> <span style=color:#a6e22e>Xapi_xenops</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> localhost <span style=color:#f92672>=</span> Helpers.get_localhost <span style=color:#f92672>~__</span>context <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> remote <span style=color:#f92672>=</span> remote_of_dest dest <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(* Copy mode means we don&#39;t destroy the VM on the source host. We also don&#39;t
</span></span></span><span style=display:flex><span><span style=color:#75715e>     	   copy over the RRDs/messages *)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> copy <span style=color:#f92672>=</span> <span style=color:#66d9ef>try</span> bool_of_string <span style=color:#f92672>(</span>List.assoc <span style=color:#e6db74>&#34;copy&#34;</span> options<span style=color:#f92672>)</span> <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> false <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>It begins by getting the local host reference, deciding whether we&rsquo;re copying or moving, and converting the input <code>dest</code> parameter from an untyped string association list to a typed record, <code>remote</code>, which is declared further up the file:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>type</span> remote <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  rpc <span style=color:#f92672>:</span> Rpc.call <span style=color:#f92672>-&gt;</span> Rpc.response<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  session <span style=color:#f92672>:</span> API.ref_session<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  sm_url <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  xenops_url <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  master_url <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  remote_ip <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span> <span style=color:#75715e>(* IP address *)</span>
</span></span><span style=display:flex><span>  remote_master_ip <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span> <span style=color:#75715e>(* IP address *)</span>
</span></span><span style=display:flex><span>  dest_host <span style=color:#f92672>:</span> API.ref_host<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div><p>this contains:</p><ul><li>A function, <code>rpc</code>, for calling XenAPI RPCs on the destination</li><li>A <code>session</code> valid on the destination</li><li>A <code>sm_url</code> on which SMAPIv2 APIs can be called on the destination</li><li>A <code>master_url</code> on which XenAPI commands can be called (not currently used)</li><li>The IP address, <code>remote_ip</code>, of the destination host</li><li>The IP address, <code>remote_master_ip</code>, of the master of the destination pool</li></ul><p>Next, we determine which VDIs to copy:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#75715e>(* The first thing to do is to create mirrors of all the disks on the remote.
</span></span></span><span style=display:flex><span><span style=color:#75715e>     We look through the VM&#39;s VBDs and all of those of the snapshots. We then
</span></span></span><span style=display:flex><span><span style=color:#75715e>     compile a list of all of the associated VDIs, whether we mirror them or not
</span></span></span><span style=display:flex><span><span style=color:#75715e>     (mirroring means we believe the VDI to be active and new writes should be
</span></span></span><span style=display:flex><span><span style=color:#75715e>     mirrored to the destination - otherwise we just copy it)
</span></span></span><span style=display:flex><span><span style=color:#75715e>     We look at the VDIs of the VM, the VDIs of all of the snapshots, and any
</span></span></span><span style=display:flex><span><span style=color:#75715e>     suspend-image VDIs. *)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vm_uuid <span style=color:#f92672>=</span> Db.VM.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vbds <span style=color:#f92672>=</span> Db.VM.get_VBDs <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vifs <span style=color:#f92672>=</span> Db.VM.get_VIFs <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> snapshots <span style=color:#f92672>=</span> Db.VM.get_snapshots <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vm_and_snapshots <span style=color:#f92672>=</span> vm <span style=color:#f92672>::</span> snapshots <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> snapshots_vbds <span style=color:#f92672>=</span> List.flatten <span style=color:#f92672>(</span>List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> self <span style=color:#f92672>-&gt;</span> Db.VM.get_VBDs <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>)</span> snapshots<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> snapshot_vifs <span style=color:#f92672>=</span> List.flatten <span style=color:#f92672>(</span>List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> self <span style=color:#f92672>-&gt;</span> Db.VM.get_VIFs <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>)</span> snapshots<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>we now decide whether we&rsquo;re intra-pool or not, and if we&rsquo;re intra-pool whether we&rsquo;re migrating onto the same host (localhost migrate). Intra-pool is decided by trying to do a lookup of our current host uuid on the destination pool.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> is_intra_pool <span style=color:#f92672>=</span> <span style=color:#66d9ef>try</span> ignore<span style=color:#f92672>(</span>Db.Host.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>dest_host<span style=color:#f92672>);</span> true <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> false <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> is_same_host <span style=color:#f92672>=</span> is_intra_pool <span style=color:#f92672>&amp;&amp;</span> remote<span style=color:#f92672>.</span>dest_host <span style=color:#f92672>==</span> localhost <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>if</span> copy <span style=color:#f92672>&amp;&amp;</span> is_intra_pool <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.operation_not_allowed<span style=color:#f92672>,</span> <span style=color:#f92672>[</span> <span style=color:#e6db74>&#34;Copy mode is disallowed on intra pool storage migration, try efficient alternatives e.g. VM.copy/clone.&#34;</span><span style=color:#f92672>]));</span></span></span></code></pre></div><p>Having got all of the VBDs of the VM, we now need to find the associated VDIs, filtering out empty CDs, and decide whether we&rsquo;re going to copy them or mirror them - read-only VDIs can be copied but RW VDIs must be mirrored.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vms_vdis <span style=color:#f92672>=</span> List.filter_map <span style=color:#f92672>(</span>vdi_filter <span style=color:#f92672>__</span>context true<span style=color:#f92672>)</span> vbds <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>where <code>vdi_filter</code> is defined earler:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#75715e>(* We ignore empty or CD VBDs - nothing to do there. Possible redundancy here:
</span></span></span><span style=display:flex><span><span style=color:#75715e>   I don&#39;t think any VBDs other than CD VBDs can be &#39;empty&#39; *)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> vdi_filter <span style=color:#f92672>__</span>context allow_mirror vbd <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>if</span> Db.VBD.get_empty <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vbd <span style=color:#f92672>||</span> Db.VBD.get_type <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vbd <span style=color:#f92672>=</span> <span style=color:#f92672>`</span><span style=color:#a6e22e>CD</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>then</span> <span style=color:#a6e22e>None</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> do_mirror <span style=color:#f92672>=</span> allow_mirror <span style=color:#f92672>&amp;&amp;</span> <span style=color:#f92672>(</span>Db.VBD.get_mode <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vbd <span style=color:#f92672>=</span> <span style=color:#f92672>`</span><span style=color:#a6e22e>RW</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> vm <span style=color:#f92672>=</span> Db.VBD.get_VM <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vbd <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> vdi <span style=color:#f92672>=</span> Db.VBD.get_VDI <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vbd <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Some</span> <span style=color:#f92672>(</span>get_vdi_mirror <span style=color:#f92672>__</span>context vm vdi do_mirror<span style=color:#f92672>)</span></span></span></code></pre></div><p>This in turn calls <code>get_vdi_mirror</code> which gathers together some important info:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> get_vdi_mirror <span style=color:#f92672>__</span>context vm vdi do_mirror <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> snapshot_of <span style=color:#f92672>=</span> Db.VDI.get_snapshot_of <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> size <span style=color:#f92672>=</span> Db.VDI.get_virtual_size <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> xenops_locator <span style=color:#f92672>=</span> Xapi_xenops.xenops_vdi_locator <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> location <span style=color:#f92672>=</span> Db.VDI.get_location <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dp <span style=color:#f92672>=</span> Storage_access.presentative_datapath_of_vbd <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>vm <span style=color:#f92672>~</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> sr <span style=color:#f92672>=</span> Db.SR.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:(</span>Db.VDI.get_SR <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>{</span>vdi<span style=color:#f92672>;</span> dp<span style=color:#f92672>;</span> location<span style=color:#f92672>;</span> sr<span style=color:#f92672>;</span> xenops_locator<span style=color:#f92672>;</span> size<span style=color:#f92672>;</span> snapshot_of<span style=color:#f92672>;</span> do_mirror<span style=color:#f92672>}</span></span></span></code></pre></div><p>The record is helpfully commented above:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>type</span> vdi_mirror <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  vdi <span style=color:#f92672>:</span> <span style=color:#f92672>[</span> <span style=color:#f92672>`</span><span style=color:#a6e22e>VDI</span> <span style=color:#f92672>]</span> API.Ref.t<span style=color:#f92672>;</span>           <span style=color:#75715e>(* The API reference of the local VDI *)</span>
</span></span><span style=display:flex><span>  dp <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>                        <span style=color:#75715e>(* The datapath the VDI will be using if the VM is running *)</span>
</span></span><span style=display:flex><span>  location <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>                  <span style=color:#75715e>(* The location of the VDI in the current SR *)</span>
</span></span><span style=display:flex><span>  sr <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>                        <span style=color:#75715e>(* The VDI&#39;s current SR uuid *)</span>
</span></span><span style=display:flex><span>  xenops_locator <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>            <span style=color:#75715e>(* The &#39;locator&#39; xenops uses to refer to the VDI on the current host *)</span>
</span></span><span style=display:flex><span>  size <span style=color:#f92672>:</span> Int64.t<span style=color:#f92672>;</span>                     <span style=color:#75715e>(* Size of the VDI *)</span>
</span></span><span style=display:flex><span>  snapshot_of <span style=color:#f92672>:</span> <span style=color:#f92672>[</span> <span style=color:#f92672>`</span><span style=color:#a6e22e>VDI</span> <span style=color:#f92672>]</span> API.Ref.t<span style=color:#f92672>;</span>   <span style=color:#75715e>(* API&#39;s snapshot_of reference *)</span>
</span></span><span style=display:flex><span>  do_mirror <span style=color:#f92672>:</span> <span style=color:#66d9ef>bool</span><span style=color:#f92672>;</span>                   <span style=color:#75715e>(* Whether we should mirror or just copy the VDI *)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div><p><code>xenops_locator</code> is <code>&lt;sr uuid>/&lt;vdi uuid></code>, and <code>dp</code> is <code>vbd/&lt;domid>/&lt;device></code> if the VM is running and <code>vbd/&lt;vm_uuid>/&lt;vdi_uuid></code> if not.</p><p>So now we have a list of these records for all VDIs attached to the VM. For these we check explicitly that they&rsquo;re all defined in the <code>vdi_map</code>, the mapping of VDI references to their destination SR references.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  check_vdi_map <span style=color:#f92672>~__</span>context vms_vdis vdi_map<span style=color:#f92672>;</span></span></span></code></pre></div><p>We then figure out the VIF map:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span> <span style=color:#66d9ef>let</span> vif_map <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> is_intra_pool <span style=color:#66d9ef>then</span> vif_map
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span> infer_vif_map <span style=color:#f92672>~__</span>context <span style=color:#f92672>(</span>vifs <span style=color:#f92672>@</span> snapshot_vifs<span style=color:#f92672>)</span> vif_map
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>More sanity checks: We can&rsquo;t do a storage migration if any of the VDIs is a reset-on-boot one - since the state will be lost on the destination when it&rsquo;s attached:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#75715e>(* Block SXM when VM has a VDI with on_boot=reset *)</span>
</span></span><span style=display:flex><span>  List.<span style=color:#f92672>(</span>iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vconf <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> vdi <span style=color:#f92672>=</span> vconf<span style=color:#f92672>.</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> <span style=color:#f92672>(</span>Db.VDI.get_on_boot <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi <span style=color:#f92672>==`</span>reset<span style=color:#f92672>)</span> <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.vdi_on_boot_mode_incompatible_with_operation<span style=color:#f92672>,</span> <span style=color:#f92672>[</span>Ref.string_of vdi<span style=color:#f92672>])))</span> vms_vdis<span style=color:#f92672>)</span> <span style=color:#f92672>;</span></span></span></code></pre></div><p>We now consider all of the VDIs associated with the snapshots. As for the VM&rsquo;s VBDs above, we end up with a <code>vdi_mirror</code> list. Note we pass <code>false</code> to the <code>allow_mirror</code> parameter of the <code>get_vdi_mirror</code> function as none of these snapshot VDIs will ever require mirrorring.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> snapshots_vdis <span style=color:#f92672>=</span> List.filter_map <span style=color:#f92672>(</span>vdi_filter <span style=color:#f92672>__</span>context false<span style=color:#f92672>)</span></span></span></code></pre></div><p>Finally we get all of the suspend-image VDIs from all snapshots as well as the actual VM, since it might be suspended itself:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>snapshots_vbds <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> suspends_vdis <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    List.fold_left
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> acc vm <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>if</span> Db.VM.get_power_state <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#f92672>=</span> <span style=color:#f92672>`</span><span style=color:#a6e22e>Suspended</span>
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>let</span> vdi <span style=color:#f92672>=</span> Db.VM.get_suspend_VDI <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>let</span> sr <span style=color:#f92672>=</span> Db.VDI.get_SR <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>if</span> is_intra_pool <span style=color:#f92672>&amp;&amp;</span> Helpers.host_has_pbd_for_sr <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>host<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>dest_host <span style=color:#f92672>~</span>sr
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>then</span> acc
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>else</span> <span style=color:#f92672>(</span>get_vdi_mirror <span style=color:#f92672>__</span>context vm vdi false<span style=color:#f92672>)::</span> acc
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>else</span> acc<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>      [] vm_and_snapshots <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Sanity check that we can see all of the suspend-image VDIs on this host:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span> <span style=color:#75715e>(* Double check that all of the suspend VDIs are all visible on the source *)</span>
</span></span><span style=display:flex><span>  List.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vdi_mirror <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> sr <span style=color:#f92672>=</span> Db.VDI.get_SR <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vdi_mirror<span style=color:#f92672>.</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> not <span style=color:#f92672>(</span>Helpers.host_has_pbd_for_sr <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>host<span style=color:#f92672>:</span>localhost <span style=color:#f92672>~</span>sr<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span> <span style=color:#f92672>(</span>Api_errors.suspend_image_not_accessible<span style=color:#f92672>,</span> <span style=color:#f92672>[</span> Ref.string_of vdi_mirror<span style=color:#f92672>.</span>vdi <span style=color:#f92672>])))</span> suspends_vdis<span style=color:#f92672>;</span></span></span></code></pre></div><p>Next is a fairly complex piece that determines the destination SR for all of these VDIs. We don&rsquo;t require API uses to decide destinations for all of the VDIs on snapshots and hence we have to make some decisions here:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dest_pool <span style=color:#f92672>=</span> List.hd <span style=color:#f92672>(</span>XenAPI.Pool.get_all remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> default_sr_ref <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    XenAPI.Pool.get_default_SR remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session dest_pool <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> suspend_sr_ref <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> pool_suspend_SR <span style=color:#f92672>=</span> XenAPI.Pool.get_suspend_image_SR remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session dest_pool
</span></span><span style=display:flex><span>    <span style=color:#f92672>and</span> host_suspend_SR <span style=color:#f92672>=</span> XenAPI.Host.get_suspend_image_sr remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session remote<span style=color:#f92672>.</span>dest_host <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> pool_suspend_SR <span style=color:#f92672>&lt;&gt;</span> Ref.null <span style=color:#66d9ef>then</span> pool_suspend_SR <span style=color:#66d9ef>else</span> host_suspend_SR <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(* Resolve placement of unspecified VDIs here - unspecified VDIs that
</span></span></span><span style=display:flex><span><span style=color:#75715e>            are &#39;snapshot_of&#39; a specified VDI go to the same place. suspend VDIs
</span></span></span><span style=display:flex><span><span style=color:#75715e>            that are unspecified go to the suspend_sr_ref defined above *)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> extra_vdis <span style=color:#f92672>=</span> suspends_vdis <span style=color:#f92672>@</span> snapshots_vdis <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> extra_vdi_map <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    List.map
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vconf <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>let</span> dest_sr_ref <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>let</span> is_mapped <span style=color:#f92672>=</span> List.mem_assoc vconf<span style=color:#f92672>.</span>vdi vdi_map
</span></span><span style=display:flex><span>           <span style=color:#f92672>and</span> snapshot_of_is_mapped <span style=color:#f92672>=</span> List.mem_assoc vconf<span style=color:#f92672>.</span>snapshot_of vdi_map
</span></span><span style=display:flex><span>           <span style=color:#f92672>and</span> is_suspend_vdi <span style=color:#f92672>=</span> List.mem vconf suspends_vdis
</span></span><span style=display:flex><span>           <span style=color:#f92672>and</span> remote_has_suspend_sr <span style=color:#f92672>=</span> suspend_sr_ref <span style=color:#f92672>&lt;&gt;</span> Ref.null
</span></span><span style=display:flex><span>           <span style=color:#f92672>and</span> remote_has_default_sr <span style=color:#f92672>=</span> default_sr_ref <span style=color:#f92672>&lt;&gt;</span> Ref.null <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>let</span> log_prefix <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>             Printf.sprintf <span style=color:#e6db74>&#34;Resolving VDI-&gt;SR map for VDI %s:&#34;</span> <span style=color:#f92672>(</span>Db.VDI.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>if</span> is_mapped <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>             debug <span style=color:#e6db74>&#34;%s VDI has been specified in the map&#34;</span> log_prefix<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>             List.assoc vconf<span style=color:#f92672>.</span>vdi vdi_map
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>end</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>if</span> snapshot_of_is_mapped <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>             debug <span style=color:#e6db74>&#34;%s Snapshot VDI has entry in map for it&#39;s snapshot_of link&#34;</span> log_prefix<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>             List.assoc vconf<span style=color:#f92672>.</span>snapshot_of vdi_map
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>end</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>if</span> is_suspend_vdi <span style=color:#f92672>&amp;&amp;</span> remote_has_suspend_sr <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>             debug <span style=color:#e6db74>&#34;%s Mapping suspend VDI to remote suspend SR&#34;</span> log_prefix<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>             suspend_sr_ref
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>end</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>if</span> is_suspend_vdi <span style=color:#f92672>&amp;&amp;</span> remote_has_default_sr <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>             debug <span style=color:#e6db74>&#34;%s Remote suspend SR not set, mapping suspend VDI to remote default SR&#34;</span> log_prefix<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>             default_sr_ref
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>end</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>if</span> remote_has_default_sr <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>             debug <span style=color:#e6db74>&#34;%s Mapping unspecified VDI to remote default SR&#34;</span> log_prefix<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>             default_sr_ref
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>end</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>             error <span style=color:#e6db74>&#34;%s VDI not in VDI-&gt;SR map and no remote default SR is set&#34;</span> log_prefix<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>             <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.vdi_not_in_map<span style=color:#f92672>,</span> <span style=color:#f92672>[</span> Ref.string_of vconf<span style=color:#f92672>.</span>vdi <span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span>           <span style=color:#66d9ef>end</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>         <span style=color:#f92672>(</span>vconf<span style=color:#f92672>.</span>vdi<span style=color:#f92672>,</span> dest_sr_ref<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>      extra_vdis <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>At the end of this we&rsquo;ve got all of the VDIs that need to be copied and destinations for all of them:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vdi_map <span style=color:#f92672>=</span> vdi_map <span style=color:#f92672>@</span> extra_vdi_map <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> all_vdis <span style=color:#f92672>=</span> vms_vdis <span style=color:#f92672>@</span> extra_vdis <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(* The vdi_map should be complete at this point - it should include all the
</span></span></span><span style=display:flex><span><span style=color:#75715e>     VDIs in the all_vdis list. *)</span></span></span></code></pre></div><p>Now we gather some final information together:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  assert_no_cbt_enabled_vdi_migrated <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>vdi_map<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dbg <span style=color:#f92672>=</span> Context.string_of_task <span style=color:#f92672>__</span>context <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> <span style=color:#a6e22e>Xapi_xenops_queue</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> queue_name <span style=color:#f92672>=</span> queue_of_vm <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>XenopsAPI</span> <span style=color:#f92672>=</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>val</span> make_client queue_name <span style=color:#f92672>:</span> <span style=color:#a6e22e>XENOPS</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> remote_vdis <span style=color:#f92672>=</span> ref [] <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> ha_always_run_reset <span style=color:#f92672>=</span> not is_intra_pool <span style=color:#f92672>&amp;&amp;</span> Db.VM.get_ha_always_run <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> cd_vbds <span style=color:#f92672>=</span> find_cds_to_eject <span style=color:#f92672>__</span>context vdi_map vbds <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  eject_cds <span style=color:#f92672>__</span>context cd_vbds<span style=color:#f92672>;</span></span></span></code></pre></div><p>check there&rsquo;s no CBT (we can&rsquo;t currently migrate the CBT metadata), make our client to talk to Xenopsd, make a mutable list of remote VDIs (which I think is redundant right now), decide whether we need to do anything for HA (we disable HA protection for this VM on the destination until it&rsquo;s fully migrated) and eject any CDs from the VM.</p><p>Up until now this has mostly been gathering info (aside from the ejecting CDs bit), but now we&rsquo;ll start to do some actions, so we begin a <code>try-catch</code> block:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>try</span></span></span></code></pre></div><p>but we&rsquo;ve still got a bit of thinking to do: we sort the VDIs to copy based on age/size:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#75715e>(* Sort VDIs by size in principle and then age secondly. This gives better
</span></span></span><span style=display:flex><span><span style=color:#75715e>       chances that similar but smaller VDIs would arrive comparatively
</span></span></span><span style=display:flex><span><span style=color:#75715e>       earlier, which can serve as base for incremental copying the larger
</span></span></span><span style=display:flex><span><span style=color:#75715e>       ones. *)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> compare_fun v1 v2 <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> r <span style=color:#f92672>=</span> Int64.compare v1<span style=color:#f92672>.</span>size v2<span style=color:#f92672>.</span>size <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> r <span style=color:#f92672>=</span> 0 <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> t1 <span style=color:#f92672>=</span> Date.to_float <span style=color:#f92672>(</span>Db.VDI.get_snapshot_time <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>v1<span style=color:#f92672>.</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> t2 <span style=color:#f92672>=</span> Date.to_float <span style=color:#f92672>(</span>Db.VDI.get_snapshot_time <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>v2<span style=color:#f92672>.</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        compare t1 t2
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span> r <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> all_vdis <span style=color:#f92672>=</span> all_vdis <span style=color:#f92672>|&gt;</span> List.sort compare_fun <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> total_size <span style=color:#f92672>=</span> List.fold_left <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> acc vconf <span style=color:#f92672>-&gt;</span> Int64.add acc vconf<span style=color:#f92672>.</span>size<span style=color:#f92672>)</span> 0L all_vdis <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> so_far <span style=color:#f92672>=</span> ref 0L <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>OK, let&rsquo;s copy/mirror:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    with_many <span style=color:#f92672>(</span>vdi_copy_fun <span style=color:#f92672>__</span>context dbg vdi_map remote is_intra_pool remote_vdis so_far total_size copy<span style=color:#f92672>)</span> all_vdis <span style=color:#f92672>@@</span> <span style=color:#66d9ef>fun</span> all_map <span style=color:#f92672>-&gt;</span></span></span></code></pre></div><p>The copy functions are written such that they take continuations. This it to make the error handling simpler - each individual component function can perform its setup and execute the continuation. In the event of an exception coming from the continuation it can then unroll its bit of state and rethrow the exception for the next layer to handle.</p><p><code>with_many</code> is a simple helper function for nesting invocations of functions that take continuations. It has the delightful type:</p><div class="wrap-code highlight"><pre tabindex=0><code>(&#39;a -&gt; (&#39;b -&gt; &#39;c) -&gt; &#39;c) -&gt; &#39;a list -&gt; (&#39;b list -&gt; &#39;c) -&gt; &#39;c</code></pre></div><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#75715e>(* Helper function to apply a &#39;with_x&#39; function to a list *)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> <span style=color:#66d9ef>rec</span> with_many withfn many fn <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>rec</span> inner l acc <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>match</span> l <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> [] <span style=color:#f92672>-&gt;</span> fn acc
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> x<span style=color:#f92672>::</span>xs <span style=color:#f92672>-&gt;</span> withfn x <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> y <span style=color:#f92672>-&gt;</span> inner xs <span style=color:#f92672>(</span>y<span style=color:#f92672>::</span>acc<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span> inner many []</span></span></code></pre></div><p>As an example of its operation, imagine our withfn is as follows:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> withfn x c <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  Printf.printf <span style=color:#e6db74>&#34;Starting withfn: x=%d</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>    c <span style=color:#f92672>(</span>string_of_int x<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    Printf.printf <span style=color:#e6db74>&#34;Handling exception for x=%d</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e<span style=color:#f92672>;;</span></span></span></code></pre></div><p>applying this gives the output:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>utop <span style=color:#f92672>#</span> with_many withfn <span style=color:#f92672>[</span>1<span style=color:#f92672>;</span>2<span style=color:#f92672>;</span>3<span style=color:#f92672>;</span>4<span style=color:#f92672>]</span> <span style=color:#f92672>(</span>String.concat <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>);;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>1
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>2
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>3
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>4
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> <span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;4,3,2,1&#34;</span></span></span></code></pre></div><p>whereas raising an exception in the continutation results in the following:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>utop <span style=color:#f92672>#</span> with_many with_fn <span style=color:#f92672>[</span>1<span style=color:#f92672>;</span>2<span style=color:#f92672>;</span>3<span style=color:#f92672>;</span>4<span style=color:#f92672>]</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> failwith <span style=color:#e6db74>&#34;error&#34;</span><span style=color:#f92672>);;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>1
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>2
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>3
</span></span><span style=display:flex><span><span style=color:#a6e22e>Starting</span> <span style=color:#66d9ef>with</span> fn<span style=color:#f92672>:</span> x<span style=color:#f92672>=</span>4
</span></span><span style=display:flex><span><span style=color:#a6e22e>Handling</span> <span style=color:#66d9ef>exception</span> <span style=color:#66d9ef>for</span> x<span style=color:#f92672>=</span>4
</span></span><span style=display:flex><span><span style=color:#a6e22e>Handling</span> <span style=color:#66d9ef>exception</span> <span style=color:#66d9ef>for</span> x<span style=color:#f92672>=</span>3
</span></span><span style=display:flex><span><span style=color:#a6e22e>Handling</span> <span style=color:#66d9ef>exception</span> <span style=color:#66d9ef>for</span> x<span style=color:#f92672>=</span>2
</span></span><span style=display:flex><span><span style=color:#a6e22e>Handling</span> <span style=color:#66d9ef>exception</span> <span style=color:#66d9ef>for</span> x<span style=color:#f92672>=</span>1
</span></span><span style=display:flex><span><span style=color:#a6e22e>Exception</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>Failure</span> <span style=color:#e6db74>&#34;error&#34;</span><span style=color:#f92672>.</span></span></span></code></pre></div><p>All the real action is in <code>vdi_copy_fun</code>, which copies or mirrors a single VDI:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> vdi_copy_fun <span style=color:#f92672>__</span>context dbg vdi_map remote is_intra_pool remote_vdis so_far total_size copy vconf continuation <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  TaskHelper.exn_if_cancelling <span style=color:#f92672>~__</span>context<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> <span style=color:#a6e22e>Storage_access</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dest_sr_ref <span style=color:#f92672>=</span> List.assoc vconf<span style=color:#f92672>.</span>vdi vdi_map <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dest_sr_uuid <span style=color:#f92672>=</span> XenAPI.SR.get_uuid remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session dest_sr_ref <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(* Plug the destination shared SR into destination host and pool master if unplugged.
</span></span></span><span style=display:flex><span><span style=color:#75715e>     Plug the local SR into destination host only if unplugged *)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dest_pool <span style=color:#f92672>=</span> List.hd <span style=color:#f92672>(</span>XenAPI.Pool.get_all remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> master_host <span style=color:#f92672>=</span> XenAPI.Pool.get_master remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session dest_pool <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> pbds <span style=color:#f92672>=</span> XenAPI.SR.get_PBDs remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session dest_sr_ref <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> pbd_host_pair <span style=color:#f92672>=</span> List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> pbd <span style=color:#f92672>-&gt;</span> <span style=color:#f92672>(</span>pbd<span style=color:#f92672>,</span> XenAPI.PBD.get_host remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session pbd<span style=color:#f92672>))</span> pbds <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> hosts_to_be_attached <span style=color:#f92672>=</span> <span style=color:#f92672>[</span>master_host<span style=color:#f92672>;</span> remote<span style=color:#f92672>.</span>dest_host<span style=color:#f92672>]</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> pbds_to_be_plugged <span style=color:#f92672>=</span> List.filter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> <span style=color:#f92672>(_,</span> host<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span>List.mem host hosts_to_be_attached<span style=color:#f92672>)</span> <span style=color:#f92672>&amp;&amp;</span> <span style=color:#f92672>(</span>XenAPI.Host.get_enabled remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session host<span style=color:#f92672>))</span> pbd_host_pair <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  List.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> <span style=color:#f92672>(</span>pbd<span style=color:#f92672>,</span> <span style=color:#f92672>_)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> not <span style=color:#f92672>(</span>XenAPI.PBD.get_currently_attached remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session pbd<span style=color:#f92672>)</span> <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        XenAPI.PBD.plug remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session pbd<span style=color:#f92672>)</span> pbds_to_be_plugged<span style=color:#f92672>;</span></span></span></code></pre></div><p>It begins by attempting to ensure the SRs we require are definitely attached on the destination host and on the destination pool master.</p><p>There&rsquo;s now a little logic to support the case where we have cross-pool SRs and the VDI is already visible to the destination pool. Since this is outside our normal support envelope there is a key in xapi_globs that has to be set (via xapi.conf) to enable this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>rec</span> dest_vdi_exists_on_sr vdi_uuid sr_ref retry <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> dest_vdi_ref <span style=color:#f92672>=</span> XenAPI.VDI.get_by_uuid remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session vdi_uuid <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> dest_vdi_sr_ref <span style=color:#f92672>=</span> XenAPI.VDI.get_SR remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session dest_vdi_ref <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> dest_vdi_sr_ref <span style=color:#f92672>=</span> sr_ref <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        true
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>        false
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> retry <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>          XenAPI.SR.scan remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session sr_ref<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          dest_vdi_exists_on_sr vdi_uuid sr_ref false
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>        false
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(* CP-4498 added an unsupported mode to use cross-pool shared SRs - the initial
</span></span></span><span style=display:flex><span><span style=color:#75715e>     use case is for a shared raw iSCSI SR (same uuid, same VDI uuid) *)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vdi_uuid <span style=color:#f92672>=</span> Db.VDI.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> mirror <span style=color:#f92672>=</span> <span style=color:#66d9ef>if</span> <span style=color:#f92672>!</span>Xapi_globs.relax_xsm_sr_check <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> <span style=color:#f92672>(</span>dest_sr_uuid <span style=color:#f92672>=</span> vconf<span style=color:#f92672>.</span>sr<span style=color:#f92672>)</span> <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>          <span style=color:#75715e>(* Check if the VDI uuid already exists in the target SR *)</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> <span style=color:#f92672>(</span>dest_vdi_exists_on_sr vdi_uuid dest_sr_ref true<span style=color:#f92672>)</span> <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>            false
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>            failwith <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;SR UUID matches on destination but VDI does not exist&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>        true
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span>not is_intra_pool<span style=color:#f92672>)</span> <span style=color:#f92672>||</span> <span style=color:#f92672>(</span>dest_sr_uuid <span style=color:#f92672>&lt;&gt;</span> vconf<span style=color:#f92672>.</span>sr<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>The check also covers the case where we&rsquo;re doing an intra-pool migration and not copying all of the disks, in which case we don&rsquo;t need to do anything for that disk.</p><p>We now have a wrapper function that creates a new datapath and passes it to a continuation function. On error it handles the destruction of the datapath:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> with_new_dp cont <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> dp <span style=color:#f92672>=</span> Printf.sprintf <span style=color:#f92672>(</span><span style=color:#66d9ef>if</span> vconf<span style=color:#f92672>.</span>do_mirror <span style=color:#66d9ef>then</span> <span style=color:#e6db74>&#34;mirror_%s&#34;</span> <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;copy_%s&#34;</span><span style=color:#f92672>)</span> vconf<span style=color:#f92672>.</span>dp <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> cont dp
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span><span style=color:#66d9ef>try</span> SMAPI.DP.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp <span style=color:#f92672>~</span>allow_leak<span style=color:#f92672>:</span>false <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> info <span style=color:#e6db74>&#34;Failed to cleanup datapath: %s&#34;</span> dp<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>raise</span> e <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>and now a helper that, given a remote VDI uuid, looks up the reference on the remote host and gives it to a continuation function. On failure of the continuation it will destroy the remote VDI:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> with_remote_vdi remote_vdi cont <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Executing remote scan to ensure VDI is known to xapi&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    XenAPI.SR.scan remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session dest_sr_ref<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> query <span style=color:#f92672>=</span> Printf.sprintf <span style=color:#e6db74>&#34;(field </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>location</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>=</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>%s</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>) and (field </span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>SR</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>=</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>%s</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>)&#34;</span> remote_vdi <span style=color:#f92672>(</span>Ref.string_of dest_sr_ref<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> vdis <span style=color:#f92672>=</span> XenAPI.VDI.get_all_records_where remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session query <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> remote_vdi_ref <span style=color:#f92672>=</span> <span style=color:#66d9ef>match</span> vdis <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> [] <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.vdi_location_missing<span style=color:#f92672>,</span> <span style=color:#f92672>[</span>Ref.string_of dest_sr_ref<span style=color:#f92672>;</span> remote_vdi<span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> h <span style=color:#f92672>::</span> [] <span style=color:#f92672>-&gt;</span> debug <span style=color:#e6db74>&#34;Found remote vdi reference: %s&#34;</span> <span style=color:#f92672>(</span>Ref.string_of <span style=color:#f92672>(</span>fst h<span style=color:#f92672>));</span> fst h
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.location_not_unique<span style=color:#f92672>,</span> <span style=color:#f92672>[</span>Ref.string_of dest_sr_ref<span style=color:#f92672>;</span> remote_vdi<span style=color:#f92672>]))</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> cont remote_vdi_ref
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span><span style=color:#66d9ef>try</span> XenAPI.VDI.destroy remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session remote_vdi_ref <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> error <span style=color:#e6db74>&#34;Failed to destroy remote VDI&#34;</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>raise</span> e <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>another helper to gather together info about a mirrored VDI:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> get_mirror_record <span style=color:#f92672>?</span>new_dp remote_vdi remote_vdi_reference <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>{</span> mr_dp <span style=color:#f92672>=</span> new_dp<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_mirrored <span style=color:#f92672>=</span> mirror<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_local_sr <span style=color:#f92672>=</span> vconf<span style=color:#f92672>.</span>sr<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_local_vdi <span style=color:#f92672>=</span> vconf<span style=color:#f92672>.</span>location<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_remote_sr <span style=color:#f92672>=</span> dest_sr_uuid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_remote_vdi <span style=color:#f92672>=</span> remote_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_local_xenops_locator <span style=color:#f92672>=</span> vconf<span style=color:#f92672>.</span>xenops_locator<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_remote_xenops_locator <span style=color:#f92672>=</span> Xapi_xenops.xenops_vdi_locator_of_strings dest_sr_uuid remote_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_local_vdi_reference <span style=color:#f92672>=</span> vconf<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      mr_remote_vdi_reference <span style=color:#f92672>=</span> remote_vdi_reference <span style=color:#f92672>}</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>and finally the really important function:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> mirror_to_remote new_dp <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> task <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> not vconf<span style=color:#f92672>.</span>do_mirror <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        SMAPI.DATA.copy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>location <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>new_dp <span style=color:#f92672>~</span>url<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>sm_url <span style=color:#f92672>~</span>dest<span style=color:#f92672>:</span>dest_sr_uuid
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>(* Though we have no intention of &#34;write&#34;, here we use the same mode as the
</span></span></span><span style=display:flex><span><span style=color:#75715e>           associated VBD on a mirrored VDIs (i.e. always RW). This avoids problem
</span></span></span><span style=display:flex><span><span style=color:#75715e>           when we need to start/stop the VM along the migration. *)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> read_write <span style=color:#f92672>=</span> true <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>(* DP set up is only essential for MIRROR.start/stop due to their open ended pattern.
</span></span></span><span style=display:flex><span><span style=color:#75715e>           It&#39;s not necessary for copy which will take care of that itself. *)</span>
</span></span><span style=display:flex><span>        ignore<span style=color:#f92672>(</span>SMAPI.VDI.attach <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>new_dp <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>location <span style=color:#f92672>~</span>read_write<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        SMAPI.VDI.activate <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>new_dp <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>location<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        ignore<span style=color:#f92672>(</span>Storage_access.register_mirror <span style=color:#f92672>__</span>context vconf<span style=color:#f92672>.</span>location<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        SMAPI.DATA.MIRROR.start <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>vconf<span style=color:#f92672>.</span>location <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>new_dp <span style=color:#f92672>~</span>url<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>sm_url <span style=color:#f92672>~</span>dest<span style=color:#f92672>:</span>dest_sr_uuid
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>end</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> mapfn x <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> total <span style=color:#f92672>=</span> Int64.to_float total_size <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> done_ <span style=color:#f92672>=</span> Int64.to_float <span style=color:#f92672>!</span>so_far <span style=color:#f92672>/.</span> total <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> remaining <span style=color:#f92672>=</span> Int64.to_float vconf<span style=color:#f92672>.</span>size <span style=color:#f92672>/.</span> total <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      done_ <span style=color:#f92672>+.</span> x <span style=color:#f92672>*.</span> remaining <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> <span style=color:#a6e22e>Storage_access</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> task_result <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>      task <span style=color:#f92672>|&gt;</span> register_task <span style=color:#f92672>__</span>context
</span></span><span style=display:flex><span>      <span style=color:#f92672>|&gt;</span> add_to_progress_map mapfn
</span></span><span style=display:flex><span>      <span style=color:#f92672>|&gt;</span> wait_for_task dbg
</span></span><span style=display:flex><span>      <span style=color:#f92672>|&gt;</span> remove_from_progress_map
</span></span><span style=display:flex><span>      <span style=color:#f92672>|&gt;</span> unregister_task <span style=color:#f92672>__</span>context
</span></span><span style=display:flex><span>      <span style=color:#f92672>|&gt;</span> success_task dbg <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> mirror_id<span style=color:#f92672>,</span> remote_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> not vconf<span style=color:#f92672>.</span>do_mirror <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> vdi <span style=color:#f92672>=</span> task_result <span style=color:#f92672>|&gt;</span> vdi_of_task dbg <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        remote_vdis <span style=color:#f92672>:=</span> vdi<span style=color:#f92672>.</span>vdi <span style=color:#f92672>::</span> <span style=color:#f92672>!</span>remote_vdis<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>None</span><span style=color:#f92672>,</span> vdi<span style=color:#f92672>.</span>vdi
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> mirrorid <span style=color:#f92672>=</span> task_result <span style=color:#f92672>|&gt;</span> mirror_of_task dbg <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> m <span style=color:#f92672>=</span> SMAPI.DATA.MIRROR.stat <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>id<span style=color:#f92672>:</span>mirrorid <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>Some</span> mirrorid<span style=color:#f92672>,</span> m<span style=color:#f92672>.</span>Mirror.dest_vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    so_far <span style=color:#f92672>:=</span> Int64.add <span style=color:#f92672>!</span>so_far vconf<span style=color:#f92672>.</span>size<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Local VDI %s %s to %s&#34;</span> vconf<span style=color:#f92672>.</span>location <span style=color:#f92672>(</span><span style=color:#66d9ef>if</span> vconf<span style=color:#f92672>.</span>do_mirror <span style=color:#66d9ef>then</span> <span style=color:#e6db74>&#34;mirrored&#34;</span> <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;copied&#34;</span><span style=color:#f92672>)</span> remote_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    mirror_id<span style=color:#f92672>,</span> remote_vdi <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>This is the bit that actually starts the mirroring or copying. Before the call to mirror we call <code>VDI.attach</code> and <code>VDI.activate</code> locally to ensure that if the VM is shutdown then the detach/deactivate there doesn&rsquo;t kill the mirroring process.</p><p>Note the parameters to the SMAPI call are <code>sr</code> and <code>vdi</code>, locating the local VDI and SM backend, <code>new_dp</code>, the datapath we&rsquo;re using for the mirroring, <code>url</code>, which is the remote url on which SMAPI calls work, and <code>dest</code>, the destination SR uuid. These are also the arguments to <code>copy</code> above too.</p><p>There&rsquo;s a little function to calculate the overall progress of the task, and the function waits until the completion of the task before it continues. The function <code>success_task</code> will raise an exception if the task failed. For <code>DATA.mirror</code>, completion implies both that the disk data has been copied to the destination and that all local writes are being mirrored to the destination. Hence more cleanup must be done on cancellation. In contrast, if the <code>DATA.copy</code> path had been taken then the operation at this point has completely finished.</p><p>The result of this function is an optional mirror id and the remote VDI uuid.</p><p>Next, there is a <code>post_mirror</code> function:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> post_mirror mirror_id mirror_record <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> continuation mirror_record <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span><span style=color:#66d9ef>match</span> mirror_id <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span> mid <span style=color:#f92672>-&gt;</span> ignore<span style=color:#f92672>(</span>Storage_access.unregister_mirror mid<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>|</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>-&gt;</span> ()<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> mirror <span style=color:#f92672>&amp;&amp;</span> not <span style=color:#f92672>(</span>Xapi_fist.storage_motion_keep_vdi () <span style=color:#f92672>||</span> copy<span style=color:#f92672>)</span> <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        Helpers.call_api_functions <span style=color:#f92672>~__</span>context <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> rpc session_id <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>            XenAPI.VDI.destroy rpc session_id vconf<span style=color:#f92672>.</span>vdi<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>      result
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> mirror_failed <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>match</span> mirror_id <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span> mid <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          ignore<span style=color:#f92672>(</span>Storage_access.unregister_mirror mid<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> m <span style=color:#f92672>=</span> SMAPI.DATA.MIRROR.stat <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>id<span style=color:#f92672>:</span>mid <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>(</span><span style=color:#66d9ef>try</span> SMAPI.DATA.MIRROR.stop <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>id<span style=color:#f92672>:</span>mid <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> ()<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>          m<span style=color:#f92672>.</span>Mirror.failed
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>-&gt;</span> false <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> mirror_failed <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.mirror_failed<span style=color:#f92672>,[</span>Ref.string_of vconf<span style=color:#f92672>.</span>vdi<span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>raise</span> e <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>This is poorly named - it is post mirror <em>and</em> copy. The aim of this function is to destroy the source VDIs on successful completion of the continuation function, which will have migrated the VM to the destination. In its exception handler it will stop the mirroring, but before doing so it will check to see if the mirroring process it was looking after has itself failed, and raise <code>mirror_failed</code> if so. This is because a failed mirror can result in a range of actual errors, and we decide here that the failed mirror was probably the root cause.</p><p>These functions are assembled together at the end of the <code>vdi_copy_fun</code> function:</p><div class="wrap-code highlight"><pre tabindex=0><code>   if mirror then
    with_new_dp (fun new_dp -&gt;
        let mirror_id, remote_vdi = mirror_to_remote new_dp in
        with_remote_vdi remote_vdi (fun remote_vdi_ref -&gt;
            let mirror_record = get_mirror_record ~new_dp remote_vdi remote_vdi_ref in
            post_mirror mirror_id mirror_record))
  else
    let mirror_record = get_mirror_record vconf.location (XenAPI.VDI.get_by_uuid remote.rpc remote.session vdi_uuid) in
    continuation mirror_record</code></pre></div><p>again, <code>mirror</code> here is poorly named, and means mirror <em>or</em> copy.</p><p>Once all of the disks have been mirrored or copied, we jump back to the body of <code>migrate_send</code>. We split apart the mirror records according to the source of the VDI:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>      <span style=color:#66d9ef>let</span> was_from vmap <span style=color:#f92672>=</span> List.exists <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vconf <span style=color:#f92672>-&gt;</span> vconf<span style=color:#f92672>.</span>vdi <span style=color:#f92672>=</span> vmap<span style=color:#f92672>.</span>mr_local_vdi_reference<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> suspends_map<span style=color:#f92672>,</span> snapshots_map<span style=color:#f92672>,</span> vdi_map <span style=color:#f92672>=</span> List.fold_left <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> <span style=color:#f92672>(</span>suspends<span style=color:#f92672>,</span> snapshots<span style=color:#f92672>,</span> vdis<span style=color:#f92672>)</span> vmap <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> was_from vmap suspends_vdis <span style=color:#66d9ef>then</span>  vmap <span style=color:#f92672>::</span> suspends<span style=color:#f92672>,</span> snapshots<span style=color:#f92672>,</span> vdis
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>if</span> was_from vmap snapshots_vdis <span style=color:#66d9ef>then</span> suspends<span style=color:#f92672>,</span> vmap <span style=color:#f92672>::</span> snapshots<span style=color:#f92672>,</span> vdis
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>else</span> suspends<span style=color:#f92672>,</span> snapshots<span style=color:#f92672>,</span> vmap <span style=color:#f92672>::</span> vdis
</span></span><span style=display:flex><span>        <span style=color:#f92672>)</span> <span style=color:#f92672>(</span>[]<span style=color:#f92672>,</span>[]<span style=color:#f92672>,</span>[]<span style=color:#f92672>)</span> all_map <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>then we reassemble all_map from this, for some reason:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>let</span> all_map <span style=color:#f92672>=</span> List.concat <span style=color:#f92672>[</span>suspends_map<span style=color:#f92672>;</span> snapshots_map<span style=color:#f92672>;</span> vdi_map<span style=color:#f92672>]</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Now we need to update the snapshot-of links:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>     <span style=color:#75715e>(* All the disks and snapshots have been created in the remote SR(s),
</span></span></span><span style=display:flex><span><span style=color:#75715e>       * so update the snapshot links if there are any snapshots. *)</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> snapshots_map <span style=color:#f92672>&lt;&gt;</span> [] <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        update_snapshot_info <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>url<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>sm_url <span style=color:#f92672>~</span>vdi_map <span style=color:#f92672>~</span>snapshots_map<span style=color:#f92672>;</span></span></span></code></pre></div><p>I&rsquo;m not entirely sure why this is done in this layer as opposed to in the storage layer.</p><p>A little housekeeping:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>     <span style=color:#66d9ef>let</span> xenops_vdi_map <span style=color:#f92672>=</span> List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> mirror_record <span style=color:#f92672>-&gt;</span> <span style=color:#f92672>(</span>mirror_record<span style=color:#f92672>.</span>mr_local_xenops_locator<span style=color:#f92672>,</span> mirror_record<span style=color:#f92672>.</span>mr_remote_xenops_locator<span style=color:#f92672>))</span> all_map <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#75715e>(* Wait for delay fist to disappear *)</span>
</span></span><span style=display:flex><span>      wait_for_fist <span style=color:#f92672>__</span>context Xapi_fist.pause_storage_migrate <span style=color:#e6db74>&#34;pause_storage_migrate&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      TaskHelper.exn_if_cancelling <span style=color:#f92672>~__</span>context<span style=color:#f92672>;</span></span></span></code></pre></div><p>the <code>fist</code> thing here simply allows tests to put in a delay at this specific point.</p><p>We also check the task to see if we&rsquo;ve been cancelled and raise an exception if so.</p><p>The VM metadata is now imported into the remote pool, with all the XenAPI level objects remapped:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> new_vm <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> is_intra_pool
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>then</span> vm
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>          <span style=color:#75715e>(* Make sure HA replaning cycle won&#39;t occur right during the import process or immediately after *)</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> () <span style=color:#f92672>=</span> <span style=color:#66d9ef>if</span> ha_always_run_reset <span style=color:#66d9ef>then</span> XenAPI.Pool.ha_prevent_restarts_for <span style=color:#f92672>~</span>rpc<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>rpc <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>seconds<span style=color:#f92672>:(</span>Int64.of_float <span style=color:#f92672>!</span>Xapi_globs.ha_monitor_interval<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#75715e>(* Move the xapi VM metadata to the remote pool. *)</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> vms <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> vdi_map <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>              List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> mirror_record <span style=color:#f92672>-&gt;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>                    local_vdi_reference <span style=color:#f92672>=</span> mirror_record<span style=color:#f92672>.</span>mr_local_vdi_reference<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                    remote_vdi_reference <span style=color:#f92672>=</span> <span style=color:#a6e22e>Some</span> mirror_record<span style=color:#f92672>.</span>mr_remote_vdi_reference<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                  <span style=color:#f92672>})</span>
</span></span><span style=display:flex><span>                all_map <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> vif_map <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>              List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> <span style=color:#f92672>(</span>vif<span style=color:#f92672>,</span> network<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>                    local_vif_reference <span style=color:#f92672>=</span> vif<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                    remote_network_reference <span style=color:#f92672>=</span> network<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                  <span style=color:#f92672>})</span>
</span></span><span style=display:flex><span>                vif_map <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> vgpu_map <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>              List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> <span style=color:#f92672>(</span>vgpu<span style=color:#f92672>,</span> gpu_group<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>                    local_vgpu_reference <span style=color:#f92672>=</span> vgpu<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                    remote_gpu_group_reference <span style=color:#f92672>=</span> gpu_group<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                  <span style=color:#f92672>})</span>
</span></span><span style=display:flex><span>                vgpu_map
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            inter_pool_metadata_transfer <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>remote <span style=color:#f92672>~</span>vm <span style=color:#f92672>~</span>vdi_map
</span></span><span style=display:flex><span>              <span style=color:#f92672>~</span>vif_map <span style=color:#f92672>~</span>vgpu_map <span style=color:#f92672>~</span>dry_run<span style=color:#f92672>:</span>false <span style=color:#f92672>~</span>live<span style=color:#f92672>:</span>true <span style=color:#f92672>~</span>copy
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> vm <span style=color:#f92672>=</span> List.hd vms <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> () <span style=color:#f92672>=</span> <span style=color:#66d9ef>if</span> ha_always_run_reset <span style=color:#66d9ef>then</span> XenAPI.VM.set_ha_always_run <span style=color:#f92672>~</span>rpc<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>rpc <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#f92672>~</span><span style=color:#66d9ef>value</span><span style=color:#f92672>:</span>false <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#75715e>(* Reserve resources for the new VM on the destination pool&#39;s host *)</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> () <span style=color:#f92672>=</span> XenAPI.Host.allocate_resources_for_vm remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session remote<span style=color:#f92672>.</span>dest_host vm true <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          vm <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>More waiting for fist points:</p><div class="wrap-code highlight"><pre tabindex=0><code>     wait_for_fist __context Xapi_fist.pause_storage_migrate2 &#34;pause_storage_migrate2&#34;;

      (* Attach networks on remote *)
      XenAPI.Network.attach_for_vm ~rpc:remote.rpc ~session_id:remote.session ~host:remote.dest_host ~vm:new_vm;</code></pre></div><p>also make sure all the networks are plugged for the VM on the destination.
Next we create the xenopsd-level vif map, equivalent to the vdi_map above:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#75715e>(* Create the vif-map for xenops, linking VIF devices to bridge names on the remote *)</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> xenops_vif_map <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> vifs <span style=color:#f92672>=</span> XenAPI.VM.get_VIFs <span style=color:#f92672>~</span>rpc<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>rpc <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>new_vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vif <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> vifr <span style=color:#f92672>=</span> XenAPI.VIF.get_record <span style=color:#f92672>~</span>rpc<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>rpc <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vif <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> bridge <span style=color:#f92672>=</span> Xenops_interface.Network.<span style=color:#a6e22e>Local</span>
</span></span><span style=display:flex><span>                <span style=color:#f92672>(</span>XenAPI.Network.get_bridge <span style=color:#f92672>~</span>rpc<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>rpc <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vifr<span style=color:#f92672>.</span>API.vIF_network<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            vifr<span style=color:#f92672>.</span>API.vIF_device<span style=color:#f92672>,</span> bridge
</span></span><span style=display:flex><span>          <span style=color:#f92672>)</span> vifs
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Now we destroy any extra mirror datapaths we set up previously:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>     <span style=color:#75715e>(* Destroy the local datapaths - this allows the VDIs to properly detach, invoking the migrate_finalize calls *)</span>
</span></span><span style=display:flex><span>      List.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> mirror_record <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> mirror_record<span style=color:#f92672>.</span>mr_mirrored
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>match</span> mirror_record<span style=color:#f92672>.</span>mr_dp <span style=color:#66d9ef>with</span> <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span> dp <span style=color:#f92672>-&gt;</span>  SMAPI.DP.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp <span style=color:#f92672>~</span>allow_leak<span style=color:#f92672>:</span>false <span style=color:#f92672>|</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>-&gt;</span> ()<span style=color:#f92672>)</span> all_map<span style=color:#f92672>;</span></span></span></code></pre></div><p>More housekeeping:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    SMPERF.debug <span style=color:#e6db74>&#34;vm.migrate_send: migration initiated vm:%s&#34;</span> vm_uuid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#75715e>(* In case when we do SXM on the same host (mostly likely a VDI
</span></span></span><span style=display:flex><span><span style=color:#75715e>         migration), the VM&#39;s metadata in xenopsd will be in-place updated
</span></span></span><span style=display:flex><span><span style=color:#75715e>         as soon as the domain migration starts. For these case, there
</span></span></span><span style=display:flex><span><span style=color:#75715e>         will be no (clean) way back from this point. So we disable task
</span></span></span><span style=display:flex><span><span style=color:#75715e>         cancellation for them here.
</span></span></span><span style=display:flex><span><span style=color:#75715e>       *)</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> is_same_host <span style=color:#66d9ef>then</span> <span style=color:#f92672>(</span>TaskHelper.exn_if_cancelling <span style=color:#f92672>~__</span>context<span style=color:#f92672>;</span> TaskHelper.set_not_cancellable <span style=color:#f92672>~__</span>context<span style=color:#f92672>);</span></span></span></code></pre></div><p>Finally we get to the memory-image part of the migration:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>      <span style=color:#75715e>(* It&#39;s acceptable for the VM not to exist at this point; shutdown commutes with storage migrate *)</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>          Xapi_xenops.Events_from_xenopsd.with_suppressed queue_name dbg vm_uuid
</span></span><span style=display:flex><span>            <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>               <span style=color:#66d9ef>let</span> xenops_vgpu_map <span style=color:#f92672>=</span> <span style=color:#75715e>(* can raise VGPU_mapping *)</span>
</span></span><span style=display:flex><span>                 infer_vgpu_map <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>remote new_vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>               migrate_with_retry
</span></span><span style=display:flex><span>                 <span style=color:#f92672>~__</span>context queue_name dbg vm_uuid xenops_vdi_map
</span></span><span style=display:flex><span>                 xenops_vif_map xenops_vgpu_map remote<span style=color:#f92672>.</span>xenops_url<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>               Xapi_xenops.Xenopsd_metadata.delete <span style=color:#f92672>~__</span>context vm_uuid<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> Xenops_interface.<span style=color:#a6e22e>Does_not_exist</span> <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;VM&#34;</span><span style=color:#f92672>,_)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> Xenops_interface.<span style=color:#a6e22e>Does_not_exist</span> <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;extra&#34;</span><span style=color:#f92672>,_)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          info <span style=color:#e6db74>&#34;%s: VM %s stopped being live during migration&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;vm_migrate_send&#34;</span> vm_uuid
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#a6e22e>VGPU_mapping</span><span style=color:#f92672>(</span>msg<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          info <span style=color:#e6db74>&#34;%s: VM %s - can&#39;t infer vGPU map: %s&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;vm_migrate_send&#34;</span> vm_uuid msg<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>raise</span> Api_errors.
</span></span><span style=display:flex><span>                  <span style=color:#f92672>(</span><span style=color:#a6e22e>Server_error</span>
</span></span><span style=display:flex><span>                     <span style=color:#f92672>(</span>vm_migrate_failed<span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>                      <span style=color:#f92672>([</span> vm_uuid
</span></span><span style=display:flex><span>                       <span style=color:#f92672>;</span> Helpers.get_localhost_uuid ()
</span></span><span style=display:flex><span>                       <span style=color:#f92672>;</span> Db.Host.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>dest_host
</span></span><span style=display:flex><span>                       <span style=color:#f92672>;</span> <span style=color:#e6db74>&#34;The VM changed its power state during migration&#34;</span>
</span></span><span style=display:flex><span>                       <span style=color:#f92672>])))</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;Migration complete&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      SMPERF.debug <span style=color:#e6db74>&#34;vm.migrate_send: migration complete vm:%s&#34;</span> vm_uuid<span style=color:#f92672>;</span></span></span></code></pre></div><p>Now we tidy up after ourselves:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>      <span style=color:#75715e>(* So far the main body of migration is completed, and the rests are
</span></span></span><span style=display:flex><span><span style=color:#75715e>         updates, config or cleanup on the source and destination. There will
</span></span></span><span style=display:flex><span><span style=color:#75715e>         be no (clean) way back from this point, due to these destructive
</span></span></span><span style=display:flex><span><span style=color:#75715e>         changes, so we don&#39;t want user intervention e.g. task cancellation.
</span></span></span><span style=display:flex><span><span style=color:#75715e>       *)</span>
</span></span><span style=display:flex><span>      TaskHelper.exn_if_cancelling <span style=color:#f92672>~__</span>context<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      TaskHelper.set_not_cancellable <span style=color:#f92672>~__</span>context<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      XenAPI.VM.pool_migrate_complete remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session new_vm remote<span style=color:#f92672>.</span>dest_host<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      detach_local_network_for_vm <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>vm <span style=color:#f92672>~</span>destination<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>dest_host<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      Xapi_xenops.refresh_vm <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm<span style=color:#f92672>;</span></span></span></code></pre></div><p>the function <code>pool_migrate_complete</code> is called on the destination host, and consists of a few things that ordinarily would be set up during VM.start or the like:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> pool_migrate_complete <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>vm <span style=color:#f92672>~</span>host <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> id <span style=color:#f92672>=</span> Db.VM.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  debug <span style=color:#e6db74>&#34;VM.pool_migrate_complete %s&#34;</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dbg <span style=color:#f92672>=</span> Context.string_of_task <span style=color:#f92672>__</span>context <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> queue_name <span style=color:#f92672>=</span> Xapi_xenops_queue.queue_of_vm <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>if</span> Xapi_xenops.vm_exists_in_xenopsd queue_name dbg id <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>    Cpuid_helpers.update_cpu_flags <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>vm <span style=color:#f92672>~</span>host<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Xapi_xenops.set_resident_on <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Xapi_xenops.add_caches id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Xapi_xenops.refresh_vm <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Monitor_dbcalls_cache.clear_cache_for_vm <span style=color:#f92672>~</span>vm_uuid<span style=color:#f92672>:</span>id
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>end</span></span></span></code></pre></div><p>More tidying up, remapping some remaining VBDs and clearing state on the sender:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>      <span style=color:#75715e>(* Those disks that were attached at the point the migration happened will have been
</span></span></span><span style=display:flex><span><span style=color:#75715e>         remapped by the Events_from_xenopsd logic. We need to remap any other disks at
</span></span></span><span style=display:flex><span><span style=color:#75715e>         this point here *)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> is_intra_pool
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>        List.iter
</span></span><span style=display:flex><span>          <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vm&#39; <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>             intra_pool_vdi_remap <span style=color:#f92672>~__</span>context vm&#39; all_map<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>             intra_pool_fix_suspend_sr <span style=color:#f92672>~__</span>context remote<span style=color:#f92672>.</span>dest_host vm&#39;<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>          vm_and_snapshots<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#75715e>(* If it&#39;s an inter-pool migrate, the VBDs will still be &#39;currently-attached=true&#39;
</span></span></span><span style=display:flex><span><span style=color:#75715e>         because we supressed the events coming from xenopsd. Destroy them, so that the
</span></span></span><span style=display:flex><span><span style=color:#75715e>         VDIs can be destroyed *)</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> not is_intra_pool <span style=color:#f92672>&amp;&amp;</span> not copy
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>then</span> List.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vbd <span style=color:#f92672>-&gt;</span> Db.VBD.destroy <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vbd<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>vbds <span style=color:#f92672>@</span> snapshots_vbds<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      new_vm
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>The remark about the <code>Events_from_xenopsd</code> is that we have a thread watching for events that are emitted by xenopsd, and we resynchronise xapi&rsquo;s state according to xenopsd&rsquo;s state for several fields for which xenopsd is considered the canonical source of truth. One of these is the exact VDI the VBD is associated with.</p><p>The suspend_SR field of the VM is set to the source&rsquo;s value, so we reset that.</p><p>Now we move the RRDs:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>if</span> not copy <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>      Rrdd_proxy.migrate_rrd <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>remote_address<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>remote_ip <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:(</span>Ref.string_of remote<span style=color:#f92672>.</span>session<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>~</span>vm_uuid<span style=color:#f92672>:</span>vm_uuid <span style=color:#f92672>~</span>host_uuid<span style=color:#f92672>:(</span>Ref.string_of remote<span style=color:#f92672>.</span>dest_host<span style=color:#f92672>)</span> ()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span></span></span></code></pre></div><p>This can be done for intra- and inter- pool migrates in the same way, simplifying the logic.</p><p>However, for messages and blobs we have to only migrate them for inter-pool migrations:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>   <span style=color:#66d9ef>if</span> not is_intra_pool <span style=color:#f92672>&amp;&amp;</span> not copy <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>      <span style=color:#75715e>(* Replicate HA runtime flag if necessary *)</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> ha_always_run_reset <span style=color:#66d9ef>then</span> XenAPI.VM.set_ha_always_run <span style=color:#f92672>~</span>rpc<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>rpc <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>new_vm <span style=color:#f92672>~</span><span style=color:#66d9ef>value</span><span style=color:#f92672>:</span>true<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#75715e>(* Send non-database metadata *)</span>
</span></span><span style=display:flex><span>      Xapi_message.send_messages <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>cls<span style=color:#f92672>:`</span><span style=color:#a6e22e>VM</span> <span style=color:#f92672>~</span>obj_uuid<span style=color:#f92672>:</span>vm_uuid
</span></span><span style=display:flex><span>        <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>remote_address<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>remote_master_ip<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      Xapi_blob.migrate_push <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>rpc<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>rpc
</span></span><span style=display:flex><span>        <span style=color:#f92672>~</span>remote_address<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>remote_master_ip <span style=color:#f92672>~</span>session_id<span style=color:#f92672>:</span>remote<span style=color:#f92672>.</span>session <span style=color:#f92672>~</span>old_vm<span style=color:#f92672>:</span>vm <span style=color:#f92672>~</span>new_vm <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#75715e>(* Signal the remote pool that we&#39;re done *)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span></span></span></code></pre></div><p>Lastly, we destroy the VM record on the source:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    Helpers.call_api_functions <span style=color:#f92672>~__</span>context <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> rpc session_id <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> not is_intra_pool <span style=color:#f92672>&amp;&amp;</span> not copy <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>          info <span style=color:#e6db74>&#34;Destroying VM ref=%s uuid=%s&#34;</span> <span style=color:#f92672>(</span>Ref.string_of vm<span style=color:#f92672>)</span> vm_uuid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          Xapi_vm_lifecycle.force_state_reset <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#f92672>~</span><span style=color:#66d9ef>value</span><span style=color:#f92672>:`</span><span style=color:#a6e22e>Halted</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          List.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> self <span style=color:#f92672>-&gt;</span> Db.VM.destroy <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>)</span> vm_and_snapshots
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    SMPERF.debug <span style=color:#e6db74>&#34;vm.migrate_send exiting vm:%s&#34;</span> vm_uuid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    new_vm</span></span></code></pre></div><p>The exception handler still has to clean some state, but mostly things are handled in the CPS functions declared above:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    error <span style=color:#e6db74>&#34;Caught %s: cleaning up&#34;</span> <span style=color:#f92672>(</span>Printexc.to_string e<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>(* We do our best to tidy up the state left behind *)</span>
</span></span><span style=display:flex><span>    Events_from_xenopsd.with_suppressed queue_name dbg vm_uuid <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> <span style=color:#f92672>_,</span> state <span style=color:#f92672>=</span> XenopsAPI.VM.stat dbg vm_uuid <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> Xenops_interface.<span style=color:#f92672>(</span>state<span style=color:#f92672>.</span>Vm.power_state <span style=color:#f92672>=</span> <span style=color:#a6e22e>Suspended</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>            debug <span style=color:#e6db74>&#34;xenops: %s: shutting down suspended VM&#34;</span> vm_uuid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>            Xapi_xenops.shutdown <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>vm <span style=color:#a6e22e>None</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> ()<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> not is_intra_pool <span style=color:#f92672>&amp;&amp;</span> Db.is_valid_ref <span style=color:#f92672>__</span>context vm <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>      List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> self <span style=color:#f92672>-&gt;</span> Db.VM.get_uuid <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>)</span> vm_and_snapshots
</span></span><span style=display:flex><span>      <span style=color:#f92672>|&gt;</span> List.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> self <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> vm_ref <span style=color:#f92672>=</span> XenAPI.VM.get_by_uuid remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session self <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            info <span style=color:#e6db74>&#34;Destroying stale VM uuid=%s on destination host&#34;</span> self<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>            XenAPI.VM.destroy remote<span style=color:#f92672>.</span>rpc remote<span style=color:#f92672>.</span>session vm_ref
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span> error <span style=color:#e6db74>&#34;Caught %s while destroying VM uuid=%s on destination host&#34;</span> <span style=color:#f92672>(</span>Printexc.to_string e<span style=color:#f92672>)</span> self<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> task <span style=color:#f92672>=</span> Context.get_task_id <span style=color:#f92672>__</span>context <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> oc <span style=color:#f92672>=</span> Db.Task.get_other_config <span style=color:#f92672>~__</span>context <span style=color:#f92672>~</span>self<span style=color:#f92672>:</span>task <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> List.mem_assoc <span style=color:#e6db74>&#34;mirror_failed&#34;</span> oc <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> failed_vdi <span style=color:#f92672>=</span> List.assoc <span style=color:#e6db74>&#34;mirror_failed&#34;</span> oc <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> vconf <span style=color:#f92672>=</span> List.find <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vconf <span style=color:#f92672>-&gt;</span> vconf<span style=color:#f92672>.</span>location<span style=color:#f92672>=</span>failed_vdi<span style=color:#f92672>)</span> vms_vdis <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;Mirror failed for VDI: %s&#34;</span> failed_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.mirror_failed<span style=color:#f92672>,[</span>Ref.string_of vconf<span style=color:#f92672>.</span>vdi<span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    TaskHelper.exn_if_cancelling <span style=color:#f92672>~__</span>context<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>begin</span> <span style=color:#66d9ef>match</span> e <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> Storage_interface.<span style=color:#a6e22e>Backend_error</span><span style=color:#f92672>(</span>code<span style=color:#f92672>,</span> params<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>code<span style=color:#f92672>,</span> params<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> Storage_interface.<span style=color:#a6e22e>Unimplemented</span><span style=color:#f92672>(</span>code<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.unimplemented_in_sm_backend<span style=color:#f92672>,</span> <span style=color:#f92672>[</span>code<span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> Xenops_interface.<span style=color:#a6e22e>Cancelled</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> TaskHelper.raise_cancelled <span style=color:#f92672>~__</span>context
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>raise</span> e
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span></span></span></code></pre></div><p>Failures during the migration can result in the VM being in a suspended state. There&rsquo;s no point leaving it like this since there&rsquo;s nothing that can be done to resume it, so we force shut it down.</p><p>We also try to remove the VM record from the destination if we managed to send it there.</p><p>Finally we check for mirror failure in the task - this is set by the events thread watching for events from the storage layer, in <a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/storage_access.ml#L1169-L1207 target=_blank>storage_access.ml</a></p><h2 id=storage-code>Storage code</h2><p>The part of the code that is conceptually in the storage layer, but physically in xapi, is located in
<a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/storage_migrate.ml target=_blank>storage_migrate.ml</a>. There are logically a few separate parts to this file:</p><ul><li>A <a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/storage_migrate.ml#L34-L204 target=_blank>stateful module</a> for persisting state across xapi restarts.</li><li>Some general <a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/storage_migrate.ml#L206-L281 target=_blank>helper functions</a></li><li>Some quite specific <a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/storage_migrate.ml#L206-L281 target=_blank>helper</a> <a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/storage_migrate.ml#L738-L791 target=_blank>functions</a> related to actions to be taken on deactivate/detach</li><li>An <a href=https://github.com/xapi-project/xen-api/blob/f75d51e7a3eff89d952330ec1a739df85a2895e2/ocaml/xapi/storage_migrate.ml#L793-L818 target=_blank>NBD handler</a></li><li>The implementations of the SMAPIv2 <a href=https://github.com/xapi-project/xcp-idl/blob/master/storage/storage_interface.ml#L430-L460 target=_blank>mirroring APIs</a></li></ul><p>Let&rsquo;s start by considering the way the storage APIs are intended to be used.</p><h3 id=copying-a-vdi>Copying a VDI</h3><p><code>DATA.copy</code> takes several parameters:</p><ul><li><code>dbg</code> - a debug string</li><li><code>sr</code> - the source SR (a uuid)</li><li><code>vdi</code> - the source VDI (a uuid)</li><li><code>dp</code> - <strong>unused</strong></li><li><code>url</code> - a URL on which SMAPIv2 API calls can be made</li><li><code>sr</code> - the destination SR in which the VDI should be copied</li></ul><p>and returns a parameter of type <code>Task.id</code>. The API call is intended to be called in an asynchronous fashion - ie., the caller makes the call, receives the task ID back and polls or uses the event mechanism to wait until the task has completed. The task may be cancelled via the <code>Task.cancel</code> API call. The result of the operation is obtained by calling TASK.stat, which returns a record:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>type</span> t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>		id<span style=color:#f92672>:</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		dbg<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		ctime<span style=color:#f92672>:</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		state<span style=color:#f92672>:</span> state<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		subtasks<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>string</span> <span style=color:#f92672>*</span> state<span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		debug_info<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>string</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		backtrace<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>}</span></span></span></code></pre></div><p>Where the <code>state</code> field contains the result once the task has completed:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>type</span> async_result_t <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Vdi_info</span> <span style=color:#66d9ef>of</span> vdi_info
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Mirror_id</span> <span style=color:#66d9ef>of</span> Mirror.id
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>type</span> completion_t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	duration <span style=color:#f92672>:</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	result <span style=color:#f92672>:</span> async_result_t option
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>type</span> state <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Pending</span> <span style=color:#66d9ef>of</span> <span style=color:#66d9ef>float</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Completed</span> <span style=color:#66d9ef>of</span> completion_t
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Failed</span> <span style=color:#66d9ef>of</span> Rpc.t</span></span></code></pre></div><p>Once the result has been obtained from the task, the task should be destroyed via the <code>TASK.destroy</code> API call.</p><p>The implementation uses the <code>url</code> parameter to make SMAPIv2 calls to the destination SR. This is used, for example, to invoke a VDI.create call if necessary. The URL contains an authentication token within it (valid for the duration of the XenAPI call that caused this DATA.copy API call).</p><p>The implementation tries to minimize the amount of data copied by looking for related VDIs on the destination SR. See below for more details.</p><h3 id=mirroring-a-vdi>Mirroring a VDI</h3><p><code>DATA.MIRROR.start</code> takes a similar set of parameters to that of copy:</p><ul><li><code>dbg</code> - a debug string</li><li><code>sr</code> - the source SR (a uuid)</li><li><code>vdi</code> - the source VDI (a uuid)</li><li><code>dp</code> - the datapath on which the VDI has been attached</li><li><code>url</code> - a URL on which SMAPIv2 API calls can be made</li><li><code>sr</code> - the destination SR in which the VDI should be copied</li></ul><p>Similar to copy above, this returns a task id. The task &lsquo;completes&rsquo; once the mirror has been set up - that is, at any point afterwards we can detach the disk and the destination disk will be identical to the source. Unlike for copy the operation is ongoing after the API call completes, since new writes need to be mirrored to the destination. Therefore the completion type of the mirror operation is <code>Mirror_id</code> which contains a handle on which further API calls related to the mirror call can be made. For example <a href=https://github.com/xapi-project/xcp-idl/blob/a999ef6191629c8f68377f7c412ee98fc6a39dea/storage/storage_interface.ml#L446 target=_blank>MIRROR.stat</a> whose signature is:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>MIRROR.stat<span style=color:#f92672>:</span> dbg<span style=color:#f92672>:</span>debug_info <span style=color:#f92672>-&gt;</span> id<span style=color:#f92672>:</span>Mirror.id <span style=color:#f92672>-&gt;</span> Mirror.t</span></span></code></pre></div><p>The return type of this call is a record containing information about the mirror:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>type</span> state <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Receiving</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Sending</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>|</span> <span style=color:#a6e22e>Copying</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>type</span> t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	source_vdi <span style=color:#f92672>:</span> vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	dest_vdi <span style=color:#f92672>:</span> vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	state <span style=color:#f92672>:</span> state <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	failed <span style=color:#f92672>:</span> <span style=color:#66d9ef>bool</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div><p>Note that state is a list since the initial phase of the operation requires both copying and mirroring.</p><p>Additionally the mirror can be cancelled using the <code>MIRROR.stop</code> API call.</p><h3 id=code-walkthrough>Code walkthrough</h3><p>let&rsquo;s go through the implementation of <code>copy</code>:</p><h4 id=datacopy>DATA.copy</h4><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> copy <span style=color:#f92672>~</span>task <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>dp <span style=color:#f92672>~</span>url <span style=color:#f92672>~</span>dest <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  debug <span style=color:#e6db74>&#34;copy sr:%s vdi:%s url:%s dest:%s&#34;</span> sr vdi url dest<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> remote_url <span style=color:#f92672>=</span> Http.Url.of_string url <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>Remote</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>Client</span><span style=color:#f92672>(</span><span style=color:#66d9ef>struct</span> <span style=color:#66d9ef>let</span> rpc <span style=color:#f92672>=</span> rpc <span style=color:#f92672>~</span>srcstr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;smapiv2&#34;</span> <span style=color:#f92672>~</span>dststr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;dst_smapiv2&#34;</span> remote_url <span style=color:#66d9ef>end</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Here we are constructing a module <code>Remote</code> on which we can do SMAPIv2 calls directly on the destination.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>try</span></span></span></code></pre></div><p>Wrap the whole function in an exception handler.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#75715e>(* Find the local VDI *)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> vdis <span style=color:#f92672>=</span> Local.SR.scan <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> local_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>try</span> List.find <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> x <span style=color:#f92672>-&gt;</span> x<span style=color:#f92672>.</span>vdi <span style=color:#f92672>=</span> vdi<span style=color:#f92672>)</span> vdis
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>with</span> <span style=color:#a6e22e>Not_found</span> <span style=color:#f92672>-&gt;</span> failwith <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;Local VDI %s not found&#34;</span> vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>We first find the metadata for our source VDI by doing a local SMAPIv2 call <code>SR.scan</code>. This returns a list of VDI metadata, out of which we extract the VDI we&rsquo;re interested in.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>try</span></span></span></code></pre></div><p>Another exception handler. This looks redundant to me right now.</p><div class="wrap-code highlight"><pre tabindex=0><code>      let similar_vdis = Local.VDI.similar_content ~dbg ~sr ~vdi in
      let similars = List.map (fun vdi -&gt; vdi.content_id) similar_vdis in
      debug &#34;Similar VDIs to %s = [ %s ]&#34; vdi (String.concat &#34;; &#34; (List.map (fun x -&gt; Printf.sprintf &#34;(vdi=%s,content_id=%s)&#34; x.vdi x.content_id) similar_vdis));</code></pre></div><p>Here we look for related VDIs locally using the <code>VDI.similar_content</code> SMAPIv2 API call. This searches for related VDIs and returns an ordered list where the most similar is first in the list. It returns both clones and snapshots, and hence is more general than simply following <code>snapshot_of</code> links.</p><div class="wrap-code highlight"><pre tabindex=0><code>      let remote_vdis = Remote.SR.scan ~dbg ~sr:dest in
      (** We drop cbt_metadata VDIs that do not have any actual data *)
      let remote_vdis = List.filter (fun vdi -&gt; vdi.ty &lt;&gt; &#34;cbt_metadata&#34;) remote_vdis in

      let nearest = List.fold_left
          (fun acc content_id -&gt; match acc with
             | Some x -&gt; acc
             | None -&gt;
               try Some (List.find (fun vdi -&gt; vdi.content_id = content_id &amp;&amp; vdi.virtual_size &lt;= local_vdi.virtual_size) remote_vdis)
               with Not_found -&gt; None) None similars in

      debug &#34;Nearest VDI: content_id=%s vdi=%s&#34;
        (Opt.default &#34;None&#34; (Opt.map (fun x -&gt; x.content_id) nearest))
        (Opt.default &#34;None&#34; (Opt.map (fun x -&gt; x.vdi) nearest));</code></pre></div><p>Here we look for VDIs on the destination with the same <code>content_id</code> as one of the locally similar VDIs. We will use this as a base image and only copy deltas to the destination. This is done by cloning the VDI on the destination and then using <code>sparse_dd</code> to find the deltas from our local disk to our local copy of the content_id disk and streaming these to the destination. Note that we need to ensure the VDI is smaller than the one we want to copy since we can&rsquo;t resize disks downwards in size.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>      <span style=color:#66d9ef>let</span> remote_base <span style=color:#f92672>=</span> <span style=color:#66d9ef>match</span> nearest <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span> vdi <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          debug <span style=color:#e6db74>&#34;Cloning VDI %s&#34;</span> vdi<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> vdi_clone <span style=color:#f92672>=</span> Remote.VDI.clone <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi_info<span style=color:#f92672>:</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> vdi_clone<span style=color:#f92672>.</span>virtual_size <span style=color:#f92672>&lt;&gt;</span> local_vdi<span style=color:#f92672>.</span>virtual_size <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> new_size <span style=color:#f92672>=</span> Remote.VDI.resize <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>vdi_clone<span style=color:#f92672>.</span>vdi <span style=color:#f92672>~</span>new_size<span style=color:#f92672>:</span>local_vdi<span style=color:#f92672>.</span>virtual_size <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            debug <span style=color:#e6db74>&#34;Resize remote VDI %s to %Ld: result %Ld&#34;</span> vdi_clone<span style=color:#f92672>.</span>vdi local_vdi<span style=color:#f92672>.</span>virtual_size new_size<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          vdi_clone
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          debug <span style=color:#e6db74>&#34;Creating a blank remote VDI&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          Remote.VDI.create <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi_info<span style=color:#f92672>:{</span> local_vdi <span style=color:#66d9ef>with</span> sm_config <span style=color:#f92672>=</span> [] <span style=color:#f92672>}</span>  <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>If we&rsquo;ve found a base VDI we clone it and resize it immediately. If there&rsquo;s nothing on the destination already we can use, we just create a new VDI. Note that the calls to create and clone may well fail if the destination host is not the SRmaster. This is <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi/storage_migrate.ml#L214-L229 target=_blank>handled purely in the <code>rpc</code> function</a>:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> <span style=color:#66d9ef>rec</span> rpc <span style=color:#f92672>~</span>srcstr <span style=color:#f92672>~</span>dststr url call <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> XMLRPC_protocol.rpc <span style=color:#f92672>~</span>transport<span style=color:#f92672>:(</span>transport_of_url url<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>~</span>srcstr <span style=color:#f92672>~</span>dststr <span style=color:#f92672>~</span>http<span style=color:#f92672>:(</span>xmlrpc <span style=color:#f92672>~</span>version<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;1.0&#34;</span> <span style=color:#f92672>?</span>auth<span style=color:#f92672>:(</span>Http.Url.auth_of url<span style=color:#f92672>)</span> <span style=color:#f92672>~</span>query<span style=color:#f92672>:(</span>Http.Url.get_query_params url<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>Http.Url.get_uri url<span style=color:#f92672>))</span> call
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>if</span> not result<span style=color:#f92672>.</span>Rpc.success <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Got failure: checking for redirect&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Call was: %s&#34;</span> <span style=color:#f92672>(</span>Rpc.string_of_call call<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;result.contents: %s&#34;</span> <span style=color:#f92672>(</span>Jsonrpc.to_string result<span style=color:#f92672>.</span>Rpc.contents<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>match</span> Storage_interface.Exception.exnty_of_rpc result<span style=color:#f92672>.</span>Rpc.contents <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> Storage_interface.Exception.<span style=color:#a6e22e>Redirect</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Some</span> ip<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> Http.<span style=color:#a6e22e>Url</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> newurl <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>match</span> url <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Http</span> h<span style=color:#f92672>,</span> d<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>(</span><span style=color:#a6e22e>Http</span> <span style=color:#f92672>{</span>h <span style=color:#66d9ef>with</span> host<span style=color:#f92672>=</span>ip<span style=color:#f92672>},</span> d<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          remote_url ip <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;Redirecting to ip: %s&#34;</span> ip<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> r <span style=color:#f92672>=</span> rpc <span style=color:#f92672>~</span>srcstr <span style=color:#f92672>~</span>dststr newurl call <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;Successfully redirected. Returning&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      r
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;Not a redirect&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      result
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>else</span> result</span></span></code></pre></div><p>Back to the copy function:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>      <span style=color:#66d9ef>let</span> remote_copy <span style=color:#f92672>=</span> copy&#39; <span style=color:#f92672>~</span>task <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>url <span style=color:#f92672>~</span>dest <span style=color:#f92672>~</span>dest_vdi<span style=color:#f92672>:</span>remote_base<span style=color:#f92672>.</span>vdi <span style=color:#f92672>|&gt;</span> vdi_info <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>This calls the actual data copy part. See below for more on that.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>      <span style=color:#66d9ef>let</span> snapshot <span style=color:#f92672>=</span> Remote.VDI.snapshot <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi_info<span style=color:#f92672>:</span>remote_copy <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      Remote.VDI.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>remote_copy<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>Some</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Vdi_info</span> snapshot<span style=color:#f92672>)</span></span></span></code></pre></div><p>Finally we snapshot the remote VDI to ensure we&rsquo;ve got a VDI of type &lsquo;snapshot&rsquo; on the destination, and we delete the non-snapshot VDI.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      error <span style=color:#e6db74>&#34;Caught %s: copying snapshots vdi&#34;</span> <span style=color:#f92672>(</span>Printexc.to_string e<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Internal_error</span> <span style=color:#f92672>(</span>Printexc.to_string e<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>|</span> <span style=color:#a6e22e>Backend_error</span><span style=color:#f92672>(</span>code<span style=color:#f92672>,</span> params<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>|</span> Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>code<span style=color:#f92672>,</span> params<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Backend_error</span><span style=color:#f92672>(</span>code<span style=color:#f92672>,</span> params<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>|</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Internal_error</span><span style=color:#f92672>(</span>Printexc.to_string e<span style=color:#f92672>))</span></span></span></code></pre></div><p>The exception handler does nothing - so we leak remote VDIs if the exception happens after we&rsquo;ve done our cloning :-(</p><h4 id=datacopy_into>DATA.copy_into</h4><p>Let&rsquo;s now look at the data-copying part. This is common code shared between <code>VDI.copy</code>, <code>VDI.copy_into</code> and <code>MIRROR.start</code> and hence has some duplication of the calls made above.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> copy_into <span style=color:#f92672>~</span>task <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>url <span style=color:#f92672>~</span>dest <span style=color:#f92672>~</span>dest_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  copy&#39; <span style=color:#f92672>~</span>task <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>url <span style=color:#f92672>~</span>dest <span style=color:#f92672>~</span>dest_vdi</span></span></code></pre></div><p><code>copy_into</code> is a stub and just calls <code>copy'</code></p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> copy&#39; <span style=color:#f92672>~</span>task <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>url <span style=color:#f92672>~</span>dest <span style=color:#f92672>~</span>dest_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> remote_url <span style=color:#f92672>=</span> Http.Url.of_string url <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>Remote</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>Client</span><span style=color:#f92672>(</span><span style=color:#66d9ef>struct</span> <span style=color:#66d9ef>let</span> rpc <span style=color:#f92672>=</span> rpc <span style=color:#f92672>~</span>srcstr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;smapiv2&#34;</span> <span style=color:#f92672>~</span>dststr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;dst_smapiv2&#34;</span> remote_url <span style=color:#66d9ef>end</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  debug <span style=color:#e6db74>&#34;copy local=%s/%s url=%s remote=%s/%s&#34;</span> sr vdi url dest dest_vdi<span style=color:#f92672>;</span></span></span></code></pre></div><p>This call takes roughly the same parameters as the ``DATA.copy` call above, except it specifies the destination VDI.
Once again we construct a module to do remote SMAPIv2 calls</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#75715e>(* Check the remote SR exists *)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> srs <span style=color:#f92672>=</span> Remote.SR.list <span style=color:#f92672>~</span>dbg <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>if</span> not<span style=color:#f92672>(</span>List.mem dest srs<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>then</span> failwith <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;Remote SR %s not found&#34;</span> dest<span style=color:#f92672>);</span></span></span></code></pre></div><p>Sanity check.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vdis <span style=color:#f92672>=</span> Remote.SR.scan <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> remote_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> List.find <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> x <span style=color:#f92672>-&gt;</span> x<span style=color:#f92672>.</span>vdi <span style=color:#f92672>=</span> dest_vdi<span style=color:#f92672>)</span> vdis
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> <span style=color:#a6e22e>Not_found</span> <span style=color:#f92672>-&gt;</span> failwith <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;Remote VDI %s not found&#34;</span> dest_vdi<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Find the metadata of the destination VDI</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> dest_content_id <span style=color:#f92672>=</span> remote_vdi<span style=color:#f92672>.</span>content_id <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>If we&rsquo;ve got a local VDI with the same content_id as the destination, we only need copy the deltas, so we make a note of the destination content ID here.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#75715e>(* Find the local VDI *)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vdis <span style=color:#f92672>=</span> Local.SR.scan <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> local_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> List.find <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> x <span style=color:#f92672>-&gt;</span> x<span style=color:#f92672>.</span>vdi <span style=color:#f92672>=</span> vdi<span style=color:#f92672>)</span> vdis
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> <span style=color:#a6e22e>Not_found</span> <span style=color:#f92672>-&gt;</span> failwith <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;Local VDI %s not found&#34;</span> vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  debug <span style=color:#e6db74>&#34;copy local=%s/%s content_id=%s&#34;</span> sr vdi local_vdi<span style=color:#f92672>.</span>content_id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  debug <span style=color:#e6db74>&#34;copy remote=%s/%s content_id=%s&#34;</span> dest dest_vdi remote_vdi<span style=color:#f92672>.</span>content_id<span style=color:#f92672>;</span></span></span></code></pre></div><p>Find the source VDI metadata.</p><div class="wrap-code highlight"><pre tabindex=0><code>  if local_vdi.virtual_size &gt; remote_vdi.virtual_size then begin
    (* This should never happen provided the higher-level logic is working properly *)
    error &#34;copy local=%s/%s virtual_size=%Ld &gt; remote=%s/%s virtual_size = %Ld&#34; sr vdi local_vdi.virtual_size dest dest_vdi remote_vdi.virtual_size;
    failwith &#34;local VDI is larger than the remote VDI&#34;;
  end;</code></pre></div><p>Sanity check - the remote VDI can&rsquo;t be smaller than the source.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> on_fail <span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>unit</span> <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>unit</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span> ref <span style=color:#f92672>=</span> ref [] <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>We do some ugly error handling here by keeping a mutable list of operations to perform in the event of a failure.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> base_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> x <span style=color:#f92672>=</span> <span style=color:#f92672>(</span>List.find <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> x <span style=color:#f92672>-&gt;</span> x<span style=color:#f92672>.</span>content_id <span style=color:#f92672>=</span> dest_content_id<span style=color:#f92672>)</span> vdis<span style=color:#f92672>).</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;local VDI %s has content_id = %s; we will perform an incremental copy&#34;</span> x dest_content_id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>Some</span> x
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;no local VDI has content_id = %s; we will perform a full copy&#34;</span> dest_content_id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>None</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>See if we can identify a local VDI with the same <code>content_id</code> as the destination. If not, no problem.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> remote_dp <span style=color:#f92672>=</span> Uuid.string_of_uuid <span style=color:#f92672>(</span>Uuid.make_uuid ()<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> base_dp <span style=color:#f92672>=</span> Uuid.string_of_uuid <span style=color:#f92672>(</span>Uuid.make_uuid ()<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> leaf_dp <span style=color:#f92672>=</span> Uuid.string_of_uuid <span style=color:#f92672>(</span>Uuid.make_uuid ()<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Construct some <code>datapaths</code> - named reasons why the VDI is attached - that we will pass to <code>VDI.attach/activate</code>.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>let</span> dest_vdi_url <span style=color:#f92672>=</span> Http.Url.set_uri remote_url <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;%s/nbd/%s/%s/%s&#34;</span> <span style=color:#f92672>(</span>Http.Url.get_uri remote_url<span style=color:#f92672>)</span> dest dest_vdi remote_dp<span style=color:#f92672>)</span> <span style=color:#f92672>|&gt;</span> Http.Url.to_string <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;copy remote=%s/%s NBD URL = %s&#34;</span> dest dest_vdi dest_vdi_url<span style=color:#f92672>;</span></span></span></code></pre></div><p>Here we are constructing a URI that we use to connect to the destination xapi. The handler for this particular path will verify the credentials and then pass the connection on to tapdisk which will behave as a NBD server. The VDI has to be attached and activated for this to work, unlike the new NBD handler in <code>xapi-nbd</code> that is smarter. The handler for this URI is declared <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi/storage_migrate.ml#L858-L884 target=_blank>in this file</a></p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>let</span> id<span style=color:#f92672>=</span>State.copy_id_of <span style=color:#f92672>(</span>sr<span style=color:#f92672>,</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Persisting state for copy (id=%s)&#34;</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    State.add id State.<span style=color:#f92672>(</span><span style=color:#a6e22e>Copy_op</span> Copy_state.<span style=color:#f92672>({</span>
</span></span><span style=display:flex><span>        base_dp<span style=color:#f92672>;</span> leaf_dp<span style=color:#f92672>;</span> remote_dp<span style=color:#f92672>;</span> dest_sr<span style=color:#f92672>=</span>dest<span style=color:#f92672>;</span> copy_vdi<span style=color:#f92672>=</span>remote_vdi<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span> remote_url<span style=color:#f92672>=</span>url<span style=color:#f92672>}));</span></span></span></code></pre></div><p>Since we&rsquo;re about to perform a long-running operation that is stateful, we persist the state here so that if xapi is restarted we can cancel the operation and not leak VDI attaches. Normally in xapi code we would be doing VBD.plug operations to persist the state in the xapi db, but this is storage code so we have to use a different mechanism.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    SMPERF.debug <span style=color:#e6db74>&#34;mirror.copy: copy initiated local_vdi:%s dest_vdi:%s&#34;</span> vdi dest_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    Pervasiveext.finally <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        debug <span style=color:#e6db74>&#34;activating RW datapath %s on remote=%s/%s&#34;</span> remote_dp dest dest_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        ignore<span style=color:#f92672>(</span>Remote.VDI.attach <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>dest_vdi <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>remote_dp <span style=color:#f92672>~</span>read_write<span style=color:#f92672>:</span>true<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        Remote.VDI.activate <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>remote_dp <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>dest_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        with_activated_disk <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>base_vdi <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>base_dp
</span></span><span style=display:flex><span>          <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> base_path <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>             with_activated_disk <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:(</span><span style=color:#a6e22e>Some</span> vdi<span style=color:#f92672>)</span> <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>leaf_dp
</span></span><span style=display:flex><span>               <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> src <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>                  <span style=color:#66d9ef>let</span> dd <span style=color:#f92672>=</span> Sparse_dd_wrapper.start <span style=color:#f92672>~</span>progress_cb<span style=color:#f92672>:(</span>progress_callback 0<span style=color:#f92672>.</span>05 0<span style=color:#f92672>.</span>9 task<span style=color:#f92672>)</span> <span style=color:#f92672>?</span>base<span style=color:#f92672>:</span>base_path true <span style=color:#f92672>(</span>Opt.unbox src<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>                      dest_vdi_url remote_vdi<span style=color:#f92672>.</span>virtual_size <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>                  Storage_task.with_cancel task
</span></span><span style=display:flex><span>                    <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span> Sparse_dd_wrapper.cancel dd<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>                    <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>                       <span style=color:#66d9ef>try</span> Sparse_dd_wrapper.wait dd
</span></span><span style=display:flex><span>                       <span style=color:#66d9ef>with</span> Sparse_dd_wrapper.<span style=color:#a6e22e>Cancelled</span> <span style=color:#f92672>-&gt;</span> Storage_task.raise_cancelled task<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>               <span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>         Remote.DP.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>remote_dp <span style=color:#f92672>~</span>allow_leak<span style=color:#f92672>:</span>false<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>         State.remove_copy id
</span></span><span style=display:flex><span>      <span style=color:#f92672>);</span></span></span></code></pre></div><p>In this chunk of code we attach and activate the disk on the remote SR via the SMAPI, then locally attach and activate both the VDI we&rsquo;re copying and the base image we&rsquo;re copying deltas from (if we&rsquo;ve got one). We then call <code>sparse_dd</code> to copy the data to the remote NBD URL. There is some logic to update progress indicators and to cancel the operation if the SMAPIv2 call <code>TASK.cancel</code> is called.</p><p>Once the operation has terminated (either on success, error or cancellation), we remove the local attach and activations in the <code>with_activated_disk</code> function and the remote attach and activation by destroying the datapath on the remote SR. We then remove the persistent state relating to the copy.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    SMPERF.debug <span style=color:#e6db74>&#34;mirror.copy: copy complete local_vdi:%s dest_vdi:%s&#34;</span> vdi dest_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;setting remote=%s/%s content_id &lt;- %s&#34;</span> dest dest_vdi local_vdi<span style=color:#f92672>.</span>content_id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Remote.VDI.set_content_id <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>dest_vdi <span style=color:#f92672>~</span>content_id<span style=color:#f92672>:</span>local_vdi<span style=color:#f92672>.</span>content_id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>(* PR-1255: XXX: this is useful because we don&#39;t have content_ids by default *)</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;setting local=%s/%s content_id &lt;- %s&#34;</span> sr local_vdi<span style=color:#f92672>.</span>vdi local_vdi<span style=color:#f92672>.</span>content_id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Local.VDI.set_content_id <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>local_vdi<span style=color:#f92672>.</span>vdi <span style=color:#f92672>~</span>content_id<span style=color:#f92672>:</span>local_vdi<span style=color:#f92672>.</span>content_id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Some</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Vdi_info</span> remote_vdi<span style=color:#f92672>)</span></span></span></code></pre></div><p>The last thing we do is to set the local and remote content_id. The local set_content_id is there because the content_id of the VDI is constructed from the location if it is unset in the <a href=https://github.com/xapi-project/xen-api/blob/3bf897b3accfc172f365689c3c6927746e059177/ocaml/xapi/storage_access.ml#L69-L72 target=_blank>storage_access.ml</a> module of xapi (still part of the storage layer)</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    error <span style=color:#e6db74>&#34;Caught %s: performing cleanup actions&#34;</span> <span style=color:#f92672>(</span>Printexc.to_string e<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    perform_cleanup_actions <span style=color:#f92672>!</span>on_fail<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e</span></span></code></pre></div><p>Here we perform the list of cleanup operations. Theoretically. It seems we don&rsquo;t ever actually set this to anything, so this is dead code.</p><h4 id=datamirrorstart>DATA.MIRROR.start</h4><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> start&#39; <span style=color:#f92672>~</span>task <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>dp <span style=color:#f92672>~</span>url <span style=color:#f92672>~</span>dest <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  debug <span style=color:#e6db74>&#34;Mirror.start sr:%s vdi:%s url:%s dest:%s&#34;</span> sr vdi url dest<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  SMPERF.debug <span style=color:#e6db74>&#34;mirror.start called sr:%s vdi:%s url:%s dest:%s&#34;</span> sr vdi url dest<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> remote_url <span style=color:#f92672>=</span> Http.Url.of_string url <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>Remote</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>Client</span><span style=color:#f92672>(</span><span style=color:#66d9ef>struct</span> <span style=color:#66d9ef>let</span> rpc <span style=color:#f92672>=</span> rpc <span style=color:#f92672>~</span>srcstr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;smapiv2&#34;</span> <span style=color:#f92672>~</span>dststr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;dst_smapiv2&#34;</span> remote_url <span style=color:#66d9ef>end</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#75715e>(* Find the local VDI *)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vdis <span style=color:#f92672>=</span> Local.SR.scan <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> local_vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> List.find <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> x <span style=color:#f92672>-&gt;</span> x<span style=color:#f92672>.</span>vdi <span style=color:#f92672>=</span> vdi<span style=color:#f92672>)</span> vdis
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> <span style=color:#a6e22e>Not_found</span> <span style=color:#f92672>-&gt;</span> failwith <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;Local VDI %s not found&#34;</span> vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>As with the previous calls, we make a remote module for SMAPIv2 calls on the destination, and we find local VDI metadata via <code>SR.scan</code></p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> id <span style=color:#f92672>=</span> State.mirror_id_of <span style=color:#f92672>(</span>sr<span style=color:#f92672>,</span>local_vdi<span style=color:#f92672>.</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Mirror ids are deterministically constructed.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#75715e>(* A list of cleanup actions to perform if the operation should fail. *)</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> on_fail <span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>unit</span> <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>unit</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span> ref <span style=color:#f92672>=</span> ref [] <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>This <code>on_fail</code> list is actually used.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> similar_vdis <span style=color:#f92672>=</span> Local.VDI.similar_content <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> similars <span style=color:#f92672>=</span> List.filter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> x <span style=color:#f92672>-&gt;</span> x <span style=color:#f92672>&lt;&gt;</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>(</span>List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> vdi <span style=color:#f92672>-&gt;</span> vdi<span style=color:#f92672>.</span>content_id<span style=color:#f92672>)</span> similar_vdis<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Similar VDIs to %s = [ %s ]&#34;</span> vdi <span style=color:#f92672>(</span>String.concat <span style=color:#e6db74>&#34;; &#34;</span> <span style=color:#f92672>(</span>List.map <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> x <span style=color:#f92672>-&gt;</span> Printf.sprintf <span style=color:#e6db74>&#34;(vdi=%s,content_id=%s)&#34;</span> x<span style=color:#f92672>.</span>vdi x<span style=color:#f92672>.</span>content_id<span style=color:#f92672>)</span> similar_vdis<span style=color:#f92672>));</span></span></span></code></pre></div><p>As with copy we look locally for similar VDIs. However, rather than use that here we actually pass this information on to the destination SR via the <code>receive_start</code> internal SMAPIv2 call:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>let</span> result_ty <span style=color:#f92672>=</span> Remote.DATA.MIRROR.receive_start <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi_info<span style=color:#f92672>:</span>local_vdi <span style=color:#f92672>~</span>id <span style=color:#f92672>~</span>similar<span style=color:#f92672>:</span>similars <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> <span style=color:#66d9ef>match</span> result_ty <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>        Mirror.<span style=color:#a6e22e>Vhd_mirror</span> x <span style=color:#f92672>-&gt;</span> x
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>This gives the destination SR a chance to say what sort of migration it can support. We only support <code>Vhd_mirror</code> style migrations which require the destination to support the <code>compose</code> SMAPIv2 operation. The type of <code>x</code> is a record:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>type</span> mirror_receive_result_vhd_t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	mirror_vdi <span style=color:#f92672>:</span> vdi_info<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	mirror_datapath <span style=color:#f92672>:</span> dp<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	copy_diffs_from <span style=color:#f92672>:</span> content_id option<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	copy_diffs_to <span style=color:#f92672>:</span> vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	dummy_vdi <span style=color:#f92672>:</span> vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span></span></span></code></pre></div><p>Field descriptions:</p><ul><li><code>mirror_vdi</code> is the VDI to which new writes should be mirrored.</li><li><code>mirror_datapath</code> is the remote datapath on which the VDI has been attached and activated. This is required to construct the remote NBD url</li><li><code>copy_diffs_from</code> represents the source base VDI to be used for the non-mirrored data copy.</li><li><code>copy_diffs_to</code> is the remote VDI to copy those diffs to</li><li><code>dummy_vdi</code> exists to prevent leaf-coalesce on the <code>mirror_vdi</code></li></ul><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#75715e>(* Enable mirroring on the local machine *)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> mirror_dp <span style=color:#f92672>=</span> result<span style=color:#f92672>.</span>Mirror.mirror_datapath <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> uri <span style=color:#f92672>=</span> <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;/services/SM/nbd/%s/%s/%s&#34;</span> dest result<span style=color:#f92672>.</span>Mirror.mirror_vdi<span style=color:#f92672>.</span>vdi mirror_dp<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> dest_url <span style=color:#f92672>=</span> Http.Url.set_uri remote_url uri <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> request <span style=color:#f92672>=</span> Http.Request.make <span style=color:#f92672>~</span>query<span style=color:#f92672>:(</span>Http.Url.get_query_params dest_url<span style=color:#f92672>)</span> <span style=color:#f92672>~</span>version<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;1.0&#34;</span> <span style=color:#f92672>~</span>user_agent<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;smapiv2&#34;</span> Http.<span style=color:#a6e22e>Put</span> uri <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> transport <span style=color:#f92672>=</span> Xmlrpc_client.transport_of_url dest_url <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>This is where we connect to the NBD server on the destination.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Searching for data path: %s&#34;</span> dp<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> attach_info <span style=color:#f92672>=</span> Local.DP.attach_info <span style=color:#f92672>~</span>dbg<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;nbd&#34;</span> <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>dp <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Got it!&#34;</span><span style=color:#f92672>;</span></span></span></code></pre></div><p>we need the local <code>attach_info</code> to find the local tapdisk so we can send it the connected NBD socket.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    on_fail <span style=color:#f92672>:=</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span> Remote.DATA.MIRROR.receive_cancel <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>id<span style=color:#f92672>)</span> <span style=color:#f92672>::</span> <span style=color:#f92672>!</span>on_fail<span style=color:#f92672>;</span></span></span></code></pre></div><p>This should probably be set directly after the call to <code>receive_start</code></p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>let</span> tapdev <span style=color:#f92672>=</span> <span style=color:#66d9ef>match</span> tapdisk_of_attach_info attach_info <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span> tapdev <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        debug <span style=color:#e6db74>&#34;Got tapdev&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> pid <span style=color:#f92672>=</span> Tapctl.get_tapdisk_pid tapdev <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> path <span style=color:#f92672>=</span> Printf.sprintf <span style=color:#e6db74>&#34;/var/run/blktap-control/nbdclient%d&#34;</span> pid <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        with_transport transport <span style=color:#f92672>(</span>with_http request <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> <span style=color:#f92672>(</span>response<span style=color:#f92672>,</span> s<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>            debug <span style=color:#e6db74>&#34;Here inside the with_transport&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> control_fd <span style=color:#f92672>=</span> Unix.socket Unix.<span style=color:#a6e22e>PF_UNIX</span> Unix.<span style=color:#a6e22e>SOCK_STREAM</span> 0 <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>            finally
</span></span><span style=display:flex><span>              <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>                 debug <span style=color:#e6db74>&#34;Connecting to path: %s&#34;</span> path<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                 Unix.connect control_fd <span style=color:#f92672>(</span>Unix.<span style=color:#a6e22e>ADDR_UNIX</span> path<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>                 <span style=color:#66d9ef>let</span> msg <span style=color:#f92672>=</span> dp <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>                 <span style=color:#66d9ef>let</span> len <span style=color:#f92672>=</span> String.length msg <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>                 <span style=color:#66d9ef>let</span> written <span style=color:#f92672>=</span> Unixext.send_fd control_fd msg 0 len [] s <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>                 debug <span style=color:#e6db74>&#34;Sent fd&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                 <span style=color:#66d9ef>if</span> written <span style=color:#f92672>&lt;&gt;</span> len <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>                   error <span style=color:#e6db74>&#34;Failed to transfer fd to %s&#34;</span> path<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>                   failwith <span style=color:#e6db74>&#34;foo&#34;</span>
</span></span><span style=display:flex><span>                 <span style=color:#66d9ef>end</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>                 Unix.close control_fd<span style=color:#f92672>)));</span>
</span></span><span style=display:flex><span>        tapdev
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        failwith <span style=color:#e6db74>&#34;Not attached&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>in</span></span></span></code></pre></div><p>Here we connect to the remote NBD server, then pass that connected fd to the local tapdisk that is using the disk. This fd is passed with a name that is later used to tell tapdisk to start using it - we use the datapath name for this.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Adding to active local mirrors: id=%s&#34;</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> alm <span style=color:#f92672>=</span> State.Send_state.<span style=color:#f92672>({</span>
</span></span><span style=display:flex><span>        url<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        dest_sr<span style=color:#f92672>=</span>dest<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        remote_dp<span style=color:#f92672>=</span>mirror_dp<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        local_dp<span style=color:#f92672>=</span>dp<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        mirror_vdi<span style=color:#f92672>=</span>result<span style=color:#f92672>.</span>Mirror.mirror_vdi<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        remote_url<span style=color:#f92672>=</span>url<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        tapdev<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        failed<span style=color:#f92672>=</span>false<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        watchdog<span style=color:#f92672>=</span><span style=color:#a6e22e>None</span><span style=color:#f92672>})</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    State.add id <span style=color:#f92672>(</span>State.<span style=color:#a6e22e>Send_op</span> alm<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Added&#34;</span><span style=color:#f92672>;</span></span></span></code></pre></div><p>As for copy we persist some state to disk to say that we&rsquo;re doing a mirror so we can undo any state changes after a toolstack restart.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;About to snapshot VDI = %s&#34;</span> <span style=color:#f92672>(</span>string_of_vdi_info local_vdi<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> local_vdi <span style=color:#f92672>=</span> add_to_sm_config local_vdi <span style=color:#e6db74>&#34;mirror&#34;</span> <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;nbd:&#34;</span> <span style=color:#f92672>^</span> dp<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> local_vdi <span style=color:#f92672>=</span> add_to_sm_config local_vdi <span style=color:#e6db74>&#34;base_mirror&#34;</span> id <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> snapshot <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>      Local.VDI.snapshot <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi_info<span style=color:#f92672>:</span>local_vdi
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> Storage_interface.<span style=color:#a6e22e>Backend_error</span><span style=color:#f92672>(</span>code<span style=color:#f92672>,</span> <span style=color:#f92672>_)</span> <span style=color:#66d9ef>when</span> code <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;SR_BACKEND_FAILURE_44&#34;</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.sr_source_space_insufficient<span style=color:#f92672>,</span> <span style=color:#f92672>[</span> sr <span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>raise</span> e
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Done!&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    SMPERF.debug <span style=color:#e6db74>&#34;mirror.start: snapshot created, mirror initiated vdi:%s snapshot_of:%s&#34;</span>
</span></span><span style=display:flex><span>      snapshot<span style=color:#f92672>.</span>vdi local_vdi<span style=color:#f92672>.</span>vdi <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    on_fail <span style=color:#f92672>:=</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span> Local.VDI.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>snapshot<span style=color:#f92672>.</span>vdi<span style=color:#f92672>)</span> <span style=color:#f92672>::</span> <span style=color:#f92672>!</span>on_fail<span style=color:#f92672>;</span></span></span></code></pre></div><p>This bit inserts into <code>sm_config</code> the name of the fd we passed earlier to do mirroring. This is interpreted by the python SM backends and passed on the <code>tap-ctl</code> invocation to unpause the disk. This causes all new writes to be mirrored via NBD to the file descriptor passed earlier.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>rec</span> inner () <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>        debug <span style=color:#e6db74>&#34;tapdisk watchdog&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> alm_opt <span style=color:#f92672>=</span> State.find_active_local_mirror id <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>match</span> alm_opt <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span> alm <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> stats <span style=color:#f92672>=</span> Tapctl.stats <span style=color:#f92672>(</span>Tapctl.create ()<span style=color:#f92672>)</span> tapdev <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> stats<span style=color:#f92672>.</span>Tapctl.Stats.nbd_mirror_failed <span style=color:#f92672>=</span> 1 <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>            Updates.add <span style=color:#f92672>(</span>Dynamic.<span style=color:#a6e22e>Mirror</span> id<span style=color:#f92672>)</span> updates<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          alm<span style=color:#f92672>.</span>State.Send_state.watchdog <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>Some</span> <span style=color:#f92672>(</span>Scheduler.one_shot scheduler <span style=color:#f92672>(</span>Scheduler.<span style=color:#a6e22e>Delta</span> 5<span style=color:#f92672>)</span> <span style=color:#e6db74>&#34;tapdisk_watchdog&#34;</span> inner<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>-&gt;</span> ()
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>in</span> inner ()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span></span></span></code></pre></div><p>This is the watchdog that runs <code>tap-ctl stats</code> every 5 seconds watching <code>mirror_failed</code> for evidence of a failure in the mirroring code. If it detects one the only thing it does is to notify that the state of the mirroring has changed. This will be picked up by the thread in xapi that is monitoring the state of the mirror. It will then issue a <code>MIRROR.stat</code> call which will return the state of the mirror including the information that it has failed.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    on_fail <span style=color:#f92672>:=</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span> stop <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>id<span style=color:#f92672>)</span> <span style=color:#f92672>::</span> <span style=color:#f92672>!</span>on_fail<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>(* Copy the snapshot to the remote *)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> new_parent <span style=color:#f92672>=</span> Storage_task.with_subtask task <span style=color:#e6db74>&#34;copy&#34;</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        copy&#39; <span style=color:#f92672>~</span>task <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>snapshot<span style=color:#f92672>.</span>vdi <span style=color:#f92672>~</span>url <span style=color:#f92672>~</span>dest <span style=color:#f92672>~</span>dest_vdi<span style=color:#f92672>:</span>result<span style=color:#f92672>.</span>Mirror.copy_diffs_to<span style=color:#f92672>)</span> <span style=color:#f92672>|&gt;</span> vdi_info <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Local VDI %s == remote VDI %s&#34;</span> snapshot<span style=color:#f92672>.</span>vdi new_parent<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span></span></span></code></pre></div><p>This is where we copy the VDI returned by the snapshot invocation to the remote VDI called <code>copy_diffs_to</code>. We only copy deltas, but we rely on <code>copy'</code> to figure out which disk the deltas should be taken from, which it does via the <code>content_id</code> field.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    Remote.VDI.compose <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi1<span style=color:#f92672>:</span>result<span style=color:#f92672>.</span>Mirror.copy_diffs_to <span style=color:#f92672>~</span>vdi2<span style=color:#f92672>:</span>result<span style=color:#f92672>.</span>Mirror.mirror_vdi<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Remote.VDI.remove_from_sm_config <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>result<span style=color:#f92672>.</span>Mirror.mirror_vdi<span style=color:#f92672>.</span>vdi <span style=color:#f92672>~</span>key<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;base_mirror&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Local VDI %s now mirrored to remote VDI: %s&#34;</span> local_vdi<span style=color:#f92672>.</span>vdi result<span style=color:#f92672>.</span>Mirror.mirror_vdi<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span></span></span></code></pre></div><p>Once the copy has finished we invoke the <code>compose</code> SMAPIv2 call that composes the diffs from the mirror with the base image copied from the snapshot.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Destroying dummy VDI %s on remote&#34;</span> result<span style=color:#f92672>.</span>Mirror.dummy_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Remote.VDI.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr<span style=color:#f92672>:</span>dest <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>result<span style=color:#f92672>.</span>Mirror.dummy_vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    debug <span style=color:#e6db74>&#34;Destroying snapshot %s on src&#34;</span> snapshot<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    Local.VDI.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi<span style=color:#f92672>:</span>snapshot<span style=color:#f92672>.</span>vdi<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Some</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Mirror_id</span> id<span style=color:#f92672>)</span></span></span></code></pre></div><p>we can now destroy the dummy vdi on the remote (which will cause a leaf-coalesce in due course), and we destroy the local snapshot here (which will also cause a leaf-coalesce in due course, providing we don&rsquo;t destroy it first). The return value from the function is the mirror_id that we can use to monitor the state or cancel the mirror.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>|</span> <span style=color:#a6e22e>Sr_not_attached</span><span style=color:#f92672>(</span>sr_uuid<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    error <span style=color:#e6db74>&#34; Caught exception %s:%s. Performing cleanup.&#34;</span> Api_errors.sr_not_attached sr_uuid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    perform_cleanup_actions <span style=color:#f92672>!</span>on_fail<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span>Api_errors.<span style=color:#a6e22e>Server_error</span><span style=color:#f92672>(</span>Api_errors.sr_not_attached<span style=color:#f92672>,[</span>sr_uuid<span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>|</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    error <span style=color:#e6db74>&#34;Caught %s: performing cleanup actions&#34;</span> <span style=color:#f92672>(</span>Api_errors.to_string e<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>    perform_cleanup_actions <span style=color:#f92672>!</span>on_fail<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e</span></span></code></pre></div><p>The exception handler just cleans up afterwards.</p><p>This is not the end of the story, since we need to detach the remote datapath being used for mirroring when we detach this end. The hook function is in <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi/storage_migrate.ml#L775-L791 target=_blank>storage_migrate.ml</a>:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> post_detach_hook <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>~</span>dp <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> State.<span style=color:#a6e22e>Send_state</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> id <span style=color:#f92672>=</span> State.mirror_id_of <span style=color:#f92672>(</span>sr<span style=color:#f92672>,</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  State.find_active_local_mirror id <span style=color:#f92672>|&gt;</span>
</span></span><span style=display:flex><span>  Opt.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> r <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> remote_url <span style=color:#f92672>=</span> Http.Url.of_string r<span style=color:#f92672>.</span>url <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>Remote</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>Client</span><span style=color:#f92672>(</span><span style=color:#66d9ef>struct</span> <span style=color:#66d9ef>let</span> rpc <span style=color:#f92672>=</span> rpc <span style=color:#f92672>~</span>srcstr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;smapiv2&#34;</span> <span style=color:#f92672>~</span>dststr<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;dst_smapiv2&#34;</span> remote_url <span style=color:#66d9ef>end</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> t <span style=color:#f92672>=</span> Thread.create <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          debug <span style=color:#e6db74>&#34;Calling receive_finalize&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          log_and_ignore_exn
</span></span><span style=display:flex><span>            <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span> Remote.DATA.MIRROR.receive_finalize <span style=color:#f92672>~</span>dbg<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;Mirror-cleanup&#34;</span> <span style=color:#f92672>~</span>id<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>          debug <span style=color:#e6db74>&#34;Finished calling receive_finalize&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          State.remove_local_mirror id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          debug <span style=color:#e6db74>&#34;Removed active local mirror: %s&#34;</span> id
</span></span><span style=display:flex><span>        <span style=color:#f92672>)</span> () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      Opt.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> id <span style=color:#f92672>-&gt;</span> Scheduler.cancel scheduler id<span style=color:#f92672>)</span> r<span style=color:#f92672>.</span>watchdog<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;Created thread %d to call receive finalize and dp destroy&#34;</span> <span style=color:#f92672>(</span>Thread.id t<span style=color:#f92672>))</span></span></span></code></pre></div><p>This removes the persistent state and calls <code>receive_finalize</code> on the destination. The body of that functions is:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> receive_finalize <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>id <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> recv_state <span style=color:#f92672>=</span> State.find_active_receive_mirror id <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> State.<span style=color:#a6e22e>Receive_state</span> <span style=color:#66d9ef>in</span> Opt.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> r <span style=color:#f92672>-&gt;</span> Local.DP.destroy <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp<span style=color:#f92672>:</span>r<span style=color:#f92672>.</span>leaf_dp <span style=color:#f92672>~</span>allow_leak<span style=color:#f92672>:</span>false<span style=color:#f92672>)</span> recv_state<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  State.remove_receive_mirror id</span></span></code></pre></div><p>which removes the persistent state on the destination and destroys the datapath associated with the mirror.</p><p>Additionally, there is also a pre-deactivate hook. The rationale for this is that we want to detect any failures to write that occur right at the end of the SXM process. So if there is a mirror operation going on, before we deactivate we wait for tapdisk to flush its queue of outstanding requests, then we query whether there has been a mirror failure. The code is just above the detach hook in <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi/storage_migrate.ml#L738-L773 target=_blank>storage_migrate.ml</a>:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> pre_deactivate_hook <span style=color:#f92672>~</span>dbg <span style=color:#f92672>~</span>dp <span style=color:#f92672>~</span>sr <span style=color:#f92672>~</span>vdi <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> State.<span style=color:#a6e22e>Send_state</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> id <span style=color:#f92672>=</span> State.mirror_id_of <span style=color:#f92672>(</span>sr<span style=color:#f92672>,</span>vdi<span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> start <span style=color:#f92672>=</span> Mtime_clock.counter () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> get_delta () <span style=color:#f92672>=</span> Mtime_clock.count start <span style=color:#f92672>|&gt;</span> Mtime.Span.to_s <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  State.find_active_local_mirror id <span style=color:#f92672>|&gt;</span>
</span></span><span style=display:flex><span>  Opt.iter <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> s <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>(* We used to pause here and then check the nbd_mirror_failed key. Now, we poll
</span></span></span><span style=display:flex><span><span style=color:#75715e>				   until the number of outstanding requests has gone to zero, then check the
</span></span></span><span style=display:flex><span><span style=color:#75715e>				   status. This avoids confusing the backend (CA-128460) *)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>open</span> <span style=color:#a6e22e>Tapctl</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> ctx <span style=color:#f92672>=</span> create () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>rec</span> wait () <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> get_delta () <span style=color:#f92672>&gt;</span> reqs_outstanding_timeout <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>Timeout</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>let</span> st <span style=color:#f92672>=</span> stats ctx s<span style=color:#f92672>.</span>tapdev <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> st<span style=color:#f92672>.</span>Stats.reqs_outstanding <span style=color:#f92672>&gt;</span> 0
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>then</span> <span style=color:#f92672>(</span>Thread.delay 1<span style=color:#f92672>.</span>0<span style=color:#f92672>;</span> wait ()<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>else</span> st
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> st <span style=color:#f92672>=</span> wait () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>        debug <span style=color:#e6db74>&#34;Got final stats after waiting %f seconds&#34;</span> <span style=color:#f92672>(</span>get_delta ()<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> st<span style=color:#f92672>.</span>Stats.nbd_mirror_failed <span style=color:#f92672>=</span> 1
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>          error <span style=color:#e6db74>&#34;tapdisk reports mirroring failed&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          s<span style=color:#f92672>.</span>failed <span style=color:#f92672>&lt;-</span> true
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>end</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> <span style=color:#a6e22e>Timeout</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        error <span style=color:#e6db74>&#34;Timeout out after %f seconds waiting for tapdisk to complete all outstanding requests&#34;</span> <span style=color:#f92672>(</span>get_delta ()<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        s<span style=color:#f92672>.</span>failed <span style=color:#f92672>&lt;-</span> true
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        error <span style=color:#e6db74>&#34;Caught exception while finally checking mirror state: %s&#34;</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>(</span>Printexc.to_string e<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>        s<span style=color:#f92672>.</span>failed <span style=color:#f92672>&lt;-</span> true
</span></span><span style=display:flex><span>    <span style=color:#f92672>)</span></span></span></code></pre></div><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=xe-cli-architecture>XE CLI architecture</h1><div class="box notices cstyle info"><div class=box-label><i class="fa-fw fas fa-info-circle"></i> Info</div><div class=box-content><p>The links in this page point to the source files of xapi
<a href=https://github.com/xapi-project/xen-api/tree/v1.132.0 target=_blank>v1.132.0</a>, not to the
latest source code. Meanwhile, the CLI server code in xapi has been moved to a
library separate from the main xapi binary, and has its own subdirectory
<code>ocaml/xapi-cli-server</code>.</p></div></div><h2 id=architecture>Architecture</h2><ul><li><p><strong>The actual CLI</strong> is a very lightweight binary in
<a href=https://github.com/xapi-project/xen-api/tree/v1.132.0/ocaml/xe-cli target=_blank>ocaml/xe-cli</a></p><ul><li>It is just a dumb client, that does everything that xapi tells
it to do</li><li>This is a security issue<ul><li>We must trust the xenserver that we connect to, because it
can tell xe to read local files, download files, &mldr;</li></ul></li><li>When it is first called, it takes the few command-line arguments
it needs, and then passes the rest to xapi in a HTTP PUT request<ul><li>Each argument is in a separate line</li></ul></li><li>Then it loops doing what xapi tells it to do, in a loop, until
xapi tells it to exit or an exception happens</li></ul></li><li><p><strong>The protocol</strong> description is in
<a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi-cli-protocol/cli_protocol.ml target=_blank>ocaml/xapi-cli-protocol/cli_protocol.ml</a></p><ul><li>The CLI has such a protocol that one binary can talk to multiple
versions of xapi as long as their CLI protocol versions are
compatible</li><li>and the CLI can be changed without updating the xe binary</li><li>and also for performance reasons, it is more efficient this way
than by having a CLI that makes XenAPI calls</li></ul></li><li><p><strong>Xapi</strong></p><ul><li>The HTTP POST request is sent to the <code>/cli</code> URL</li><li>In <code>Xapi.server_init</code>, xapi <a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi/xapi.ml#L804 target=_blank>registers the appropriate function
to handle these
requests</a>,
defined in <a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi/xapi.ml#L589 target=_blank>common_http_handlers in the same
file</a>:
<code>Xapi_cli.handler</code></li><li>The relevant code is in <code>ocaml/xapi/records.ml</code>,
<code>ocaml/xapi/cli_*.ml</code><ul><li>CLI object definitions are in <code>records.ml</code>, command
definitions in <code>cli_frontend.ml</code> (in
<a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi/cli_frontend.ml#L72 target=_blank>cmdtable_data</a>),
implementations of commands in <code>cli_operations.ml</code></li></ul></li><li>When a command is received, it is parsed into a command name and
a parameter list of key-value pairs<ul><li>and the command table
<a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi/xapi_cli.ml#L157 target=_blank>is</a>
<a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi/cli_frontend.ml#L3005 target=_blank>populated
lazily</a>
from the commands defined in <code>cmdtable_data</code> in
<code>cli_frontend.ml</code>, and <a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi/cli_operations.ml#L740 target=_blank>automatically
generated</a>
low-level parameter commands (the ones defined in <a href=http://docs.citrix.com/content/dam/docs/en-us/xenserver/xenserver-7-0/downloads/xenserver-7-0-administrators-guide.pdf target=_blank>section
A.3.2 of the XenServer Administrator&rsquo;s
Guide</a>)
are also added for a list of standard classes</li><li>the command table maps command names to records that contain
the implementation of the command, among other things</li></ul></li><li>Then the command name <a href=https://github.com/xapi-project/xen-api/blob/v1.132.0/ocaml/xapi/xapi_cli.ml#L86 target=_blank>is looked
up</a>
in the command table, and the corresponding operation is
executed with the parsed key-value parameter list passed to it</li></ul></li></ul><h2 id=walk-through-cli-handler-in-xapi-external-calls>Walk-through: CLI handler in xapi (external calls)</h2><h3 id=definitions-for-the-http-handler>Definitions for the HTTP handler</h3><pre><code>Constants.cli_uri = &quot;/cli&quot;

Datamodel.http_actions = [...;
  (&quot;post_cli&quot;, (Post, Constants.cli_uri, false, [], _R_READ_ONLY, []));
...]

(* these public http actions will NOT be checked by RBAC *)
(* they are meant to be used in exceptional cases where RBAC is already *)
(* checked inside them, such as in the XMLRPC (API) calls *)
Datamodel.public_http_actions_with_no_rbac_check` = [...
  &quot;post_cli&quot;;  (* CLI commands -&gt; calls XMLRPC *)
...]

Xapi.common_http_handlers = [...;
  (&quot;post_cli&quot;, (Http_svr.BufIO Xapi_cli.handler));
...]

Xapi.server_init () =
  ...
  &quot;Registering http handlers&quot;, [], (fun () -&gt; List.iter Xapi_http.add_handler common_http_handlers);
  ...
</code></pre><p>Due to there definitions, <code>Xapi_http.add_handler</code> does not perform RBAC checks for <code>post_cli</code>. This means that the CLI handler does not use <code>Xapi_http.assert_credentials_ok</code> when a request comes in, as most other handlers do. The reason is that RBAC checking is delegated to the actual XenAPI calls that are being done by the commands in <code>Cli_operations</code>.</p><p>This means that the <code>Xapi_http.add_handler call</code> so resolves to simply:</p><pre><code>Http_svr.Server.add_handler server Http.Post &quot;/cli&quot; (Http_svr.BufIO Xapi_cli.handler))
</code></pre><p>&mldr;which means that the function <code>Xapi_cli.handler</code> is called directly when an HTTP POST request with path <code>/cli</code> comes in.</p><h3 id=high-level-request-processing>High-level request processing</h3><p><code>Xapi_cli.handler</code>:</p><ul><li>Reads the body of the HTTP request, limitted to <code>Xapi_globs.http_limit_max_cli_size = 200 * 1024</code> characters.</li><li>Sends a protocol version string to the client: <code>"XenSource thin CLI protocol"</code> plus binary encoded major (0) and (2) minor numbers.</li><li>Reads the protocol version from the client and exits with an error if it does not match the above.</li><li>Calls <code>Xapi_cli.parse_session_and_args</code> with the request&rsquo;s body to extract the session ref, if there.</li><li>Calls <code>Cli_frontend.parse_commandline</code> to parse the rest of the command line from the body.</li><li>Calls <code>Xapi_cli.exec_command</code> to execute the command.</li><li>On error, calls <code>exception_handler</code>.</li></ul><p><code>Xapi_cli.parse_session_and_args</code>:</p><ul><li>Is passed the request body and reads it line by line. Each line is considered an argument.</li><li>Removes any CR chars from the end of each argument.</li><li>If the first arg starts with <code>session_id=</code>, the the bit after this prefix is considered to be a session reference.</li><li>Returns the session ref (if there) and (remaining) list of args.</li></ul><p><code>Cli_frontend.parse_commandline</code>:</p><ul><li>Returns the command name and assoc list of param names and values. It handles <code>--name</code> and <code>-flag</code> arguments by turning them into key/value string pairs.</li></ul><p><code>Xapi_cli.exec_command</code>:</p><ul><li>Finds username/password params.</li><li>Get the rpc function: this is the so-called &ldquo;<code>fake_rpc</code> callback&rdquo;, which does not use the network or HTTP at all, but goes straight to <code>Api_server.callback1</code> (the XenAPI RPC entry point). This function is used by the CLI handler to do loopback XenAPI calls.</li><li>Logs the parsed xe command, omitting sensitive data.</li><li>Continues as <code>Xapi_cli.do_rpcs</code></li><li>Looks up the command name in the command table from <code>Cli_frontend</code> (raises an error if not found).</li><li>Checks if all required params have been supplied (raises an error if not).</li><li>Checks that the host is a pool master (raises an error if not).</li><li>Depending on the command, a <code>session.login_with_password</code> or <code>session.slave_local_login_with_password</code> XenAPI call is made with the supplied username and password. If the authentication passes, then a session reference is returned for the RBAC role that belongs to the user. This session is used to do further XenAPI calls.</li><li>Next, the implementation of the command in <code>Cli_operations</code> is executed.</li></ul><h3 id=command-implementations>Command implementations</h3><p>The various commands are implemented in <code>cli_operations.ml</code>. These functions are only called after user authentication has passed (see above). However, RBAC restrictions are only enforced inside any XenAPI calls that are made, and <em>not</em> on any of the other code in <code>cli_operations.ml</code>.</p><p>The type of each command implementation function is as follows (see <code>cli_cmdtable.ml</code>):</p><pre><code>type op =
  Cli_printer.print_fn -&gt;
  (Rpc.call -&gt; Rpc.response) -&gt;
  API.ref_session -&gt; ((string*string) list) -&gt; unit
</code></pre><p>So each function receives a printer for sending text output to the xe client, and rpc function and session reference for doing XenAPI calls, and a key/value pair param list. Here is a typical example:</p><pre><code>let bond_create printer rpc session_id params =
  let network = List.assoc &quot;network-uuid&quot; params in
  let mac = List.assoc_default &quot;mac&quot; params &quot;&quot; in
  let network = Client.Network.get_by_uuid rpc session_id network in
  let pifs = List.assoc &quot;pif-uuids&quot; params in
  let uuids = String.split ',' pifs in
  let pifs = List.map (fun uuid -&gt; Client.PIF.get_by_uuid rpc session_id uuid) uuids in
  let mode = Record_util.bond_mode_of_string (List.assoc_default &quot;mode&quot; params &quot;&quot;) in
  let properties = read_map_params &quot;properties&quot; params in
  let bond = Client.Bond.create rpc session_id network pifs mac mode properties in
  let uuid = Client.Bond.get_uuid rpc session_id bond in
  printer (Cli_printer.PList [ uuid])
</code></pre><ul><li>The necessary parameters are looked up in <code>params</code> using <code>List.assoc</code> or similar.</li><li>UUIDs are translated into reference by <code>get_by_uuid</code> XenAPI calls (note that the <code>Client</code> module is the XenAPI client, and functions in there require the rpc function and session reference).</li><li>Then the main API call is made (<code>Client.Bond.create</code> in this case).</li><li>Further API calls may be made to output data for the client, and passed to the <code>printer</code>.</li></ul><p>This is the common case for CLI operations: they do API calls based on the parameters that were passed in.</p><p>However, other commands are more complicated, for example <code>vm_import/export</code> and <code>vm_migrate</code>. These contain a lot more logic in the CLI commands, and also send commands to the client to instruct it to read or write files and/or do HTTP calls.</p><p>Yet other commands do not actually do any XenAPI calls, but instead get &ldquo;helpful&rdquo; information from other places. Example: <code>diagnostic_gc_stats</code>, which displays statistics from xapi&rsquo;s OCaml GC.</p><h2 id=tutorials>Tutorials</h2><p>The following tutorials show how to extend the CLI (and XenAPI):</p><ul><li><a href=/new-docs/xapi/guides/howtos/add-field/>Adding a field</a></li><li><a href=/new-docs/xapi/guides/howtos/add-function/>Adding an operation</a></li></ul><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=xenopsd>Xenopsd</h1><p>Xenopsd is the VM manager of the XAPI Toolstack.
Xenopsd is responsible for:</p><ul><li>Starting, stopping, rebooting, suspending, resuming, migrating VMs.</li><li>(Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices.</li><li>Setting up VM consoles.</li><li>Running bootloaders.</li><li>Setting QoS parameters.</li><li>Configuring SMBIOS tables.</li><li>Handling crashes.</li><li>etc.</li></ul><p>Check out the <a href=/new-docs/xenopsd/features.html>full features list</a>.</p><p>The code is in <code>ocaml/xenopsd</code>.</p><h2 id=principles>Principles</h2><ol><li>Do no harm: Xenopsd should never touch domains/VMs which it hasn&rsquo;t been
asked to manage. This means that it can co-exist with other VM managers
such as &lsquo;xl&rsquo; and &rsquo;libvirt&rsquo;.</li><li>Be independent: Xenopsd should be able to work in isolation. In particular
the loss of some other component (e.g. the network) should not by itself
prevent VMs being managed locally (including shutdown and reboot).</li><li>Asynchronous by default: Xenopsd exposes task monitoring and offers
cancellation for all operations. Xenopsd ensures that the system is always
in a manageable state after an operation has been cancelled.</li><li>Avoid state duplication: where another component owns some state, Xenopsd
will always defer to it. We will avoid creating out-of-sync caches of
this state.</li><li>Be debuggable: Xenopsd will expose diagnostic APIs and tools to allow
its internal state to be inspected and modified.</li></ol><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Xenopsd</h1><article class=default><header class=headline></header><h1 id=architecture>Architecture</h1><p>Xenopsd instances run on a host and manage VMs on behalf of clients. This
picture shows 3 different Xenopsd instances: 2 named &ldquo;xenopsd-xc&rdquo; and 1 named
&ldquo;xenopsd-xenlight&rdquo;.</p><p><a href=#image-29ea70da2431820c9a223a35974a0468 class=lightbox-link><img src=/new-docs/xenopsd/architecture/host.svg alt="Where xenopsd fits on a host" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-29ea70da2431820c9a223a35974a0468><img src=/new-docs/xenopsd/architecture/host.svg alt="Where xenopsd fits on a host" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Each instance is responsible for managing a disjoint set of VMs. Clients should
never ask more than one Xenopsd to manage the same VM.
Managing a VM means:</p><ul><li>handling start/shutdown/suspend/resume/migrate/reboot</li><li>allowing devices (disks, nics, PCI cards, vCPUs etc) to be manipulated</li><li>providing updates to clients when things change (reboots, console becomes
available, guest agent says something etc).</li></ul><p>For a full list of features, consult the <a href=/new-docs/xenopsd/architecture/features.html>features list</a>.</p><p>Each Xenopsd instance has a unique name on the host. A typical name is</p><ul><li>org.xen.xcp.xenops.classic</li><li>org.xen.xcp.xenops.xenlight</li></ul><p>A higher-level tool, such as <a href=https://github.com/xapi-project/xen-api target=_blank>xapi</a>
will associate VMs with individual Xenopsd names.</p><p>Running multiple Xenopsds is necessary because</p><ul><li>The virtual hardware supported by different technologies (libxc, libxl, qemu)
is expected to be different. We can guarantee the virtual hardware is stable
across a rolling upgrade by running the VM on the old Xenopsd. We can then switch
Xenopsds later over a VM reboot when the VM admin is happy with it. If the
VM admin is unhappy then we can reboot back to the original Xenopsd again.</li><li>The suspend/resume/migrate image formats will differ across technologies
(again libxc vs libxl) and it will be more reliable to avoid switching
technology over a migrate.</li><li>In the future different security domains may have different Xenopsd instances
providing even stronger isolation guarantees between domains than is possible
today.</li></ul><p>Communication with Xenopsd is handled through a Xapi-global library:
<a href=https://github.com/xapi-project/xcp-idl target=_blank>xcp-idl</a>. This library supports</p><ul><li>message framing: by default using HTTP but a binary framing format is
available</li><li>message encoding: by default we use JSON but XML is also available</li><li>RPCs over Unix domain sockets and persistent queues.</li></ul><p>This library allows the communication details to be changed without having to
change all the Xapi clients and servers.</p><p>Xenopsd has a number of &ldquo;backends&rdquo; which perform the low-level VM operations
such as (on Xen) &ldquo;create domain&rdquo; &ldquo;hotplug disk&rdquo; &ldquo;destroy domain&rdquo;. These backends
contain all the hypervisor-specific code including</p><ul><li>connecting to Xenstore</li><li>opening the libxc /proc/xen/privcmd interface</li><li>initialising libxl contexts</li></ul><p>The following diagram shows the internal structure of Xenopsd:</p><p><a href=#image-716f5ff72ba8794320cb51e620ffb031 class=lightbox-link><img src=/new-docs/xenopsd/architecture/xenopsd.svg alt="Inside xenopsd" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-716f5ff72ba8794320cb51e620ffb031><img src=/new-docs/xenopsd/architecture/xenopsd.svg alt="Inside xenopsd" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>At the top of the diagram two client RPC have been sent: one to start a VM
and the other to fetch the latest events. The RPCs are all defined in
<a href=https://github.com/xapi-project/xcp-idl/blob/master/xen/xenops_interface.ml target=_blank>xcp-idl/xen/xenops_interface.ml</a>.
The RPCs are received by the Xenops_server module and decomposed into
&ldquo;micro-ops&rdquo; (labelled &ldquo;μ op&rdquo;). These micro ops represent actions like</p><ul><li>create a Xen domain (recall a Xen domain is an empty shell with no memory)</li><li>build a Xen domain: this is where the kernel or hvmloader is copied in</li><li>launch a device model: this is where a qemu instance is started (if one is
required)</li><li>hotplug a device: this involves writing the frontend and backend trees to
Xenstore</li><li>unpause a domain (recall a Xen domain is created in the paused state)</li></ul><p>Each of these micro-ops is represented by a function call in a &ldquo;backend plugin&rdquo;
interface. The micro-ops are enqueued in queues, one queue per VM. There is a
thread pool (whose size can be changed dynamically by the admin) which pulls
micro-ops from the VM queues and calls the corresponding backend function.</p><p>The active backend (there can only be one backend per Xenopsd instance)
executes the micro-ops. The Xenops_server_xen backend in the picture above
talks to libxc, libxl and qemu to create and destroy domains. The backend
also talks to other Xapi services, in particular</p><ul><li>it registers datasources with xcp-rrdd, telling xcp-rrdd to measure I/O
throughput and vCPU utilisation</li><li>it reserves memory for new domains by talking to squeezed</li><li>it makes disks available by calling SMAPIv2 VDI.{at,de}tach, VDI.{,de}activate</li><li>it launches subprocesses by talking to forkexecd (avoiding problems with
accidental fd capture)</li></ul><p>Xenopsd backends are also responsible for monitoring running VMs. In the
Xenops_server_xen backend this is done by watching Xenstore for</p><ul><li>@releaseDomain watch events</li><li>device hotplug status changes</li></ul><p>When such an event happens (for example: @releaseDomain sent when a domain
requests a reboot) the corresponding operation does not happen inline. Instead
the event is rebroadcast upwards to Xenops_server as a signal (for example:
&ldquo;VM <em>id</em> needs some attention&rdquo;) and a &ldquo;VM_stat&rdquo; micro-op is queued in the
appropriate queue. Xenopsd does not allow operations to run on the same VM
in parallel and enforces this by:</p><ul><li>pushing all operations pertaining to a VM to the same queue</li><li>associating each VM queue to at-most-one worker pool thread</li></ul><p>The event takes the form &ldquo;VM <em>id</em> needs some attention&rdquo; and not &ldquo;VM <em>id</em> needs
to be rebooted&rdquo; because, by the time the queue is flushed, the VM may well now
be in a different state. Perhaps rather than being rebooted it now needs to
be shutdown; or perhaps the domain is now in a good state because the reboot
has already happened. The signals sent by the backend to the Xenops_server are
a bit like event channel notifications in the Xen ring protocols: they are
requests to ask someone to perform work, they don&rsquo;t themselves describe the work
that needs to be done.</p><p>An implication of this design is that it should always be possible to answer
the question, &ldquo;what operation should be performed to get the VM into a valid state?&rdquo;.
If an operation is cancelled half-way through or if Xenopsd is suddenly restarted,
it will ask the question about all the VMs and perform the necessary operations.
The operations must be designed carefully to make this work. For example if Xenopsd
is restarted half-way through starting a VM, it must be obvious on restart that
the VM should either be forcibly shutdown or rebooted to make it a valid state
again. Note: we don&rsquo;t demand that operations are performed as transactions;
we only demand that the state they leave the system be &ldquo;sensible&rdquo; in the sense
that the admin will recognise it and be able to continue their work.</p><p>Sometimes this can be achieved through careful ordering of side-effects
within the operations, taking advantage of artifacts of the system such as:</p><ul><li>a domain which has not been fully created will have total vCPU time = 0 and
will be paused. If we see one of these we should reboot it because it may
not be fully intact.</li></ul><p>In the absense of &ldquo;tells&rdquo; from the system, operations are expected to journal
their intentions and support restart after failure.</p><p>There are three categories of metadata associated with VMs:</p><ol><li>system metadata: this is created as a side-effect of starting VMs. This
includes all the information about active disks and nics stored in Xenstore
and the list of running domains according to Xen.</li><li>VM: this is the configuration to use when the VM is started or rebooted.
This is like a &ldquo;config file&rdquo; for the VM.</li><li>VmExtra: this is the runtime configuration of the VM. When VM configuration
is changed it often cannot be applied immediately; instead the VM continues
to run with the previous configuration. We need to track the runtime
configuration of the VM in order for suspend/resume and migrate to work. It
is also useful to be able to tell a client, &ldquo;on next reboot this value will
be <em>x</em> but currently it is <em>x-1</em>&rdquo;.</li></ol><p>VM and VmExtra metadata is stored by Xenopsd in the domain 0 filesystem, in
a simple directory hierarchy.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=design>Design</h1><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Design</h1><article class=default><header class=headline></header><h1 id=events>Events</h1><ul><li>ids rather than data; inherently coalescable</li><li>blocking poll + async operations implies a client needs 2 connections</li><li>coarse granularity</li><li>similarity and differences with: XenAPI, event channels, xenstore watches</li></ul><p><a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467 target=_blank>https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467</a></p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=hooks>Hooks</h1><p>There are a number of hook points at which xenopsd may execute certain scripts. These scripts are found in hook-specific directories of the form <code>/etc/xapi.d/&lt;hookname>/</code>. All executable scripts in these directories are run with the following arguments:</p><pre><code>&lt;script.sh&gt; -reason &lt;reason&gt; -vmuuid &lt;uuid of VM&gt;
</code></pre><p>The scripts are executed in filename-order. By convention, the filenames are usually of the form <code>10resetvdis</code>.</p><p>The hook points are:</p><pre><code>vm-pre-shutdown
vm-pre-migrate
vm-post-migrate (Dundee only)
vm-pre-start
vm-pre-reboot
vm-pre-resume
vm-post-resume (Dundee only)
vm-post-destroy
</code></pre><p>and the reason codes are:</p><pre><code>clean-shutdown
hard-shutdown
clean-reboot
hard-reboot
suspend
source -- passed to pre-migrate hook on source host
destination -- passed to post-migrate hook on destination (Dundee only)
none
</code></pre><p>For example, in order to execute a script on VM shutdown, it would be sufficient to create the script in the post-destroy hook point:</p><pre><code>/etc/xapi.d/vm-post-destroy/01myscript.sh
</code></pre><p>containing</p><pre><code>#!/bin/bash
echo I was passed $@ &gt; /tmp/output
</code></pre><p>And when, for example, VM e30d0050-8f15-e10d-7613-cb2d045c8505 is shut-down, the script is executed:</p><pre><code>[vagrant@localhost ~]$ sudo xe vm-shutdown --force uuid=e30d0050-8f15-e10d-7613-cb2d045c8505
[vagrant@localhost ~]$ cat /tmp/output
I was passed -vmuuid e30d0050-8f15-e10d-7613-cb2d045c8505 -reason hard-shutdown
</code></pre><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pvs-proxy-ovs-rules>PVS Proxy OVS Rules</h1><h1 id=rule-design>Rule Design</h1><p>The Open vSwitch (OVS) daemon implements a programmable switch.
XenServer uses it to re-direct traffic between three entities:</p><ul><li>PVS server - identified by its IP address</li><li>a local VM - identified by its MAC address</li><li>a local Proxy - identified by its MAC address</li></ul><p>VM and PVS server are unaware of the Proxy; xapi configures OVS to
redirect traffic between PVS and VM to pass through the proxy.</p><p>OVS uses rules that match packets. Rules are organised in sets called
tables. A rule can be used to match a packet and to inject it into
another rule set/table table such that a packet can be matched again.</p><p>Furthermore, a rule can set registers associated with a packet which that
can be matched in subsequent rules. In that way, a packet can be tagged
such that it will only match specific rules downstream that match the
tag.</p><p>Xapi configures 3 rule sets:</p><h2 id=table-0---entry-rules>Table 0 - Entry Rules</h2><p>Rules match UDP traffic between VM/PVS, Proxy/VM, and PVS/VM where the
PVS server is identified by its IP and all other components by their MAC
address. All packets are tagged with the direction they are going and
re-submitted into Table 101 which handles ports.</p><h2 id=table-101---port-rules>Table 101 - Port Rules</h2><p>Rules match UDP traffic going to a specific port of the PVS server and
re-submit it into Table 102.</p><h2 id=table-102---exit-rules>Table 102 - Exit Rules</h2><p>These rules implement the redirection:</p><ul><li>Rules matching packets coming from VM to PVS are directed to the Proxy.</li><li>Rules matching packets coming from PVS to VM are directed to the Proxy.</li><li>Rules matching packets coming from the Proxy are already addressed
properly (to the VM) are handled normally.</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=requirements-for-suspend-image-framing>Requirements for suspend image framing</h1><p>We are currently (Dec 2013) undergoing a transition from the &lsquo;classic&rsquo; xenopsd
backend (built upon calls to libxc) to the &lsquo;xenlight&rsquo; backend built on top of
the officially supported libxl API.</p><p>During this work, we have come across an incompatibility between the suspend
images created using the &lsquo;classic&rsquo; backend and those created using the new
libxl-based backend. This needed to be fixed to enable RPU to any new version
of XenServer.</p><h2 id=historic-classic-stack>Historic &lsquo;classic&rsquo; stack</h2><p>Prior to this work, xenopsd was involved in the construction of the suspend
image and we ended up with an image with the following format:</p><pre><code>+-----------------------------+
| &quot;XenSavedDomain\n&quot;          |  &lt;-- added by xenopsd-classic
|-----------------------------|
|  Memory image dump          |  &lt;-- libxc
|-----------------------------|
| &quot;QemuDeviceModelRecord\n&quot;   |
|  &lt;size of following record&gt; |  &lt;-- added by xenopsd-classic
|  (a 32-bit big-endian int)  |
|-----------------------------|
| &quot;QEVM&quot;                      |  &lt;-- libxc/qemu
|  Qemu device record         |
+-----------------------------+
</code></pre><p>We have also been carrying a patch in the Xen patchqueue against
xc_domain_restore. This patch (revert_qemu_tail.patch) stopped
xc_domain_restore from attempting to read past the memory image dump. At which
point xenopsd-classic would just take over and restore what it had put there.</p><h2 id=requirements-for-new-stack>Requirements for new stack</h2><p>For xenopsd-xenlight to work, we need to operate without the
revert_qemu_tail.patch since libxl assumes it is operating on top of an
upstream libxc.</p><p>We need the following relationship between suspend images created on one
backend being able to be restored on another backend. Where the backends are
old-classic (OC), new-classic (NC) and xenlight (XL). Obviously all suspend
images created on any backend must be able to be restored on the same backend:</p><pre><code>                OC _______ NC _______ XL
                 \  &gt;&gt;&gt;&gt;&gt;      &gt;&gt;&gt;&gt;&gt;  /
                  \__________________/
                    &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</code></pre><p>It turns out this was not so simple. After removing the patch against
xc_domain_restore and allowing libxc to restore the hvm_buffer_tail, we found
that supsend images created with OC (detailed in the previous section) are not
of a valid format for two reasons:</p><pre><code>i. The &quot;XenSavedDomain\n&quot; was extraneous;
</code></pre><p>ii. The Qemu signature section (prior to the record) is not of valid form.</p><p>It turns out that the section with the Qemu signature can be one of the
following:</p><pre><code>a. &quot;QemuDeviceModelRecord&quot; (NB. no newline) followed by the record to EOF;
b. &quot;DeviceModelRecord0002&quot; then a uint32_t length followed by record;
c. &quot;RemusDeviceModelState&quot; then a uint32_t length followed by record;
</code></pre><p>The old-classic (OC) backend not only uses an invalid signature (since it
contains a trailing newline) but it also includes a length, <em>and</em> the length is
in big-endian when the uint32_t is seen to be little-endian.</p><p>We considered creating a proxy for the fd in the incompatible cases but since
this would need to be a 22-lookahead byte-by-byte proxy this was deemed
impracticle. Instead we have made patched libxc with a much simpler patch to
understand this legacy format.</p><p>Because peek-ahead is not possible on pipes, the patch for (ii) needed to be
applied at a point where the hvm tail had been read completely. We piggy-backed
on the point after (a) had been detected. At this point the remainder of the fd
is buffered (only around 7k) and the magic &ldquo;QEVM&rdquo; is expected at the head of
this buffer. So we simply added a patch to check if there was a pesky newline
and the buffer[5:8] was &ldquo;QEVM&rdquo; and if it was we could discard the first
5 bytes:</p><pre><code>                              0    1    2    3    4    5   6   7   8
Legacy format from OC:  [...| \n | \x | \x | \x | \x | Q | E | V | M |...]

Required at this point: [...|  Q |  E |  V |  M |...]
</code></pre><h2 id=changes-made>Changes made</h2><p>To make the above use-cases work, we have made the following changes:</p><pre><code>1. Make new-classic (NC) not restore Qemu tail (let libxc do it)
    xenopsd.git:ef3bf4b

2. Make new-classic use valid signature (b) for future restore images
    xenopsd.git:9ccef3e

3. Make xc_domain_restore in libxc understand legacy xenopsd (OC) format
    xen-4.3.pq.hg:libxc-restore-legacy-image.patch

4. Remove revert-qemu-tail.patch from Xen patchqueue
    xen-4.3.pq.hg:3f0e16f2141e

5. Make xenlight (XL) use &quot;XenSavedDomain\n&quot; start-of-image signature
    xenopsd.git:dcda545
</code></pre><p>This has made the required use-cases work as follows:</p><pre><code>                OC __134__ NC __245__ XL
                 \  &gt;&gt;&gt;&gt;&gt;      &gt;&gt;&gt;&gt;&gt;  /
                  \_______345________/
                    &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</code></pre><p>And the suspend-resume on same backends work by virtue of:</p><pre><code>OC --&gt; OC : Just works
NC --&gt; NC : By 1,2,4
XL --&gt; XL : By 4 (5 is used but not required)
</code></pre><h2 id=new-components>New components</h2><p>The output of the changes above are:</p><ul><li>A new xenops-xc binary for NC</li><li>A new xenops-xl binary for XL</li><li>A new libxenguest.4.3 for both of NC and XL</li></ul><h2 id=future-considerations>Future considerations</h2><p>This should serve as a useful reference when considering making changes to the
suspend image in any way.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=suspend-image-framing-format>Suspend image framing format</h1><p>Example suspend image layout:</p><pre><code>+----------------------------+
| 1. Suspend image signature |
+============================+
| 2.0 Xenops header          |
| 2.1 Xenops record          |
+============================+
| 3.0 Libxc header           |
| 3.1 Libxc record           |
+============================+
| 4.0 Qemu header            |
| 4.1 Qemu save record       |
+============================+
| 5.0 End_of_image footer    |
+----------------------------+
</code></pre><p>A suspend image is now constucted as a series of header-record pairs. The
initial signature (1.) is used to determine whether we are dealing with the
unstructured, &ldquo;legacy&rdquo; suspend image or the new, structured format.</p><p>Each header is two 64-bit integers: the first identifies the header type and
the second is the length of the record that follows in bytes. The following
types have been defined (the ones marked with a (*) have yet to be
implemented):</p><pre><code>* Xenops       : Metadata for the suspend image
* Libxc        : The result of a xc_domain_save
* Libxl*       : Not implemented
* Libxc_legacy : Marked as a libxc record saved using pre-Xen-4.5
* Qemu_trad    : The qemu save file for the Qemu used in XenServer
* Qemu_xen*    : Not implemented
* Demu*        : Not implemented
* End_of_image : A footer marker to denote the end of the suspend image
</code></pre><p>Some of the above types do not have the notion of a length since they cannot be
known upfront before saving and also are delegated to other layers of the stack
on restoring. Specifically these are the memory image sections, libxc and
libxl.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=tasks>Tasks</h1><p>Some operations performed by Xenopsd are blocking, for example:</p><ul><li>suspend/resume/migration</li><li>attaching disks (where the SMAPI VDI.attach/activate calls can perform network
I/O)</li></ul><p>We want to be able to</p><ul><li>present the user with an idea of progress (perhaps via a &ldquo;progress bar&rdquo;)</li><li>allow the user to cancel a blocked operation that is taking too long</li><li>associate logging with the user/client-initiated actions that spawned them</li></ul><h2 id=principles>Principles</h2><ul><li>all operations which may block (the vast majority) should be written in an
asynchronous style i.e. the operations should immediately return a Task id</li><li>all operations should guarantee to respond to a cancellation request in a
bounded amount of time (30s)</li><li>when cancelled, the system should always be left in a valid state</li><li>clients are responsible for destroying Tasks when they are finished with the
results</li></ul><h2 id=types>Types</h2><p>A task has a state, which may be Pending, Completed or failed:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>type</span> async_result <span style=color:#f92672>=</span> <span style=color:#66d9ef>unit</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>type</span> completion_t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>		duration <span style=color:#f92672>:</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		result <span style=color:#f92672>:</span> async_result option
</span></span><span style=display:flex><span>	<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>type</span> state <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>Pending</span> <span style=color:#66d9ef>of</span> <span style=color:#66d9ef>float</span>
</span></span><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>Completed</span> <span style=color:#66d9ef>of</span> completion_t
</span></span><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>Failed</span> <span style=color:#66d9ef>of</span> Rpc.t</span></span></code></pre></div><p>When a task is Failed, we assocate it with a marshalled exception (a value of type
Rpc.t). This exception must be one from the set defined in the
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L46 target=_blank>Xenops_interface</a>.
To see how they are marshalled, see
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L564 target=_blank>Xenops_server</a>.</p><p>From the point of view of a client, a Task has the immutable type (which can be
queried with a <code>Task.stat</code>):</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>type</span> t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>		id<span style=color:#f92672>:</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		dbg<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		ctime<span style=color:#f92672>:</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		state<span style=color:#f92672>:</span> state<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		subtasks<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>string</span> <span style=color:#f92672>*</span> state<span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		debug_info<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>string</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>}</span></span></span></code></pre></div><p>where</p><ul><li>id is a unique (integer) id generated by Xenopsd. This is how a Task is
represented to clients</li><li>dbg is a client-provided debug key which will be used in log lines, allowing
lines from the same Task to be associated together</li><li>ctime is the creation time</li><li>state is the current state (Pending/Completed/Failed)</li><li>subtasks lists logical internal sub-operations for debugging</li><li>debug_info includes miscellaneous key/value pairs used for debugging</li></ul><p>Internally, Xenopsd uses a
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L73 target=_blank>mutable record type</a>
to track Task state. This is broadly similar to the interface type except</p><ul><li>the state is mutable: this allows Tasks to complete</li><li>the task contains a &ldquo;do this now&rdquo; thunk</li><li>there is a &ldquo;cancelling&rdquo; boolean which is toggled to request a cancellation.</li><li>there is a list of cancel callbacks</li><li>there are some fields related to &ldquo;cancel points&rdquo;</li></ul><h2 id=persistence>Persistence</h2><p>The Tasks are intended to represent activities associated with in-memory queues
and threads. Therefore the active Tasks are kept in memory in a map, and will
be lost over a process restart. This is desirable since we will also lose the
queued items and the threads, so there is no need to resync on start.</p><p>Note that every operation must ensure that the state of the system is recoverable
on restart by not leaving it in an invalid state. It is not necessary to either
guarantee to complete or roll-back a Task. Tasks are not expected to be
transactional.</p><h2 id=lifecycle-of-a-task>Lifecycle of a Task</h2><p>All Tasks returned by API functions are created as part of the enqueue functions:
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1451 target=_blank>queue_operation_*</a>.
Even operations which are performed internally are normally wrapped in Tasks by
the function
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1451 target=_blank>immediate_operation</a>.</p><p>A queued operation will be processed by one of the
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L554 target=_blank>queue worker threads</a>.
It will</p><ul><li>set the thread-local debug key to the Task.dbg</li><li>call <code>task.Xenops_task.run</code>, taking care to catch exceptions and update
the <code>task.Xenops_task.state</code></li><li>unset the thread-local debug key</li><li>generate an event on the Task to provoke clients to query the current state.</li></ul><p>Task implementations must update their progress as they work. For the common
case of a compound operation like <code>VM_start</code> which is decomposed into
multiple &ldquo;micro-ops&rdquo; (e.g. <code>VM_create</code> <code>VM_build</code>) there is a useful
helper function
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1092 target=_blank>perform_atomics</a>
which divides the progress &lsquo;bar&rsquo; into sections, where each &ldquo;micro-op&rdquo; can have
a different size (<code>weight</code>). A progress callback function is passed into
each Xenopsd backend function so it can be updated with fine granulatiry. For
example note the arguments to
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1092 target=_blank>B.VM.save</a></p><p>Clients are expected to destroy Tasks they are responsible for creating. Xenopsd
cannot do this on their behalf because it does not know if they have successfully
queried the Task status/result.</p><p>When Xenopsd is a client of itself, it will take care to destroy the Task
properly, for example see
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1451 target=_blank>immediate_operation</a>.</p><h2 id=cancellation>Cancellation</h2><p>The goal of cancellation is to unstick a blocked operation and to return the
system to <em>some</em> valid state, not any valid state in particular.
Xenopsd does not treat operations as transactions;
when an operation is cancelled it may</p><ul><li>fully complete (e.g. if it was about to do this anyway)</li><li>fully abort (e.g. if it had made no progress)</li><li>enter some other valid state (e.g. if it had gotten half way through)</li></ul><p>Xenopsd will never leave the system in an invalid state after cancellation.</p><p>Every Xenopsd operation should unblock and return the system to a valid state within
a reasonable amount of time after a cancel request. This should be as quick as possible
but up to 30s may be acceptable.
Bear in mind that a human is probably impatiently watching a UI say &ldquo;please wait&rdquo;
and which doesn&rsquo;t have any notion of progress itself. Keep it quick!</p><p>Cancellation is triggered by TASK.cancel which calls
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L194 target=_blank>cancel</a>.
This</p><ul><li>sets the cancelling boolean</li><li>calls all registered cancel callbacks</li></ul><p>Implementations respond to cancellation by</p><ul><li>if running: periodically call <a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L213 target=_blank>check_cancelling</a></li><li>if about to block: register a suitable cancel callback safely with <a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L224 target=_blank>with_cancel</a>.</li></ul><p>Xenopsd&rsquo;s libxc backend can block in 2 different ways, and therefore has 2 different
types of cancel callback:</p><ol><li>cancellable Xenstore watches</li><li>cancellable subprocesses</li></ol><p>Xenstore watches are used for device hotplug and unplug. Xenopsd has to wait for
the backend or for a udev script to do something. If that blocks then we need
a way to cancel the watch. The easiest way to cancel a watch is to watch an
additional path (a &ldquo;cancel path&rdquo;) and delete it, see
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/xc/cancel_utils.ml#L117 target=_blank>cancellable_watch</a>.
The &ldquo;cancel paths&rdquo; are placed within the VM&rsquo;s Xenstore directory to ensure that
cleanup code which does <code>xenstore-rm</code> will automatically &ldquo;cancel&rdquo; all outstanding
watches. Note that we trigger a cancel by deleting rather than creating, to avoid
racing with delete and creating orphaned Xenstore entries.</p><p>Subprocesses are used for suspend/resume/migrate. Xenopsd hands file descriptors
to libxenguest by running a subprocess and passing the fds to it. Xenopsd therefore
gets the process id and can send it a signal to cancel it. See
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/xc/cancel_utils.ml#L117 target=_blank>Cancellable_subprocess.run</a>.</p><h2 id=testing-with-cancel-points>Testing with cancel points</h2><p>Cancellation is difficult to test, as it is completely asynchronous. Therefore
Xenopsd has some built-in cancellation testing infrastructure known as &ldquo;cancel points&rdquo;.
A &ldquo;cancel point&rdquo; is a point in the code where a <code>Cancelled</code> exception could
be thrown, either by checking the cancelling boolean or as a side-effect of
a cancel callback. The
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L216 target=_blank>check_cancelling</a>
function increments a counter every time it passes one of these points, and
this value is returned to clients in the
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L135 target=_blank>Task.debug_info</a>.</p><p>A <a href=https://github.com/xapi-project/xen-api/blob/a365545c3b113fcd4bedecbc9146d4b6e3efbb04/ocaml/xapi/cancel_tests.ml target=_blank>test harness</a>
runs a series of operations. Each operation is first run all the way through to
completion to discover the total number of cancel points. The operation is then
re-run with a
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L84 target=_blank>request to cancel at a particular point</a>.
The test then waits for the system to stabilise and verifies that it appears to be
in a valid state.</p><h2 id=preventing-tasks-leaking>Preventing Tasks leaking</h2><p>The client who creates a Task must destroy it when the Task is finished, and
they have processed the result. What if a client like xapi is restarted while
a Task is running?</p><p>We assume that, if xapi is talking to a xenopsd, then xapi completely owns it.
Therefore xapi should destroy any completed tasks that it doesn&rsquo;t recognise.</p><p>If a user wishes to manage VMs with xenopsd in parallel with xapi, the user
should run a separate xenopsd.</p><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=features>Features</h1><h2 id=general>General</h2><ul><li>Pluggable backends including<ul><li>xc: drives Xen via libxc and xenguest</li><li>simulator: simulates operations for component-testing</li></ul></li><li>Supports running multiple instances and backends on the same host, looking
after different sets of VMs</li><li>Extensive configuration via command-line (see manpage) and config
file</li><li>Command-line tool for easy VM administration and troubleshooting</li><li>User-settable degree of concurrency to get VMs started quickly</li></ul><h2 id=vms>VMs</h2><ul><li>VM start/shutdown/reboot</li><li>VM suspend/resume/checkpoint/migrate</li><li>VM pause/unpause</li><li>VM s3suspend/s3resume</li><li>customisable SMBIOS tables for OEM-locked VMs</li><li>hooks for 3rd party extensions:<ul><li>pre-start</li><li>pre-destroy</li><li>post-destroy</li><li>pre-reboot</li></ul></li><li>per-VM xenguest replacement</li><li>suppression of VM reboot loops</li><li>live vCPU hotplug and unplug</li><li>vCPU to pCPU affinity setting</li><li>vCPU QoS settings (weight and cap for the Xen credit2 scheduler)</li><li>DMC memory-ballooning support</li><li>support for storage driver domains</li><li>live update of VM shadow memory</li><li>guest-initiated disk/nic hotunplug</li><li>guest-initiated disk eject</li><li>force disk/nic unplug</li><li>support for &lsquo;surprise-removable&rsquo; devices</li><li>disk QoS configuration</li><li>nic QoS configuration</li><li>persistent RTC</li><li>two-way guest agent communication for monitoring and control</li><li>network carrier configuration</li><li>port-locking for nics</li><li>text and VNC consoles over TCP and Unix domain sockets</li><li>PV kernel and ramdisk whitelisting</li><li>configurable VM videoram</li><li>programmable action-after-crash behaviour including: shutting down
the VM, taking a crash dump or leaving the domain paused for inspection</li><li>ability to move nics between bridges/switches</li><li>advertises the VM memory footprints</li><li>PCI passthrough</li><li>support for discrete emulators (e.g. &lsquo;demu&rsquo;)</li><li>PV keyboard and mouse</li><li>qemu stub domains</li><li>cirrus and stdvga graphics cards</li><li>HVM serial console (useful for debugging)</li><li>support for vGPU</li><li>workaround for &lsquo;spurious page faults&rsquo; kernel bug</li><li>workaround for &lsquo;machine address size&rsquo; kernel bug</li></ul><h2 id=hosts>Hosts</h2><ul><li>CPUid masking for heterogenous pools: reports true features and current
features</li><li>Host console reading</li><li>Hypervisor version and capabilities reporting</li><li>Host CPU querying</li></ul><h2 id=apis>APIs</h2><ul><li>versioned json-rpc API with feature advertisements</li><li>clients can disconnect, reconnect and easily resync with the latest
VM state without losing updates</li><li>all operations have task control including<ul><li>asychronous cancellation: for both subprocesses and xenstore watches</li><li>progress updates</li><li>subtasks</li><li>per-task debug logs</li></ul></li><li>asynchronous event watching API</li><li>advertises VM metrics<ul><li>memory usage</li><li>balloon driver co-operativeness</li><li>shadow memory usage</li><li>domain ids</li></ul></li><li>channel passing (via sendmsg(2)) for efficient memory image copying</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=operation-walk-throughs>Operation Walk-Throughs</h1><p>Let&rsquo;s trace through interesting operations to see how the whole system
works.</p><ul><li><a href=/new-docs/xenopsd/walkthroughs/VM.start.md>Starting a VM</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate.md>Migrating a VM</a></li><li>Shutting down a VM and waiting for it to happen</li><li>A VM wants to reboot itself</li><li>A disk is hotplugged</li><li>A disk refuses to hotunplug</li><li>A VM is suspended</li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Operation Walk-Throughs</h1><article class=default><header class=headline></header><h1 id=live-migration-sequence-diagram>Live Migration Sequence Diagram</h1><div class="mermaid align-left">sequenceDiagram
autonumber
participant tx as sender
participant rx0 as receiver thread 0
participant rx1 as receiver thread 1
participant rx2 as receiver thread 2
activate tx
tx->>rx0: VM.import_metadata
tx->>tx: Squash memory to dynamic-min
tx->>rx1: HTTP /migrate/vm
activate rx1
rx1->>rx1: VM_receive_memory&lt;br/>VM_create (00000001)&lt;br/>VM_restore_vifs
rx1->>tx: handshake (control channel)&lt;br/>Synchronisation point 1
tx->>rx2: HTTP /migrate/mem
activate rx2
rx2->>tx: handshake (memory channel)&lt;br/>Synchronisation point 1-mem
tx->>rx1: handshake (control channel)&lt;br/>Synchronisation point 1-mem ACK
rx2->>rx1: memory fd
tx->>rx1: VM_save/VM_restore&lt;br/>Synchronisation point 2
tx->>tx: VM_rename
rx1->>rx2: exit
deactivate rx2
tx->>rx1: handshake (control channel)&lt;br/>Synchronisation point 3
rx1->>rx1: VM_rename&lt;br/>VM_restore_devices&lt;br/>VM_unpause&lt;br/>VM_set_domain_action_request
rx1->>tx: handshake (control channel)&lt;br/>Synchronisation point 4
deactivate rx1
tx->>tx: VM_shutdown&lt;br/>VM_remove
deactivate tx</div><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=walkthrough-migrating-a-vm>Walkthrough: Migrating a VM</h1><p>A XenAPI client wishes to migrate a VM from one host to another within
the same pool.</p><p>The client will issue a command to migrate the VM and it will be dispatched
by the autogenerated <code>dispatch_call</code> function from <strong>xapi/server.ml</strong>. For
more information about the generated functions you can have a look to
<a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/idl/ocaml_backend target=_blank>XAPI IDL model</a>.</p><p>The command will trigger the operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/lib/xenops_server.ml#L2572 target=_blank>VM_migrate</a>
that has low level operations performed by the backend. These atomics operations
that we will describe in the documentation are:</p><ul><li>VM.restore</li><li>VM.rename</li><li>VBD.set_active</li><li>VBD.plug</li><li>VIF.set_active</li><li>VGPU.set_active</li><li>VM.create_device_model</li><li>PCI.plug</li><li>VM.set_domain_action_request</li></ul><p>The command have serveral parameters such as: should it be ran asynchronously,
should it be forwared to another host, how arguments should be marshalled and
so on. A new thread is created by <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xapi/server_helpers.ml#L55 target=_blank>xapi/server_helpers.ml</a>
to handle the command asynchronously. At this point the helper also check if
the command should be passed to the <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi/message_forwarding.ml target=_blank>message forwarding</a>
layer in order to be executed on another host (the destination) or locally if
we are already at the right place.</p><p>It will finally reach <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xapi/api_server.ml#L242 target=_blank>xapi/api_server.ml</a> that will take the action
of posted a command to the message broker <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/message-switch target=_blank>message switch</a>.
It is a JSON-RPC HTTP request sends on a Unix socket to communicate between some
XAPI daemons. In the case of the migration this message sends by <strong>XAPI</strong> will be
consumed by the <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd target=_blank>xenopsd</a>
daemon that will do the job of migrating the VM.</p><h1 id=the-migration-of-the-vm>The migration of the VM</h1><p>The migration is an asynchronous task and a thread is created to handle this task.
The tasks&rsquo;s reference is returned to the client, which can then check
its status until completion.</p><p>As we see in the introduction the <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd target=_blank>xenopsd</a>
daemon will pop the operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/lib/xenops_server.ml#L2572 target=_blank>VM_migrate</a>
from the message broker.</p><p>Only one backend is know available that interacts with libxc, libxenguest
and xenstore. It is the <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd/xc target=_blank>xc backend</a>.</p><p>The entities that need to be migrated are: <em>VDI</em>, <em>VIF</em>, <em>VGPU</em> and <em>PCI</em> components.</p><p>During the migration process the destination domain will be built with the same
uuid than the original VM but the last part of the UUID will be
<code>XXXXXXXX-XXXX-XXXX-XXXX-000000000001</code>. The original domain will be removed using
<code>XXXXXXXX-XXXX-XXXX-XXXX-000000000000</code>.</p><p>There are some points called <em>hooks</em> at which <code>xenopsd</code> can execute some script.
Before starting a migration a command is send to the original domain to execute
a pre migrate script if it exists.</p><p>Before starting the migration a command is sent to Qemu using the Qemu Machine Protocol (QMP)
to check that the domain can be suspended (see <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/device_common.ml target=_blank>xenopsd/xc/device_common.ml</a>).
After checking with Qemu that the VM is suspendable we can start the migration.</p><h2 id=importing-metadata>Importing metadata</h2><p>As for <em>hooks</em>, commands to source domain are sent using <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/libs/stunnel target=_blank>stunnel</a> a daemon which
is used as a wrapper to manage SSL encryption communication between two hosts on the same
pool. To import metada an XML RPC command is sent to the original domain.</p><p>Once imported it will give us a reference id and will allow to build the new domain
on the destination using the temporary VM uuid <code>XXXXXXXX-XXXX-XXXX-XXXX-000000000001</code>
where <code>XXX...</code> is the reference id of the original VM.</p><h2 id=setting-memory>Setting memory</h2><p>One of the first thing to do is to setup the memory. The backend will check that there
is no ballooning operation in progress. At this point the migration can fail if a
ballooning operation is in progress and takes too much time.</p><p>Once memory checked the daemon will get the state of the VM (running, halted, &mldr;) and
information about the VM are retrieve by the backend like the maximum memory the domain
can consume but also information about quotas for example.
Information are retrieve by the backend from xenstore.</p><p>Once this is complete, we can restore VIF and create the domain.</p><p>The synchronisation of the memory is the first point of synchronisation and everythin
is ready for VM migration.</p><h2 id=vm-migration>VM Migration</h2><p>After receiving memory we can set up the destination domain. If we have a vGPU we need to kick
off its migration process. We will need to wait the acknowledge that indicates that the entry
for the GPU has been well initialized. before starting the main VM migration.</p><p>Their is a mechanism of handshake for synchronizing between the source and the
destination. Using the handshake protocol the receiver inform the sender of the
request that everything has been setup and ready to save/restore.</p><h3 id=vm-restore>VM restore</h3><p>VM restore is a low level atomic operation <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L2684 target=_blank>VM.restore</a>.
This operation is represented by a function call to <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/domain.ml#L1540 target=_blank>backend</a>.
It uses <strong>Xenguest</strong>, a low-level utility from XAPI toolstack, to interact with the Xen hypervisor
and libxc for sending a request of migration to the <strong>emu-manager</strong>.</p><p>After sending the request results coming from <strong>emu-manager</strong> are collected
by the main thread. It blocks until results are received.</p><p>During the live migration, <strong>emu-manager</strong> helps in ensuring the correct state
transitions for the devices and handling the message passing for the VM as
it&rsquo;s moved between hosts. This includes making sure that the state of the
VM&rsquo;s virtual devices, like disks or network interfaces, is correctly moved over.</p><h3 id=vm-renaming>VM renaming</h3><p>Once all operations are done we can rename the VM on the target from its temporary
name to its real UUID. This operation is another low level atomic one
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L1667 target=_blank>VM.rename</a>
that will take care of updating the xenstore on the destination.</p><p>The next step is the restauration of devices and unpause the domain.</p><h3 id=restoring-remaining-devices>Restoring remaining devices</h3><p>Restoring devices starts by activating VBD using the low level atomic operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3674 target=_blank>VBD.set_active</a>. It is an update of Xenstore. VBDs that are read-write must
be plugged before read-only ones. Once activated the low level atomic operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3721 target=_blank>VBD.plug</a>
is called. VDI are attached and activate.</p><p>Next devices are VIFs that are set as active <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L4296 target=_blank>VIF.set_active</a> and plug <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L4394 target=_blank>VIF.plug</a>.
If there are VGPUs we will set them as active now using the atomic <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3490 target=_blank>VGPU.set_active</a>.</p><p>We are almost done. The next step is to create the device model</p><h4 id=create-device-model>create device model</h4><p>Create device model is done by using the atomic operation <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L2375 target=_blank>VM.create_device_model</a>. This
will configure <strong>qemu-dm</strong> and started. This allow to manage PCI devices.</p><h4 id=pci-plug>PCI plug</h4><p><a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3399 target=_blank>PCI.plug</a>
is executed by the backend. It plugs a PCI device and advertise it to QEMU if this option is set. It is
the case for NVIDIA SR-IOV vGPUS.</p><p>At this point devices have been restored. The new domain is considered survivable. We can
unpause the domain and performs last actions</p><h3 id=unpause-and-done>Unpause and done</h3><p>Unpause is done by managing the state of the domain using bindings to <a href="https://xenbits.xen.org/gitweb/?p=xen.git;a=blob;f=tools/libs/ctrl/xc_domain.c;h=f2d9d14b4d9f24553fa766c5dcb289f88d684bb0;hb=HEAD#l76" target=_blank>xenctrl</a>.
Once hypervisor has unpaused the domain some actions can be requested using <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3172 target=_blank>VM.set_domain_action_request</a>.
It is a path in xenstore. By default no action is done but a reboot can be for example
initiated.</p><p>Previously we spoke about some points called <em>hooks</em> at which <code>xenopsd</code> can execute some script. There
is also a hook to run a post migrate script. After the execution of the script if there is one
the migration is almost done. The last step is a handskake to seal the success of the migration
and the old VM can now be cleaned.</p><h1 id=links>Links</h1><p>Some links are old but even if many changes occured they are relevant for a global understanding
of the XAPI toolstack.</p><ul><li><a href=https://xapi-project.github.io/xapi/architecture.html target=_blank>XAPI architecture</a></li><li><a href=https://wiki.xenproject.org/wiki/XAPI_Dispatch target=_blank>XAPI dispatcher</a></li><li><a href=https://xapi-project.github.io/xenopsd/architecture.html target=_blank>Xenopsd architecture</a></li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=walkthrough-starting-a-vm>Walkthrough: Starting a VM</h1><p>A Xenopsd client wishes to start a VM. They must first tell Xenopsd the VM
configuration to use. A VM configuration is broken down into objects:</p><ul><li>VM: A device-less Virtual Machine</li><li>VBD: A virtual block device for a VM</li><li>VIF: A virtual network interface for a VM</li><li>PCI: A virtual PCI device for a VM</li></ul><p>Treating devices as first-class objects is convenient because we wish to expose
operations on the devices such as hotplug, unplug, eject (for removable media),
carrier manipulation (for network interfaces) etc.</p><p>The &ldquo;add&rdquo; functions in the Xenopsd interface cause Xenopsd to create the
objects:</p><ul><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L420 target=_blank>VM.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L464 target=_blank>VBD.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L475 target=_blank>VIF.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L457 target=_blank>PCI.add</a></li></ul><p>In the case of <a href=https://github.com/xapi-project/xen-api target=_blank>xapi</a>, there are a set
of functions which
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L380 target=_blank>convert between the XenAPI objects and the Xenopsd objects</a>.
The two interfaces are slightly different because they have different expected
users:</p><ul><li>the XenAPI has many clients which are updated on long release cycles. The
main property needed is backwards compatibility, so that new release of xapi
remain compatible with these older clients. Quite often we will chose to
&ldquo;grandfather in&rdquo; some poorly designed interface simply because we wish to
avoid imposing churn on 3rd parties.</li><li>the Xenopsd API clients are all open-source and are part of the xapi-project.
These clients can be updated as the API is changed. The main property needed
is to keep the interface clean, so that it properly hides the complexity
of dealing with Xen from other components.</li></ul><p>The Xenopsd &ldquo;VM.add&rdquo; function has code like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>let</span> add&#39; x <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>		debug <span style=color:#e6db74>&#34;VM.add %s&#34;</span> <span style=color:#f92672>(</span>Jsonrpc.to_string <span style=color:#f92672>(</span>rpc_of_t x<span style=color:#f92672>));</span>
</span></span><span style=display:flex><span>		DB.write x<span style=color:#f92672>.</span>id x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>B</span> <span style=color:#f92672>=</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>val</span> get_backend () <span style=color:#f92672>:</span> <span style=color:#a6e22e>S</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>		B.VM.add x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		x<span style=color:#f92672>.</span>id</span></span></code></pre></div><p>This function does 2 things:</p><ul><li>it stores the VM configuration in the &ldquo;database&rdquo;</li><li>it tells the &ldquo;backend&rdquo; that the VM exists</li></ul><p>The Xenopsd database is really a set of config files in the filesystem. All
objects belonging to a VM (recall we only have VMs, VBDs, VIFs, PCIs and not
stand-alone entities like disks) and are placed into a subdirectory named after
the VM e.g.:</p><div class="wrap-code highlight"><pre tabindex=0><code># ls /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701
config	vbd.xvda  vbd.xvdb
# cat /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701/config
{&#34;id&#34;: &#34;7b719ce6-0b17-9733-e8ee-dbc1e6e7b701&#34;, &#34;name&#34;: &#34;fedora&#34;,
 ...
}</code></pre></div><p>Xenopsd doesn&rsquo;t have as persistent a notion of a VM as xapi, it is expected that
all objects are deleted when the host is rebooted. However the objects should
be persisted over a simple Xenopsd restart, which is why the objects are stored
in the filesystem.</p><p>Aside: it would probably be more appropriate to store the metadata in Xenstore
since this has the exact object lifetime we need. This will require a more
performant Xenstore to realise.</p><p>Every running Xenopsd process is linked with a single backend. Currently backends
exist for:</p><ul><li>Xen via libxc, libxenguest and xenstore</li><li>Xen via libxl, libxc and xenstore</li><li>Xen via libvirt</li><li>KVM by direct invocation of qemu</li><li>Simulation for testing</li></ul><p>From here we shall assume the use of the &ldquo;Xen via libxc, libxenguest and xenstore&rdquo; (a.k.a.
&ldquo;Xenopsd classic&rdquo;) backend.</p><p>The backend <a href=https://github.com/xapi-project/xenopsd/blob/2a476c132c0b5732f9b224316b851a1b4d57520b/xc/xenops_server_xen.ml#L719 target=_blank>VM.add</a>
function checks whether the VM we have to manage already exists &ndash; and if it does
then it ensures the Xenstore configuration is intact. This Xenstore configuration
is important because at any time a client can query the state of a VM with
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L438 target=_blank>VM.stat</a>
and this relies on certain Xenstore keys being present.</p><p>Once the VM metadata has been registered with Xenopsd, the client can call
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L443 target=_blank>VM.start</a>.
Like all potentially-blocking Xenopsd APIs, this function returns a Task id.
Please refer to the <a href=/new-docs/xenopsd/walkthroughs/VM.start/../design/Tasks.html>Task handling design</a> for a general
overview of how tasks are handled.</p><p>Clients can poll the state of a task by calling <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L404 target=_blank>TASK.stat</a>
but most clients will prefer to use the event system instead.
Please refer to the <a href=/new-docs/xenopsd/walkthroughs/VM.start/../design/Events.html>Event handling design</a> for a general
overview of how events are handled.</p><p>The event model is similar to the XenAPI: clients call a blocking
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L487 target=_blank>UPDATES.get</a>
passing in a token which represents the point in time when the last UPDATES.get
returned. The call blocks until some objects have changed state, and these object
ids are returned (NB in the XenAPI the current object states are returned)
The client must then call the relevant &ldquo;stat&rdquo; function, in this
case <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L404 target=_blank>TASK.stat</a></p><p>The client will be able to see the task make progress and use this to &ndash; for example &ndash;
populate a progress bar in a UI. If the client needs to cancel the task then it
can call the <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L405 target=_blank>TASK.cancel</a>;
again see the <a href=/new-docs/xenopsd/walkthroughs/VM.start/../design/Tasks.html>Task handling design</a> to understand how this is
implemented.</p><p>When the Task has completed successfully, then calls to *.stat will show:</p><ul><li>the power state is Paused</li><li>exactly one valid Xen domain id</li><li>all VBDs have active = plugged = true</li><li>all VIFs have active = plugged = true</li><li>all PCI devices have plugged = true</li><li>at least one active console</li><li>a valid start time</li><li>valid &ldquo;targets&rdquo; for memory and vCPU</li></ul><p>Note: before a Task completes, calls to *.stat will show partial updates e.g.
the power state may be Paused but none of the disks may have become plugged.
UI clients must choose whether they are happy displaying this in-between state
or whether they wish to hide it and pretend the whole operation has happened
transactionally. If a particular client wishes to perform side-effects in
response to Xenopsd state changes &ndash; for example to clean up an external resource
when a VIF becomes unplugged &ndash; then it must be very careful to avoid responding
to these in-between states. Generally it is safest to passively report these
values without driving things directly from them. Think of them as status lights
on the front panel of a PC: fine to look at but it&rsquo;s not a good idea to wire
them up to actuators which actually do things.</p><p>Note: the Xenopsd implementation guarantees that, if it is restarted at any point
during the start operation, on restart the VM state shall be &ldquo;fixed&rdquo; by either
(i) shutting down the VM; or (ii) ensuring the VM is intact and running.</p><p>In the case of <a href=https://github.com/xapi-project/xen-api target=_blank>xapi</a> every Xenopsd
Task id bound one-to-one with a XenAPI task by the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1831 target=_blank>sync_with_task</a>.
The function <a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1450 target=_blank>update_task</a>
is called when xapi receives a notification that a Xenopsd Task has changed state,
and updates the corresponding XenAPI task.
Xapi launches exactly one thread per Xenopsd instance (&ldquo;queue&rdquo;) to monitor for
background events via the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467 target=_blank>events_watch</a>
while each thread performing a XenAPI call waits for its specific Task to complete
via the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L30 target=_blank>event_wait</a>.</p><p>It is the responsibility of the client to call
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L406 target=_blank>TASK.destroy</a>
when the Task is nolonger needed. Xenopsd won&rsquo;t destroy the task because it contains
the success/failure result of the operation which is needed by the client.</p><p>What happens when a Xenopsd receives a VM.start request?</p><p>When Xenopsd receives the request it adds it to the appropriate per-VM queue
via the function
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1744 target=_blank>queue_operation</a>.
To understand this and other internal details of Xenopsd, consult the
<a href=/new-docs/xenopsd/walkthroughs/VM.start/../architecture.html>architecture description</a>.
The <a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1457 target=_blank>queue_operation_int</a>
function looks like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> queue_operation_int dbg id op <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>let</span> task <span style=color:#f92672>=</span> Xenops_task.add tasks dbg <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> t <span style=color:#f92672>-&gt;</span> perform op t<span style=color:#f92672>;</span> <span style=color:#a6e22e>None</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>	Redirector.push id <span style=color:#f92672>(</span>op<span style=color:#f92672>,</span> task<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>	task</span></span></code></pre></div><p>The &ldquo;task&rdquo; is a record containing Task metadata plus a &ldquo;do it now&rdquo; function
which will be executed by a thread from the thread pool. The
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L396 target=_blank>module Redirector</a>
takes care of:</p><ul><li>pushing operations to the right queue</li><li>ensuring at most one worker thread is working on a VM&rsquo;s operations</li><li>reducing the queue size by coalescing items together</li><li>providing a diagnostics interface</li></ul><p>Once a thread from the worker pool becomes free, it will execute the &ldquo;do it now&rdquo;
function. In the example above this is <code>perform op t</code> where <code>op</code> is
<code>VM_start vm</code> and <code>t</code> is the Task. The function
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1198 target=_blank>perform</a>
has fragments like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>VM_start</span> id <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>			debug <span style=color:#e6db74>&#34;VM.start %s&#34;</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>			perform_atomics <span style=color:#f92672>(</span>atomics_of_operation op<span style=color:#f92672>)</span> t<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>			VM_DB.signal id</span></span></code></pre></div><p>Each &ldquo;operation&rdquo; (e.g. <code>VM_start vm</code>) is decomposed into &ldquo;micro-ops&rdquo; by the
function
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L739 target=_blank>atomics_of_operation</a>
where the micro-ops are small building-block actions common to the higher-level
operations. Each operation corresponds to a list of &ldquo;micro-ops&rdquo;, where there is
no if/then/else. Some of the &ldquo;micro-ops&rdquo; may be a no-op depending on the VM
configuration (for example a PV domain may not need a qemu). In the case of
<code>VM_start vm</code> this decomposes into the sequence:</p><h2 id=1-run-the-vm_pre_start-scripts>1. run the &ldquo;VM_pre_start&rdquo; scripts</h2><p>The <code>VM_hook_script</code> micro-op runs the corresponding &ldquo;hook&rdquo; scripts. The
code is all in the
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/lib/xenops_hooks.ml target=_blank>Xenops_hooks</a>
module and looks for scripts in the hardcoded path <code>/etc/xapi.d</code>.</p><h2 id=2-create-a-xen-domain>2. create a Xen domain</h2><p>The <code>VM_create</code> micro-op calls the <code>VM.create</code> function in the backend.
In the classic Xenopsd backend the
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L633 target=_blank>VM.create_exn</a>
function must</p><ol><li>check if we&rsquo;re creating a domain for a fresh VM or resuming an existing one:
if it&rsquo;s a resume then the domain configuration stored in the VmExtra database
table must be used</li><li>ask <em>squeezed</em> to create a memory &ldquo;reservation&rdquo; big enough to hold the VM
memory. Unfortunately the domain cannot be created until the memory is free
because domain create often fails in low-memory conditions. This means the
&ldquo;reservation&rdquo; is associated with our &ldquo;session&rdquo; with squeezed; if Xenopsd
crashes and restarts the reservation will be freed automatically.</li><li>create the Domain via the libxc hypercall</li><li>&ldquo;transfer&rdquo; the squeezed reservation to the domain such that squeezed will
free the memory if the domain is destroyed later</li><li>compute and set an initial balloon target depending on the amount of memory
reserved (recall we ask for a range between <em>dynamic_min</em> and <em>dynamic_max</em>)</li><li>apply the &ldquo;suppress spurious page faults&rdquo; workaround if requested</li><li>set the &ldquo;machine address size&rdquo;</li><li>&ldquo;hotplug&rdquo; the vCPUs. This operates a lot like memory ballooning &ndash; Xen creates
lots of vCPUs and then the guest is asked to only use some of them. Every VM
therefore starts with the &ldquo;VCPUs_max&rdquo; setting and co-operative hotplug is
used to reduce the number. Note there is no enforcement mechanism: a VM which
cheats and uses too many vCPUs would have to be caught by looking at the
performance statistics.</li></ol><h2 id=3-build-the-domain>3. build the domain</h2><p>On a Xen system a domain is created empty, and memory is actually allocated
from the host in the &ldquo;build&rdquo; phase via functions in <em>libxenguest</em>. The
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L994 target=_blank>VM.build_domain_exn</a>
function must</p><ol><li>run pygrub (or eliloader) to extract the kernel and initrd, if necessary</li><li>invoke the <em>xenguest</em> binary to interact with libxenguest.</li><li>apply the <code>cpuid</code> configuration</li><li>store the current domain configuration on disk &ndash; it&rsquo;s important to know
the difference between the configuration you started with and the configuration
you would use after a reboot because some properties (such as maximum memory
and vCPUs) as fixed on create.</li></ol><p>The xenguest binary was originally
a separate binary for two reasons: (i) the libxenguest functions weren&rsquo;t
threadsafe since they used lots of global variables; and (ii) the libxenguest
functions used to have a different, incompatible license, which prevent us
linking. Both these problems have been resolved but we still shell out to
the xenguest binary.</p><p>The xenguest binary has also evolved to configure more of the initial domain
state. It also <a href=https://github.com/xapi-project/ocaml-xen-lowlevel-libs/blob/master/xenguest-4.4/xenguest_stubs.c#L42 target=_blank>reads Xenstore</a>
and configures</p><ul><li>the vCPU affinity</li><li>the vCPU credit2 weight/cap parameters</li><li>whether the NX bit is exposed</li><li>whether the viridian CPUID leaf is exposed</li><li>whether the system has PAE or not</li><li>whether the system has ACPI or not</li><li>whether the system has nested HVM or not</li><li>whether the system has an HPET or not</li></ul><h2 id=4-mark-each-vbd-as-active>4. mark each VBD as &ldquo;active&rdquo;</h2><p>VBDs and VIFs are said to be &ldquo;active&rdquo; when they are intended to be used by a
particular VM, even if the backend/frontend connection hasn&rsquo;t been established,
or has been closed. If someone calls <code>VBD.stat</code> or <code>VIF.stat</code> then
the result includes both &ldquo;active&rdquo; and &ldquo;plugged&rdquo;, where &ldquo;plugged&rdquo; is true if
the frontend/backend connection is established.
For example xapi will
set <a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1300 target=_blank>VBD.currently_attached</a>
to &ldquo;active || plugged&rdquo;. The &ldquo;active&rdquo; flag is conceptually very similar to the
traditional &ldquo;online&rdquo; flag (which is not documented in the upstream Xen tree
as of Oct/2014 but really should be) except that on unplug, one would set
the &ldquo;online&rdquo; key to &ldquo;0&rdquo; (false) <em>first</em> before initiating the hotunplug. By
contrast the &ldquo;active&rdquo; flag is set to false <em>after</em> the unplug i.e. &ldquo;set_active&rdquo;
calls bracket plug/unplug. If the &ldquo;active&rdquo; flag was set before the unplug
attempt then as soon as the frontend/backend connection is removed clients
would see the VBD as completely dissociated from the VM &ndash; this would be misleading
because Xenopsd will not have had time to use the storage API to release locks
on the disks. By doing all the cleanup before setting &ldquo;active&rdquo; to false, clients
can be assured that the disks are now free to be reassigned.</p><h2 id=5-handle-non-persistent-disks>5. handle non-persistent disks</h2><p>A non-persistent disk is one which is reset to a known-good state on every
VM start. The <code>VBD_epoch_begin</code> is the signal to perform any necessary reset.</p><h2 id=6-plug-vbds>6. plug VBDs</h2><p>The <code>VBD_plug</code> micro-op will plug the VBD into the VM. Every VBD is plugged
in a carefully-chosen order.
Generally, plug order is important for all types of devices. For VBDs, we must
work around the deficiency in the storage interface where a VDI, once attached
read/only, cannot be attached read/write. Since it is legal to attach the same
VDI with multiple VBDs, we must plug them in such that the read/write VBDs
come first. From the guest&rsquo;s point of view the order we plug them doesn&rsquo;t
matter because they are indexed by the Xenstore device id (e.g. 51712 = xvda).</p><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1631 target=_blank>VBD.plug</a>
will</p><ul><li>call <code>VDI.attach</code> and <code>VDI.activate</code> in the storage API to make the
devices ready (start the tapdisk processes etc)</li><li>add the Xenstore frontend/backend directories containing the block device
info</li><li>add the extra xenstore keys returned by the <code>VDI.attach</code> call that are
needed for SCSIid passthrough which is needed to support VSS</li><li>write the VBD information to the Xenopsd database so that future calls to
<em>VBD.stat</em> can be told about the associated disk (this is needed so clients
like xapi can cope with CD insert/eject etc)</li><li>if the qemu is going to be in a different domain to the storage, a frontend
device in the qemu domain is created.</li></ul><p>The Xenstore keys are written by the functions
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L486 target=_blank>Device.Vbd.add_async</a>
and
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L550 target=_blank>Device.Vbd.add_wait</a>.
In a Linux domain (such as dom0) when the backend directory is created, the kernel
creates a &ldquo;backend device&rdquo;. Creating any device will cause a kernel UEVENT to fire
which is picked up by udev. The udev rules run a script whose only job is to
stat(2) the device (from the &ldquo;params&rdquo; key in the backend) and write the major
and minor number to Xenstore for blkback to pick up. (Aside: FreeBSD doesn&rsquo;t do
any of this, instead the FreeBSD kernel module simply opens the device in the
&ldquo;params&rdquo; key). The script also writes the backend key &ldquo;hotplug-status=connected&rdquo;.
We currently wait for this key to be written so that later calls to <em>VBD.stat</em>
will return with &ldquo;plugged=true&rdquo;. If the call returns before this key is written
then sometimes we receive an event, call <em>VBD.stat</em> and conclude erroneously
that a spontaneous VBD unplug occurred.</p><h2 id=7-mark-each-vif-as-active>7. mark each VIF as &ldquo;active&rdquo;</h2><p>This is for the same reason as VBDs are marked &ldquo;active&rdquo;.</p><h2 id=8-plug-vifs>8. plug VIFs</h2><p>Again, the order matters. Unlike VBDs,
there is no read/write read/only constraint and the devices
have unique indices (0, 1, 2, &mldr;) <em>but</em> Linux kernels have often (always?)
ignored the actual index and instead relied on the order of results from the
<code>xenstore-ls</code> listing. The order that xenstored returns the items happens
to be the order the nodes were created so this means that (i) xenstored must
continue to store directories as ordered lists rather than maps (which would
be more efficient); and (ii) Xenopsd must make sure to plug the vifs in
the same order. Note that relying on ethX device numbering has always been a
bad idea but is still common. I bet if you change this lots of tests will
suddenly start to fail!</p><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1945 target=_blank>VIF.plug_exn</a>
will</p><ul><li>compute the port locking configuration required and write this to a well-known
location in the filesystem where it can be read from the udev scripts. This
really should be written to Xenstore instead, since this scheme doesn&rsquo;t work
with driver domains.</li><li>add the Xenstore frontend/backend directories containing the network device
info</li><li>write the VIF information to the Xenopsd database so that future calls to
<em>VIF.stat</em> can be told about the associated network</li><li>if the qemu is going to be in a different domain to the storage, a frontend
device in the qemu domain is created.</li></ul><p>Similarly to the VBD case, the function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L642 target=_blank>Device.Vif.add</a>
will write the Xenstore keys and wait for the &ldquo;hotplug-status=connected&rdquo; key.
We do this because we cannot apply the port locking rules until the backend
device has been created, and we cannot know the rules have been applied
until after the udev script has written the key. If we didn&rsquo;t wait for it then
the VM might execute without all the port locking properly configured.</p><h2 id=9-create-the-device-model>9. create the device model</h2><p>The <code>VM_create_device_model</code> micro-op will create a qemu device model if</p><ul><li>the VM is HVM; or</li><li>the VM uses a PV keyboard or mouse (since only qemu currently has backend
support for these devices).</li></ul><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1090 target=_blank>VM.create_device_model_exn</a>
will</p><ul><li>(if using a qemu stubdom) it will create and build the qemu domain</li><li>compute the necessary qemu arguments and launch it.</li></ul><p>Note that qemu (aka the &ldquo;device model&rdquo;) is created after the VIFs and VBDs have
been plugged but before the PCI devices have been plugged. Unfortunately qemu
traditional infers the needed emulated hardware by inspecting the Xenstore
VBD and VIF configuration and assuming that we want one emulated device per
PV device, up to the natural limits of the emulated buses (i.e. there can be
at most 4 IDE devices: {primary,secondary}{master,slave}). Not only does this
create an ordering dependency that needn&rsquo;t exist &ndash; and which impacts migration
downtime &ndash; but it also completely ignores the plain fact that, on a Xen system,
qemu can be in a different domain than the backend disk and network devices.
This hack only works because we currently run everything in the same domain.
There is an option (off by default) to list the emulated devices explicitly
on the qemu command-line. If we switch to this by default then we ought to be
able to start up qemu early, as soon as the domain has been created (qemu will
need to know the domain id so it can map the I/O request ring).</p><h2 id=10-plug-pci-devices>10. plug PCI devices</h2><p>PCI devices are treated differently to VBDs and VIFs.
If we are attaching the device to an
HVM guest then instead of relying on the traditional Xenstore frontend/backend
state machine we instead send RPCs to qemu requesting they be hotplugged. Note
the domain is paused at this point, but qemu still supports PCI hotplug/unplug.
The reasons why this doesn&rsquo;t follow the standard Xenstore model are known only
to the people who contributed this support to qemu.
Again the order matters because it determines the position of the virtual device
in the VM.</p><p>Note that Xenopsd doesn&rsquo;t know anything about the PCI devices; concepts such
as &ldquo;GPU groups&rdquo; belong to higher layers, such as xapi.</p><h2 id=11-mark-the-domain-as-alive>11. mark the domain as alive</h2><p>A design principle of Xenopsd is that it should tolerate failures such as being
suddenly restarted. It guarantees to always leave the system in a valid state,
in particular there should never be any &ldquo;half-created VMs&rdquo;. We achieve this for
VM start by exploiting the mechanism which is necessary for reboot. When a VM
wishes to reboot it causes the domain to exit (via SCHEDOP_shutdown) with a
&ldquo;reason code&rdquo; of &ldquo;reboot&rdquo;. When Xenopsd sees this event <code>VM_check_state</code>
operation is queued. This operation calls
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1443 target=_blank>VM.get_domain_action_request</a>
to ask the question, &ldquo;what needs to be done to make this VM happy now?&rdquo;. The
implementation checks the domain state for shutdown codes and also checks a
special Xenopsd Xenstore key. When Xenopsd creates a Xen domain it sets this
key to &ldquo;reboot&rdquo; (meaning &ldquo;please reboot me if you see me&rdquo;) and when Xenopsd
finishes starting the VM it clears this key. This means that if Xenopsd crashes
while starting a VM, the new Xenopsd will conclude that the VM needs to be rebooted
and will clean up the current domain and create a fresh one.</p><h2 id=12-unpause-the-domain>12. unpause the domain</h2><p>A Xenopsd VM.start will always leave the domain paused, so strictly speaking
this is a separate &ldquo;operation&rdquo; queued by the client (such as xapi) after the
VM.start has completed. The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L808 target=_blank>VM.unpause</a>
is reassuringly simple:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>		<span style=color:#66d9ef>if</span> di<span style=color:#f92672>.</span>Xenctrl.total_memory_pages <span style=color:#f92672>=</span> 0n <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Domain_not_built</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>		Domain.unpause <span style=color:#f92672>~</span>xc di<span style=color:#f92672>.</span>Xenctrl.domid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		Opt.iter
</span></span><span style=display:flex><span>			<span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> stubdom_domid <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>				Domain.unpause <span style=color:#f92672>~</span>xc stubdom_domid
</span></span><span style=display:flex><span>			<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>get_stubdom <span style=color:#f92672>~</span>xs di<span style=color:#f92672>.</span>Xenctrl.domid<span style=color:#f92672>)</span></span></span></code></pre></div><footer class=footline></footer></article></section></section><article class=default><header class=headline></header><h1 id=networkd>Networkd</h1><p>The <code>xcp-networkd</code> daemon (hereafter simply called &ldquo;networkd&rdquo;) is a component in the xapi toolstack that is responsible for configuring network interfaces and virtual switches (bridges) on a host.</p><p>The code is in <code>ocaml/networkd</code>.</p><h2 id=principles>Principles</h2><ol><li><p><strong>Distro-agnostic</strong>. Networkd is meant to work on at least CentOS/RHEL as well a Debian/Ubuntu based distros. It therefore should not use any network configuration features specific to those distros.</p></li><li><p><strong>Stateless</strong>. By default, networkd should not maintain any state. If you ask networkd anything about a network interface or bridge, or any other network sub-system property, it will always query the underlying system (e.g. an IP address), rather than returning any cached state. However, if you want networkd to configure networking at host boot time, the you can ask it to remember your configuration you have set for any interface or bridge you choose.</p></li><li><p><strong>Idempotent</strong>. It should be possible to call any networkd function multiple times without breaking things. For example, calling a function to set an IP address on an interface twice in a row should have the same outcome as calling it just once.</p></li><li><p><strong>Do no harm</strong>. Networkd should only configure what you ask it to configure. This means that it can co-exist with other network managers.</p></li></ol><h2 id=usage>Usage</h2><p>Networkd is a daemon that is typically started at host-boot time. In the same way as the other daemons in the xapi toolstack, it is controlled by RPC requests. It typically receives requests from the xapi daemon, on behalf of which it configures host networking.</p><p>Networkd&rsquo;s RCP API is fully described by the <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi-idl/network/network_interface.ml target=_blank>network_interface.ml</a> file. The API has two main namespaces: <code>Interface</code> and <code>Bridge</code>, which are implemented in two modules in <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/networkd/bin/network_server.ml target=_blank>network_server.ml</a>.</p><p>In line with other xapi daemons, all API functions take an argument of type <code>debug_info</code> (a string) as their first argument. The debug string appears in any log lines that are produced as a side effort of calling the function.</p><h2 id=network-interface-api>Network Interface API</h2><p>The Interface API has functions to query and configure properties of Linux network devices, such as IP addresses, and bringing them up or down. Most Interface functions take a <code>name</code> string as a reference to a network interface as their second argument, which is expected to be the name of the Linux network device. There is also a special function, called <code>Interface.make_config</code>, that is able to configure a number of interfaces at once. It takes an argument called <code>config</code> of type <code>(iface * interface_config_t) list</code>, where <code>iface</code> is an interface name, and <code>interface_config_t</code> is a compound type containing the full configuration for an interface (as far as networkd is able to configure them), currently defined as follows:</p><div class="wrap-code highlight"><pre tabindex=0><code>type interface_config_t = {
	ipv4_conf: ipv4;
	ipv4_gateway: Unix.inet_addr option;
	ipv6_conf: ipv6;
	ipv6_gateway: Unix.inet_addr option;
	ipv4_routes: (Unix.inet_addr * int * Unix.inet_addr) list;
	dns: Unix.inet_addr list * string list;
	mtu: int;
	ethtool_settings: (string * string) list;
	ethtool_offload: (string * string) list;
	persistent_i: bool;
}</code></pre></div><p>When the function returns, it should have completely configured the interface, and have brought it up. The idempotency principle applies to this function, which means that it can be used to successively modify interface properties; any property that has not changed will effectively be ignored. In fact, <code>Interface.make_config</code> is the main function that xapi uses to configure interfaces, e.g. as a result of a <code>PIF.plug</code> or a <code>PIF.reconfigure_ip</code> call.</p><p>Also note the <code>persistent</code> property in the interface config. When an interface is made &ldquo;persistent&rdquo;, this means that any configuration that is set on it is remembered by networkd, and the interface config is written to disk. When networkd is started, it will read the persistent config and call <code>Interface.make_config</code> on it in order to apply it (see Startup below).</p><p><em>The full networkd API should be documented separately somewhere on this site.</em></p><h2 id=bridge-api>Bridge API</h2><p>The Bridge API functions are all about the management of virtual switches, also known as &ldquo;bridges&rdquo;. The shape of the Bridge API roughly follows that of the Open vSwitch in that it treats a bridge as a collection of &ldquo;ports&rdquo;, where a port can contain one or more &ldquo;interfaces&rdquo;.</p><p>NIC bonding and VLANs are all configured on the Bridge level. There are functions for creating and destroying bridges, adding and removing ports, and configuring bonds and VLANs. Like interfaces, bridges and ports are addressed by name in the Bridge functions. Analogous to the Interface function with the same name, there is a <code>Bridge.make_config</code> function, and bridges can be made <code>persistent</code>.</p><div class="wrap-code highlight"><pre tabindex=0><code>type port_config_t = {
	interfaces: iface list;
	bond_properties: (string * string) list;
	bond_mac: string option;
}
type bridge_config_t = {
	ports: (port * port_config_t) list;
	vlan: (bridge * int) option;
	bridge_mac: string option;
	other_config: (string * string) list;
	persistent_b: bool;
}</code></pre></div><h2 id=backends>Backends</h2><p>Networkd currently has two different backends: the &ldquo;Linux bridge&rdquo; backend and the &ldquo;Open vSwitch&rdquo; backend. The former is the &ldquo;classic&rdquo; backend based on the bridge module that is available in the Linux kernel, plus additional standard Linux functionality for NIC bonding and VLANs. The latter backend is newer and uses the <a href=http://www.openvswitch.org target=_blank>Open vSwitch (OVS)</a> for bridging as well as other functionality. Which backend is currently in use is defined by the file <code>/etc/xensource/network.conf</code>, which is read by networkd when it starts. The choice of backend (currently) only affects the Bridge API: every function in it has a separate implementation for each backend.</p><h2 id=low-level-interfaces>Low-level Interfaces</h2><p>Networkd uses standard networking commands and interfaces that are available in most modern Linux distros, rather than relying on any distro-specific network tools (see the distro-agnostic principle). These are tools such as <code>ip</code> (iproute2), <code>dhclient</code> and <code>brctl</code>, as well as the <code>sysfs</code> files system, and <code>netlink</code> sockets. To control the OVS, the <code>ovs-*</code> command line tools are used. All low-level functions are called from <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/networkd/lib/network_utils.ml target=_blank>network_utils.ml</a>.</p><h2 id=configuration-on-startup>Configuration on Startup</h2><p>Networkd, periodically as well as on shutdown, writes the current configuration of all bridges and interfaces (see above) in a JSON format to a file called <code>networkd.db</code> (currently in <code>/var/lib/xcp</code>). The contents of the file are completely described by the following type:</p><div class="wrap-code highlight"><pre tabindex=0><code>type config_t = {
	interface_config: (iface * interface_config_t) list;
	bridge_config: (bridge * bridge_config_t) list;
	gateway_interface: iface option;
	dns_interface: iface option;
}</code></pre></div><p>The <code>gateway_interface</code> and <code>dns_interface</code> in the config are global host-level options to define from which interfaces the default gateway and DNS configuration is taken. This is especially important when multiple interfaces are configured by DHCP.</p><p>When networkd starts up, it first reads <code>network.conf</code> to determine the network backend. It subsequently attempts to parse <code>networkd.db</code>, and tries to call <code>Bridge.make_config</code> and <code>Interface.make_config</code> on it, with a special options to only apply the config for <code>persistent</code> bridges and interfaces, as well as bridges related to those (for example, if a VLAN bridge is configured, then also its parent bridge must be configured).</p><p>Networkd also supports upgrades from older versions of XenServer that used a network configuration script called <code>interface-configure</code>. If <code>networkd.db</code> is not found on startup, then networkd attempts to call this tool (via the <code>/etc/init.d/management-interface</code> script) in order to set up networking at boot time. This is normally followed immediately by a call from xapi instructing networkd to take over.</p><p>Finally, if no network config (old or new) is found on disk at all, networkd looks for a XenServer &ldquo;firstboot&rdquo; data file, which is written by XenServer&rsquo;s host installer, and tries to apply it to set up the management interface.</p><h2 id=monitoring>Monitoring</h2><p>Besides the ability to configure bridges and network interfaces, networkd has facilities for monitoring interfaces and bonds. When networkd starts, a monitor thread is started, which does several things (see <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/networkd/bin/network_monitor_thread.ml target=_blank>network_monitor_thread.ml</a>):</p><ul><li>Every 5 seconds, it gathers send/receive counters and link state of all network interfaces. It then writes these stats to a shared-memory file, to be picked up by other components such as <code>xcp-rrdd</code> and <code>xapi</code> (see documentation about &ldquo;xenostats&rdquo; elsewhere).</li><li>It monitors NIC bonds, and sends alerts through xapi in case of link state changes within a bond.</li><li>It uses <code>ip monitor address</code> to watch for an IP address changes, and if so, it calls xapi (<code>Host.signal_networking_change</code>) for it to update the IP addresses of the PIFs in its database that were configured by DHCP.</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=squeezed>Squeezed</h1><p>Squeezed is the XAPI Toolstack&rsquo;s host memory manager (aka balloon driver).
Squeezed uses ballooning to move memory between running VMs, to avoid wasting
host memory.</p><h2 id=principles>Principles</h2><ol><li>Avoid wasting host memory: unused memory should be put to use by returning
it to VMs.</li><li>Memory should be shared in proportion to the configured policy.</li><li>Operate entirely at the level of domains (not VMs), and be independent of
Xen toolstack.</li></ol><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Squeezed</h1><article class=default><header class=headline></header><h1 id=architecture>Architecture</h1><p>Squeezed is responsible for managing the memory on a single host. Squeezed
&ldquo;balances&rdquo; memory between VMs according to a policy written to Xenstore.</p><p>The following diagram shows the internals of Squeezed:</p><p><a href=#image-263bc191ccf46ec849c396c214ce44a0 class=lightbox-link><img src=/new-docs/squeezed/architecture/squeezed.png alt="Internals of squeezed" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-263bc191ccf46ec849c396c214ce44a0><img src=/new-docs/squeezed/architecture/squeezed.png alt="Internals of squeezed" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>At the center of squeezed is an abstract model of a Xen host. The model
includes:</p><ul><li>The amount of already-used host memory (used by fixed overheads such as Xen
and the crash kernel).</li><li>Per-domain memory policy specifically <code>dynamic-min</code> and <code>dynamic-max</code> which
together describe a range, within which the domain&rsquo;s actual used memory
should remain.</li><li>Per-domain calibration data which allows us to compute the necessary balloon
target value to achive a particular memory usage value.</li></ul><p>Squeezed is a single-threaded program which receives commands from xenopsd over
a Unix domain socket. When Xenopsd wishes to start a new VM, squeezed will be
asked to create a &ldquo;reservation&rdquo;. Note this is different to the Xen notion of a
reservation. A squeezed reservation consists of an amount of memory squeezed
will guarantee to keep free labelled with an id. When Xenopsd later creates the
domain to notionally use the reservation, the reservation is &ldquo;transferred&rdquo; to
the domain before the domain is built.</p><p>Squeezed will also wake up every 30s and attempt to rebalance the memory on a
host. This is useful to correct imbalances caused by balloon drivers
temporarily failing to reach their targets. Note that ballooning is
fundamentally a co-operative process, so squeezed must handle cases where the
domains refuse to obey commands.</p><p>The &ldquo;output&rdquo; of squeezed is a list of &ldquo;actions&rdquo; which include:</p><ul><li>Set domain x&rsquo;s <code>memory/target</code> to a new value.</li><li>Set the <code>maxmem</code> of a domain to a new value (as a hard limit beyond which the
domain cannot allocate).</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=design>Design</h1><p>Squeezed is a single host memory ballooning daemon. It helps by:</p><ol><li>Allowing VM memory to be adjusted dynamically without having to reboot;
and</li><li>Avoiding wasting memory by keeping everything fully utilised, while retaining
the ability to take memory back to start new VMs.</li></ol><p>Squeezed currently includes a simple <a href=#ballooning-policy>Ballooning policy</a>
which serves as a useful default. The policy is written with respect to an
abstract <a href=#the-memory-model>Xen memory model</a>, which is based on a number of
<a href=#environmental-assumptions>assumptions about the environment</a>, for example
that most domains have co-operative balloon drivers. In theory the policy could
be replaced later with something more sophisticated (for example see
[xenballoond](<a href=https://github.com/avsm/xen-unstable/blob/master/tools/xenballoon/ target=_blank>https://github.com/avsm/xen-unstable/blob/master/tools/xenballoon/</a>
xenballoond.README)).</p><p>The <a href=#toolstack-interface>Toolstack interface</a> is used by Xenopsd to free
memory for starting new VMs. Although the only known client is Xenopsd, the
interface can in theory be used by other clients. Multiple clients can safely
use the interface at the same time.</p><p>The <a href=#the-structure-of-the-daemon>internal structure</a> consists of a
single-thread event loop. To see how it works end-to-end, consult the
<a href=#example-operation>example</a>.</p><p>No software is ever perfect; to understand the flaws in Squeezed, please
consult the <a href=#issues>list of issues</a>.</p><h1 id=environmental-assumptions>Environmental assumptions</h1><ol><li><p>The Squeezed daemon runs within a Xen domain 0 and
communicates to xenstored via a Unix domain socket. Therefore
Squeezed
is granted full access to xenstore, enabling it to modify every
domain’s <code>memory/target</code>.</p></li><li><p>The Squeezed daemon calls <code>setmaxmem</code> in order to cap the amount of memory
a domain can use. This relies on a patch to xen which allows <code>maxmem</code> to
be set lower than <code>totpages</code> See Section <a href=#use-of-maxmem>maxmem</a> for more
information.</p></li><li><p>The Squeezed daemon
assumes that only domains which write <code>control/feature-balloon</code> into
xenstore can respond to ballooning requests. It will not ask any
other domains to balloon.</p></li><li><p>The Squeezed daemon
assumes that the memory used by a domain is: (i) that listed in
<code>domain_getinfo</code> as <code>totpages</code>; (ii) shadow as given by
<code>shadow_allocation_get</code>; and (iii) a small (few KiB) of
miscellaneous Xen structures
(e.g. for domains, vcpus) which are invisible.</p></li><li><p>The Squeezed daemon
assumes that a domain which is created with a particular
<code>memory/target</code> (and <code>startmem</code>, to within rounding error) will
reach a stable value of <code>totpages</code> before writing
<code>control/feature-balloon</code>. The daemon writes this value to
<code>memory/memory-offset</code> for future reference.</p><ul><li>The Squeezed daemon
does not know or care exactly what causes the difference between
<code>totpages</code> and <code>memory/target</code> and it does <em>not</em>
expect it to remain constant across Xen releases. It
only expects the value to remain constant over the lifetime of a
domain.</li></ul></li><li><p>The Squeezed daemon
assumes that the balloon driver has hit its target when difference
between <code>memory/target</code> and <code>totpages</code> equals the <code>memory-offset</code>
value.</p><ul><li>Corrollary: to make a domain with a responsive balloon driver
currenty using <code>totpages</code> allocate or free
<em>x</em>
it suffices to
set <code>memory/target</code> to
<em>x+<code>totpages</code>-<code>memoryoffset</code></em>
and wait for the
balloon driver to finish. See Section <a href=#the-memory-model>memory model</a> for more
detail.</li></ul></li><li><p>The Squeezed daemon must
maintain a “slush fund” of memory (currently 9MiB) which it must
prevent any domain from allocating. Since (i) some Xen operations (such
as domain creation) require memory within a physical address range
(e.g. less than 4GiB) and (ii) since Xen preferentially
allocates memory outside these ranges, it follows that by preventing
guests from allocating <em>all</em> host memory (even
transiently) we guarantee that memory from within these special
ranges is always available. Squeezed operates in
<a href=#two-phase-target-setting>two phases</a>: first causing memory to be freed; and
second causing memory to be allocated.</p></li><li><p>The Squeezed daemon
assumes that it may set <code>memory/target</code> to any value within range:
<code>memory/dynamic-max</code> to <code>memory/dynamic-min</code></p></li><li><p>The Squeezed daemon
assumes that the probability of a domain booting successfully may be
increased by setting <code>memory/target</code> closer to <code>memory/static-max</code>.</p></li><li><p>The Squeezed daemon
assumes that, if a balloon driver has not made any visible progress
after 5 seconds, it is effectively <em>inactive</em>. Active
domains will be expected to pick up the slack.</p></li></ol><h1 id=toolstack-interface>Toolstack interface</h1><p>The toolstack interface introduces the concept of a <em>reservation</em>.
A <em>reservation</em> is: an amount of host free memory tagged
with an associated <em>reservation id</em>. Note this is an
internal Squeezed concept and Xen is
completely unaware of it. When the daemon is moving memory between
domains, it always aims to keep</p><p><a href=#image-9794c6c7483983163e888b470b498944 class=lightbox-link><img src=/new-docs/squeezed/design/hostfreemem.svg alt="host free memory &amp;gt;= s + sum_i(reservation_i)" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-9794c6c7483983163e888b470b498944><img src=/new-docs/squeezed/design/hostfreemem.svg alt="host free memory &amp;gt;= s + sum_i(reservation_i)" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>where <em>s</em> is the size of the “slush fund” (currently 9MiB) and
<a href=#image-7fac60a91654cc42d3d85433cdc156e5 class=lightbox-link><img src=/new-docs/squeezed/design/reservation.svg alt=reservation_t class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-7fac60a91654cc42d3d85433cdc156e5><img src=/new-docs/squeezed/design/reservation.svg alt=reservation_t class="lightbox-image noborder lightbox noshadow" loading=lazy></a>
is the amount corresponding to the <em>i</em>th
reservation.</p><p>As an aside: Earlier versions of Squeezed always
associated memory with a Xen domain. Unfortunately
this required domains to be created before memory was freed which was
problematic because domain creation requires small amounts of contiguous
frames. Rather than implement some form of memory defragmentation,
Squeezed and Xenopsd were
modified to free memory before creating a domain. This necessitated
making memory <em>reservations</em> first-class stand-alone
entities.</p><p>Once a <em>reservation</em> is made (and the corresponding memory
is freed), it can be <em>transferred</em> to a domain created by a
toolstack. This associates the <em>reservation</em> with that
domain so that, if the domain is destroyed, the
<em>reservation</em> is also freed. Note that Squeezed is careful not
to count both a domain’s <em>reservation</em> and its <code>totpages</code>
during e.g. domain building: instead it considers the domain’s
allocation to be the maximum of <em>reservation</em> and
<code>totpages</code>.</p><p>The size of a <em>reservation</em> may either be specified exactly
by the caller or the caller may provide a memory range. If a range is
provided the daemon will allocate at least as much as the minimum value
provided and as much as possible up to the maximum. By allocating as
much memory as possible to the domain, the probability of a successful
boot is increased.</p><p>Clients of the Squeezed provide a string
name when they log in. All untransferred reservations made by a client
are automatically deleted when a client logs in. This prevents memory
leaks where a client crashes and loses track of its own reservation ids.</p><p>The interface looks like this:</p><pre><code>string session_id login(
  string client_name
)

string reservation_id reserve_memory(
  string client_name,
  int kib
)

int amount, string reservation_id reserve_memory_range(
  string client_name,
  int min,
  int max
)

void delete_reservation(
  string client_name,
  string reservation_id
)

void transfer_reservation_to_domain(
  string client_name,
  string reservation_id,
  int domid
)
</code></pre><p><a href=https://github.com/xapi-project/xenopsd/blob/bf4f8d13ded299b56e55a4b36221ada3dfa0b2b1/xc/xenops_server_xen.ml#L353 target=_blank>The Xenopsd code</a> in pseudocode works as follows:</p><pre><code> r_id = reserve_memory_range(&quot;xenopsd&quot;, min, max);
 try:
    d = domain_create()
    transfer_reservation_to_domain(&quot;xenopsd&quot;, r_id, d)
 with:
    delete_reservation(&quot;xenopsd&quot;, r_id)
</code></pre><p>The interface is currently implemented using a trivial RPC protocol
over a Unix domain socket in domain 0.</p><h1 id=ballooning-policy>Ballooning policy</h1><p>This section describes the very simple default policy currently
built-into Squeezed.</p><p>Every domain has a pair of values written into xenstore:
<code>memory/dynamic-min</code> and <code>memory/dynamic-max</code> with the following
meanings:</p><ul><li><code>memory/dynamic-min</code> the lowest value that Squeezed is allowed
to set <code>memory/target</code>. The administrator should make this as low as
possible but high enough to ensure that the applications inside the
domain actually work.</li><li><code>memory/dynamic-max</code>
the highest value that Squeezed is allowed
to set <code>memory/target</code>. This can be used to dynamically cap the
amount of memory a domain can use.</li></ul><p>If all balloon drivers are responsive then Squeezed daemon allocates
memory proportionally, so that each domain has the same value of:
<a href=#image-2cd2ad17012dbcca9b15822717e69755 class=lightbox-link><img src=/new-docs/squeezed/design/fraction.svg alt=target-min/(max-min) class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-2cd2ad17012dbcca9b15822717e69755><img src=/new-docs/squeezed/design/fraction.svg alt=target-min/(max-min) class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>So:</p><ul><li><p>if memory is plentiful then all domains will have
<code>memory/target</code>=<code>memory/dynamic-max</code></p></li><li><p>if memory is scarce then all domains will have
<code>memory/target</code>=<code>memory/dynamic-min</code></p></li></ul><p>Note that the values of <code>memory/target</code> suggested by the policy are
ideal values. In many real-life situations (e.g. when a balloon driver
fails to make progress and is declared <em>inactive</em>) the
<code>memory/target</code> values will be different.</p><p>Note that, by default, domain 0 has
<code>memory/dynamic-min</code>=<code>memory/dynamic-max</code>, effectively disabling
ballooning. Clearly a more sophisticated policy would be required here
since ballooning down domain 0 as extra domains are started would be
counterproductive while backends and control interfaces remain in
domain 0.</p><h1 id=the-memory-model>The memory model</h1><p>Squeezed
considers a ballooning-aware domain (i.e. one which has written
the <code>feature-balloon</code> flag into xenstore) to be completely described by
the parameters:</p><ul><li><p><code>dynamic-min</code>: policy value written to <code>memory/dynamic-min</code> in xenstore by a
toolstack (see Section <a href=#ballooning-policy>Ballooning policy</a>)</p></li><li><p><code>dynamic-max</code>: policy value written to <code>memory/dynamic-max</code> in xenstore by a
toolstack (see Section <a href=#ballooning-policy>Ballooning policy</a>)</p></li><li><p><code>target</code>: balloon driver target written to <code>memory/target</code> in xenstore by
Squeezed.</p></li><li><p><code>totpages</code>: instantaneous number of pages used by the domain as returned by
the hypercall <code>domain_getinfo</code></p></li><li><p><code>memory-offset</code>: constant difference between <code>target</code> and <code>totpages</code> when the
balloon driver believes no ballooning is necessary: where
<code>memory-offset</code> = <code>totpages</code> - <code>target</code> when the balloon driver believes it
has reached its target.</p></li><li><p><code>maxmem</code>: upper limit on <code>totpages</code>: where <code>totpages</code> &lt;= <code>maxmem</code></p></li></ul><p>For convenience we define a <code>adjusted-target</code> to be the <em>target</em> value necessary
to cause a domain currently using <code>totpages</code> to maintain this value
indefinitely so <code>adjusted-target</code> = <code>totpages</code> - <code>memory-offset</code>.</p><p>The Squeezed
daemon believes that:</p><ul><li><p>a domain should be ballooning iff
<code>adjusted-target</code> &lt;> <code>target</code> (unless it has become <em>inactive</em>)</p></li><li><p>a domain has hit its target iff
<code>adjusted-target</code> = <code>target</code> (to within 1 page);</p></li><li><p>if a domain has
<code>target</code> = <code>x</code> then, when ballooning
is complete, it will have
<code>totpages</code> = <code>memory-offset</code> + <code>x</code>; and therefore</p></li><li><p>to cause a domain to free <code>y</code> it sufficies to set
<code>target</code> := <code>totpages</code> - <code>memory-offset</code> - <code>y</code>.</p></li></ul><p>The Squeezed
daemon considers non-ballooning aware domains (i.e. those which have not
written <code>feature-balloon</code>) to be represented by pairs of:</p><ul><li><p><code>totpages</code>: instantaneous number of pages used by the domain as returned by
<code>domain_getinfo</code></p></li><li><p><code>reservation</code>: memory initially freed for this domain by Squeezed after a
<code>transfer_reservation_to_domid</code> call</p></li></ul><p>Note that non-ballooning aware domains will always have
<code>startmem</code> = <code>target</code>
since the domain will not be
instructed to balloon. Since a domain which is being built will have
0 &lt;= <code>totpages</code> &lt;= <code>reservation</code>, Squeezed computes
<a href=#image-f87d7e965dc6293ac9b9211942eb88c9 class=lightbox-link><img src=/new-docs/squeezed/design/unused.svg alt="unused(i)=reservation(i)-totpages" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-f87d7e965dc6293ac9b9211942eb88c9><img src=/new-docs/squeezed/design/unused.svg alt="unused(i)=reservation(i)-totpages" class="lightbox-image noborder lightbox noshadow" loading=lazy></a>
and subtracts this from its model of the host’s free memory, ensuring
that it doesn’t accidentally reallocate this memory for some other
purpose.</p><p>The Squeezed
daemon believes that:</p><ul><li><p>all guest domains start out as non-ballooning aware domains where
<code>target</code>=<code>reservation</code>=<code>startmem</code>$;</p></li><li><p>some guest domains become ballooning-aware during their boot
sequence i.e. when they write <code>feature-balloon</code></p></li></ul><p>The Squeezed
daemon considers a host to be represented by:</p><ul><li><p>ballooning domains: a set of domains which Squeezed will instruct
to balloon;</p></li><li><p>other domains: a set of booting domains and domains which have no
balloon drivers (or whose balloon drivers have failed)</p></li><li><p>a &ldquo;slush fund&rdquo; of low memory required for Xen</p></li><li><p><code>physinfo.free_pages</code> total amount of memory instantanously free
(including both <code>free_pages</code> and <code>scrub_pages</code>)</p></li><li><p>reservations: batches of free memory which are not (yet) associated
with any domain</p></li></ul><p>The Squeezed
daemon considers memory to be unused (i.e. not allocated for any useful
purpose) if it is neither in use by a domain nor reserved.</p><h1 id=the-main-loop>The main loop</h1><p>The main loop is triggered by either:</p><ol><li><p>the arrival of an allocation request on the toolstack interface; or</p></li><li><p>the policy engine – polled every 10s – deciding that a target
<a href=https://github.com/xapi-project/squeezed/blob/7a5601d1543bd27e1e390a0a4f0a50aa531760e6/src/memory_server.ml#L60 target=_blank>adjustment is needed</a>.</p></li></ol><p>Each iteration of the main loop generates the following actions:</p><ol><li><p>Domains which were active but have failed to make progress towards
their target in 5s are declared <em>inactive</em>. These
domains then have:
<code>maxmem</code> set to the minimum of <code>target</code> and <code>totpages</code>.</p></li><li><p>Domains which were inactive but have started to make progress
towards their target are declared <em>active</em>. These
domains then have: <code>maxmem</code> set to <code>target</code>.</p></li><li><p>Domains which are currently active have new targets computed
according to the policy (see Section <a href=#ballooning-policy>Ballooning policy</a>). Note that
inactive domains are ignored and not expected to balloon.</p></li></ol><p>Note that domains remain classified as <em>inactive</em> only
during one run of the main loop. Once the loop has terminated all
domains are optimistically assumed to be <em>active</em> again.
Therefore should a domain be classified as <em>inactive</em> once,
it will get many later chances to respond.</p><p>The targets are set in <a href=#two-phase-target-setting>two phases</a>.
The <a href=#use-of-maxmem>maxmem</a> is used to prevent domains suddenly allocating
more memory than we want them to.</p><p>The main loop has a notion of a host free memory “target”, similar to
the existing domain memory <code>target</code>. When we are trying to free memory
(e.g. for starting a new VM), the host free memory “target” is
increased. When we are trying to distribute memory among guests
(e.g. after a domain has shutdown and freed lots of memory), the host
free memory “target” is low. Note the host free memory “target” is
always at least several MiB to ensure that some host free memory with
physical address less than 4GiB is free (see <a href=#two-phase-target-setting>Two phase target setting</a> for
related information).</p><p>The main loop terminates when all <em>active</em> domains have
reached their targets (this could be because all domains responded or
because they all wedged and became inactive); and the policy function
hasn’t suggested any new target changes. There are three possible
results:</p><ol><li><p>Success if the host free memory is near enough its “target”;</p></li><li><p>Failure if the operation is simply impossible within the policy
limits (i.e. <code>dynamic_min</code> values are too high;</p></li><li><p>Failure if the operation failed because one or more domains became
<em>inactive</em> and this prevented us from reaching our host
free memory “target”.</p></li></ol><p>Note that, since only <em>active</em> domains have their targets
set, the system effectively rewards domains which refuse to free memory
(<em>inactive</em>) and punishes those which do free memory
(<em>active</em>). This effect is countered by signalling to the
admin which domains/VMs aren’t responding so they can take corrective
action. To achieve this, the daemon monitors the list of
<em>inactive</em> domains and if a domain is
<em>inactive</em> for more than 20s it writes a flag into xenstore
<code>memory/uncooperative</code>. This key can be monitored and used to generate
an alert, if desired.</p><h2 id=two-phase-target-setting>Two phase target setting</h2><p>The following diagram shows how a system with two domains can evolve if domain
<code>memory/target</code> values are increased for some domains and decreased for
others, at the same time. Each graph shows two domains (domain 1 and
domain 2) and a host. For a domain, the square box shows its
<code>adjusted-totpages</code> and the arrow indicates the direction of the
<code>memory/target</code>. For the host the square box indicates total free
memory. Note the highlighted state where the host’s free memory is
temporarily exhausted</p><p><a href=#image-b13f774969bacd4247583dd8cd200888 class=lightbox-link><img src=/new-docs/squeezed/design/twophase.svg alt="Two phase target setting" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-b13f774969bacd4247583dd8cd200888><img src=/new-docs/squeezed/design/twophase.svg alt="Two phase target setting" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>In the
initial state (at the top of the diagram), there are two domains, one
which has been requested to use more memory and the other requested to
use less memory. In effect the memory is to be transferred from one
domain to the other. In the final state (at the bottom of the diagram),
both domains have reached their respective targets, the memory has been
transferred and the host free memory is at the same value it was
initially. However the system will not move atomically from the initial
state to the final: there are a number of possible transient in-between
states, two of which have been drawn in the middle of the diagram. In
the left-most transient state the domain which was asked to
<em>free</em> memory has freed all the memory requested: this is
reflected in the large amount of host memory free. In the right-most
transient state the domain which was asked to <em>allocate</em>
memory has allocated all the memory requested: now the host’s free
memory has hit zero.</p><p>If the host’s free memory hits zero then Xen has been forced to
give all memory to guests, including memory less than 4GiB which is critical
for allocating certain structures. Even if we ask a domain to free
memory via the balloon driver there is no guarantee that it will free
the <em>useful</em> memory. This leads to an annoying failure mode
where operations such as creating a domain free due to <code>ENOMEM</code> despite
the fact that there is apparently lots of memory free.</p><p>The solution to this problem is to adopt a two-phase <code>memory/target</code>
setting policy. The Squeezed daemon forces
domains to free memory first before allowing domains to allocate,
in-effect forcing the system to move through the left-most state in the
diagram above.</p><h2 id=use-of-maxmem>Use of maxmem</h2><p>The Xen
domain <code>maxmem</code> value is used to limit memory allocations by the domain.
The rules are:</p><ol><li><p>if the domain has never been run and is paused then
<code>maxmem</code> is set to <code>reservation</code> (reservations were described
in the <a href=#toolstack-interface>Toolstack interface</a> section above);</p><ul><li><p>these domains are probably still being built and we must let
them allocate their <code>startmem</code></p></li><li><p><strong>FIXME</strong>: this &ldquo;never been run&rdquo; concept pre-dates the
<code>feature-balloon</code> flag: perhaps we should use the
<code>feature-balloon</code> flag instead.</p></li></ul></li><li><p>if the domain is running and the balloon driver is thought to be
working then <code>maxmem</code> is set to <code>target</code>; and</p><ul><li>there may be a delay between lowering a target and the domain
noticing so we prevent the domain from allocating memory when it
should in fact be deallocating.</li></ul></li><li><p>if the domain is running and the balloon driver is thought to be
inactive then
<code>maxmem</code> is set to the minimum of <code>target</code> and <code>actual</code>.</p><ul><li><p>if the domain is using more memory than it should then we allow
it to make progress down towards its target; however</p></li><li><p>if the domain is using less memory than it should then we must
prevent it from suddenly waking up and allocating more since we
have probably just given it to someone else</p></li><li><p><strong>FIXME</strong>: should we reduce the target to leave the domain in a
neutral state instead of asking it to allocate and fail forever?</p></li></ul></li></ol><h1 id=example-operation>Example operation</h1><p>The diagram shows an initial system state comprising 3 domains on a
single host. The state is not ideal; the domains each have the same
policy settings (<code>dynamic-min</code> and <code>dynamic-max</code>) and yet are using
differing values of <code>adjusted-totpages</code>. In addition the host has more
memory free than desired. The second diagram shows the result of
computing ideal target values and the third diagram shows the result
after targets have been set and the balloon drivers have
responded.</p><p><a href=#image-22d9da892808ed4ef6e79d19a25bbc6c class=lightbox-link><img src=/new-docs/squeezed/design/calculation.svg alt=calculation class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-22d9da892808ed4ef6e79d19a25bbc6c><img src=/new-docs/squeezed/design/calculation.svg alt=calculation class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The scenario above includes 3 domains (domain 1,
domain 2, domain 3) on a host. Each of the domains has a non-ideal
<code>adjusted-totpages</code> value.</p><p>Recall we also have the policy constraint that:
<code>dynamic-min</code> &lt;= <code>target</code> &lt;= <code>dynamic-max</code>
Hypothetically if we reduce <code>target</code> by
<code>target</code>-<code>dynamic-min</code> (i.e. by setting
<code>target</code> to <code>dynamic-min</code>) then we should reduce
<code>totpages</code> by the same amount, freeing this much memory on the host. In
the upper-most graph in the diagram above, the total amount of memory
which would be freed if we set each of the 3 domain’s
<code>target</code> to <code>dynamic-min</code> is:
<code>d1</code> + <code>d2</code> + <code>d3</code>. In this hypothetical
situation we would now have
<code>x</code> + <code>s</code> + <code>d1</code> + <code>d2</code> + <code>d3</code> free on the host where
<code>s</code> is the host slush fund and <code>x</code> is completely unallocated. Since we
always want to keep the host free memory above <code>s</code>, we are free to
return <code>x</code> + <code>d1</code> + <code>d2</code> + <code>d3</code> to guests. If we
use the default built-in proportional policy then, since all domains
have the same <code>dynamic-min</code> and <code>dynamic-max</code>, each gets the same
fraction of this free memory which we call <code>g</code>:
<a href=#image-a1b3c2a6b69c31b87b6a6d1d84ef0b29 class=lightbox-link><img src=/new-docs/squeezed/design/g.svg alt="definition of g" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-a1b3c2a6b69c31b87b6a6d1d84ef0b29><img src=/new-docs/squeezed/design/g.svg alt="definition of g" class="lightbox-image noborder lightbox noshadow" loading=lazy></a>
For each domain, the ideal balloon target is now
<code>target</code> = <code>dynamic-min</code> + <code>g</code>.
Squeezed does not set all the targets at once: this would allow the
allocating domains to race with the deallocating domains, potentially allowing
all low memory to be allocated. Therefore Squeezed sets the
targets in <a href=#two-phase-target-setting>two phases</a>.</p><h1 id=the-structure-of-the-daemon>The structure of the daemon</h1><p>Squeezed is a single-threaded daemon which is started by an <code>init.d</code>
script. It sits waiting for incoming requests on its toolstack interface
and checks every 10s whether all domain targets are set to the ideal
values
(recall the <a href=#ballooning-policy>Ballooning policy</a>). If an allocation request
arrives or if the domain targets require adjusting then it calls into
the module
<a href=https://github.com/xapi-project/squeezed/blob/master/src/squeeze_xen.ml target=_blank>squeeze_xen.ml</a>.</p><p>The module
<a href=https://github.com/xapi-project/squeezed/blob/master/src/squeeze_xen.ml target=_blank>src/squeeze_xen.ml</a>
contains code which inspects the state of the host (through hypercalls
and reading xenstore) and creates a set of records describing the
current state of the host and all the domains. Note this snapshot of
state is not atomic – it is pieced together from multiple hypercalls and
xenstore reads – we assume that the errors generated are small and we
ignore them. These records are passed into the
<a href=https://github.com/xapi-project/squeezed/blob/master/lib/squeeze.ml target=_blank>squeeze.ml</a>
module where they
are processed and converted into a list of <em>actions</em> i.e.
(i) updates to <code>memory/target</code> and; (ii) declarations that particular
domains have become <em>inactive</em> or <em>active</em>.
The rationale for separating the Xen interface from the
main ballooning logic was to make testing easier: the module
<a href=https://github.com/xapi-project/squeezed/blob/master/test/squeeze_test.ml target=_blank>test/squeeze_test.ml</a>
contains a simple simulator which allows various edge-cases to be
checked.</p><h1 id=issues>Issues</h1><ul><li><p>If a linux domU kernel has the netback, blkback or blktap modules
then they away pages via <code>alloc_empty_pages_and_pagevec()</code> during
boot. This interacts with the balloon driver to break the assumption
that, reducing the target by <code>x</code> from a neutral value should free
<code>x</code> amount of memory.</p></li><li><p>Polling the state of the host (particular the xenstore contents) is
a bit inefficient. Perhaps we should move the policy values
<code>dynamic_min</code> and <code>dynamic_max</code> to a separate place in the xenstore
tree and use watches instead.</p></li><li><p>The memory values given to the domain builder are in units of MiB.
We may wish to similarly quantise the <code>target</code> value or check that
the <code>memory-offset</code> calculation still works.</p></li><li><p>The Xen
patch queue reintroduces the lowmem emergency pool. This was an
attempt to prevent guests from allocating lowmem before we switched
to a two-phase target setting procedure. This patch can probably be
removed.</p></li><li><p>It seems unnecessarily evil to modify an <em>inactive</em>
domain’s <code>maxmem</code> leaving <code>maxmem</code> less than <code>target</code>, causing
the guest to attempt allocations forwever. It’s probably neater to
move the <code>target</code> at the same time.</p></li><li><p>Declaring a domain <em>active</em> just because it makes small
amounts of progress shouldn’t be enough. Otherwise a domain could
free 1 byte (or maybe 1 page) every 5s.</p></li><li><p>Likewise, declaring a domain “uncooperative” only if it has been
<em>inactive</em> for 20s means that a domain could alternate
between <em>inactive</em> for 19s and <em>active</em>
for 1s and not be declared “uncooperative”.</p></li></ul><h2 id=document-history>Document history</h2><table><thead><tr><th>Version</th><th>Date</th><th>Change</th></tr></thead><tbody><tr><td>0.2</td><td>10th Nov 2014</td><td>Update to markdown</td></tr><tr><td>0.1</td><td>9th Nov 2009</td><td>Initial version</td></tr></tbody></table><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=xapi-guard>Xapi-guard</h1><p>The <code>xapi-guard</code> daemon is the component in the xapi toolstack that is responsible for handling persistence requests from VMs (domains).
Currently these are UEFI vars and vTPM updates.</p><p>The code is in <code>ocaml/xapi-guard</code>.
When the daemon managed only with UEFI updates it was called <code>varstored-guard</code>.
Some files and package names still use the previous name.</p><h2 id=principles>Principles</h2><ol><li>Calls from domains must be limited in privilege to do certain API calls, and
to read and write from their corresponding VM in xapi&rsquo;s database only.</li><li>Xenopsd is able to control xapi-guard through message switch, this access is
not limited.</li><li>Listening to domain socket is restored whenever the daemon restarts to minimize disruption of running domains.</li><li>Disruptions to requests when xapi is unavailable is minimized.
The startup procedure is not blocked by the availability of xapi, and write requests from domains must not fail because xapi is unavailable.</li></ol><h2 id=overview>Overview</h2><p>Xapi-guard forwards calls from domains to xapi to persist UEFI variables, and update vTPMs.
To do this, it listens to 1 socket per service (varstored, or swtpm) per domain.
To create these sockets before the domains are running, it listens to a message-switch socket.
This socket listens to calls from xenopsd, which orchestrates the domain creation.</p><p>To protect the domains from xapi being unavailable transiently, xapi-guard provides an on-disk cache for vTPM writes.
This cache acts as a buffer and stores the requests temporarily until xapi can be contacted again.
This situation usually happens when xapi is being restarted as part of an update.
SWTPM, the vTPM daemon, reads the contents of the TPM from xapi-guard on startup, suspend, and resume.
During normal operation SWTPM does not send read requests from xapi-guard.</p><h2 id=structure>Structure</h2><p>The cache module consists of two Lwt threads, one that writes to disk, and another one that reads from disk.
The writer is triggered when a VM writes to the vTPM.
It never blocks if xapi is unreachable, but responds as soon as the data has been stored either by xapi or on the local disk, such that the VM receives a timely response to the write request.
Both try to send the requests to xapi, depending on the state, to attempt write all the cached data back to xapi, and stop using the cache.
The threads communicate through a bounded queue, this is done to limit the amount of memory used.
This queue is a performance optimisation, where the writer informs the reader precisely which are the names of the cache files, such that the reader does not need to list the cache directory.
And a full queue does not mean data loss, just a loss of performance; vTPM writes are still cached.</p><p>This means that the cache operates in three modes:</p><ul><li>Direct: during normal operation the disk is not used at all</li><li>Engaged: both threads use the queue to order events</li><li>Disengaged: A thread dumps request to disk while the other reads the cache
until it&rsquo;s empty</li></ul><div class="mermaid align-center">---
title: Cache State
---
stateDiagram-v2
Disengaged
note right of Disengaged
Writer doesn't add requests to queue
Reader reads from cache and tries to push to xapi
end note
Direct
note left of Direct
Writer bypasses cache, send to xapi
Reader waits
end note
Engaged
note right of Engaged
Writer writes to cache and adds requests to queue
Reader reads from queue and tries to push to xapi
end note
[*] --> Disengaged
Disengaged --> Disengaged : Reader pushed pending TPMs to xapi, in the meantime TPMs appeared in the cache
Disengaged --> Direct : Reader pushed pending TPMs to xapi, cache is empty
Direct --> Direct : Writer receives TPM, sent to xapi
Direct --> Engaged : Writer receives TPM, error when sent to xapi
Engaged --> Direct : Reader sent TPM to xapi, finds an empty queue
Engaged --> Engaged : Writer receives TPM, queue is not full
Engaged --> Disengaged : Writer receives TPM, queue is full</div><h2 id=startup>Startup</h2><p>At startup, there&rsquo;s a dedicated routine to transform the existing contents of the cache.
This is currently done because the timestamp reference change on each boot.
This means that the existing contents might have timestamps considered more recent than timestamps of writes coming from running events, leading to missing content updates.
This must be avoided and instead the updates with offending timestamps are renamed to a timestamp taken from the current timestamp, ensuring a consistent
ordering.
The routine is also used to keep a minimal file tree: unrecognised files are deleted, temporary files created to ensure atomic writes are left untouched, and empty directories are deleted.
This mechanism can be changed in the future to migrate to other formats.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=design-documents>Design Documents</h1><div><strong>Key: </strong><span class="label label-default">Revision</span>
<span class="label label-danger">Proposed</span>
<span class="label label-warning">Confirmed</span>
<span class="label label-success">Released (vA.B)</span>
<span class="label label-info">Unrecognised status</span></div><table class="table table-striped table-condensed"><tbody><tr><td><a href=/new-docs/design/snapshot-revert/ class="btn btn-link">Improving snapshot revert behaviour</a>
<span class="label label-default">v1</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/multiple-cluster-managers/ class="btn btn-link">Multiple Cluster Managers</a>
<span class="label label-default">v2</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/sr-level-rrds/ class="btn btn-link">SR-Level RRDs</a>
<span class="label label-default">v11</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/backtraces/ class="btn btn-link">Backtrace support</a>
<span class="label label-default">v1</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/aggr-storage-reboots/ class="btn btn-link">Aggregated Local Storage and Host Reboots</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/coverage/ class="btn btn-link">Code Coverage Profiling</a>
<span class="label label-default">v2</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/distributed-database/ class="btn btn-link">Distributed database</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/fcoe-nics/ class="btn btn-link">FCoE capable NICs</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/local-database/ class="btn btn-link">Local database</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/management-interface-on-vlan/ class="btn btn-link">Management Interface on VLAN</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/multiple-device-emulators/ class="btn btn-link">Multiple device emulators</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/ocfs2/ class="btn btn-link">OCFS2 storage</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/patches-in-vdis/ class="btn btn-link">patches in VDIs</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/pci-passthrough/ class="btn btn-link">PCI passthrough support</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/pool-wide-ssh/ class="btn btn-link">Pool-wide SSH</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/xenopsd_events/ class="btn btn-link">Process events from xenopsd in a timely manner</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/plugin-protocol-v3/ class="btn btn-link">RRDD plugin protocol v3</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/schedule-snapshot/ class="btn btn-link">Schedule Snapshot Design</a>
<span class="label label-default">v2</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/emulated-pci-spec/ class="btn btn-link">Specifying Emulated PCI Devices</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/thin-lvhd/ class="btn btn-link">thin LVHD storage</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/xenprep/ class="btn btn-link">XenPrep</a>
<span class="label label-default">v2</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/pool-certificates/ class="btn btn-link">TLS vertification for intra-pool communications</a>
<span class="label label-default">v2</span>
<span class="label
label-success">released (22.6.0)</span></td></tr><tr><td><a href=/new-docs/design/tunnelling/ class="btn btn-link">Tunnelling API design</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (5.6 fp1)</span></td></tr><tr><td><a href=/new-docs/design/heterogeneous-pools/ class="btn btn-link">Heterogeneous pools</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (5.6)</span></td></tr><tr><td><a href=/new-docs/design/emergency-network-reset/ class="btn btn-link">Emergency Network Reset Design</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.0.2)</span></td></tr><tr><td><a href=/new-docs/design/bonding-improvements/ class="btn btn-link">Bonding Improvements design</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.0)</span></td></tr><tr><td><a href=/new-docs/design/gpu-passthrough/ class="btn btn-link">GPU pass-through support</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.0)</span></td></tr><tr><td><a href=/new-docs/design/integrated-gpu-passthrough/ class="btn btn-link">Integrated GPU passthrough support</a>
<span class="label label-default">v3</span>
<span class="label
label-success">released (6.5 sp1)</span></td></tr><tr><td><a href=/new-docs/design/pif-properties/ class="btn btn-link">GRO and other properties of PIFs</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.5)</span></td></tr><tr><td><a href=/new-docs/design/archival-redesign/ class="btn btn-link">RRDD archival redesign</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7,0)</span></td></tr><tr><td><a href=/new-docs/design/cpu-levelling-v2/ class="btn btn-link">CPU feature levelling 2.0</a>
<span class="label label-default">v7</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/gpu-support-evolution/ class="btn btn-link">GPU support evolution</a>
<span class="label label-default">v3</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/plugin-protocol-v2/ class="btn btn-link">RRDD plugin protocol v2</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/vgpu-type-identifiers/ class="btn btn-link">VGPU type identifiers</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/virt-hw-platform-vn/ class="btn btn-link">Virtual Hardware Platform Version</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/smapiv3/ class="btn btn-link">SMAPIv3</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.6)</span></td></tr><tr><td><a href=/new-docs/design/user-certificates/ class="btn btn-link">User-installable host certificates</a>
<span class="label label-default">v2</span>
<span class="label
label-success">released (8.2)</span></td></tr><tr><td><a href=/new-docs/design/RDP/ class="btn btn-link">RDP control</a>
<span class="label label-default">v2</span>
<span class="label
label-success">released (xenserver 6.5 sp1)</span></td></tr></tbody></table><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Design Documents</h1><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/144>#144</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Included some open questions under Xapi point 2</td></tr><tr><td><span class="label label-default">v3</span></td><td>Added new error, task, and assumptions</td></tr></table></header><h1 id=aggregated-local-storage-and-host-reboots>Aggregated Local Storage and Host Reboots</h1><h2 id=introduction>Introduction</h2><p>When hosts use an aggregated local storage SR, then disks are going to be mirrored to several different hosts in the pool (RAID). This ensures that if a host goes down (e.g. due to a reboot after installing a hotfix or upgrade, or when &ldquo;fenced&rdquo; by the HA feature), all disk contents in the SR are still accessible. This also means that if all disks are mirrored to just two hosts (worst-case scenario), just one host may be down at any point in time to keep the SR fully available.</p><p>When a node comes back up after a reboot, it will resynchronise all its disks with the related mirrors on the other hosts in the pool. This syncing takes some time, and only after this is done, we may consider the host &ldquo;up&rdquo; again, and allow another host to be shut down.</p><p>Therefore, when installing a hotfix to a pool that uses aggregated local storage, or doing a rolling pool upgrade, we need to make sure that we do hosts one-by-one, and we wait for the storage syncing to finish before doing the next.</p><p>This design aims to provide guidance and protection around this by blocking hosts to be shut down or rebooted from the XenAPI except when safe, and setting the <code>host.allowed_operations</code> field accordingly.</p><h2 id=xenapi>XenAPI</h2><p>If an aggregated local storage SR is in use, and one of the hosts is rebooting or down (for whatever reason), or resynchronising its storage, the operations <code>reboot</code> and <code>shutdown</code> will be removed from the <code>host.allowed_operations</code> field of <em>all</em> hosts in the pool that have a PBD for the SR.</p><p>This is a conservative approach in that assumes that this kind of SR tolerates only one node &ldquo;failure&rdquo;, and assumes no knowledge about how the SR distributes its mirrors. We may refine this in future, in order to allow some hosts to be down simultaneously.</p><p>The presence of the <code>reboot</code> operation in <code>host.allowed_operations</code> indicates whether the <code>host.reboot</code> XenAPI call is allowed or not (similarly for <code>shutdown</code> and <code>host.shutdown</code>). It will not, of course, prevent anyone from rebooting a host from the dom0 console or power switch.</p><p>Clients, such as XenCenter, can use <code>host.allowed_operations</code>, when applying an update to a pool, to guide them when it is safe to update and reboot the next host in the sequence.</p><p>In case <code>host.reboot</code> or <code>host.shutdown</code> is called while the storage is busy resyncing mirrors, the call will fail with a new error <code>MIRROR_REBUILD_IN_PROGRESS</code>.</p><h2 id=xapi>Xapi</h2><p>Xapi needs to be able to:</p><ol><li>Determine whether aggregated local storage is in use; this just means that a PBD for such an SR present.<ul><li>TBD: To avoid SR-specific code in xapi, the storage backend should tell us whether it is an aggregated local storage SR.</li></ul></li><li>Determine whether the storage system is resynchronising its mirrors; it will need to be able to query the storage backend for this kind of information.<ul><li>Xapi will poll for this and will reflect that a resync is happening by creating a <code>Task</code> for it (in the DB). This task can be used to track progress, if available.</li><li>The exact way to get the syncing information from the storage backend is SR specific. The check may be implemented in a separate script or binary that xapi calls from the polling thread. Ideally this would be integrated with the storage backend.</li></ul></li><li>Update <code>host.allowed_operations</code> for all hosts in the pool according to the rules described above. This comes down to updating the function <code>valid_operations</code> in <code>xapi_host_helpers.ml</code>, and will need to use a combination of the functionality from the two points above, plus and indication of host liveness from <code>host_metrics.live</code>.</li><li>Trigger an update of the allowed operations when a host shuts down or reboots (due to a XenAPI call or otherwise), and when it has finished resynchronising when back up. Triggers must be in the following places (some may already be present, but are listed for completeness, and to confirm this):<ul><li>Wherever <code>host_metrics.live</code> is updated to detect pool slaves going up and down (probably at least in <code>Db_gc.check_host_liveness</code> and <code>Xapi_ha</code>).</li><li>Immediately when a <code>host.reboot</code> or <code>host.shutdown</code> call is executed: <code>Message_forwarding.Host.{reboot,shutdown,with_host_operation}</code>.</li><li>When a storage resync is starting or finishing.</li></ul></li></ol><p>All of the above runs on the pool master (= SR master) only.</p><h2 id=assumptions>Assumptions</h2><p>The above will be safe if the storage cluster is equal to the XenServer pool. In general, however, it may be desirable to have a storage cluster that is larger than the pool, have multiple XS pools on a single cluster, or even share the cluster with other kinds of nodes.</p><p>To ensure that the storage is &ldquo;safe&rdquo; in these scenarios, xapi needs to be able to ask the storage backend:</p><ol><li>if a mirror is being rebuilt &ldquo;somewhere&rdquo; in the cluster, AND</li><li>if &ldquo;some node&rdquo; in the cluster is offline (even if the node is not in the XS pool).</li></ol><p>If the cluster is equal to the pool, then xapi can do point 2 without asking the storage backend, which will simplify things. For the moment, we assume that the storage cluster is equal to the XS pool, to avoid making things too complicated (while still need to keep in mind that we may change this in future).</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr></table></header><h1 id=backtrace-support>Backtrace support</h1><p>We want to make debugging easier by recording exception backtraces which are</p><ul><li>reliable</li><li>cross-process (e.g. xapi to xenopsd)</li><li>cross-language</li><li>cross-host (e.g. master to slave)</li></ul><p>We therefore need</p><ul><li>to ensure that backtraces are captured in our OCaml and python code</li><li>a marshalling format for backtraces</li><li>conventions for storing and retrieving backtraces</li></ul><h1 id=backtraces-in-ocaml>Backtraces in OCaml</h1><p>OCaml has fast exceptions which can be used for both</p><ul><li>control flow i.e. fast jumps from inner scopes to outer scopes</li><li>reporting errors to users (e.g. the toplevel or an API user)</li></ul><p>To keep the exceptions fast, exceptions and backtraces are decoupled:
there is a single active backtrace per-thread at any one time. If you
have caught an exception and then throw another exception, the backtrace
buffer will be reinitialised, destroying your previous records. For example
consider a &lsquo;finally&rsquo; function:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> finally f cleanup <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> f () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    result
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e <span style=color:#75715e>(* &lt;-- backtrace starts here now *)</span></span></span></code></pre></div><p>This function performs some action (i.e. <code>f ()</code>) and guarantees to
perform some cleanup action (<code>cleanup ()</code>) whether or not an exception
is thrown. This is a common pattern to ensure resources are freed (e.g.
closing a socket or file descriptor). Unfortunately the <code>raise e</code> in
the exception handler loses the backtrace context: when the exception
gets to the toplevel, <code>Printexc.get_backtrace ()</code> will point at the
<code>finally</code> rather than the real cause of the error.</p><p>We will use a variant of the solution proposed by
<a href=http://gallium.inria.fr/blog/a-library-to-record-ocaml-backtraces/ target=_blank>Jacques-Henri Jourdan</a>
where we will record backtraces when we catch exceptions, before the
buffer is reinitialised. Our <code>finally</code> function will now look like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> finally f cleanup <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> f () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    result
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    Backtrace.is_important e<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e</span></span></code></pre></div><p>The function <code>Backtrace.is_important e</code> associates the exception <code>e</code>
with the current backtrace before it gets deleted.</p><p>Xapi always has high-level exception handlers or other wrappers around all the
threads it spawns. In particular Xapi tries really hard to associate threads
with active tasks, so it can prefix all log lines with a task id. This helps
admins see the related log lines even when there is lots of concurrent activity.
Xapi also tries very hard to label other threads with names for the same reason
(e.g. <code>db_gc</code>). Every thread should end up being wrapped in <code>with_thread_named</code>
which allows us to catch exceptions and log stacktraces from <code>Backtrace.get</code>
on the way out.</p><h2 id=ocaml-design-guidelines>OCaml design guidelines</h2><p>Making nice backtraces requires us to think when we write our exception raising
and handling code. In particular:</p><ul><li>If a function handles an exception and re-raise it, you must call
<code>Backtrace.is_important e</code> with the exception to capture the backtrace first.</li><li>If a function raises a different exception (e.g. <code>Not_found</code> becoming a XenAPI
<code>INTERNAL_ERROR</code>) then you must use <code>Backtrace.reraise &lt;old> &lt;new></code> to
ensure the backtrace is preserved.</li><li>All exceptions should be printable &ndash; if the generic printer doesn&rsquo;t do a good
enough job then register a custom printer.</li><li>If you are the last person who will see an exception (because you aren&rsquo;t going
to rethrow it) then you <em>may</em> log the backtrace via <code>Debug.log_backtrace e</code>
<em>if and only if</em> you reasonably expect the resulting backtrace to be helpful
and not spammy.</li><li>If you aren&rsquo;t the last person who will see an exception (because you are going
to rethrow it or another exception), then <em>do not</em> log the backtrace; the
next handler will do that.</li><li>All threads should have a final exception handler at the outermost level
for example <code>Debug.with_thread_named</code> will do this for you.</li></ul><h1 id=backtraces-in-python>Backtraces in python</h1><p>Python exceptions behave similarly to the OCaml ones: if you raise a new
exception while handling an exception, the backtrace buffer is overwritten.
Therefore the same considerations apply.</p><h2 id=python-design-guidelines>Python design guidelines</h2><p>The function <a href=https://docs.python.org/2/library/sys.html#sys.exc_info target=_blank>sys.exc_info()</a>
can be used to capture the traceback associated with the last exception.
We must guarantee to call this before constructing another exception. In
particular, this does not work:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>  <span style=color:#66d9ef>raise</span> MyException(sys<span style=color:#f92672>.</span>exc_info())</span></span></code></pre></div><p>Instead you must capture the traceback first:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>  exc_info <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>exc_info()
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>raise</span> MyException(exc_info)</span></span></code></pre></div><h1 id=marshalling-backtraces>Marshalling backtraces</h1><p>We need to be able to take an exception thrown from python code, gather
the backtrace, transmit it to an OCaml program (e.g. xenopsd) and glue
it onto the end of the OCaml backtrace. We will use a simple json marshalling
format for the raw backtrace data consisting of</p><ul><li>a string summary of the error (e.g. an exception name)</li><li>a list of filenames</li><li>a corresponding list of lines</li></ul><p>(Note we don&rsquo;t use the more natural list of pairs as this confuses the
&ldquo;rpclib&rdquo; code generating library)</p><p>In python:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    results <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;error&#34;</span>: str(s[<span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;files&#34;</span>: files,
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;lines&#34;</span>: lines,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    print json<span style=color:#f92672>.</span>dumps(results)</span></span></code></pre></div><p>In OCaml:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>type</span> error <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    error<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    files<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    lines<span style=color:#f92672>:</span> <span style=color:#66d9ef>int</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span> <span style=color:#66d9ef>with</span> rpc
</span></span><span style=display:flex><span>  print_string <span style=color:#f92672>(</span>Jsonrpc.to_string <span style=color:#f92672>(</span>rpc_of_error <span style=color:#f92672>...))</span></span></span></code></pre></div><h1 id=retrieving-backtraces>Retrieving backtraces</h1><p>Backtraces will be written to syslog as usual. However it will also be
possible to retrieve the information via the CLI to allow diagnostic
tools to be written more easily.</p><h2 id=the-cli>The CLI</h2><p>We add a global CLI argument &ldquo;&ndash;trace&rdquo; which requests the backtrace be
printed, if one is available:</p><div class="wrap-code highlight"><pre tabindex=0><code># xe vm-start vm=hvm --trace
Error code: SR_BACKEND_FAILURE_202
Error parameters: , General backend error [opterr=exceptions must be old-style classes or derived from BaseException, not str],
Raised Server_error(SR_BACKEND_FAILURE_202, [ ; General backend error [opterr=exceptions must be old-style classes or derived from BaseException, not str];  ])
Backtrace:
0/50 EXT @ st30 Raised at file /opt/xensource/sm/SRCommand.py, line 110
1/50 EXT @ st30 Called from file /opt/xensource/sm/SRCommand.py, line 159
2/50 EXT @ st30 Called from file /opt/xensource/sm/SRCommand.py, line 263
3/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1486
4/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 83
5/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1519
6/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1567
7/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1065
8/50 EXT @ st30 Called from file /opt/xensource/sm/EXTSR.py, line 221
9/50 xenopsd-xc @ st30 Raised by primitive operation at file &#34;lib/storage.ml&#34;, line 32, characters 3-26
10/50 xenopsd-xc @ st30 Called from file &#34;lib/task_server.ml&#34;, line 176, characters 15-19
11/50 xenopsd-xc @ st30 Raised at file &#34;lib/task_server.ml&#34;, line 184, characters 8-9
12/50 xenopsd-xc @ st30 Called from file &#34;lib/storage.ml&#34;, line 57, characters 1-156
13/50 xenopsd-xc @ st30 Called from file &#34;xc/xenops_server_xen.ml&#34;, line 254, characters 15-63
14/50 xenopsd-xc @ st30 Called from file &#34;xc/xenops_server_xen.ml&#34;, line 1643, characters 15-76
15/50 xenopsd-xc @ st30 Called from file &#34;lib/xenctrl.ml&#34;, line 127, characters 13-17
16/50 xenopsd-xc @ st30 Re-raised at file &#34;lib/xenctrl.ml&#34;, line 127, characters 56-59
17/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 937, characters 3-54
18/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1103, characters 4-71
19/50 xenopsd-xc @ st30 Called from file &#34;list.ml&#34;, line 84, characters 24-34
20/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1098, characters 2-367
21/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1203, characters 3-46
22/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1441, characters 3-9
23/50 xenopsd-xc @ st30 Raised at file &#34;lib/xenops_server.ml&#34;, line 1452, characters 9-10
24/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1458, characters 48-60
25/50 xenopsd-xc @ st30 Called from file &#34;lib/task_server.ml&#34;, line 151, characters 15-26
26/50 xapi @ st30 Raised at file &#34;xapi_xenops.ml&#34;, line 1719, characters 11-14
27/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
28/50 xapi @ st30 Raised at file &#34;xapi_xenops.ml&#34;, line 2005, characters 13-14
29/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
30/50 xapi @ st30 Raised at file &#34;xapi_xenops.ml&#34;, line 1785, characters 15-16
31/50 xapi @ st30 Called from file &#34;message_forwarding.ml&#34;, line 233, characters 25-44
32/50 xapi @ st30 Called from file &#34;message_forwarding.ml&#34;, line 915, characters 15-67
33/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
34/50 xapi @ st30 Raised at file &#34;lib/pervasiveext.ml&#34;, line 26, characters 9-12
35/50 xapi @ st30 Called from file &#34;message_forwarding.ml&#34;, line 1205, characters 21-199
36/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
37/50 xapi @ st30 Raised at file &#34;lib/pervasiveext.ml&#34;, line 26, characters 9-12
38/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
9/50 xapi @ st30 Raised at file &#34;rbac.ml&#34;, line 236, characters 10-15
40/50 xapi @ st30 Called from file &#34;server_helpers.ml&#34;, line 75, characters 11-41
41/50 xapi @ st30 Raised at file &#34;cli_util.ml&#34;, line 78, characters 9-12
42/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
43/50 xapi @ st30 Raised at file &#34;lib/pervasiveext.ml&#34;, line 26, characters 9-12
44/50 xapi @ st30 Called from file &#34;cli_operations.ml&#34;, line 1889, characters 2-6
45/50 xapi @ st30 Re-raised at file &#34;cli_operations.ml&#34;, line 1898, characters 10-11
46/50 xapi @ st30 Called from file &#34;cli_operations.ml&#34;, line 1821, characters 14-18
47/50 xapi @ st30 Called from file &#34;cli_operations.ml&#34;, line 2109, characters 7-526
48/50 xapi @ st30 Called from file &#34;xapi_cli.ml&#34;, line 113, characters 18-56
49/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9</code></pre></div><p>One can automatically set &ldquo;&ndash;trace&rdquo; for a whole shell session as follows:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export XE_EXTRA_ARGS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;--trace&#34;</span></span></span></code></pre></div><h2 id=the-xenapi>The XenAPI</h2><p>We already store error information in the XenAPI &ldquo;Task&rdquo; object and so we
can store backtraces at the same time. We shall add a field &ldquo;backtrace&rdquo;
which will have type &ldquo;string&rdquo; but which will contain s-expression encoded
backtrace data. Clients should not attempt to parse this string: its
contents may change in future. The reason it is different from the json
mentioned before is that it also contains host and process information
supplied by Xapi, and may be extended in future to contain other diagnostic
information.</p><h2 id=the-xenopsd-api>The Xenopsd API</h2><p>We already store error information in the xenopsd API &ldquo;Task&rdquo; objects,
we can extend these to store the backtrace in an additional field (&ldquo;backtrace&rdquo;).
This field will have type &ldquo;string&rdquo; but will contain s-expression encoded
backtrace data.</p><h2 id=the-smapiv1-api>The SMAPIv1 API</h2><p>Errors in SMAPIv1 are returned as XMLRPC &ldquo;Faults&rdquo; containing a code and
a status line. Xapi transforms these into XenAPI exceptions usually of the
form <code>SR_BACKEND_FAILURE_&lt;code></code>. We can extend the SM backends to use the
XenAPI exception type directly: i.e. to marshal exceptions as dictionaries:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>  results <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Status&#34;</span>: <span style=color:#e6db74>&#34;Failure&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;ErrorDescription&#34;</span>: [ code, param1, <span style=color:#f92672>...</span>, paramN ]
</span></span><span style=display:flex><span>  }</span></span></code></pre></div><p>We can then define a new backtrace-carrying error:</p><ul><li>code = <code>SR_BACKEND_FAILURE_WITH_BACKTRACE</code></li><li>param1 = json-encoded backtrace</li><li>param2 = code</li><li>param3 = reason</li></ul><p>which is internally transformed into <code>SR_BACKEND_FAILURE_&lt;code></code> and
the backtrace is appended to the current Task backtrace. From the client&rsquo;s
point of view the final exception should look the same, but Xapi will have
a chance to see and log the whole backtrace.</p><p>As a side-effect, it is possible for SM plugins to throw XenAPI errors directly,
without interpretation by Xapi.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.0)</span></td></tr></table></header><h1 id=bonding-improvements-design>Bonding Improvements design</h1><p>This document describes design details for the
PR-1006 requirements.</p><h1 id=xapi-and-xenapi>XAPI and XenAPI</h1><h2 id=creating-a-bond>Creating a Bond</h2><h3 id=current-behaviour-on-bond-creation>Current Behaviour on Bond creation</h3><p>Steps for a user to create a bond:</p><ol><li>Shutdown all VMs with VIFs using the interfaces that will be bonded,
in order to unplug those VIFs.</li><li>Create a Network to be used by the bond: <code>Network.create</code></li><li>Call <code>Bond.create</code> with a ref to this Network, a list of refs of
slave PIFs, and a MAC address to use.</li><li>Call <code>PIF.reconfigure_ip</code> to configure the bond master.</li><li>Call <code>Host.management_reconfigure</code> if one of the slaves is the
management interface. This command will call <code>interface-reconfigure</code>
to bring up the master and bring down the slave PIFs, thereby
activating the bond. Otherwise, call <code>PIF.plug</code> to activate the
bond.</li></ol><p><code>Bond.create</code> XenAPI call:</p><ol><li><p>Remove duplicates in the list of slaves.</p></li><li><p>Validate the following:</p><ul><li>Slaves must not be in a bond already.</li><li>Slaves must not be VLAN masters.</li><li>Slaves must be on the same host.</li><li>Network does not already have a PIF on the same host as the
slaves.</li><li>The given MAC is valid.</li></ul></li><li><p>Create the master PIF object.</p><ul><li>The device name of this PIF is <code>bond</code><em>x</em>, with <em>x</em> the smallest
unused non-negative integer.</li><li>The MAC of the first-named slave is used if no MAC was
specified.</li></ul></li><li><p>Create the Bond object, specifying a reference to the master. The
value of the <code>PIF.master_of</code> field on the master is dynamically
computed on request.</p></li><li><p>Set the <code>PIF.bond_slave_of</code> fields of the slaves. The value of the
<code>Bond.slaves</code> field is dynamically computed on request.</p></li></ol><h3 id=new-behaviour-on-bond-creation>New Behaviour on Bond creation</h3><p>Steps for a user to create a bond:</p><ol><li>Create a Network to be used by the bond: <code>Network.create</code></li><li>Call <code>Bond.create</code> with a ref to this Network, a list of refs of
slave PIFs, and a MAC address to use.<br>The new bond will automatically be plugged if one of the slaves was
plugged.</li></ol><p>In the following, for a host <em>h</em>, a <em>VIF-to-move</em> is a VIF associated
with a VM that is either</p><ul><li>running, suspended or paused on <em>h</em>, OR</li><li>halted, and <em>h</em> is the only host that the VM can be started on.</li></ul><p>The <code>Bond.create</code> XenAPI call is updated to do the following:</p><ol><li><p>Remove duplicates in the list of slaves.</p></li><li><p>Validate the following, and raise an exception if any of these check
fails:</p><ul><li>Slaves must not be in a bond already.</li><li>Slaves must not be VLAN masters.</li><li>Slaves must not be Tunnel access PIFs.</li><li>Slaves must be on the same host.</li><li>Network does not already have a PIF on the same host as the
slaves.</li><li>The given MAC is valid.</li></ul></li><li><p>Try unplugging all currently attached VIFs of the set of VIFs that
need to be moved. Roll back and raise an exception of one of the
VIFs cannot be unplugged (e.g. due to the absence of PV drivers in
the VM).</p></li><li><p>Determine the <em>primary slave</em>: the management PIF (if among the
slaves), or the first slave with IP configuration.</p></li><li><p>Create the master PIF object.</p><ul><li>The device name of this PIF is <code>bond</code><em>x</em>, with <em>x</em> the smallest
unused non-negative integer.</li><li>The MAC of the primary slave is used if no MAC was specified.</li><li>Include the IP configuration of the primary slave.</li><li>If any of the slaves has <code>PIF.disallow_unplug = true</code>, this will
be copied to the master.</li></ul></li><li><p>Create the Bond object, specifying a reference to the master. The
value of the <code>PIF.master_of</code> field on the master is dynamically
computed on request. Also a reference to the primary slave is
written to <code>Bond.primary_slave</code> on the new Bond object.</p></li><li><p>Set the <code>PIF.bond_slave_of</code> fields of the slaves. The value of the
<code>Bond.slaves</code> field is dynamically computed on request.</p></li><li><p>Move VLANs, plus the VIFs-to-move on them, to the master.</p><ul><li>If all VLANs on the slaves have different tags, all VLANs will
be moved to the bond master, while the same Network is used. The
network effectively moves up to the bond and therefore no VIFs
need to be moved.</li><li>If multiple VLANs on different slaves have the same tag, they
necessarily have different Networks as well. Only one VLAN with
this tag is created on the bond master. All VIFs-to-move on the
remaining VLAN networks are moved to the Network that was moved
up.</li></ul></li><li><p>Move Tunnels to the master. The tunnel Networks move up with the
tunnels. As tunnel keys are different for all tunnel networks, there
are no complications as in the VLAN case.</p></li><li><p>Move VIFs-to-move on the slaves to the master.</p></li><li><p>If one of the slaves is the current management interface, move
management to the master; the master will automatically be plugged.
If none of the slaves is the management interface, plug the master
if any of the slaves was plugged. In both cases, the slaves will
automatically be unplugged.</p></li><li><p>On all slaves, reset the IP configuration and set <code>disallow_unplug</code>
to false.</p></li></ol><p><em>Note: &ldquo;moving&rdquo; a VIF, VLAN or tunnel means &ldquo;re-creating somewhere else,
and destroying the old one&rdquo;.</em></p><h2 id=destroying-a-bond>Destroying a Bond</h2><h3 id=current-behaviour-on-bond-destruction>Current Behaviour on Bond destruction</h3><p>Steps for a user to destroy a bond:</p><ol><li>If the management interface is on the bond, move it to another PIF
using <code>PIF.reconfigure_ip</code> and <code>Host.management_reconfigure</code>.
Otherwise, no <code>PIF.unplug</code> needs to be called on the bond master, as
<code>Bond.destroy</code> does this automatically.</li><li>Call <code>Bond.destroy</code> with a ref to the Bond object.</li><li>If desired, bring up the former slave PIFs by calls to <code>PIF.plug</code>
(this is does not happen automatically).</li></ol><p><code>Bond.destroy</code> XenAPI call:</p><ol><li><p>Validate the following constraints:</p><ul><li>No VLANs are attached to the bond master.</li><li>The bond master is not the management PIF.</li></ul></li><li><p>Bring down the master PIF and clean up the underlying network
devices.</p></li><li><p>Remove the Bond and master PIF objects.</p></li></ol><h3 id=new-behaviour-on-bond-destruction>New Behaviour on Bond destruction</h3><p>Steps for a user to destroy a bond:</p><ol><li>Call <code>Bond.destroy</code> with a ref to the Bond object.</li><li>If desired, move VIFs/VLANs/tunnels/management from (former) primary
slave to other PIFs.</li></ol><p><code>Bond.destroy</code> XenAPI call is updated to do the following:</p><ol><li>Try unplugging all currently attached VIFs of the set of VIFs that
need to be moved. Roll back and raise an exception of one of the
VIFs cannot be unplugged (e.g. due to the absence of PV drivers in
the VM).</li><li>Copy the IP configuration of the master to the primary slave.</li><li>Move VLANs, with their Networks, to the primary slave.</li><li>Move Tunnels, with their Networks, to the primary slave.</li><li>Move VIFs-to-move on the master to the primary slave.</li><li>If the master is the current management interface, move management
to the primary slave. The primary slave will automatically be
plugged.</li><li>If the master was plugged, plug the primary slave. This will
automatically clean up the underlying devices of the bond.</li><li>If the master has <code>PIF.disallow_unplug = true</code>, this will be copied
to the primary slave.</li><li>Remove the Bond and master PIF objects.</li></ol><h2 id=using-bond-slaves>Using Bond Slaves</h2><h3 id=current-behaviour-for-bond-slaves>Current Behaviour for Bond Slaves</h3><ul><li>It possible to plug any existing PIF, even bond slaves. Any other
PIFs that cannot be attached at the same time as the PIF that is
being plugged, are automatically unplugged.</li><li>Similarly, it is possible to make a bond slave the management
interface. Any other PIFs that cannot be attached at the same time
as the PIF that is being plugged, are automatically unplugged.</li><li>It is possible to have a VIF on a Network associated with a bond
slave. When the VIF&rsquo;s VM is started, or the VIF is hot-plugged, the
PIF is relies on is automatically plugged, and any other PIFs that
cannot be attached at the same time as this PIF are automatically
unplugged.</li><li>It is possible to have a VLAN on a bond slave, though the bond
(master) and the VLAN may not be simultaneously attached. This is
not currently enforced (which may be considered a bug).</li></ul><h3 id=new-behaviour-for-bond-slaves>New behaviour for Bond Slaves</h3><ul><li>It is no longer possible to plug a bond slave. The exception
CANNOT_PLUG_BOND_SLAVE is raised when trying to do so.</li><li>It is no longer possible to make a bond slave the management
interface. The exception CANNOT_PLUG_BOND_SLAVE is raised when
trying to do so.</li><li>It is still possible to have a VIF on the Network of a bond slave.
However, it is not possible to start such a VIF&rsquo;s VM on a host, if
this would need a bond slave to be plugged. Trying this will result
in a CANNOT_PLUG_BOND_SLAVE exception. Likewise, it is not
possible to hot-plug such a VIF.</li><li>It is no longer possible to place a VLAN on a bond slave. The
exception CANNOT_ADD_VLAN_TO_BOND_SLAVE is raised when trying
to do so.</li><li>It is no longer possible to place a tunnel on a bond slave. The
exception CANNOT_ADD_TUNNEL_TO_BOND_SLAVE is raised when trying
to do so.</li></ul><h2 id=actions-on-start-up>Actions on Start-up</h2><h3 id=current-behaviour-on-start-up>Current Behaviour on Start-up</h3><p>When a pool slave starts up, bonds and VLANs on the pool master are
replicated on the slave:</p><ul><li>Create all VLANs that the master has, but the slave has not. VLANs
are identified by their tag, the device name of the slave PIF, and
the Networks of the master and slave PIFs.</li><li>Create all bonds that the master has, but the slave has not. If the
interfaces needed for the bond are not all available on the slave, a
partial bond is created. If some of these interface are already
bonded on the slave, this bond is destroyed first.</li></ul><h3 id=new-behaviour-on-start-up>New Behaviour on Start-up</h3><ul><li>The current VLAN/tunnel/bond recreation code is retained, as it uses
the new Bond.create and Bond.destroy functions, and therefore does
what it needs to do.</li><li>Before VLAN/tunnel/bond recreation, any violations of the rules
defined in R2 are rectified, by moving VIFs, VLANs, tunnels or
management up to bonds.</li></ul><h1 id=cli>CLI</h1><p>The behaviour of the <code>xe</code> CLI commands <code>bond-create</code>, <code>bond-destroy</code>,
<code>pif-plug</code>, and <code>host-management-reconfigure</code> is changed to match their
associated XenAPI calls.</p><h1 id=xencenter>XenCenter</h1><p>XenCenter already automatically moves the management interface when a
bond is created or destroyed. This is no longer necessary, as the
<code>Bond.create/destroy</code> calls already do this. XenCenter only needs to
copy any <code>PIF.other_config</code> keys that is needs between primary slave and
bond master.</p><h1 id=manual-tests>Manual Tests</h1><ul><li>Create a bond of two interfaces&mldr;<ul><li>without VIFs/VLANs/management on them;</li><li>with management on one of them;</li><li>with a VLAN on one of them;</li><li>with two VLANs on two different interfaces, having the same VLAN
tag;</li><li>with a VIF associated with a halted VM on one of them;</li><li>with a VIF associated with a running VM (with and without PV
drivers) on one of them.</li></ul></li><li>Destroy a bond of two interfaces&mldr;<ul><li>without VIFs/VLANs/management on it;</li><li>with management on it;</li><li>with a VLAN on it;</li><li>with a VIF associated with a halted VM on it;</li><li>with a VIF associated with a running VM (with and without PV
drivers) on it.</li></ul></li><li>In a pool of two hosts, having VIFs/VLANs/management on the
interfaces of the pool slave, create a bond on the pool master, and
restart XAPI on the slave.</li><li>Restart XAPI on a host with a networking configuration that has
become illegal due to these requirements.</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=code-coverage-profiling>Code Coverage Profiling</h1><p>We would like to add optional coverage profiling to existing <a href=http://ocaml.org target=_blank>OCaml</a>
projects in the context of <a href=https://github.com/xenserver target=_blank>XenServer</a> and <a href=https://github.com/xapi-project target=_blank>XenAPI</a>. This article
presents how we do it.</p><p>Binaries instrumented for coverage profiling in the XenServer project
need to run in an environment where several services act together as
they provide operating-system-level services. This makes it a little
harder than profiling code that can be profiled and executed in
isolation.</p><h2 id=tldr>TL;DR</h2><p>To build binaries with coverage profiling, do:</p><pre><code>./configure --enable-coverage
make 
</code></pre><p>Binaries will log coverage data to <code>/tmp/bisect*.out</code> from which a
coverage report can be generated in <code>coverage/</code>:</p><pre><code>bisect-ppx-report -I _build -html coverage /tmp/bisect*.out
</code></pre><h2 id=profiling-framework-bisect-ppx>Profiling Framework Bisect-PPX</h2><p>The open-source <a href=https://github.com/aantron/bisect_ppx target=_blank>BisectPPX</a> instrumentation framework uses extension
points (PPX) in the <a href=http://ocaml.org target=_blank>OCaml</a> compiler to instrument code during
compilation. Instrumented code for a binary is then compiled as usual
and logs during execution data to in-memory data structures. Before an
instrumented binary terminates, it writes the logged data to a file.
This data can then be analysed with the <code>bisect-ppx-report</code> tool, to
produce a summary of annotated code that highlights what part of a
codebase was executed.</p><p><a href=https://github.com/aantron/bisect_ppx target=_blank>BisectPPX</a> has several desirable properties:</p><ul><li>a robust code base that is well tested</li><li>it is easy to integrate into the compilation pipeline (see below)</li><li>is specific to the <a href=http://ocaml.org target=_blank>OCaml</a> language; an expression-oriented language
like OCaml doesn&rsquo;t fit the traditional statement coverage well</li><li>it is actively maintained</li><li>is generates useful reports for interactive and non-interactive use
that help to improve code coverage</li></ul><p><a href=#image-8b4b1b7aa9a48eca6df1189165642113 class=lightbox-link><img src=/new-docs/design/coverage/./coverage-screenshot.png alt="Coverage Analysis" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-8b4b1b7aa9a48eca6df1189165642113><img src=/new-docs/design/coverage/./coverage-screenshot.png alt="Coverage Analysis" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Red parts indicate code that wasn&rsquo;t executed whereas green parts were.
Hovering over a dark green spot reveals how often that point was
executed.</p><p>The individual steps of instrumenting code with <a href=https://github.com/aantron/bisect_ppx target=_blank>BisectPPX</a> are greatly
abstracted by OCamlfind (OCaml&rsquo;s library manager) and OCamlbuild
(OCaml&rsquo;s compilation manager):</p><pre><code># write code
vim example.ml

# build it with instrumentation from bisect_ppx
ocamlbuild -use-ocamlfind -pkg bisect_ppx -pkg unix example.native

# execute it - generates files ./bisect*.out
./example.native

# generate report
bisect-ppx-report -I _build -html coverage bisect000*

# view coverage/index.html

Summary:
 - 'binding' points: 2/2 (100.00%)
 - 'sequence' points: 10/10 (100.00%)
 - 'match/function' points: 5/8 (62.50%)
 - total: 17/20 (85.00%)
</code></pre><p>The fourth step generates a HTML report in <code>coverage/</code>. All it takes is
to declare to <a href=https://github.com/ocaml/ocamlbuild/blob/master/manual/manual.adoc target=_blank>OCamlbuild</a> that a module depends on <code>bisect_ppx</code> and it
will be instrumented during compilation. Behind the scenes <code>ocamlfind</code>
makes sure that the compiler uses a preprocessing step that instruments
the code.</p><h2 id=signal-handling>Signal Handling</h2><p>During execution the code instrumentation leads to the collection of
data. This code registers a function with <code>at_exit</code> that writes the data
to <code>bisect*.out</code> when <code>exit</code> is called. A binary can terminate without
calling <code>exit</code> and in that case the file would not be written. It is
therefore important to make sure that <code>exit</code> is called. If this does not
happen naturally, for example in the context of a daemon that is
terminated by receiving the <code>TERM</code> signal, a signal handler must be
installed:</p><pre><code>let stop signal =
  printf &quot;caught signal %d\n&quot; signal;
  exit 0

Sys.set_signal Sys.sigterm (Sys.Signal_handle stop)
</code></pre><h2 id=dumping-coverage-information-at-runtime>Dumping coverage information at runtime</h2><p>By default coverage data can only be dumped at exit, which is inconvenient if you have a test-suite
that needs to reuse a long running daemon, and starting/stopping it each time is not feasible.</p><p>In such cases we need an API to dump coverage at runtime, which <em>is</em> provided by <code>bisect_ppx >= 1.3.0</code>.
However each daemon will need to set up a way to listen to an event that triggers this coverage dump,
furthermore it is desirable to make runtime coverage dumping compiled in conditionally to be absolutely sure
that production builds do <em>not</em> use coverage preprocessed code.</p><p>Hence instead of duplicating all this build logic in each daemon (<code>xapi</code>, <code>xenopsd</code>, etc.) provide this
functionality in a common library <code>xapi-idl</code> that:</p><ul><li>logs a message on startup so we know it is active</li><li>sets BISECT_FILE environment variable to dump coverage in the appropriate place</li><li>listens on <code>org.xen.xapi.coverage.&lt;name></code> message queue for runtime coverage dump commands:<ul><li>sending <code>dump &lt;Number></code> will cause runtime coverage to be dumped to a file
named <code>bisect-&lt;name>-&lt;random>.&lt;Number>.out</code></li><li>sending <code>reset</code> will cause the runtime coverage counters to be reset</li></ul></li></ul><p>Daemons that use <code>Xcp_service.configure2</code> (e.g. <code>xenopsd</code>) will benefit from this runtime trigger automatically,
provided they are themselves preprocessed with <code>bisect_ppx</code>.</p><p>Since we are interested in collecting coverage data for system-wide test-suite runs we need a way to trigger
dumping of coverage data centrally, and a good candidate for that is <code>xapi</code> as the top-level daemon.</p><p>It will call <code>Xcp_coverage.dispatcher_init ()</code>, which listens on <code>org.xen.xapi.coverage.dispatch</code> and
dispatches the coverage dump command to all message queues under <code>org.xen.xapi.coverage.*</code> except itself.</p><p>On production, and regular builds all of this is a no-op, ensured by using separate <code>lib/coverage/disabled.ml</code> and <code>lib/coverage/enabled.ml</code>
files which implement the same interface, and choosing which one to use at build time.</p><h2 id=where-data-is-written>Where Data is Written</h2><p>By default, <a href=https://github.com/aantron/bisect_ppx target=_blank>BisectPPX</a> writes data in a binary&rsquo;s current working
directory as <code>bisectXXXX.out</code>. It doesn&rsquo;t overwrite existing files and
files from several runs can be combined during analysis. However, this
name and the location can be inconvenient when multiple programs share a
directory.</p><p><a href=https://github.com/aantron/bisect_ppx target=_blank>BisectPPX</a>&rsquo;s default can be overridden with the <code>BISECT_FILE</code>
environment variable. This can happen on the command line:</p><pre><code>BISECT_FILE=/tmp/example ./example.native
</code></pre><p>In the context of XenServer we could do this in startup scripts.
However, we added a bit of code</p><pre><code>val Coverage.init: string -&gt; unit
</code></pre><p>that sets the environment variable from inside the program. The files
are written to a temporary directory (respecting <code>$TMP</code> or using <code>/tmp</code>)
and uses the <code>string</code>-typed argument to include it in the name. To be
effective, this function must be called before the programs exits. For
clarity it is called at the begin of program execution.</p><h2 id=instrumenting-an-oasis-project>Instrumenting an Oasis Project</h2><p>While instrumentation is easy on the level of a small file or project it
is challenging in a bigger project. We decided to focus on projects that
are build with the <a href=http://oasis.forge.ocamlcore.org target=_blank>Oasis</a> build and packaging manager. These have a
well-defined structure and compilation process that is controlled by a
central <code>_oasis</code> file. This file describes for each library and binary
its dependencies at a package level. From this, <a href=http://oasis.forge.ocamlcore.org target=_blank>Oasis</a> generates a
<code>configure</code> script and compilation rules for the <a href=https://github.com/ocaml/ocamlbuild/blob/master/manual/manual.adoc target=_blank>OCamlbuild</a> system.
<a href=http://oasis.forge.ocamlcore.org target=_blank>Oasis</a> is designed that the generated files can be shipped without
requiring <a href=http://oasis.forge.ocamlcore.org target=_blank>Oasis</a> itself being available.</p><p>Goals for instrumentation are:</p><ul><li>what files are instrumented should be obvious and easy to manage</li><li>instrumentation must be optional, yet easy to activate</li><li>avoid methods that require to keep several files in sync like multiple
<code>_oasis</code> files</li><li>avoid separate Git branches for instrumented and non-instrumented
code</li></ul><p>In the ideal case, we could introduce a configuration switch
<code>./configure --enable-coverage</code> that would prepare compilation for
coverage instrumentation. While <a href=http://oasis.forge.ocamlcore.org target=_blank>Oasis</a> supports the creation of such
switches, they cannot be used to control build dependencies like
compiling a file with or without package <code>bisec_ppx</code>. We have chosen a
different method:</p><p>A <code>Makefile</code> target <code>coverage</code> augments the <code>_tags</code> file to include the
rules in file <code>_tags.coverage</code> that cause files to be instrumented:</p><pre><code>make coverage # prepare
make          # build
</code></pre><p>leads to the execution of this code during preparation:</p><pre><code>coverage: _tags _tags.coverage 
  test ! -f _tags.orig &amp;&amp; mv _tags _tags.orig || true
  cat _tags.coverage _tags.orig &gt; _tags
</code></pre><p>The file <code>_tags.coverage</code> contains two simple <a href=https://github.com/ocaml/ocamlbuild/blob/master/manual/manual.adoc target=_blank>OCamlbuild</a> rules that
could be tweaked to instrument only some files:</p><pre><code>&lt;**/*.ml{,i,y}&gt;: pkg_bisect_ppx
&lt;**/*.native&gt;:   pkg_bisect_ppx
</code></pre><p>When <code>make coverage</code> is not called, these rules are not active and
hence, code is not instrumented for coverage. We believe that this
solution to control instrumentation meets the goals from above. In
particular, what files are instrumented and when is controlled by very
few lines of declarative code that lives in the main repository of a
project.</p><h2 id=project-layout>Project Layout</h2><p>The crucial files in an <a href=http://oasis.forge.ocamlcore.org target=_blank>Oasis</a>-controlled project that is set up for
coverage analysis are:</p><pre><code>./_oasis                      - make &quot;profiling&quot; a build depdency
./_tags.coverage              - what files get instrumented
./profiling/coverage.ml       - support file, sets env var
./Makefile                    - target 'coverage'
</code></pre><p>The <code>_oasis</code> file bundles the files under <code>profiling/</code> into an internal
library which executables then depend on:</p><pre><code>	# Support files for profiling 
	Library profiling
		CompiledObject:     best
		Path:               profiling
		Install:            false
		Findlibname:        profiling
		Modules:            Coverage
		BuildDepends: 

	Executable set_domain_uuid
		CompiledObject:     best
		Path:               tools
		ByteOpt:            -warn-error +a-3
		NativeOpt:          -warn-error +a-3
		MainIs:             set_domain_uuid.ml
		Install:            false
		BuildDepends:
			xenctrl, 
			uuidm, 
			cmdliner,
			profiling			# &lt;-- here
</code></pre><p>The <code>Makefile</code> target <code>coverage</code> primes the project for a profiling build:</p><pre><code>	# make coverage - prepares for building with coverage analysis

	coverage: _tags _tags.coverage 
		test ! -f _tags.orig &amp;&amp; mv _tags _tags.orig || true
		cat _tags.coverage _tags.orig &gt; _tags
</code></pre><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v7</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Add details about VM migration and import</td></tr><tr><td><span class="label label-default">v3</span></td><td>Included and excluded use cases</td></tr><tr><td><span class="label label-default">v4</span></td><td>Rolling Pool Upgrade use cases</td></tr><tr><td><span class="label label-default">v5</span></td><td>Lots of changes to simplify the design</td></tr><tr><td><span class="label label-default">v6</span></td><td>Use case refresh based on simplified design</td></tr><tr><td><span class="label label-default">v7</span></td><td>RPU refresh based on simplified design</td></tr></table></header><h1 id=cpu-feature-levelling-20>CPU feature levelling 2.0</h1><h1 id=executive-summary>Executive Summary</h1><p>The old XS 5.6-style Heterogeneous Pool feature that is based around hardware-level CPUID masking will be replaced by a safer and more flexible software-based levelling mechanism.</p><h1 id=history>History</h1><ul><li>Original XS 5.6 design: <a href=/new-docs/design/cpu-levelling-v2/../heterogeneous-pools>heterogeneous-pools</a></li><li>Changes made in XS 5.6 FP1 for the DR feature (added CPUID checks upon migration)</li><li>XS 6.1: migration checks extended for cross-pool scenario</li></ul><h1 id=high-level-interfaces-and-behaviour>High-level Interfaces and Behaviour</h1><p>A VM can only be migrated safely from one host to another if both hosts offer the set of CPU features which the VM expects. If this is not the case, CPU features may appear or disappear as the VM is migrated, causing it to crash. The purpose of feature levelling is to hide features which the hosts do not have in common from the VM, so that it does not see any change in CPU capabilities when it is migrated.</p><p>Most pools start off with homogenous hardware, but over time it may become impossible to source new hosts with the same specifications as the ones already in the pool. The main use of feature levelling is to allow such newer, more capable hosts to be added to an existing pool while preserving the ability to migrate existing VMs to any host in the pool.</p><h2 id=principles-for-migration>Principles for Migration</h2><p>The CPU levelling feature aims to both:</p><ol><li>Make VM migrations <em>safe</em> by ensuring that a VM will see the same CPU features before and after a migration.</li><li>Make VMs as <em>mobile</em> as possible, so that it can be freely migrated around in a XenServer pool.</li></ol><p>To make migrations safe:</p><ul><li>A migration request will be blocked if the destination host does not offer the some of the CPU features that the VM currently sees.</li><li>Any additional CPU features that the destination host is able to offer will be hidden from the VM.</li></ul><p><em>Note:</em> Due to the limitations of the old Heterogeneous Pools feature, we are not able to guarantee the safety of VMs that are migrated to a Levelling-v2 host from an older host, during a rolling pool upgrade. This is because such VMs may be using CPU features that were not captured in the old feature sets, of which we are therefore unaware. However, migrations between the same two hosts, but before the upgrade, may have already been unsafe. The promise is that we will not make migrations <em>more</em> unsafe during a rolling pool upgrade.</p><p>To make VMs mobile:</p><ul><li>A VM that is started in a XenServer pool will be able to see only CPU features that are common to all hosts in the pool. The set of common CPU features is referred to in this document as the <em>pool CPU feature level</em>, or simply the <em>pool level</em>.</li></ul><h2 id=use-cases-for-pools>Use Cases for Pools</h2><ol><li><p>A user wants to add a new host to an existing XenServer pool. The new host has all the features of the existing hosts, plus extra features which the existing hosts do not. The new host will be allowed to join the pool, but its extra features will be hidden from VMs that are started on the host or migrated to it. The join does not require any host reboots.</p></li><li><p>A user wants to add a new host to an existing XenServer pool. The new host does not have all the features of the existing ones. XenCenter warns the user that adding the host to the pool is possible, but it would lower the pool&rsquo;s CPU feature level. The user accepts this and continues the join. The join does not require any host reboots. VMs that are started anywhere on the pool, from now on, will only see the features of the new host (the lowest common denominator), such that they are migratable to any host in the pool, including the new one. VMs that were running before the pool join will not be migratable to the new host, because these VMs may be using features that the new host does not have. However, after a reboot, such VMs will be fully mobile.</p></li><li><p>A user wants to add a new host to an existing XenServer pool. The new host does not have all the features of the existing ones, and at the same time, it has certain features that the pool does not have (the feature sets overlap). This is essentially a combination of the two use cases above, where the pool&rsquo;s CPU feature level will be downgraded to the intersection of the feature sets of the pool and the new host. The join does not require any host reboots.</p></li><li><p>A user wants to upgrade or repair the hardware of a host in an existing XenServer pool. After upgrade the host has all the features it used to have, plus extra features which other hosts in the pool do not have. The extra features are masked out and the host resumes its place in the pool when it is booted up again.</p></li><li><p>A user wants to upgrade or repair the hardware of a host in an existing XenServer pool. After upgrade the host has fewer features than it used to have. When the host is booted up again, the pool CPU&rsquo;s feature level will be automatically lowered, and the user will be alerted of this fact (through the usual alerting mechanism).</p></li><li><p>A user wants to remove a host from an existing XenServer pool. The host will be removed as normal after any VMs on it have been migrated away. The feature set offered by the pool will be automatically re-levelled upwards in case the host which was removed was the least capable in the pool, and additional features common to the remaining hosts will be unmasked.</p></li></ol><h2 id=rolling-pool-upgrade>Rolling Pool Upgrade</h2><ul><li><p>A VM which was running on the pool before the upgrade is expected to continue to run afterwards. However, when the VM is migrated to an upgraded host, some of the CPU features it had been using might disappear, either because they are not offered by the host or because the new feature-levelling mechanism hides them. To have the best chance for such a VM to successfully migrate (see the note under &ldquo;Principles for Migration&rdquo;), it will be given a temporary VM-level feature set providing all of the destination&rsquo;s CPU features that were unknown to XenServer before the upgrade. When the VM is rebooted it will inherit the pool-level feature set.</p></li><li><p>A VM which is started during the upgrade will be given the current pool-level feature set. The pool-level feature set may drop after the VM is started, as more hosts are upgraded and re-join the pool, however the VM is guaranteed to be able to migrate to any host which has already been upgraded. If the VM is started on the master, there is a risk that it may only be able to run on that host.</p></li><li><p>To allow the VMs with grandfathered-in flags to be migrated around in the pool, the intra pool VM migration pre-checks will compare the VM&rsquo;s feature flags to the target host&rsquo;s flags, not the pool flags. This will maximise the chance that a VM can be migrated somewhere in a heterogeneous pool, particularly in the case where only a few hosts in the pool do not have features which the VMs require.</p></li><li><p>To allow cross-pool migration, including to pool of a higher XenServer version, we will still check the VM&rsquo;s requirements against the <em>pool-level</em> features of the target pool. This is to avoid the possibility that we migrate a VM to an &lsquo;island&rsquo; in the other pool, from which it cannot be migrated any further.</p></li></ul><h2 id=xenapi-changes>XenAPI Changes</h2><h3 id=fields>Fields</h3><ul><li><code>host.cpu_info</code> is a field of type <code>(string -> string) map</code> that contains information about the CPUs in a host. It contains the following keys: <code>cpu_count</code>, <code>socket_count</code>, <code>vendor</code>, <code>speed</code>, <code>modelname</code>, <code>family</code>, <code>model</code>, <code>stepping</code>, <code>flags</code>, <code>features</code>, <code>features_after_reboot</code>, <code>physical_features</code> and <code>maskable</code>.<ul><li>The following keys are specific to hardware-based CPU masking and will be removed: <code>features_after_reboot</code>, <code>physical_features</code> and <code>maskable</code>.</li><li>The <code>features</code> key will continue to hold the current CPU features that the host is able to use. In practise, these features will be available to Xen itself and dom0; guests may only see a subset. The current format is a string of four 32-bit words represented as four groups of 8 hexadecimal digits, separated by dashes. This will change to an arbitrary number of 32-bit words. Each bit at a particular position (starting from the left) still refers to a distinct CPU feature (<code>1</code>: feature is present; <code>0</code>: feature is absent), and feature strings may be compared between hosts. The old format simply becomes a special (4 word) case of the new format, and bits in the same position may be compared between old and new feature strings.</li><li>The new key <code>features_pv</code> will be added, representing the subset of <code>features</code> that the host is able to offer to a PV guest.</li><li>The new key <code>features_hvm</code> will be added, representing the subset of <code>features</code> that the host is able to offer to an HVM guest.</li></ul></li><li>A new field <code>pool.cpu_info</code> of type <code>(string -> string) map</code> (read only) will be added. It will contain:<ul><li><code>vendor</code>: The common CPU vendor across all hosts in the pool.</li><li><code>features_pv</code>: The intersection of <code>features_pv</code> across all hosts in the pool, representing the feature set that a PV guest will see when started on the pool.</li><li><code>features_hvm</code>: The intersection of <code>features_hvm</code> across all hosts in the pool, representing the feature set that an HVM guest will see when started on the pool.</li><li><code>cpu_count</code>: the total number of CPU cores in the pool.</li><li><code>socket_count</code>: the total number of CPU sockets in the pool.</li></ul></li><li>The <code>pool.other_config:cpuid_feature_mask</code> override key will no longer have any effect on pool join or VM migration.</li><li>The field <code>VM.last_boot_CPU_flags</code> will be updated to the new format (see <code>host.cpu_info:features</code>). It will still contain the feature set that the VM was started with as well as the vendor (under the <code>features</code> and <code>vendor</code> keys respectively).</li></ul><h3 id=messages>Messages</h3><ul><li><code>pool.join</code> currently requires that the CPU vendor and feature set (according to <code>host.cpu_info:vendor</code> and <code>host.cpu_info:features</code>) of the joining host are equal to those of the pool master. This requirement will be loosened to mandate only equality in CPU vendor:<ul><li>The join will be allowed if <code>host.cpu_info:vendor</code> equals <code>pool.cpu_info:vendor</code>.</li><li>This means that xapi will additionally allow hosts that have a <em>more</em> extensive feature set than the pool (as long as the CPU vendor is common). Such hosts are transparently down-levelled to the pool level (without needing reboots).</li><li>This further means that xapi will additionally allow hosts that have a <em>less</em> extensive feature set than the pool (as long as the CPU vendor is common). In this case, the pool is transparently down-levelled to the new host&rsquo;s level (without needing reboots). Note that this does not affect any running VMs in any way; the mobility of running VMs will not be restricted, which can still migrate to any host they could migrate to before. It does mean that those running VMs will not be migratable to the new host.</li><li>The current error raised in case of a CPU mismatch is <code>POOL_HOSTS_NOT_HOMOGENEOUS</code> with <code>reason</code> argument <code>"CPUs differ"</code>. This will remain the error that is raised if the pool join fails due to incompatible CPU vendors.</li><li>The <code>pool.other_config:cpuid_feature_mask</code> override key will no longer have any effect.</li></ul></li><li><code>host.set_cpu_features</code> and <code>host.reset_cpu_features</code> will be removed: it is no longer to use the old method of CPU feature masking (CPU feature sets are controlled automatically by xapi). Calls will fail with <code>MESSAGE_REMOVED</code>.</li><li>VM lifecycle operations will be updated internally to use the new feature fields, to ensure that:<ul><li>Newly started VMs will be given CPU features according to the pool level for maximal mobility.</li><li>For safety, running VMs will maintain their feature set across migrations and suspend/resume cycles. CPU features will transparently be hidden from VMs.</li><li>Furthermore, migrate and resume will only be allowed in case the target host&rsquo;s CPUs are capable enough, i.e. <code>host.cpu_info:vendor</code> = <code>VM.last_boot_CPU_flags:vendor</code> and <code>host.cpu_info:features_{pv,hvm}</code> ⊇ <code>VM.last_boot_CPU_flags:features</code>. A <code>VM_INCOMPATIBLE_WITH_THIS_HOST</code> error will be returned otherwise (as happens today).</li><li>For cross pool migrations, to ensure maximal mobility in the target pool, a stricter condition will apply: the VM must satisfy the pool CPU level rather than just the target host&rsquo;s level: <code>pool.cpu_info:vendor</code> = <code>VM.last_boot_CPU_flags:vendor</code> and <code>pool.cpu_info:features_{pv,hvm}</code> ⊇ <code>VM.last_boot_CPU_flags:features</code></li></ul></li></ul><h2 id=cli-changes>CLI Changes</h2><p>The following changes to the <code>xe</code> CLI will be made:</p><ul><li><code>xe host-cpu-info</code> (as well as <code>xe host-param-list</code> and friends) will return the fields of <code>host.cpu_info</code> as described above.</li><li><code>xe host-set-cpu-features</code> and <code>xe host-reset-cpu-features</code> will be removed.</li><li><code>xe host-get-cpu-features</code> will still return the value of <code>host.cpu_info:features</code> for a given host.</li></ul><h1 id=low-level-implementation>Low-level implementation</h1><h2 id=xenctrl>Xenctrl</h2><p>The old <code>xc_get_boot_cpufeatures</code> hypercall will be removed, and replaced by two new functions, which are available to xenopsd through the Xenctrl module:</p><pre><code>external get_levelling_caps : handle -&gt; int64 = &quot;stub_xc_get_levelling_caps&quot;

type featureset_index = Featureset_host | Featureset_pv | Featureset_hvm
external get_featureset : handle -&gt; featureset_index -&gt; int64 array = &quot;stub_xc_get_featureset&quot;
</code></pre><p>In particular, the <code>get_featureset</code> function will be used by xapi/xenopsd to ask Xen which are the widest sets of CPU features that it can offer to a VM (PV or HVM). I don&rsquo;t think there is a use for <code>get_levelling_caps</code> yet.</p><h2 id=xenopsd>Xenopsd</h2><ul><li>Update the type <code>Host.cpu_info</code>, which contains all the fields that need to go into the <code>host.cpu_info</code> field in the xapi DB. The type already exists but is unused. Add the function <code>HOST.get_cpu_info</code> to obtain an instance of the type. Some code from xapi and the cpuid.ml from xen-api-libs can be reused.</li><li>Add a platform key <code>featureset</code> (<code>Vm.t.platformdata</code>), which xenopsd will write to xenstore along with the other platform keys (no code change needed in xenopsd). Xenguest will pick this up when a domain is created, and will apply the CPUID policy to the domain. This has the effect of masking out features that the host may have, but which have a <code>0</code> in the feature set bitmap.</li><li>Review current cpuid-related functions in <code>xc/domain.ml</code>.</li></ul><h2 id=xapi>Xapi</h2><h3 id=xapi-startup>Xapi startup</h3><ul><li>Update <code>Create_misc.create_host_cpu</code> function to use the new xenopsd call.</li><li>If the host features fall below pool level, e.g. due to a change in hardware: down-level the pool by updating <code>pool.cpu_info.features_{pv,hvm}</code>. Newly started VMs will inherit the new level; already running VMs will not be affected, but will not be able to migrate to this host.</li><li>To notify the admin of this event, an API alert (message) will be set: <code>pool_cpu_features_downgraded</code>.</li></ul><h3 id=vm-start>VM start</h3><ul><li>Inherit feature set from pool (<code>pool.cpu_info.features_{pv,hvm}</code>) and set <code>VM.last_boot_CPU_flags</code> (<code>cpuid_helpers.ml</code>).</li><li>The domain will be started with this CPU feature set enabled, by writing the feature set string to <code>platformdata</code> (see above).</li></ul><h3 id=vm-migrate-and-resume>VM migrate and resume</h3><ul><li>There are already CPU compatiblity checks on migration, both in-pool and cross-pool, as well as resume. Xapi compares <code>VM.last_boot_CPU_flags</code> of the VM to-migrate with <code>host.cpu_info</code> of the receiving host. Migration is only allowed if the CPU vendors and the same, and <code>host.cpu_info:features</code> ⊇ <code>VM.last_boot_CPU_flags:features</code>. The check can be overridden by setting the <code>force</code> argument to <code>true</code>.</li><li>For in-pool migrations, these checks will be updated to use the appropriate <code>features_pv</code> or <code>features_hvm</code> field.</li><li>For cross-pool migrations. These checks will be updated to use <code>pool.cpu_info</code> (<code>features_pv</code> or <code>features_hvm</code> depending on how the VM was booted) rather than <code>host.cpu_info</code>.</li><li>If the above checks pass, then the <code>VM.last_boot_CPU_flags</code> will be maintained, and the new domain will be started with the same CPU feature set enabled, by writing the feature set string to <code>platformdata</code> (see above).</li><li>In case the VM is migrated to a host with a higher xapi software version (e.g. a migration from a host that does not have CPU levelling v2), the feature string may be longer. This may happen during a rolling pool upgrade or a cross-pool migration, or when a suspended VM is resume after an upgrade. In this case, the following safety rules apply:<ul><li>Only the existing (shorter) feature string will be used to determine whether the migration will be allowed. This is the best we can do, because we are unaware of the state of the extended feature set on the older host.</li><li>The existing feature set in <code>VM.last_boot_CPU_flags</code> will be extended with the extra bits in <code>host.cpu_info:features_{pv,hvm}</code>, i.e. the widest feature set that can possibly be granted to the VM (just in case the VM was using any of these features before the migration).</li><li>Strictly speaking, a migration of a VM from host A to B that was allowed before B was upgraded, may no longer be allowed after the upgrade, due to stricter feature sets in the new implementation (from the <code>xc_get_featureset</code> hypercall). However, the CPU features that are switched off by the new implementation are features that a VM would not have been able to actually use. We therefore need a don&rsquo;t-care feature set (similar to the old <code>pool.other_config:cpuid_feature_mask</code> key) with bits that we may ignore in migration checks, and switch off after the migration. This will be a xapi config file option.</li><li>XXX: Can we actually block a cross-pool migration at the receiver end??</li></ul></li></ul><h3 id=vm-import>VM import</h3><p>The <code>VM.last_boot_CPU_flags</code> field must be upgraded to the new format (only really needed for VMs that were suspended while exported; <code>preserve_power_state=true</code>), as described above.</p><h3 id=pool-join>Pool join</h3><p>Update pool join checks according to the rules above (see <code>pool.join</code>), i.e. remove the CPU features constraints.</p><h3 id=upgrade>Upgrade</h3><ul><li>The pool level (<code>pool.cpu_info</code>) will be initialised when the pool master upgrades, and automatically adjusted if needed (downwards) when slaves are upgraded, by each upgraded host&rsquo;s started sequence (as above under &ldquo;Xapi startup&rdquo;).</li><li>The <code>VM.last_boot_CPU_flags</code> fields of running and suspended VMs will be &ldquo;upgraded&rdquo; to the new format on demand, when a VM is migrated to or resume on an upgraded host, as described above.</li></ul><h2 id=xencenter-integration>XenCenter integration</h2><ul><li>Don&rsquo;t explicitly down-level upon join anymore</li><li>Become aware of new pool join rule</li><li>Update Rolling Pool Upgrade</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=distributed-database>Distributed database</h1><p>All hosts in a pool use the shared database by sending queries to
the pool master. This creates</p><ul><li>a performance bottleneck as the pool size increases</li><li>a reliability problem when the master fails.</li></ul><p>The reliability problem can be ameliorated by running with HA enabled,
but this is not always possible.</p><p>Both problems can be addressed by observing that the database objects
correspond to distinct physical objects where eventual consistency is
perfectly ok. For example if host &lsquo;A&rsquo; is running a VM and changes the
VM&rsquo;s name, it doesn&rsquo;t matter if it takes a while before the change shows
up on host &lsquo;B&rsquo;. If host &lsquo;B&rsquo; changes its network configuration then it
doesn&rsquo;t matter how long it takes host &lsquo;A&rsquo; to notice. We would still like
the metadata to be replicated to cope with failure, but we can allow
changes to be committed locally and synchronised later.</p><p>Note the one exception to this pattern: the current SM plugins use database
fields to implement locks. This should be shifted to a special-purpose
lock acquire/release API.</p><h2 id=using-git-via-irmin>Using git via Irmin</h2><p>A git repository is a database of key=value pairs with branching history.
If we placed our host and VM metadata in git then we could <code>commit</code>
changes and <code>pull</code> and <code>push</code> them between replicas. The
<a href=https://github.com/mirage/irmin target=_blank>Irmin</a> library provides an easy programming
interface on top of git which we could link with the Xapi database layer.</p><h2 id=proposed-new-architecture>Proposed new architecture</h2><p><a href=#image-6b4aad96383c5690190c558dde7a3141 class=lightbox-link><img src=/new-docs/design/distributed-database/architecture.png alt="Pools of one" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-6b4aad96383c5690190c558dde7a3141><img src=/new-docs/design/distributed-database/architecture.png alt="Pools of one" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The diagram above shows two hosts: one a master and the other a regular host.
The XenAPI client has sent a request to the wrong host; normally this would
result in a <code>HOST_IS_SLAVE</code> error being sent to the client. In the new
world, the host is able to process the request, only contacting the master
if it is necessary to acquire a lock. Starting a VM would require a lock; but
rebooting or migrating an existing VM would not. Assuming the lock can
be acquired, then the operation is executed locally with all state updates
being made to a git topic branch.</p><p><a href=#image-a34b400b14022fef927554fb0d4b39f7 class=lightbox-link><img src=/new-docs/design/distributed-database/topic.png alt="Topic branches" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-a34b400b14022fef927554fb0d4b39f7><img src=/new-docs/design/distributed-database/topic.png alt="Topic branches" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Roughly we would have 1 topic branch per
pending XenAPI Task. Once the Task completes successfully, the topic branch
(containing the new VM state) is merged back into master.
Separately each
host will pull and push updates between each other for replication.</p><p>We would avoid merge conflicts by construction; either</p><ul><li>a host&rsquo;s configuration will always be &ldquo;owned&rdquo; by the host and it will be
an error for anyone else to merge updates to it</li><li>the master&rsquo;s locking will guarantee that a VM is running on at most one
host at a time. It will be an error for anyone else to merge updates to it.</li></ul><h2 id=what-we-gain>What we gain</h2><p>We will gain the following</p><ul><li>the master will only be a bottleneck when the number of VM locks gets
really large;</li><li>you will be able to connect XenCenter to hosts without a master and manage
them. Today such hosts are unmanageable.</li><li>the database will have a history and you&rsquo;ll be able to &ldquo;go back in time&rdquo;
either for debugging or to recover from mistakes</li><li>bugs caused by concurrent threads (in separate Tasks) confusing each other
will be vanquished. A typical failure mode is: one active thread destroys
an object; a passive thread sees the object and then tries to read it
and gets a database failure instead. Since every thread is operating a
separate Task they will all have their own branch and will be isolated from
each other.</li></ul><h2 id=what-we-lose>What we lose</h2><p>We will lose the following</p><ul><li>the ability to use the Xapi database as a &ldquo;lock&rdquo;</li><li>coherence between hosts: there will be no guarantee that an effect seen
by host &lsquo;A&rsquo; will be seen immediately by host &lsquo;B&rsquo;. In particular this means
that clients should send all their commands and <code>event.from</code> calls to
the same host (although any host will do)</li></ul><h2 id=stuff-we-need-to-build>Stuff we need to build</h2><ul><li><p>A <code>pull</code>/<code>push</code> replicator: this would have to monitor the list
of hosts in the pool and distribute updates to them in some vaguely
efficient manner. Ideally we would avoid hassling the pool master and
use some more efficient topology: perhaps a tree?</p></li><li><p>A <code>git diff</code> to XenAPI event converter: whenever a host <code>pull</code>s
updates from another it needs to convert the diff into a set of touched
objects for any <code>event.from</code> to read. We could send the changeset hash
as the <code>event.from</code> token.</p></li><li><p>Irmin nested views: since Tasks can be nested (and git branches can be
nested) we need to make sure that Irmin views can be nested.</p></li><li><p>We need to go through the xapi code and convert all mixtures of database
access and XenAPI updates into pure database calls. With the previous system
it was better to use a XenAPI to remote large chunks of database effects to
the master than to perform them locally. It will now be better to run them
all locally and merge them at the end. Additionally since a Task will have
a local branch, it won&rsquo;t be possible to see the state on a remote host
without triggering an early merge (which would harm efficiency)</p></li><li><p>We need to create a first-class locking API to use instead of the
<code>VDI.sm_config</code> locks.</p></li></ul><h2 id=prototype>Prototype</h2><p>A basic prototype has been created:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ opam pin xen-api-client git://github.com/djs55/xen-api-client#improvements
</span></span><span style=display:flex><span>$ opam pin add xapi-database git://github.com/djs55/xapi-database
</span></span><span style=display:flex><span>$ opam pin add xapi git://github.com/djs55/xen-api#schema-sexp</span></span></code></pre></div><p>The <code>xapi-database</code> is clone of the existing Xapi database code
configured to run as a separate process. There is
<a href=https://github.com/djs55/xapi-database/blob/master/core/db_git.ml#L55 target=_blank>code to convert from XML to git</a>
and
<a href=https://github.com/djs55/xapi-database/blob/master/core/db_git.ml#L186 target=_blank>an implementation of the Xapi remote database API</a>
which uses the following layout:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ git clone /xapi.db db
</span></span><span style=display:flex><span>Cloning into <span style=color:#e6db74>&#39;db&#39;</span>...
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ cd db; ls
</span></span><span style=display:flex><span>xapi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi
</span></span><span style=display:flex><span>console   host_metrics  PCI          pool     SR      user  VM
</span></span><span style=display:flex><span>host      network       PIF          session  tables  VBD   VM_metrics
</span></span><span style=display:flex><span>host_cpu  PBD           PIF_metrics  SM       task    VDI
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi/pool
</span></span><span style=display:flex><span>OpaqueRef:39adc911-0c32-9e13-91a8-43a25939110b
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi/pool/OpaqueRef<span style=color:#ae81ff>\:</span>39adc911-0c32-9e13-91a8-43a25939110b/
</span></span><span style=display:flex><span>crash_dump_SR                 __mtime           suspend_image_SR
</span></span><span style=display:flex><span>__ctime                       name_description  uuid
</span></span><span style=display:flex><span>default_SR                    name_label        vswitch_controller
</span></span><span style=display:flex><span>ha_allow_overcommit           other_config      wlb_enabled
</span></span><span style=display:flex><span>ha_enabled                    redo_log_enabled  wlb_password
</span></span><span style=display:flex><span>ha_host_failures_to_tolerate  redo_log_vdi      wlb_url
</span></span><span style=display:flex><span>ha_overcommitted              ref               wlb_username
</span></span><span style=display:flex><span>ha_plan_exists_for            _ref              wlb_verify_cert
</span></span><span style=display:flex><span>master                        restrictions
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi/pool/OpaqueRef<span style=color:#ae81ff>\:</span>39adc911-0c32-9e13-91a8-43a25939110b/other_config/
</span></span><span style=display:flex><span>cpuid_feature_mask  memory-ratio-hvm  memory-ratio-pv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ cat xapi/pool/OpaqueRef<span style=color:#ae81ff>\:</span>39adc911-0c32-9e13-91a8-43a25939110b/other_config/cpuid_feature_mask
</span></span><span style=display:flex><span>ffffff7f-ffffffff-ffffffff-ffffffff</span></span></code></pre></div><p>Notice how:</p><ul><li>every object is a directory</li><li>every key/value pair is represented as a file</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.0.2)</span></td></tr></table></header><h1 id=emergency-network-reset-design>Emergency Network Reset Design</h1><p>This document describes design details for the PR-1032 requirements.</p><p>The design consists of four parts:</p><ol><li>A new XenAPI call <code>Host.reset_networking</code>, which removes all the
PIFs, Bonds, VLANs and tunnels associated with the given host, and a
call <code>PIF.scan_bios</code> to bring back the PIFs with device names as
defined in the BIOS.</li><li>A <code>xe-reset-networking</code> script that can be executed on a XenServer
host, which prepares the reset and causes the host to reboot.</li><li>An xsconsole page that essentially does the same as
<code>xe-reset-networking</code>.</li><li>A new item in the XAPI start-up sequence, which when triggered by
<code>xe-reset-networking</code>, calls <code>Host.reset_networking</code> and re-creates
the PIFs.</li></ol><h2 id=command-line-utility>Command-Line Utility</h2><p>The <code>xe-reset-networking</code> script takes the following parameters:</p><p>DNS server for management interface. Optional; ignored if <code>--mode=dhcp</code>.</p><p>The script takes the following steps after processing the given
parameters:</p><ol><li>Inform the user that the host will be restarted, and that any
running VMs should be shut down. Make the user confirm that they
really want to reset the networking by typing &lsquo;yes&rsquo;.</li><li>Read <code>/etc/xensource/pool.conf</code> to determine whether the host is a
pool master or pool slave.</li><li>If a pool slave, update the IP address in the <code>pool.conf</code> file to
the one given in the <code>-m</code> parameter, if present.</li><li>Shut down networking subsystem (<code>service network stop</code>).</li><li>If no management device is specified, take it from
/etc/firstboot.d/data/management.conf.</li><li>If XAPI is running, stop it.</li><li>Reconfigure the management interface and associated bridge by
<code>interface-reconfigure --force</code>.</li><li>Update <code>MANAGEMENT_INTERFACE</code> and clear <code>CURRENT_INTERFACES</code> in
<code>/etc/xensource-inventory</code>.</li><li>Create the file <code>/tmp/network-reset</code> to trigger XAPI to complete the
network reset after the reboot. This file should contain the full
configuration details of the management interface as key/value pairs
(format: <code>&lt;key>=&lt;value>\n</code>), and looks similar to the firstboot data
files. The file contains at least the keys <code>DEVICE</code> and <code>MODE</code>, and
<code>IP</code>, <code>NETMASK</code>, <code>GATEWAY</code>, or <code>DNS</code> when appropriate.</li><li>Reboot</li></ol><h2 id=xapi>XAPI</h2><h3 id=xenapi>XenAPI</h3><p>A new <em>hidden</em> API call:</p><ul><li><code>Host.reset_networking</code><ul><li>Parameter: host reference <code>host</code></li><li>Calling this function removes all the PIF, Bond, VLAN and tunnel
objects associated with the given host from the master database.
All Network and VIF objects are maintained, as these do not
necessarily belong to a single host.</li></ul></li></ul><h3 id=start-up-sequence>Start-up Sequence</h3><p>After reboot, in the XAPI start-up sequence trigged by the presence of
<code>/tmp/network-reset</code>:</p><ol><li>Read the desired management configuration from <code>/tmp/network-reset</code>.</li><li>Call <code>Host.reset_networking</code> with a ref to the localhost.</li><li>Call <code>PIF.scan</code> with a ref to the localhost to recreate the
(physical) PIFs.</li><li>Call <code>PIF.reconfigure_ip</code> to configure the management interface.</li><li>Call <code>Host.management_reconfigure</code>.</li><li>Delete <code>/tmp/network-reset</code>.</li></ol><h2 id=xsconsole>xsconsole</h2><p>Add an &ldquo;Emergency Network Reset&rdquo; option under the &ldquo;Network and
Management Interface&rdquo; menu. Selecting this option will show some
explanation in the pane on the right-hand side. Pressing &lt;Enter> will
bring up a dialogue to select the interfaces to use as management
interface after the reset. After choosing a device, the dialogue
continues with configuration options like in the &ldquo;Configure Management
Interface&rdquo; dialogue. After completing the dialogue, the same steps as
listed for <code>xe-reset-networking</code> are executed.</p><h2 id=notes>Notes</h2><ul><li>On a pool slave, the management interface should be the same as on
the master (the same device name, e.g. eth0).</li><li>Resetting the networking configuration on the master should be
ideally be followed by resets of the pool slaves as well, in order
to synchronise their configuration (especially bonds/VLANs/tunnels).
Furthermore, in case the IP address of the master has changed, as a
result of a network reset or <code>Host.management_reconfigure</code>, pool
slaves may also use the network reset functionality to reconnect to
the master on its new IP.</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/120>#120</a></td></tr></table></header><h1 id=fcoe-capable-nics>FCoE capable NICs</h1><p>It has been possible to identify the NICs of a Host which can support FCoE.
This property can be listed in PIF object under capabilities field.</p><h2 id=introduction>Introduction</h2><ul><li>FCoE supported on a NIC is a hardware property. With the help of dcbtool, we can identify which NIC support FCoE.</li><li>The new field capabilities will be <code>Set(String)</code> in PIF object. For FCoE capable NIC will have string &ldquo;fcoe&rdquo; in PIF capabilities field.</li><li><code>capabilities</code> field will be ReadOnly, This field cannot be modified by user.</li></ul><h2 id=pif-object>PIF Object</h2><p>New field:</p><ul><li>Field <code>PIF.capabilities</code> will be type <code>Set(string)</code>.</li><li>Default value in PIF capabilities will have an empty set.</li></ul><h2 id=xapi-changes>Xapi Changes</h2><ul><li>Set the field capabilities &ldquo;fcoe&rdquo; depending on output of xcp-networkd call <code>get_capabilities</code>.</li><li>Field capabilities &ldquo;fcoe&rdquo; can be set during <code>introduce_internal</code> on when creating a PIF.</li><li>Field capabilities &ldquo;fcoe&rdquo; can be updated during <code>refresh_all</code> on xapi startup.</li><li>The above field will be set everytime when xapi-restart.</li></ul><h2 id=xcp-networkd-changes>XCP-Networkd Changes</h2><p>New function:</p><ul><li>String list <code>string list get_capabilties (string)</code></li><li>Argument: device_name for the PIF.</li><li>This function calls method <code>capable</code> exposed by <code>fcoe_driver.py</code> as part of dom0.</li><li>It returns string list [&ldquo;fcoe&rdquo;] or [] depending on <code>capable</code> method output.</li></ul><h2 id=defaults-installation-and-upgrade>Defaults, Installation and Upgrade</h2><ul><li>Any newly introduced PIF will have its capabilities field as empty set until <code>fcoe_driver</code> method <code>capable</code> states FCoE is supported on the NIC.</li><li>It includes PIFs obtained after a fresh install of Xenserver, as well as PIFs created using <code>PIF.introduce</code> then <code>PIF.scan</code>.</li><li>During an upgrade Xapi Restart will call <code>refresh_all</code> which then populate the capabilities field as empty set.</li></ul><h2 id=command-line-interface>Command Line Interface</h2><ul><li>The <code>PIF.capabilities</code> field is exposed through <code>xe pif-list</code> and <code>xe pif-param-list</code> as usual.</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.0)</span></td></tr></table></header><h1 id=gpu-pass-through-support>GPU pass-through support</h1><p>This document contains the software design for GPU pass-through. This
code was originally included in the version of Xapi used in XenServer 6.0.</p><h2 id=overview>Overview</h2><p>Rather than modelling GPU pass-through from a PCI perspective, and
having the user manipulate PCI devices directly, we are taking a
higher-level view by introducing a dedicated graphics model. The
graphics model is similar to the networking and storage model, in which
virtual and physical devices are linked through an intermediate
abstraction layer (e.g. the &ldquo;Network&rdquo; class in the networking model).</p><p>The basic graphics model is as follows:</p><ul><li>A host owns a number of physical GPU devices (<em>pGPUs</em>), each of
which is available for passing through to a VM.</li><li>A VM may have a virtual GPU device (<em>vGPU</em>), which means it expects
to have access to a GPU when it is running.</li><li>Identical pGPUs are grouped across a resource pool in <em>GPU groups</em>.
GPU groups are automatically created and maintained by XS.</li><li>A GPU group connects vGPUs to pGPUs in the same way as VIFs are
connected to PIFs by Network objects: for a VM <em>v</em> having a vGPU on
GPU group <em>p</em> to run on host <em>h</em>, host <em>h</em> must have a pGPU in GPU
group <em>p</em> and pass it through to VM <em>v</em>.</li><li>VM start and non-live migration rules are analogous to the network
API and follow the above rules.</li><li>In case a VM that has a vGPU is started, while no pGPU available, an
exception will occur and the VM won&rsquo;t start. As a result, in order
to guarantee that a VM always has access to a pGPU, the number of
vGPUs should not exceed the number of pGPUs in a GPU group.</li></ul><p>Currently, the following restrictions apply:</p><ul><li>Hotplug is not supported.</li><li>Suspend/resume and checkpointing (memory snapshots) are not
supported.</li><li>Live migration (XenMotion) is not supported.</li><li>No more than one GPU per VM will be supported.</li><li>Only Windows guests will be supported.</li></ul><h2 id=xenapi-changes>XenAPI Changes</h2><p>The design introduces a new generic class called <em>PCI</em> to capture state
and information about relevant PCI devices in a host. By default, xapi
would not create PCI objects for all PCI devices, but only for the ones
that are managed and configured by xapi; currently only GPU devices.</p><p>The PCI class has no fields specific to the type of the PCI device (e.g.
a graphics card or NIC). Instead, device specific objects will contain a
link to their underlying PCI device&rsquo;s object.</p><p>The new XenAPI classes and changes to existing classes are detailed
below.</p><h3 id=pci-class>PCI class</h3><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>class_id</td><td>string</td><td>PCI class ID (hidden field)</td></tr><tr><td>class_name</td><td>string</td><td>PCI class name (GPU, NIC, &mldr;)</td></tr><tr><td>vendor_id</td><td>string</td><td>Vendor ID (hidden field).</td></tr><tr><td>vendor_name</td><td>string</td><td>Vendor name.</td></tr><tr><td>device_id</td><td>string</td><td>Device ID (hidden field).</td></tr><tr><td>device_name</td><td>string</td><td>Device name.</td></tr><tr><td>host</td><td>host ref</td><td>The host that owns the PCI device.</td></tr><tr><td>pci_id</td><td>string</td><td>BDF (domain/Bus/Device/Function identifier) of the (physical) PCI function, e.g. &ldquo;0000:00:1a.1&rdquo;. The format is hhhh:hh:hh.h, where h is a hexadecimal digit.</td></tr><tr><td>functions</td><td>int</td><td>Number of (physical + virtual) functions; currently fixed at 1 (hidden field).</td></tr><tr><td>attached_VMs</td><td>VM ref set</td><td>List of VMs that have this PCI device &ldquo;currently attached&rdquo;, i.e. plugged, i.e. passed-through to (hidden field).</td></tr><tr><td>dependencies</td><td>PCI ref set</td><td>List of dependent PCI devices: all of these need to be passed-thru to the same VM (co-location).</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr></tbody></table><p><em>Hidden fields</em> are only for use by xapi internally, and not visible to
XenAPI users.</p><p>Messages: none.</p><h3 id=pgpu-class>PGPU class</h3><p>A physical GPU device (pGPU).</p><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>PCI</td><td>PCI ref</td><td>Link to the underlying PCI device.</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr><tr><td>host</td><td>host ref</td><td>The host that owns the GPU.</td></tr><tr><td>GPU_group</td><td>GPU_group ref</td><td>GPU group the pGPU is contained in. Can be Null.</td></tr></tbody></table><p>Messages: none.</p><h3 id=gpu_group-class>GPU_group class</h3><p>A group of identical GPUs across hosts. A VM that is associated with a
GPU group can use any of the GPUs in the group. A VM does not need to
install new GPU drivers if moving from one GPU to another one in the
same GPU group.</p><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>VGPUs</td><td>VGPU ref set</td><td>List of vGPUs in the group.</td></tr><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>PGPUs</td><td>PGPU ref set</td><td>List of pGPUs in the group.</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr><tr><td>name_label</td><td>string</td><td>A human-readable name.</td></tr><tr><td>name_description</td><td>string</td><td>A notes field containing human-readable description.</td></tr><tr><td>GPU_types</td><td>string set</td><td>List of GPU types (vendor+device ID) that can be in this group (hidden field).</td></tr></tbody></table><p>Messages: none.</p><h3 id=vgpu-class>VGPU class</h3><p>A virtual GPU device (vGPU).</p><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>VM</td><td>VM ref</td><td>VM that owns the vGPU.</td></tr><tr><td>GPU_group</td><td>GPU_group ref</td><td>GPU group the vGPU is contained in.</td></tr><tr><td>currently_attached</td><td>bool</td><td>Reflects whether the virtual device is currently &ldquo;connected&rdquo; to a physical device.</td></tr><tr><td>device</td><td>string</td><td>Order in which the devices are plugged into the VM. Restricted to &ldquo;0&rdquo; for now.</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr></tbody></table><p>Messages:</p><table><thead><tr><th>Prototype</th><th>Description</th><th></th></tr></thead><tbody><tr><td>VGPU ref create (GPU_group ref, string, VM ref)</td><td>Manually assign the vGPU device to the VM given a device number, and link it to the given GPU group.</td><td></td></tr><tr><td>void destroy (VGPU ref)</td><td>Remove the association between the GPU group and the VM.</td><td></td></tr></tbody></table><p>It is possible to assign more vGPUs to a group than number number of
pGPUs in the group. When a VM is started, a pGPU must be available; if
not, the VM will not start. Therefore, to guarantee that a VM has access
to a pGPU at any time, one must manually enforce that the number of
vGPUs in a GPU group does not exceed the number of pGPUs. XenCenter
might display a warning, or simply refuse to assign a vGPU, if this
constraint is violated. This is analogous to the handling of memory
availability in a pool: a VM may not be able to start if there is no
host having enough free memory.</p><h3 id=vm-class>VM class</h3><p>Fields:</p><ul><li>Deprecate (unused) <code>PCI_bus</code> field</li><li>Add field <code>VGPU ref set VGPUs</code>: List of vGPUs.</li><li>Add field <code>PCI ref set attached_PCIs</code>: List of PCI devices that are
&ldquo;currently attached&rdquo; (plugged, passed-through) (<em>hidden field</em>).</li></ul><h3 id=host-class>host class</h3><p>Fields:</p><ul><li>Add field <code>PCI ref set PCIs</code>: List of PCI devices.</li><li>Add field <code>PGPU ref set PGPUs</code>: List of physical GPU devices.</li><li>Add field <code>(string -> string) map chipset_info</code>, which contains at
least the key <code>iommu</code>. If <code>"true"</code>, this key indicates whether the
host has IOMMU/VT-d support build in, <strong>and</strong> this functionality is
enabled by Xen; the value will be <code>"false"</code> otherwise.</li></ul><h2 id=initialisation-and-operations>Initialisation and Operations</h2><h3 id=enabling-iommuvt-d>Enabling IOMMU/VT-d</h3><p>(This may not be needed in Xen 4.1. Confirm with Simon.)</p><p>Provide a command that does this:</p><ul><li><code>/opt/xensource/libexec/xen-cmdline --set-xen iommu=1</code></li><li>reboot</li></ul><h3 id=xapi-startup>Xapi startup</h3><p>Definitions:</p><ul><li>PCI devices are matched on the combination of their <code>pci_id</code>,
<code>vendor_id</code>, and <code>device_id</code>.</li></ul><p>First boot and any subsequent xapi start:</p><ol><li><p>Find out from dmesg whether IOMMU support is present and enabled in
Xen, and set <code>host.chipset_info:iommu</code> accordingly.</p></li><li><p>Detect GPU devices currently present in the host. For each:</p><ol><li>If there is no matching PGPU object yet, create a PGPU object,
and add it to a GPU group containing identical PGPUs, or a new
group.</li><li>If there is no matching PCI object yet, create one, and also
create or update the PCI objects for dependent devices.</li></ol></li><li><p>Destroy all existing PCI objects of devices that are not currently
present in the host (i.e. objects for devices that have been
replaced or removed).</p></li><li><p>Destroy all existing PGPU objects of GPUs that are not currently
present in the host. Send a XenAPI alert to notify the user of this
fact.</p></li><li><p>Update the list of <code>dependencies</code> on all PCI objects.</p></li><li><p>Sync <code>VGPU.currently_attached</code> on all <code>VGPU</code> objects.</p></li></ol><h3 id=upgrade>Upgrade</h3><p>For any VMs that have <code>VM.other_config:pci</code> set to use a GPU, create an
appropriate vGPU, and remove the <code>other_config</code> option.</p><h3 id=generic-pci-interface>Generic PCI Interface</h3><p>A generic PCI interface exposed to higher-level code, such as the
networking and GPU management modules within Xapi. This functionality
relies on Xenops.</p><p>The PCI module exposes the following functions:</p><ul><li>Check whether a PCI device has free (unassigned) functions. This is
the case if the number of assignments in <code>PCI.attached_VMs</code> is
smaller than <code>PCI.functions</code>.</li><li>Plug a PCI function into a running VM.<ol><li>Raise exception if there are no free functions.</li><li>Plug PCI device, as well as dependent PCI devices. The PCI
module must also tell device-specific modules to update the
<code>currently_attached</code> field on dependent <code>VGPU</code> objects etc.</li><li>Update <code>PCI.attached_VMs</code>.</li></ol></li><li>Unplug a PCI function from a running VM.<ol><li>Raise exception if the PCI function is not owned by (passed
through to) the VM.</li><li>Unplug PCI device, as well as dependent PCI devices. The PCI
module must also tell device-specific modules to update the
<code>currently_attached</code> field on dependent <code>VGPU</code> objects etc.</li><li>Update <code>PCI.attached_VMs</code>.</li></ol></li></ul><h3 id=construction-and-destruction>Construction and Destruction</h3><p>VGPU.create:</p><ol><li>Check license. Raise FEATURE_RESTRICTED if the GPU feature has not
been enabled.</li><li>Raise INVALID_DEVICE if the given device number is not &ldquo;0&rdquo;, or
DEVICE_ALREADY_EXISTS if (indeed) the device already exists. This
is a convenient way of enforcing that only one vGPU per VM is
supported, for now.</li><li>Create <code>VGPU</code> object in the DB.</li><li>Initialise <code>VGPU.currently_attached = false</code>.</li><li>Return a ref to the new object.</li></ol><p>VGPU.destroy:</p><ol><li>Raise OPERATION_NOT_ALLOWED if <code>VGPU.currently_attached = true</code>
and the VM is running.</li><li>Destroy <code>VGPU</code> object.</li></ol><h3 id=vm-operations>VM Operations</h3><p>VM.start(_on):</p><ol><li>If <code>host.chipset_info:iommu = "false"</code>, raise VM_REQUIRES_IOMMU.</li><li>Raise FEATURE_REQUIRES_HVM (carrying the string &ldquo;GPU passthrough
needs HVM&rdquo;) if the VM is PV rather than HVM.</li><li>For each of the VM&rsquo;s vGPUs:<ol><li>Confirm that the given host has a pGPU in its associated GPU
group. If not, raise VM_REQUIRES_GPU.</li><li>Consult the generic PCI module for all pGPUs in the group to
find out whether a suitable PCI function is available. If a
physical device is not available, raise VM_REQUIRES_GPU.</li><li>Ask PCI module to plug an available pGPU into the VM&rsquo;s domain
and set <code>VGPU.currently_attached</code> to <code>true</code>. As a side-effect,
any dependent PCI devices would be plugged.</li></ol></li></ol><p>VM.shutdown:</p><ol><li>Ask PCI module to unplug all GPU devices.</li><li>Set <code>VGPU.currently_attached</code> to <code>false</code> for all the VM&rsquo;s VGPUs.</li></ol><p>VM.suspend, VM.resume(_on):</p><ul><li>Raise VM_HAS_PCI_ATTACHED if the VM has any plugged <code>VGPU</code>
objects, as suspend/resume for VMs with GPUs is currently not
supported.</li></ul><p>VM.pool_migrate:</p><ul><li>Raise VM_HAS_PCI_ATTACHED if the VM has any plugged <code>VGPU</code>
objects, as live migration for VMs with GPUs is currently not
supported.</li></ul><p>VM.clone, VM.copy, VM.snapshot:</p><ul><li>Copy <code>VGPU</code> objects along with the VM.</li></ul><p>VM.import, VM.export:</p><ul><li>Include <code>VGPU</code> and <code>GPU_group</code> objects in the VM export format.</li></ul><p>VM.checkpoint</p><ul><li>Raise VM_HAS_PCI_ATTACHED if the VM has any plugged <code>VGPU</code>
objects, as checkpointing for VMs with GPUs is currently not
supported.</li></ul><h3 id=pool-join-and-eject>Pool Join and Eject</h3><p>Pool join:</p><ol><li><p>For each <code>PGPU</code>:</p><ol><li>Copy it to the pool.</li><li>Add it to a <code>GPU_group</code> of identical PGPUs, or a new one.</li></ol></li><li><p>Copy each <code>VGPU</code> to the pool together with the VM that owns it, and
add it to the GPU group containing the same <code>PGPU</code> as before the
join.</p></li></ol><p>Step 1 is done automatically by the xapi startup code, and step 2 is
handled by the VM export/import code. Hence, no work needed.</p><p>Pool eject:</p><ol><li><code>VGPU</code> objects will be automatically GC&rsquo;ed when the VMs are removed.</li><li>Xapi&rsquo;s startup code recreates the <code>PGPU</code> and <code>GPU_group</code> objects.</li></ol><p>Hence, no work needed.</p><h2 id=required-low-level-interface>Required Low-level Interface</h2><p>Xapi needs a way to obtain a list of all PCI devices present on a host.
For each device, xapi needs to know:</p><ul><li>The PCI ID (BDF).</li><li>The type of device (NIC, GPU, &mldr;) according to a well-defined and
stable list of device types (as in <code>/usr/share/hwdata/pci.ids</code>).</li><li>The device and vendor ID+name (currently, for PIFs, xapi looks up
the name in <code>/usr/share/hwdata/pci.ids</code>).</li><li>Which other devices/functions are required to be passed through to
the same VM (co-located), e.g. other functions of a compound PCI
device.</li></ul><h2 id=command-line-interface-xe>Command-Line Interface (xe)</h2><ul><li>xe pgpu-list</li><li>xe pgpu-param-list/get/set/add/remove/clear</li><li>xe gpu-group-list</li><li>xe gpu-group-param-list/get/set/add/remove/clear</li><li>xe vgpu-list</li><li>xe vgpu-create</li><li>xe vgpu-destroy</li><li>xe vgpu-param-list/get/set/add/remove/clear</li><li>xe host-param-get param-name=chipset-info param-key=iommu</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Documented interface changes between xapi and xenopsd for vGPU</td></tr><tr><td><span class="label label-default">v2</span></td><td>Added design for storing vGPU-to-pGPU allocation in xapi database</td></tr><tr><td><span class="label label-default">v3</span></td><td>Marked new xapi DB fields as internal-only</td></tr></table></header><h1 id=gpu-support-evolution>GPU support evolution</h1><h2 id=introduction>Introduction</h2><p>As of XenServer 6.5, VMs can be provisioned with access to graphics processors
(either emulated or passed through) in four different ways. Virtualisation of
Intel graphics processors will exist as a fifth kind of graphics processing
available to VMs. These five situations all require the VM&rsquo;s device model to be
created in subtly different ways:</p><p><strong>Pure software emulation</strong></p><ul><li>qemu is launched either with no special parameter, if the basic Cirrus
graphics processor is required, otherwise qemu is launched with the
<code>-std-vga</code> flag.</li></ul><p><strong>Generic GPU passthrough</strong></p><ul><li>qemu is launched with the <code>-priv</code> flag to turn on privilege separation</li><li>qemu can additionally be passed the <code>-std-vga</code> flag to choose the
corresponding emulated graphics card.</li></ul><p><strong>Intel integrated GPU passthrough (GVT-d)</strong></p><ul><li>As well as the <code>-priv</code> flag, qemu must be launched with the <code>-std-vga</code> and
<code>-gfx_passthru</code> flags. The actual PCI passthrough is handled separately
via xen.</li></ul><p><strong>NVIDIA vGPU</strong></p><ul><li>qemu is launched with the <code>-vgpu</code> flag</li><li>a secondary display emulator, demu, is launched with the following parameters:<ul><li><code>--domain</code> - the VM&rsquo;s domain ID</li><li><code>--vcpus</code> - the number of vcpus available to the VM</li><li><code>--gpu</code> - the PCI address of the physical GPU on which the emulated GPU will
run</li><li><code>--config</code> - the path to the config file which contains detail of the GPU to
emulate</li></ul></li></ul><p><strong>Intel vGPU (GVT-g)</strong></p><ul><li>here demu is not used, but instead qemu is launched with five parameters:<ul><li><code>-xengt</code></li><li><code>-vgt_low_gm_sz</code> - the low GM size in MiB</li><li><code>-vgt_high_gm_sz</code> - the high GM size in MiB</li><li><code>-vgt_fence_sz</code> - the number of fence registers</li><li><code>-priv</code></li></ul></li></ul><h2 id=xenopsd>xenopsd</h2><p>To handle all these possibilities, we will add some new types to xenopsd&rsquo;s
interface:</p><div class="wrap-code highlight"><pre tabindex=0><code>module Pci = struct
  type address = {
    domain: int;
    bus: int;
    device: int;
    fn: int;
  }

  ...
end

module Vgpu = struct
  type gvt_g = {
    physical_pci_address: Pci.address;
    low_gm_sz: int64;
    high_gm_sz: int64;
    fence_sz: int;
  }

  type nvidia = {
    physical_pci_address: Pci.address;
    config_file: string
  }

  type implementation =
    | GVT_g of gvt_g
    | Nvidia of nvidia

  type id = string * string

  type t = {
    id: id;
    position: int;
    implementation: implementation;
  }

  type state = {
    plugged: bool;
    emulator_pid: int option;
  }
end

module Vm = struct
  type igd_passthrough of
    | GVT_d

  type video_card =
    | Cirrus
    | Standard_VGA
    | Vgpu
    | Igd_passthrough of igd_passthrough

  ...
end

module Metadata = struct
  type t = {
    vm: Vm.t;
    vbds: Vbd.t list;
    vifs: Vif.t list;
    pcis: Pci.t list;
    vgpus: Vgpu.t list;
    domains: string option;
  }
end</code></pre></div><p>The <code>video_card</code> type is used to indicate to the function
<code>Xenops_server_xen.VM.create_device_model_config</code> how the VM&rsquo;s emulated graphics
card will be implemented. A value of <code>Vgpu</code> indicates that the VM needs to be
started with one or more virtualised GPUs - the function will need to look at
the list of GPUs associated with the VM to work out exactly what parameters to
send to qemu.</p><p>If <code>Vgpu.state.emulator_pid</code> of a plugged vGPU is <code>None</code>, this indicates that
the emulation of the vGPU is being done by qemu rather than by a separate
emulator.</p><p>n.b. adding the <code>vgpus</code> field to <code>Metadata.t</code> will break backwards compatibility
with old versions of xenopsd, so some upgrade logic will be required.</p><p>This interface will allow us to support multiple vGPUs per VM in future if
necessary, although this may also require reworking the interface between
xenopsd, qemu and demu. For now, xenopsd will throw an exception if it is asked
to start a VM with more than one vGPU.</p><h2 id=xapi>xapi</h2><p>To support the above interface, xapi will convert all of a VM&rsquo;s non-passthrough
GPUs into <code>Vgpu.t</code> objects when sending VM metadata to xenopsd.</p><p>In contrast to GVT-d, which can only be run on an Intel GPU which has been
has been hidden from dom0, GVT-g will only be allowed to run on a GPU which has
<em>not</em> been hidden from dom0.</p><p>If a GVT-g-capable GPU is detected, and it is not hidden from dom0, xapi will
create a set of VGPU_type objects to represent the vGPU presets which can run on
the physical GPU. Exactly how these presets are defined is TBD, but a likely
solution is via a set of config files as with NVIDIA vGPU.</p><p><strong>Allocation of vGPUs to physical GPUs</strong></p><p>For NVIDIA vGPU, when starting a VM, each vGPU attached to the VM is assigned
to a physical GPU as a result of capacity planning at the pool level. The
resulting configuration is stored in the VM.platform dictionary, under
specific keys:</p><ul><li><code>vgpu_pci_id</code> - the address of the physical GPU on which the vGPU will run</li><li><code>vgpu_config</code> - the path to the vGPU config file which the emulator will use</li></ul><p>Instead of storing the assignment in these fields, we will add a new
internal-only database field:</p><ul><li><code>VGPU.scheduled_to_be_resident_on (API.ref_PGPU)</code></li></ul><p>This will be set to the ref of the physical GPU on which the vGPU will run. From
here, xapi can easily obtain the GPU&rsquo;s PCI address. Capacity planning will also
take into account which vGPUs are scheduled to be resident on a physical GPU,
which will avoid races resulting from many vGPU-enabled VMs being started at
once.</p><p>The path to the config file is already stored in the <code>VGPU_type.internal_config</code>
dictionary, under the key <code>vgpu_config</code>. xapi will use this value directly
rather than copying it to VM.platform.</p><p>To support other vGPU implementations, we will add another internal-only
database field:</p><ul><li><code>VGPU_type.implementation enum(Passthrough|Nvidia|GVT_g)</code></li></ul><p>For the <code>GVT_g</code> implementation, no config file is needed. Instead,
<code>VGPU_type.internal_config</code> will contain three key-value pairs, with the keys</p><ul><li><code>vgt_low_gm_sz</code></li><li><code>vgt_high_gm_sz</code></li><li><code>vgt_fence_sz</code></li></ul><p>The values of these pairs will be used to construct a value of type
<code>Xenops_interface.Vgpu.gvt_g</code>, which will be passed down to xenopsd.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.5)</span></td></tr></table></header><h1 id=gro-and-other-properties-of-pifs>GRO and other properties of PIFs</h1><p>It has been possible to enable and disable GRO and other &ldquo;ethtool&rdquo; features on
PIFs for a long time, but there was never an official API for it. Now there is.</p><h2 id=introduction>Introduction</h2><p>The former way to enable GRO via the CLI is as follows:</p><pre><code>xe pif-param-set uuid=&lt;pif-uuid&gt; other-config:ethtool-gro=on
xe pif-plug uuid=&lt;pif-uuid&gt;
</code></pre><p>The <code>other-config</code> field is a grab-bag of options that are not clearly defined.
The options exposed through <code>other-config</code> are mostly experimental features, and
the interface is not considered stable. Furthermore, the field is read/write
and does not have any input validation, and cannot not trigger any actions
immediately. The latter is why it is needed to call <code>pif-plug</code> after setting
the <code>ethtool-gro</code> key, in order to actually make things happen.</p><h2 id=new-api>New API</h2><p>New field:</p><ul><li>Field <code>PIF.properties</code> of type <code>(string -> string) map</code>.</li><li>Physical and bond PIFs have a <code>gro</code> key in their <code>properties</code>, with possible values <code>on</code> and <code>off</code>. There are currently no other properties defined.</li><li>VLAN and Tunnel PIFs do not have any properties. They implicitly inherit the properties from the PIF they are based upon (either a physical PIF or a bond).</li><li>For backwards compatibility, if there is a <code>other-config:ethtool-gro</code> key present on the PIF, it will be treated as an override of the <code>gro</code> key in <code>PIF.properties</code>.</li></ul><p>New function:</p><ul><li>Message <code>void PIF.set_property (PIF ref, string, string)</code>.</li><li>First argument: the reference of the PIF to act on.</li><li>Second argument: the key to change in the <code>properties</code> field.</li><li>Third argument: the value to write.</li><li>The function can only be used on physical PIFs that are not bonded, and on bond PIFs. Attempts to call the function on bond slaves, VLAN PIFs, or Tunnel PIFs, fail with <code>CANNOT_CHANGE_PIF_PROPERTIES</code>.</li><li>Calls with invalid keys or values fail with <code>INVALID_VALUE</code>.</li><li>When called on a bond PIF, the key in the <code>properties</code> of the associated bond slaves will also be set to same value.</li><li>The function automatically causes the settings to be applied to the network devices (no additional <code>plug</code> is needed). This includes any VLANs that are on top of the PIF to-be-changed, as well as any bond slaves.</li></ul><h2 id=defaults-installation-and-upgrade>Defaults, Installation and Upgrade</h2><ul><li>Any newly introduced PIF will have its <code>properties</code> field set to <code>"gro" -> "on"</code>. This includes PIFs obtained after a fresh installation of XenServer, as well as PIFs created using <code>PIF.introduce</code> or <code>PIF.scan</code>. In other words, GRO will be &ldquo;on&rdquo; by default.</li><li>An upgrade from a version of XenServer that does not have the <code>PIF.properties</code> field, will give every physical and bond PIF a <code>properties</code> field set to <code>"gro" -> "on"</code>. In other words, GRO will be &ldquo;on&rdquo; by default after an upgrade.</li></ul><h2 id=bonding>Bonding</h2><ul><li>When creating a bond, the bond-slaves-to-be must all have equal <code>PIF.properties</code>. If not, the <code>bond.create</code> call will fail with <code>INCOMPATIBLE_BOND_PROPERTIES</code>.</li><li>When a bond is created successfully, the <code>properties</code> of the bond PIF will be equal to the properties of the bond slaves.</li></ul><h2 id=command-line-interface>Command Line Interface</h2><ul><li>The <code>PIF.properties</code> field is exposed through <code>xe pif-list</code> and <code>xe pif-param-list</code> as usual.</li><li>The <code>PIF.set_property</code> call is exposed through <code>xe pif-param-set</code>. For example: <code>xe pif-param-set uuid=&lt;pif-uuid> properties:gro=off</code>.</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (5.6)</span></td></tr></table></header><h1 id=heterogeneous-pools>Heterogeneous pools</h1><h1 id=notes>Notes</h1><ul><li>The <code>cpuid</code> instruction is used to obtain a CPU&rsquo;s manufacturer,
family, model, stepping and features information.</li><li>The feature bitvector is 128 bits wide: 2 times 32 bits of base
features plus 2 times 32 bits of extended features, which are
referred to as <code>base_ecx</code>, <code>base_edx</code>, <code>ext_ecx</code> and <code>ext_edx</code>
(after the registers used by <code>cpuid</code> to store the results).</li><li>The feature bits can be masked by Intel FlexMigration and AMD
Extended Migration. This means that features can be made to appear
as absent. Hence, a CPU can appear as a less-capable CPU.<ul><li>AMD Extended Migration is able to mask both base and extended
features.</li><li>Intel FlexMigration on Core 2 CPUs (Penryn) is able to mask
<strong>only the base features</strong> (<code>base_ecx</code> and <code>base_edx</code>). The
newer Nehalem and Westmere CPUs support extended-feature masking
as well.</li></ul></li><li>A process in dom0 (e.g. xapi) is able to call <code>cpuid</code> to obtain the
(possibly modified) CPU info, or can obtain this information from
Xen. Masking is done only by Xen at boot time, before any domains
are loaded.</li><li>To apply a feature mask, a dom0 process may specify the mask in the
Xen command line in the file <code>/boot/extlinux.conf</code>. After a reboot,
the mask will be enforced.</li><li>It is not possible to obtain the original features from a dom0
process, if the features have been masked. Before applying the first
mask, the process could remember/store the original feature vector,
or obtain the information from Xen.</li><li>All CPU cores on a host can be assumed to be identical. Masking will
be done simultaneously on all cores in a host.</li><li>Whether a CPU supports FlexMigration/Extended Migration can (only)
be derived from the family/model/stepping information.</li><li>XS5.5 has an exception for the EST feature in base_ecx. This flag
is ignored on pool join.</li></ul><h1 id=overview-of-xenapi-changes>Overview of XenAPI Changes</h1><h2 id=fields>Fields</h2><p>Currently, the datamodel has <code>Host_cpu</code> objects for each CPU core in a
host. As they are all identical, we are considering keeping just one CPU
record in the <code>Host</code> object itself, and deprecating the <code>Host_cpu</code>
class. For backwards compatibility, the <code>Host_cpu</code> objects will remain
as they are in MNR, but may be removed in subsequent releases.</p><p>Hence, there will be a new field called <code>Host.cpu_info</code>, a read-only
string-string map, containing the following fixed set of keys:</p><p>Indicating whether the CPU supports Intel FlexMigration or AMD Extended
Migration. There are three possible values: <code>"no"</code> means that masking is
not possible, <code>"base"</code> means that only base features can be masked, and
<code>"full"</code> means that base as well as extended features can be masked.</p><p>Note: When the <code>features</code> and <code>features_after_reboot</code> are different,
XenCenter could display a warning saying that a reboot is needed to
enforce the feature masking.</p><p>The <code>Pool.other_config:cpuid_feature_mask</code> key is recognised. If this
key is present and if it contains a value in the same format as
<code>Host.cpu_info:features</code>, the value is used to mask the feature vectors
before comparisons during any pool join in the pool it is defined on.
This can be used to white-list certain feature flags, i.e. to ignore
them when adding a new host to a pool. The default it
<code>ffffff7f-ffffffff-ffffffff-ffffffff</code>, which white-lists the EST feature
for compatibility with XS 5.5 and earlier.</p><h2 id=messages>Messages</h2><p>New messages:</p><ul><li><code>Host.set_cpu_features</code><ul><li>Parameters: Host reference <code>host</code>, new CPU feature vector
<code>features</code>.</li><li>Roles: only Pool Operator and Pool Admin.</li><li>Sets the feature vector to be used after a reboot
(<code>Host.cpu_info:features_after_reboot</code>), if <code>features</code> is valid.</li></ul></li><li><code>Host.reset_cpu_features</code><ul><li>Parameter: Host reference <code>host</code>.</li><li>Roles: only Pool Operator and Pool Admin.</li><li>Removes the feature mask, such that after a reboot all features
of the CPU are enabled.</li></ul></li></ul><h1 id=xapi>XAPI</h1><h2 id=back-end>Back-end</h2><ul><li>Xen keeps the physical (unmasked) CPU features in memory when
starts, before applying any masks. Xen exposes the physical
features, as well as the current (possibly masked) features, to
dom0/xapi via the function <code>xc_get_boot_cpufeatures</code> in libxc.</li><li>A dom0 script <code>/etc/xensource/libexec/xen-cmdline</code>, which provides a
future-proof way of modifying the Xen command-line key/value pairs.
This script has the following options, where <code>mask</code> is one of
<code>cpuid_mask_ecx</code>, <code>cpuid_mask_edx</code>, <code>cpuid_mask_ext_ecx</code> or
<code>cpuid_mask_ext_edx</code>, and <code>value</code> is <code>0xhhhhhhhh</code> (<code>h</code> is represents
a hex digit).:<ul><li><code>--list-cpuid-masks</code></li><li><code>--set-cpuid-masks mask=value mask=value</code></li><li><code>--delete-cpuid-masks mask mask</code></li></ul></li><li>A <code>restrict_cpu_masking</code> key has been added to the host licensing
restrictions map. This will be <code>true</code> when the <code>Host.edition</code> is
<code>free</code>, and <code>false</code> if it is <code>enterprise</code> or <code>platinum</code>.</li></ul><h2 id=start-up>Start-up</h2><p>The <code>Host.cpu_info</code> field is refreshed:</p><ul><li>The values for the keys <code>cpu_count</code>, <code>vendor</code>, <code>speed</code>, <code>modelname</code>,
<code>flags</code>, <code>stepping</code>, <code>model</code>, and <code>family</code> are obtained from
<code>/etc/xensource/boot_time_cpus</code> (and ultimately from
<code>/proc/cpuinfo</code>).</li><li>The values of the <code>features</code> and <code>physical_features</code> are obtained
from Xen and the <code>features_after_reboot</code> key is made equal to the
<code>features</code> field.</li><li>The value of the <code>maskable</code> key is determined by the CPU details.<ul><li>for Intel Core2 (Penryn) CPUs:
<code>family = 6 and (model = 1dh or (model = 17h and stepping >= 4))</code>
(<code>maskable = "base"</code>)</li><li>for Intel Nehalem/Westmere CPUs:
<code>family = 6 and ((model = 1ah and stepping > 2) or model = 1eh or model = 25h or model = 2ch or model = 2eh or model = 2fh)</code>
(<code>maskable = "full"</code>)</li><li>for AMD CPUs: <code>family >= 10h</code> (<code>maskable = "full"</code>)</li></ul></li></ul><h2 id=setting-masking-and-resetting-the-cpu-features>Setting (Masking) and Resetting the CPU Features</h2><ul><li>The <code>Host.set_cpu_features</code> call:<ul><li>checks whether the license of the host is Enterprise or
Platinum; throws FEATURE_RESTRICTED if not.</li><li>expects a string of 32 hexadecimal digits, optionally containing
spaces; throws INVALID_FEATURE_STRING if malformed.</li><li>checks whether the given feature vector can be formed by masking
the physical feature vector; throws INVALID_FEATURE_STRING if
not. Note that on Intel Core 2 CPUs, it is only possible to the
mask the base features!</li><li>checks whether the CPU supports FlexMigration/Extended
Migration; throws CPU_FEATURE_MASKING_NOT_SUPPORTED if not.</li><li>sets the value of <code>features_after_reboot</code> to the given feature
vector.</li><li>adds the new feature mask to the Xen command-line via the
<code>xen-cmdline</code> script. The mask is represented by one or more of
the following key/value pairs (where <code>h</code> represents a hex
digit):<ul><li><code>cpuid_mask_ecx=0xhhhhhhhh</code></li><li><code>cpuid_mask_edx=0xhhhhhhhh</code></li><li><code>cpuid_mask_ext_ecx=0xhhhhhhhh</code></li><li><code>cpuid_mask_ext_edx=0xhhhhhhhh</code></li></ul></li></ul></li><li>The <code>Host.reset_cpu_features</code> call:<ul><li>copies <code>physical_features</code> to <code>features_after_reboot</code>.</li><li>removes the feature mask from the Xen command-line via the
<code>xen-cmdline</code> script (if any).</li></ul></li></ul><h2 id=pool-join-and-eject>Pool Join and Eject</h2><ul><li><code>Pool.join</code> fails when the <code>vendor</code> and <code>feature</code> keys do not match,
and disregards any other key in <code>Host.cpu_info</code>.<ul><li>However, as XS5.5 disregards the EST flag, there is a new way to
disregard/ignore feature flags on pool join, by setting a mask
in <code>Pool.other_config:cpuid_feature_mask</code>. The value of this
field should have the same format as <code>Host.cpu_info:features</code>.
When comparing the CPUID features of the pool and the joining
host for equality, this mask is applied before the comparison.
The default is <code>ffffff7f-ffffffff-ffffffff-ffffffff</code>, which
defines the EST feature, bit 7 of the base ecx flags, as &ldquo;don&rsquo;t
care&rdquo;.</li></ul></li><li><code>Pool.eject</code> clears the database (as usual), and additionally
removes the feature mask from <code>/boot/extlinux.conf</code> (if any).</li></ul><h1 id=cli>CLI</h1><p>New commands:</p><ul><li><code>host-cpu-info</code><ul><li>Parameters: <code>uuid</code> (optional, uses localhost if absent).</li><li>Lists <code>Host.cpu_info</code> associated with the host.</li></ul></li><li><code>host-get-cpu-features</code><ul><li>Parameters: <code>uuid</code> (optional, uses localhost if absent).</li><li>Returns the value of <code>Host.cpu_info:features]</code> associated with
the host.</li></ul></li><li><code>host-set-cpu-features</code><ul><li>Parameters: <code>features</code> (string of 32 hexadecimal digits,
optionally containing spaces or dashes), <code>uuid</code> (optional, uses
localhost if absent).</li><li>Calls <code>Host.set_cpu_features</code>.</li></ul></li><li><code>host-reset-cpu-features</code><ul><li>Parameters: <code>uuid</code> (optional, uses localhost if absent).</li><li>Calls <code>Host.reset_cpu_features</code>.</li></ul></li></ul><p>The following commands will be deprecated: <code>host-cpu-list</code>,
<code>host-cpu-param-get</code>, <code>host-cpu-param-list</code>.</p><p>WARNING:</p><p>If the user is able to set any mask they like, they may end up disabling
CPU features that are required by dom0 (and probably other guest OSes),
resulting in a kernel panic when the machine restarts. Hence, using the
set function is potentially dangerous.</p><p>It is apparently not easy to find out exactly which flags are safe to
mask and which aren&rsquo;t, so we cannot prevent an API/CLI user from making
mistakes in this way. However, using XenCenter would always be safe, as
XC always copies features masks from real hosts.</p><p>If a machine ends up in such a bad state, there is a way to get out of
it. At the boot prompt (before Xen starts), you can type &ldquo;menu.c32&rdquo;,
select a boot option and alter the Xen command-line to remove the
feature masks, after which the machine will again boot normally (note:
in our set-up, there is first a PXE boot prompt; the second prompt is
the one we mean here).</p><p>The API/CLI documentation should stress the potential danger of using
this functionality, and explain how to get out of trouble again.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr></table></header><h1 id=improving-snapshot-revert-behaviour>Improving snapshot revert behaviour</h1><p>Currently there is a XenAPI <code>VM.revert</code> which reverts a &ldquo;VM&rdquo; to the state it
was in when a VM-level snapshot was taken. There is no <code>VDI.revert</code> so
<code>VM.revert</code> uses <code>VDI.clone</code> to change the state of the disks.</p><p>The use of <code>VDI.clone</code> has the side-effect of changing VDI refs and uuids.
This causes the following problems:</p><ul><li>It is difficult for clients
such as <a href=http://cloudstack.apache.org target=_blank>Apache CloudStack</a> to keep track
of the disks it is actively managing</li><li>VDI snapshot metadata (<code>VDI.snapshot_of</code> et al) has to be carefully
fixed up since all the old refs are now dangling</li></ul><p>We will fix these problems by:</p><ol><li>adding a <code>VDI.revert</code> to the SMAPIv2 and calling this from <code>VM.revert</code></li><li>defining a new SMAPIv1 operation <code>vdi_revert</code> and a corresponding capability
<code>VDI_REVERT</code></li><li>the Xapi implementation of <code>VDI.revert</code> will first try the <code>vdi_revert</code>,
and fall back to <code>VDI.clone</code> if that fails</li><li>implement <code>vdi_revert</code> for common storage types, including File and LVM-based
SRs.</li></ol><h2 id=xenapi-changes>XenAPI changes</h2><p>We will add the function <code>VDI.revert</code> with arguments:</p><ul><li>in: <code>snapshot: Ref(VDI)</code>: the snapshot to which we want to revert</li><li>in: <code>driver_params: Map(String,String)</code>: optional extra parameters</li><li>out: <code>Ref(VDI)</code> the new VDI</li></ul><p>The function will look up the VDI which this is a <code>snapshot_of</code>, and change
the VDI to have the same contents as the snapshot. The snapshot will not be
modified. If the implementation is able to revert in-place, then the reference
returned will be the VDI this is a <code>snapshot_of</code>; otherwise it is a reference
to a fresh VDI (created by the <code>VDI.clone</code> fallback path)</p><p>References:</p><ul><li>@johnelse&rsquo;s <a href=https://github.com/xapi-project/xen-api/pull/1963 target=_blank>pull request</a>
which implements this</li></ul><h2 id=smapiv1-changes>SMAPIv1 changes</h2><p>We will define the function <code>vdi_revert</code> with arguments:</p><ul><li>in: <code>sr_uuid</code>: the UUID of the SR containing both the VDI and the snapshot</li><li>in: <code>vdi_uuid</code>: the UUID of the snapshot whose contents should be duplicated</li><li>in: <code>target_uuid</code>: the UUID of the target whose contents should be replaced</li></ul><p>The function will replace the contents of the <code>target_uuid</code> VDI with the
contents of the <code>vdi_uuid</code> VDI without changing the identify of the target
(i.e. name-label, uuid and location are guaranteed to remain the same).
The <code>vdi_uuid</code> is preserved by this operation. The operation is obvoiusly
idempotent.</p><h2 id=xapi-changes>Xapi changes</h2><p>Xapi will</p><ul><li>use <code>VDI.revert</code> in the <code>VM.revert</code> code-path</li><li>expose a new <code>xe vdi-revert</code> CLI command</li><li>implement the <code>VDI.revert</code> by calling the SMAPIv1 function and falling back
to <code>VDI.clone</code> if a <code>Not_implemented</code> exception is thrown</li></ul><p>References:</p><ul><li>@johnelse&rsquo;s <a href=https://github.com/xapi-project/xen-api/pull/1963 target=_blank>pull request</a></li></ul><h2 id=sm-changes>SM changes</h2><p>We will modify</p><ul><li>SRCommand.py and VDI.py to add a new <code>vdi_revert</code> function which throws
a &rsquo;not implemented&rsquo; exception</li><li>FileSR.py to implement <code>VDI.revert</code> using a variant of the existing
snapshot/clone machinery</li><li>EXTSR.py and NFSSR.py to advertise the <code>VDI_REVERT</code> capability</li><li>LVHDSR.py to implement <code>VDI.revert</code> using a variant of the existing
snapshot/clone machinery</li><li>LVHDoISCSISR.py and LVHDoHBASR.py to advertise the <code>VDI_REVERT</code> capability</li></ul><h1 id=prototype-code>Prototype code</h1><p>Prototype code exists here:</p><ul><li><a href=https://github.com/xapi-project/xcp-idl/pull/37 target=_blank>xapi-project/xcp-idl#37</a> by @johnelse</li><li><a href=https://github.com/xapi-project/xen-api/pull/2058 target=_blank>xapi-project/xen-api#2058</a> mainly by @johnelse but with 2 extra patches from me</li><li><a href=https://github.com/djs55/sm/commit/cbc28755c9c4300ed067abc089081f58f821f504 target=_blank>Definition of SMAPIv1 vdi_revert</a></li><li><a href=https://github.com/djs55/sm/commit/eb31d6205ccd707152a5b59c9a733fd48db5316b target=_blank>Hacky implementation for EXT/NFS</a></li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.5 sp1)</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/33>#33</a></td></tr></table></header><h1 id=integrated-gpu-passthrough-support>Integrated GPU passthrough support</h1><h2 id=introduction>Introduction</h2><p>Passthrough of discrete GPUs has been
<a href=%7b%7bsite.baseurl%7d%7d/xapi/design/gpu-passthrough.html>available since XenServer 6.0</a>.
With some extensions, we will also be able to support passthrough of integrated
GPUs.</p><ul><li>Whether an integrated GPU will be accessible to dom0 or available to
passthrough to guests must be configurable via XenAPI.</li><li>Passthrough of an integrated GPU requires an extra flag to be sent to qemu.</li></ul><h2 id=host-configuration>Host Configuration</h2><p>New fields will be added (both read-only):</p><ul><li><code>PGPU.dom0_access enum(enabled|disable_on_reboot|disabled|enable_on_reboot)</code></li><li><code>host.display enum(enabled|disable_on_reboot|disabled|enable_on_reboot)</code></li></ul><p>as well as new API calls used to modify the state of these fields:</p><ul><li><code>PGPU.enable_dom0_access</code></li><li><code>PGPU.disable_dom0_access</code></li><li><code>host.enable_display</code></li><li><code>host.disable_display</code></li></ul><p>Each of these API calls will return the new state of the field e.g. calling
<code>host.disable_display</code> on a host with <code>display = enabled</code> will return
<code>disable_on_reboot</code>.</p><p>Disabling dom0 access will modify the xen commandline (using the xen-cmdline
tool) such that dom0 will not be able to access the GPU on next boot.</p><p>Calling host.disable_display will modify the xen and dom0 commandlines such
that neither will attempt to send console output to the system display device.</p><p>A state diagram for the fields <code>PGPU.dom0_access</code> and <code>host.display</code> is shown
below:</p><p><a href=#image-029ea3d86efa37f7abfc5d3b9c3e206e class=lightbox-link><img src=/new-docs/design/integrated-gpu-passthrough/integrated-gpu-passthrough.png alt="host.integrated_GPU_passthrough flow diagram" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-029ea3d86efa37f7abfc5d3b9c3e206e><img src=/new-docs/design/integrated-gpu-passthrough/integrated-gpu-passthrough.png alt="host.integrated_GPU_passthrough flow diagram" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>While it is possible for these two fields to be modified independently, a
client must disable both the host display and dom0 access to the system display
device before that device can be passed through to a guest.</p><p>Note that when a client enables or disables either of these fields, the change
can be cancelled until the host is rebooted.</p><h2 id=handling-vga_arbiter>Handling vga_arbiter</h2><p>Currently, xapi will not create a PGPU object for the PCI device with address
reported by <code>/dev/vga_arbiter</code>. This is to prevent a GPU in use by dom0 from
from being passed through to a guest. This behaviour will be changed - instead
of not creating a PGPU object at all, xapi will create a PGPU, but its
supported_VGPU_types field will be empty.</p><p>However, the PGPU&rsquo;s supported_VGPU_types will be populated as normal if:</p><ol><li>dom0 access to the GPU is disabled.</li><li>The host&rsquo;s display is disabled.</li><li>The vendor ID of the device is contained in a whitelist provided by xapi&rsquo;s
config file.</li></ol><p>A read-only field will be added:</p><ul><li><code>PGPU.is_system_display_device bool</code></li></ul><p>This will be true for a PGPU iff <code>/dev/vga_arbiter</code> reports the PGPU as the
system display device for the host on which the PGPU is installed.</p><h2 id=interfacing-with-xenopsd>Interfacing with xenopsd</h2><p>When starting a VM attached to an integrated GPU, the VM config sent to xenopsd
will contain a video_card of type IGD_passthrough. This will override the type
determined from VM.platform:vga. xapi will consider a GPU to be integrated if
both:</p><ol><li>It resides on bus 0.</li><li>The vendor ID of the device is contained in a whitelist provided by xapi&rsquo;s
config file.</li></ol><p>When xenopsd starts qemu for a VM with a video_card of type IGD_passthrough,
it will pass the flags &ldquo;-std-vga&rdquo; AND &ldquo;-gfx_passthru&rdquo;.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=local-database>Local database</h1><p>All hosts in a pool use the shared database by sending queries to
the pool master. This creates a performance bottleneck as the pool
size increases. All hosts in a pool receive a database backup from
the master periodically, every couple of hours. This creates a
reliability problem as updates may be lost if the master fails during
the window before the backup.</p><p>The reliability problem can be avoided by running with HA or the redo
log enabled, but this is not always possible.</p><p>We propose to:</p><ul><li>adapt the existing event machinery to allow every host to maintain
an up-to-date database replica;</li><li>actively cache the database locally on each host and satisfy read
operations from the cache. Most database operations are reads so
this should reduce the number of RPCs across the network.</li></ul><p>In a later phase we can move to a completely
<a href=/new-docs/design/local-database/../distributed-database>distributed database</a>.</p><h2 id=replicating-the-database>Replicating the database</h2><p>We will create a database-level variant of the existing XenAPI <code>event.from</code>
API. The new RPC will block until a database event is generated, and then
the events will be returned using the existing &ldquo;redo-log&rdquo; event types. We
will add a few second delay into the RPC to batch the updates.</p><p>We will replace the pool database download logic with an <code>event.from</code>-like
loop which fetches all the events from the master&rsquo;s database and applies
them to the local copy. The first call will naturally return the full database
contents.</p><p>We will turn on the existing &ldquo;in memory db cache&rdquo; mechanism on all hosts,
not just the master. This will be where the database updates will go.</p><p>The result should be that every host will have a <code>/var/xapi/state.db</code> file,
with writes going to the master first and then filtering down to all slaves.</p><h2 id=using-the-replica-as-a-cache>Using the replica as a cache</h2><p>We will re-use the <a href=/new-docs/design/local-database/../../toolstack/features/DR>Disaster Recovery</a> multiple
database mechanism to allow slaves to access their local database. We will
change the defalult database &ldquo;context&rdquo; to snapshot the local database,
perform reads locally and write-through to the master.</p><p>We will add an HTTP header to all forwarded XenAPI calls from the master which
will include the current database generation count. When a forwarded XenAPI
operation is received, the slave will deliberately wait until the local cache
is at least as new as this, so that we always use fresh metadata for XenAPI
calls (e.g. the VM.start uses the absolute latest VM memory size).</p><p>We will document the new database coherence policy, i.e. that writes on a host
will not immediately be seen by reads on another host. We believe that this
is only a problem when we are using the database for locking and are attempting
to hand over a lock to another host. We are already using XenAPI calls forwarded
to the master for some of this, but may need to do a bit more of this; in
particular the storage backends may need some updating.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Addition of `networkd_db` update for Upgrade</td></tr><tr><td><span class="label label-default">v3</span></td><td>More info on `networkd_db` and API Errors</td></tr></table></header><h1 id=management-interface-on-vlan>Management Interface on VLAN</h1><p>This document describes design details for the
REQ-42: Support Use of VLAN on XAPI Management Interface.</p><h1 id=xapi-and-xcp-networkd>XAPI and XCP-Networkd</h1><h3 id=creating-a-vlan>Creating a VLAN</h3><p>Creating a VLAN is already there, Lisiting the steps to create a VLAN which is used later in the document.
Steps:</p><ol><li>Check the PIFs created on a Host for physical devices <code>eth0</code>, <code>eth1</code>.
<code>xe pif-list params=uuid physical=true host-uuid=UUID</code> this will list <code>pif-UUID</code></li><li>Create a new network for the VLAN interface.
<code>xe network-create name-label=VLAN1</code>
It returns a new <code>network-UUID</code></li><li>Create a VLAN PIF.
<code>xe vlan-create pif-uuid=pif-UUID network-uuid=network-UUID vlan=VLAN-ID</code>
It returns a new VLAN PIF <code>new-pif-UUID</code></li><li>Plug the VLAN PIF.
<code>xe pif-plug uuid=new-pif-UUID</code></li><li>Configure IP on the VLAN PIF.
<code>xe pif-reconfigure-ip uuid=new-pif-UUID mode= IP= netmask= gateway= DNS= </code>This will configure IP on the PIF, here <code>mode</code> is must and other parametrs are needed on selecting mode=static</li></ol><p>Similarly, creating a vlan pif can be achieved by corresponding XenAPI calls.</p><h2 id=recognise-vlan-config-from-managementconf>Recognise VLAN config from management.conf</h2><p>For a newly installed host, If host installer was asked to put the management interface on given VLAN.
We will expect a new entry <code>VLAN=ID</code> under <code>/etc/firstboot.d/data/management.conf</code>.</p><p>Listing current contents of management.conf which will be used later in the document.
<code>LABEL</code>=<code>eth0</code> -> Represents Pyhsical device on which Management Interface must reside.
<code>MODE</code>=<code>dhcp</code>||<code>static</code> -> Represents IP configuration mode for the Management Interface. There can be other parameters like IP, NETMASK, GATEWAY and DNS when we have <code>static</code> mode.
<code>VLAN</code>=<code>ID</code> -> New entry for specifying VLAN TAG going to be configured on device <code>LABEL</code>.
Management interface going to be configured on this VLAN ID with specified mode.</p><h3 id=firstboot-script-need-to-recognise-vlan-config>Firstboot script need to recognise VLAN config</h3><p>Firstboot script <code>/etc/firstboot.d/30-prepare-networking</code> need to be updated for configuring
management interface to be on provided VLAN ID.</p><p>Steps to be followed:</p><ol><li><code>PIF.scan</code> performed in the script must have created the PIFs for the underlying pyhsical devices.</li><li>Get the PIF UUID for physical device <code>LABEL</code>.</li><li>Repeat the steps mentioned in <code>Creating a VLAN</code>, i.e. network-create, vlan-create and pif-plug. Now we have a new PIF for the VLAN.</li><li>Perform <code>pif-reconfigure-ip</code> for the new VLAN PIF.</li><li>Perform <code>host-management-reconfigure</code> using new VLAN PIF.</li></ol><h3 id=xcp-networkd-need-to-recognise-vlan-config-during-startup>XCP-Networkd need to recognise VLAN config during startup</h3><p>XCP-Networkd during first boot and boot after pool eject gets the initial network setup from the <code>management.conf</code> and <code>xensource-inventory</code> file to update the network.db for management interface info.
XCP-Networkd must honour the new VLAN config.</p><p>Steps to be followed:</p><ol><li>During startup <code>read_config</code> step tries to read the <code>/var/lib/xcp/networkd.db</code> file which is not yet created just after host installation.</li><li>Since <code>networkd.db</code> read throws <code>Read_Error</code>, it tries to read <code>network.dbcache</code> which is also not available hence it goes to read <code>read_management_conf</code> file.</li><li>There can be two possible MODE <code>static</code> or <code>dhcp</code> taken from management.conf.</li><li><code>bridge_name</code> is taken as <code>MANAGEMENT_INTERFACE</code> from xensource-inventory, further <code>bridge_config</code> and <code>interface_config</code> are build based on MODE.</li><li>Call <code>Bridge.make_config()</code> and <code>Interface.make_config()</code> are performed with respective <code>bridge_config</code> and <code>interface_config</code>.</li></ol><h2 id=updating-networkd_db-program>Updating networkd_db program</h2><p><code>networkd_db</code> provides the management interface info to the host installer during upgrade.
It reads <code>/var/lib/xcp/networkd.db</code> file to output the Management Interface information. Here we need to update the networkd_db to output the VLAN information when vlan bridge is a input.</p><p>Steps to be followed:</p><ol><li>Currently VLAN interface IP information is provided correctly on passing VLAN bridge as input.
<code>networkd_db -iface xapi0</code> this will list <code>mode</code> as dhcp or static, if mode=static then it will provide <code>ipaddr</code> and <code>netmask</code> too.</li><li>We need to udpate this program to provide VLAN ID and parent bridge info on passing VLAN bridge as input.
<code>networkd_db -bridge xapi0</code> It should output the VLAN info like:
<code>interfaces=</code>
<code>vlan=vlanID</code>
<code>parent=xenbr0</code> using the parent bridge user can identify the physical interfaces.
Here we will extract VLAN and parent bridge from <code>bridge_config</code> under <code>networkd.db</code>.</li></ol><h2 id=additional-vlan-parameter-for-emergency-network-reset>Additional VLAN parameter for Emergency Network Reset</h2><p>Detail design is mentioned on <a href=http://xapi-project.github.io/xapi/design/emergency-network-reset.html target=_blank>http://xapi-project.github.io/xapi/design/emergency-network-reset.html</a>
For using <code>xe-reset-networking</code> utility to configure management interface on VLAN, We need to add one more parameter <code>--vlan=vlanID</code> to the utility.
There are certain parameters need to be passed to this utility: &ndash;master, &ndash;device, &ndash;mode, &ndash;ip, &ndash;netmask, &ndash;gateway, &ndash;dns and new one &ndash;vlan.</p><h3 id=vlan-parameter-addition-to-xe-reset-networking>VLAN parameter addition to xe-reset-networking</h3><p>Steps to be followed:</p><ol><li>Check if <code>VLANID</code> is passed then let bridge=<code>xapi0</code>.</li><li>Write the <code>bridge=xapi0</code> into xensource-inventory file, This should work as Xapi check avialable bridges while creating networks.</li><li>Write the <code>VLAN=vlanID</code> into <code>management.conf</code> and <code>/tmp/network-reset</code>.</li><li>Modify <code>check_network_reset</code> under xapi.ml to perform steps <code>Creating a VLAN</code> and perform <code>management_reconfigure</code> on vlan pif.
Step <code>Creating a VLAN</code> must have created the VLAN record in Xapi DB similar to firstboot script.</li><li>If no VLANID is specified then retain the current one, This utility must take the management interface info from <code>networkd_db</code> program and handle the VLAN config.</li></ol><h3 id=vlan-parameter-addition-to-xsconsole-emergency-network-reset>VLAN parameter addition to xsconsole Emergency Network Reset</h3><p>Under <code>Emergency Network Reset</code> option under the <code>Network and Management Interface</code> menu.
Selecting this option will show some explanation in the pane on the right-hand side.
Pressing will bring up a dialogue to select the interfaces to use as management interface after the reset.
After choosing a device, the dialogue continues with configuration options like in the <code>Configure Management Interface</code> dialogue.
There will be an additionall option for VLAN in the dialogue.
After completing the dialogue, the same steps as listed for xe-reset-networking are executed.</p><h2 id=updating-pool-joineject-operations>Updating Pool Join/Eject operations</h2><h3 id=pool-join-while-pool-having-management-interface-on-a-vlan>Pool Join while Pool having Management Interface on a VLAN</h3><p>Currently <code>pool-join</code> fails if VLANs are present on the host joining a pool.
We need to allow pool-join only if Pool and host joining a pool both has management interface on same VLAN.</p><p>Steps to be followed:</p><ol><li>Under <code>pre_join_checks</code> update function <code>assert_only_physical_pifs</code> to check Pool master management_interface is on same VLAN.</li><li>Call <code>Host.get_management_interface</code> on Pool master and get the vlanID, match it with <code>localhost</code> management_interface VLAN ID.
If it matches then allow pool-join.</li><li>In case if there are multiple VLANs on host joining a pool, fail the pool-join gracefully.</li><li>After the pool-join, Host xapi db will get sync from pool master xapi db, This will be fine to have management interface on VLAN.</li></ol><h3 id=pool-eject-while-host-ejected-having-management-interface-on-a-vlan>Pool Eject while host ejected having Management Interface on a VLAN</h3><p>Currently managament interface VLAN config on host is not been retained in <code>xensource-inventory</code> or <code>management.conf</code> file.
We need to retain the vlanID under config files.</p><p>Steps to be followed:</p><ol><li>Under call <code>Pool.eject</code> we need to update <code>write_first_boot_management_interface_configuration_file</code> function.</li><li>Check if management_interface is on VLAN then get the VLANID from the pif.</li><li>Update the VLANID into the <code>managament.conf</code> file and the <code>bridge</code> into <code>xensource-inventory</code> file.
In order to be retained by XCP-Networkd on startup after the host is ejected.</li></ol><h2 id=new-api-for-pool-management-reconfigure>New API for Pool Management Reconfigure</h2><p>Currently there is no Pool Level API to reconfigure management_interface for all of the Hosts in a Pool at once.
API <code>Pool.management_reconfigure</code> will be needed in order to reconfigure <code>manamegemnt_interface</code> on all hosts in a Pool to the same Network either VLAN or Physical.</p><h3 id=current-behaviour-to-change-the-management-interface-on-host>Current behaviour to change the Management Interface on Host</h3><p>Currently call <code>Host.management_reconfigure</code> with VLAN pif-uuid can change the management_interface to specified VLAN.
Listing the steps to understand the workflow of <code>management_interface</code> reconfigure. We will be using <code>Host.management_reconfigure</code> call inside the new API.</p><p>Steps performed during management_reconfigure:</p><ol><li><code>bring_pif_up</code> get called for the pif.</li><li><code>xensource-inventory</code> get updated with the latest info of interface.
3 <code>update-mh-info</code> updates the management_mac into xenstore.</li><li>Http server gets restarted, even though xapi listen on all IP addresses, This new interface as <code>_the_ management</code> interface is used by slaves to connect to pool master.</li><li><code>on_dom0_networking_change</code> refreshes console URIs for the new IP address.</li><li>Xapi db is updated with new management interface info.</li></ol><h3 id=management-reconfigure-on-pool-from-physical-network-to-vlan-network-or-from-vlan-network-to-other-vlan-network-or-from-vlan-network-to-physical-network>Management Reconfigure on Pool from Physical Network to VLAN Network or from VLAN Network to Other VLAN Network or from VLAN Network to Physical Network</h3><p>Listing steps to be performed manually on each Host or Pool as a prerequisite to use the New API.
We need to make sure that new network which is going to be a management interface has PIFs configured on each Host.
In case of pyhsical network we will assume pifs are configured on each host, In case of vlan network we need to create vlan pifs on each Host.
We would assume that VLAN is available on the switch/network.</p><p>Manual steps to be performed before calling new API:</p><ol><li>Create a vlan network on pool via <code>network.create</code>, In case of pyhsical NICs network must be present.</li><li>Create a vlan pif on each host via <code>VLAN.create</code> using above network ref, physical PIF ref and vlanID, Not needed in case of pyhsical network.
Or An Alternate call <code>pool.create_VLAN</code> providing <code>device</code> and above <code>network</code> will create vlan PIFs for all hosts in a pool.</li><li>Perform <code>PIF.reconfigure_ip</code> for each new Network PIF on each Host.</li></ol><p>If User wishes to change the management interface manually on each Host in a Pool, We should allow it, There will be a guideline for that:</p><p>User can individually change management interface on each host calling <code>Host.management_reconfigure</code> using pifs on physical devices or vlan pifs.
This must be perfomed on slaves first and lastly on Master, As changing management_interface on master will disconnect slaves from master then further calls <code>Host.management_reconfigure</code> cannot be performed till master recover slaves via call <code>pool.recover_slaves</code>.</p><h3 id=api-details>API Details</h3><ul><li><code>Pool.management_reconfigure</code><ul><li>Parameter: network reference <code>network</code>.</li><li>Calling this function configures <code>management_interface</code> on each host of a pool.</li><li>For the <code>network</code> provided it will check pifs are present on each Host,
In case of VLAN network it will check vlan pifs on provided network are present on each Host of Pool.</li><li>Check IP is configured on above pifs on each Host.</li><li>If PIFs are not present or IP is not configured on PIFs this call must fail gracefully, Asking user to configure them.</li><li>Call <code>Host.management_reconfigure</code> on each slave then lastly on master.</li><li>Call <code>pool.recover_slaves</code> on master inorder to recover slaves which might have lost the connection to master.</li></ul></li></ul><h3 id=api-errors>API errors</h3><p>Possible API errors that may be raised by <code>pool.management_reconfigure</code>:</p><ul><li><code>INTERFACE_HAS_NO_IP</code> : the specified PIF (<code>pif</code> parameter) has no IP configuration. The new API checks for all PIFs on the new Network has IP configured. There might be a case when user has forgotten to configure IP on PIF on one or many of the Hosts in a Pool.</li></ul><p>New API ERROR:</p><ul><li><code>REQUIRED_PIF_NOT_PRESENT</code> : the specified Network (<code>network</code> parameter) has no PIF present on the host in pool. There might be a case when user has forgotten to create vlan pif on one or many of the Hosts in a Pool.</li></ul><h2 id=cp-tickets>CP-Tickets</h2><ol><li>CP-14027</li><li>CP-14028</li><li>CP-14029</li><li>CP-14030</li><li>CP-14031</li><li>CP-14032</li><li>CP-14033</li></ol><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial revision</td></tr><tr><td><span class="label label-default">v2</span></td><td>Short-term simplications and scope reduction</td></tr></table></header><h1 id=multiple-cluster-managers>Multiple Cluster Managers</h1><h2 id=introduction>Introduction</h2><p>Xapi currently uses a cluster manager called <a href=/new-docs/design/multiple-cluster-managers/../../features/HA/HA.html>xhad</a>. Sometimes other software comes with its own built-in way of managing clusters, which would clash with xhad (example: xhad could choose to fence node &lsquo;a&rsquo; while the other system could fence node &lsquo;b&rsquo; resulting in a total failure). To integrate xapi with this other software we have 2 choices:</p><ol><li>modify the other software to take membership information from xapi; or</li><li>modify xapi to take membership information from this other software.</li></ol><p>This document proposes a way to do the latter.</p><h2 id=xenapi-changes>XenAPI changes</h2><h3 id=new-field>New field</h3><p>We will add the following new field:</p><ul><li><code>pool.ha_cluster_stack</code> of type <code>string</code> (read-only)<ul><li>If HA is enabled, this field reflects which cluster stack is in use.</li><li>Set to <code>"xhad"</code> on upgrade, which implies that so far we have used XenServer&rsquo;s own cluster stack, called <code>xhad</code>.</li></ul></li></ul><h3 id=cluster-stack-choice>Cluster-stack choice</h3><p>We assume for now that a particular cluster manager will be mandated (only) by certain types of clustered storage, recognisable by SR type (e.g. OCFS2 or Melio). The SR backend will be able to inform xapi if the SR needs a particular cluster stack, and if so, what is the name of the stack.</p><p>When <code>pool.enable_ha</code> is called, xapi will determine which cluster stack to use based on the presence or absence of such SRs:</p><ul><li>If an SR that needs its own cluster stack is attached to the pool, then xapi will use that cluster stack.</li><li>If no SR that needs a particular cluster stack is attached to the pool, then xapi will use <code>xhad</code>.</li></ul><p>If multiple SRs that need a particular cluster stack exist, then the storage parts of xapi must ensure that no two such SRs are ever attached to a pool at the same time.</p><h3 id=new-errors>New errors</h3><p>We will add the following API error that may be raised by <code>pool.enable_ha</code>:</p><ul><li><code>INCOMPATIBLE_STATEFILE_SR</code>: the specified SRs (<code>heartbeat_srs</code> parameter) are not of the right type to hold the HA statefile for the <code>cluster_stack</code> that will be used. For example, there is a Melio SR attached to the pool, and therefore the required cluster stack is the Melio one, but the given heartbeat SR is not a Melio SR. The single parameter will be the name of the required SR type.</li></ul><p>The following new API error may be raised by <code>PBD.plug</code>:</p><ul><li><code>INCOMPATIBLE_CLUSTER_STACK_ACTIVE</code>: the operation cannot be performed because an incompatible cluster stack is active. The single parameter will be the name of the required cluster stack. This could happen (or example) if you tried to create an OCFS2 SR with XenServer HA already enabled.</li></ul><h3 id=future-extensions>Future extensions</h3><p>In future, we may add a parameter to explicitly choose the cluster stack:</p><ul><li>New parameter to <code>pool.enable_ha</code> called <code>cluster_stack</code> of type <code>string</code> which will have the default value of empty string (meaning: let the implementation choose).</li><li>With the additional parameter, <code>pool.enable_ha</code> may raise two new errors:<ul><li><code>UNKNOWN_CLUSTER_STACK</code>:
The operation cannot be performed because the requested cluster stack does not exist. The user should check the name was entered correctly and, failing that, check to see if the software is installed. The exception will have a single parameter: the name of the cluster stack which was not found.</li><li><code>CLUSTER_STACK_CONSTRAINT</code>: HA cannot be enabled with the provided cluster stack because some third-party software is already active which requires a different cluster stack setting. The two parameters are: a reference to an object (such as an SR) which has created the restriction, and the name of the cluster stack that this object requires.</li></ul></li></ul><h2 id=implementation>Implementation</h2><p>The <code>xapi.conf</code> file will have a new field: <code>cluster-stack-root</code> which will have the default value <code>/usr/libexec/xapi/cluster-stack</code>. The existing <code>xhad</code> scripts and tools will be moved to <code>/usr/libexec/xapi/cluster-stack/xhad/</code>. A hypothetical cluster stack called <code>foo</code> would be placed in <code>/usr/libexec/xapi/cluster-stack/foo/</code>.</p><p>In <code>Pool.enable_ha</code> with <code>cluster_stack="foo"</code> we will verify that the subdirectory <code>&lt;cluster-stack-root>/foo</code> exists. If it does not exist, then the call will fail with <code>UNKNOWN_CLUSTER_STACK</code>.</p><p>Alternative cluster stacks will need to conform to the exact same interface as <a href=/new-docs/design/multiple-cluster-managers/../../features/HA/HA.html>xhad</a>.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=multiple-device-emulators>Multiple device emulators</h1><p>Xen&rsquo;s <code>ioreq-server</code> feature allows for several device emulator
processes to be attached to the same domain, each emulating different
sets of virtual hardware. This makes it possible, for example, to
emulate network devices in a separate process for improved security
and isolation, or to provide special purpose emulators for particular
virtual hardware devices.</p><p><code>ioreq-server</code> is currently used in XenServer to support vGPU, where it
is configured via the legacy toolstack interface. These changes will make
multiple emulators usable in open source Xen via the new libxl interface.</p><h2 id=libxl-changes>libxl changes</h2><ul><li><p>The singleton device_model_version, device_model_stubdomain and
device_model fields in the b_info structure will be replaced by a list of
(version, stubdomain, model, arguments) tuples, one for each emulator.</p></li><li><p>libxl_domain_create_new() will be changed to spawn a new device model
for each entry in the list.</p></li></ul><p>It may also be useful to spawn the device models separately and only
attach them during domain creation. This could be supported by
making each device_model entry a union of <code>pid | parameter_tuple</code>.
If such an entry specifies a parameter tuple, it is processed as above;
if it specifies a pid, libxl_domain_create_new(), the existing device
model with that pid is attached instead.</p><h2 id=qemu-changes>QEMU changes</h2><ul><li><p>Patches to make QEMU register with Xen as an ioreq-server have been
submitted upstream, but not yet applied.</p></li><li><p>QEMU&rsquo;s <code>--machine none</code> and <code>--nodefaults</code> options should make it
possible to create an empty machine and add just a host bus, PCI bus
and device. This has not yet been fully demonstrated, so QEMU changes
may be required.</p></li></ul><h2 id=xen-changes>Xen changes</h2><ul><li>Until now, <code>ioreq-server</code> has only been used to connect one extra
device model, in addition to the default one. Multiple emulators
should work, but there is a chance that bugs will be discovered.</li></ul><h2 id=interfacing-with-xenopsd>Interfacing with xenopsd</h2><p>This functionality will only be available through the experimental
Xenlight-based xenopsd.</p><ul><li>the <code>VM_build</code> clause in the <code>atomics_of_operation</code> function will be
changed to fill in the list of emulators to be created (or attached)
in the b_info struct</li></ul><h2 id=host-configuration>Host Configuration</h2><p>vGPU support is implemented mostly in xenopsd, so no Xapi changes are
required to support vGPU through the generic device model mechanism.
Changes would be required if we decided to expose the additional device
models through the API, but in the near future it is more likely that
any additional device models will be dealt with entirely by xenopsd.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=ocfs2-storage>OCFS2 storage</h1><p>OCFS2 is a (host-)clustered filesystem which runs on top of a shared raw block
device. Hosts using OCFS2 form a cluster using a combination of network and
storage heartbeats and host fencing to avoid split-brain.</p><p>The following diagram shows the proposed architecture with <code>xapi</code>:</p><p><a href=#image-d66a6ed6319ab6404277261b12b6436d class=lightbox-link><img src=/new-docs/design/ocfs2/ocfs2.png alt="Proposed architecture" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-d66a6ed6319ab6404277261b12b6436d><img src=/new-docs/design/ocfs2/ocfs2.png alt="Proposed architecture" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Please note the following:</p><ul><li>OCFS2 is configured to use global heartbeats rather than per-mount heartbeats
because we quite often have many SRs and therefore many mountpoints</li><li>The OCFS2 global heartbeat should be collocated on the same SR as the XenServer
HA SR so that we depend on fewer SRs (the storage is a single point of failure
for OCFS2)</li><li>The OCFS2 global heartbeat should itself be a raw VDI within an LVHDSR.</li><li>Every host can be in at-most-one OCFS2 cluster i.e. the host cluster membership
is a per-host thing rather than a per-SR thing. Therefore <code>xapi</code> will be
modified to configure the cluster and manage the cluster node numbers.</li><li>Every SR will be a filesystem mount, managed by a SM plugin called &ldquo;OCFS2&rdquo;.</li><li>Xapi HA uses the <code>xhad</code> process which runs in userspace but in the realtime
scheduling class so it has priority over all other userspace tasks. <code>xhad</code>
sends heartbeats via the <code>ha_statefile</code> VDI and via UDP, and uses the
Xen watchdog for host fencing.</li><li>OCFS2 HA uses the <code>o2cb</code> kernel driver which sends heartbeats via the
<code>o2cb_statefile</code> and via TCP, fencing the host by panicing domain 0.</li></ul><h1 id=managing-o2cb>Managing O2CB</h1><p>OCFS2 uses the O2CB &ldquo;cluster stack&rdquo; which is similar to our <code>xhad</code>. To configure
O2CB we need to</p><ul><li>assign each host an integer node number (from zero)</li><li>on pool/cluster join: update the configuration on every node to include the
new node. In OCFS2 this can be done online.</li><li>on pool/cluster leave/eject: update the configuration on every node to exclude
the old node. In OCFS2 this needs to be done offline.</li></ul><p>In the current Xapi toolstack there is a single global implicit cluster called a &ldquo;Pool&rdquo;
which is used for: resource locking; &ldquo;clustered&rdquo; storage repositories and fault handling (in HA). In the long term we will allow these types of clusters to be
managed separately or all together, depending on the sophistication of the
admin and the complexity of their environment. We will take a small step in that
direction by keeping the OCFS2 O2CB cluster management code at &ldquo;arms length&rdquo;
from the Xapi Pool.join code.</p><p>In
<a href=https://github.com/xapi-project/xcp-idl target=_blank>xcp-idl</a>
we will define a new API category called &ldquo;Cluster&rdquo; (in addition to the
categories for
<a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/xen/xenops_interface.ml#L102 target=_blank>Xen domains</a>
, <a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/memory/memory_interface.ml#L38 target=_blank>ballooning</a>
, <a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/rrd/rrd_interface.ml#L76 target=_blank>stats</a>
,
<a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/network/network_interface.ml#L106 target=_blank>networking</a>
and
<a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/storage/storage_interface.ml#L51 target=_blank>storage</a>
). These APIs will only be called by Xapi on localhost. In particular they will
not be called across-hosts and therefore do not have to be backward compatible.
These are &ldquo;cluster plugin APIs&rdquo;.</p><p>We will define the following APIs:</p><ul><li><code>Plugin:Membership.create</code>: add a host to a cluster. On exit the local host cluster software
will know about the new host but it may need to be restarted before the
change takes effect<ul><li>in:<code>hostname:string</code>: the hostname of the management domain</li><li>in:<code>uuid:string</code>: a UUID identifying the host</li><li>in:<code>id:int</code>: the lowest available unique integer identifying the host
where an integer will never be re-used unless it is guaranteed that
all nodes have forgotten any previous state associated with it</li><li>in:<code>address:string list</code>: a list of addresses through which the host
can be contacted</li><li>out: Task.id</li></ul></li><li><code>Plugin:Membership.destroy</code>: removes a named host from the cluster. On exit the local
host software will know about the change but it may need to be restarted
before it can take effect<ul><li>in:<code>uuid:string</code>: the UUID of the host to remove</li></ul></li><li><code>Plugin:Cluster.query</code>: queries the state of the cluster<ul><li>out:<code>maintenance_required:bool</code>: true if there is some outstanding configuration
change which cannot take effect until the cluster is restarted.</li><li>out:<code>hosts</code>: a list of all known hosts together with a state including:
whether they are known to be alive or dead; or whether they are currently
&ldquo;excluded&rdquo; because the cluster software needs to be restarted</li></ul></li><li><code>Plugin:Cluster.start</code>: turn on the cluster software and let the local host join</li><li><code>Plugin:Cluster.stop</code>: turn off the cluster software</li></ul><p>Xapi will be modified to:</p><ul><li>add table <code>Cluster</code> which will have columns<ul><li><code>name: string</code>: this is the name of the Cluster plugin (TODO: use same
terminology as SM?)</li><li><code>configuration: Map(String,String)</code>: this will contain any cluster-global
information, overrides for default values etc.</li><li><code>enabled: Bool</code>: this is true when the cluster &ldquo;should&rdquo; be running. It
may require maintenance to synchronise changes across the hosts.</li><li><code>maintenance_required: Bool</code>: this is true when the cluster needs to
be placed into maintenance mode to resync its configuration</li></ul></li><li>add method <code>XenAPI:Cluster.enable</code> which sets <code>enabled=true</code> and waits for all
hosts to report <code>Membership.enabled=true</code>.</li><li>add method <code>XenAPI:Cluster.disable</code> which sets <code>enabled=false</code> and waits for all
hosts to report <code>Membership.enabled=false</code>.</li><li>add table <code>Membership</code> which will have columns<ul><li><code>id: int</code>: automatically generated lowest available unique integer
starting from 0</li><li><code>cluster: Ref(Cluster)</code>: the type of cluster. This will never be NULL.</li><li><code>host: Ref(host)</code>: the host which is a member of the cluster. This may
be NULL.</li><li><code>left: Date</code>: if not 1/1/1970 this means the time at which the host
left the cluster.</li><li><code>maintenance_required: Bool</code>: this is true when the Host believes the
cluster needs to be placed into maintenance mode.</li></ul></li><li>add field <code>Host.memberships: Set(Ref(Membership))</code></li><li>extend enum <code>vdi_type</code> to include <code>o2cb_statefile</code> as well as <code>ha_statefile</code></li><li>add method <code>Pool.enable_o2cb</code> with arguments<ul><li>in: <code>heartbeat_sr: Ref(SR)</code>: the SR to use for global heartbeats</li><li>in: <code>configuration: Map(String,String)</code>: available for future configuration tweaks</li><li>Like <code>Pool.enable_ha</code> this will find or create the heartbeat VDI, create the
<code>Cluster</code> entry and the <code>Membership</code> entries. All <code>Memberships</code> will have
<code>maintenance_required=true</code> reflecting the fact that the desired cluster
state is out-of-sync with the actual cluster state.</li></ul></li><li>add method <code>XenAPI:Membership.enable</code><ul><li>in: <code>self:Host</code>: the host to modify</li><li>in: <code>cluster:Cluster</code>: the cluster.</li></ul></li><li>add method <code>XenAPI:Membership.disable</code><ul><li>in: <code>self:Host</code>: the host to modify</li><li>in: <code>cluster:Cluster</code>: the cluster name.</li></ul></li><li>add a cluster monitor thread which<ul><li>watches the <code>Host.memberships</code> field and calls <code>Plugin:Membership.create</code> and
<code>Plugin:Membership.destroy</code> to keep the local cluster software up-to-date
when any host in the pool changes its configuration</li><li>calls <code>Plugin:Cluster.query</code> after an <code>Plugin:Membership:create</code> or
<code>Plugin:Membership.destroy</code> to see whether the
SR needs maintenance</li><li>when all hosts have a last start time later than a <code>Membership</code>
record&rsquo;s <code>left</code> date, deletes the <code>Membership</code>.</li></ul></li><li>modify <code>XenAPI:Pool.join</code> to resync with the master&rsquo;s <code>Host.memberships</code> list.</li><li>modify <code>XenAPI:Pool.eject</code> to<ul><li>call <code>Membership.disable</code> in the cluster plugin to stop the <code>o2cb</code> service</li><li>call <code>Membership.destroy</code> in the cluster plugin to remove every other host
from the local configuration</li><li>remove the <code>Host</code> metadata from the pool</li><li>set <code>XenAPI:Membership.left</code> to <code>NOW()</code></li></ul></li><li>modify <code>XenAPI:Host.forget</code> to<ul><li>remove the <code>Host</code> metadata from the pool</li><li>set <code>XenAPI:Membership.left</code> to <code>NOW()</code></li><li>set <code>XenAPI:Cluster.maintenance_required</code> to true</li></ul></li></ul><p>A Cluster plugin called &ldquo;o2cb&rdquo; will be added which</p><ul><li>on <code>Plugin:Membership.destroy</code><ul><li>comment out the relevant node id in cluster.conf</li><li>set the &rsquo;needs a restart&rsquo; flag</li></ul></li><li>on <code>Plugin:Membership.create</code><ul><li>if the provided node id is too high: return an error. This means the
cluster needs to be rebooted to free node ids.</li><li>if the node id is not too high: rewrite the cluster.conf using
the &ldquo;online&rdquo; tool.</li></ul></li><li>on <code>Plugin:Cluster.start</code>: find the VDI with <code>type=o2cb_statefile</code>;
add this to the &ldquo;static-vdis&rdquo; list; <code>chkconfig</code> the service on. We
will use the global heartbeat mode of <code>o2cb</code>.</li><li>on <code>Plugin:Cluster.stop</code>: stop the service; <code>chkconfig</code> the service off;
remove the &ldquo;static-vdis&rdquo; entry; leave the VDI itself alone</li><li>keeps track of the current &rsquo;live&rsquo; cluster.conf which allows it to<ul><li>report the cluster service as &rsquo;needing a restart&rsquo; (which implies
we need maintenance mode)</li></ul></li></ul><p>Summary of differences between this and xHA:</p><ul><li>we allow for the possibility that hosts can join and leave, without
necessarily taking the whole cluster down. In the case of <code>o2cb</code> we
should be able to have <code>join</code> work live and only <code>eject</code> requires
maintenance mode</li><li>rather than write explicit RPCs to update cluster configuration state
we instead use an event watch and resync pattern, which is hopefully
more robust to network glitches while a reconfiguration is in progress.</li></ul><h1 id=managing-xhad>Managing xhad</h1><p>We need to ensure <code>o2cb</code> and <code>xhad</code> do not try to conflict by fencing
hosts at the same time. We shall:</p><ul><li><p>use the default <code>o2cb</code> timeouts (hosts fence if no I/O in 60s): this
needs to be short because disk I/O <em>on otherwise working hosts</em> can
be blocked while another host is failing/ has failed.</p></li><li><p>make the <code>xhad</code> host fence timeouts much longer: 300s. It&rsquo;s much more
important that this is reliable than fast. We will make this change
globally and not just when using OCFS2.</p></li></ul><p>In the <code>xhad</code> config we will cap the <code>HeartbeatInterval</code> and <code>StatefileInterval</code>
at 5s (the default otherwise would be 31s). This means that 60 heartbeat
messages have to be lost before <code>xhad</code> concludes that the host has failed.</p><h1 id=sm-plugin>SM plugin</h1><p>The SM plugin <code>OCFS2</code> will be a file-based plugin.</p><p>TODO: which file format by default?</p><p>The SM plugin will first check whether the <code>o2cb</code> cluster is active and fail
operations if it is not.</p><h1 id=io-paths>I/O paths</h1><p>When either HA or OCFS O2CB &ldquo;fences&rdquo; the host it will look to the admin like
a host crash and reboot. We need to (in priority order)</p><ol><li>help the admin <em>prevent</em> fences by monitoring their I/O paths
and fixing issues before they lead to trouble</li><li>when a fence/crash does happen, help the admin<ul><li>tell the difference between an I/O error (admin to fix) and a software
bug (which should be reported)</li><li>understand how to make their system more reliable</li></ul></li></ol><h2 id=monitoring-io-paths>Monitoring I/O paths</h2><p>If heartbeat I/O fails for more than 60s when running <code>o2cb</code> then the host will fence.
This can happen either</p><ul><li><p>for a good reason: for example the host software may have deadlocked or someone may
have pulled out a network cable.</p></li><li><p>for a bad reason: for example a network bond link failure may have been ignored
and then the second link failed; or the heartbeat thread may have been starved of
I/O bandwidth by other processes</p></li></ul><p>Since the consequences of fencing are severe &ndash; all VMs on the host crash simultaneously &ndash;
it is important to avoid the host fencing for bad reasons.</p><p>We should recommend that all users</p><ul><li>use network bonding for their network heartbeat</li><li>use multipath for their storage heartbeat</li></ul><p>Furthermore we need to <em>help</em> users monitor their I/O paths. It&rsquo;s no good if they use
a bonded network but fail to notice when one of the paths have failed.</p><p>The current XenServer HA implementation generates the following I/O-related alerts:</p><ul><li><code>HA_HEARTBEAT_APPROACHING_TIMEOUT</code> (priority 5 &ldquo;informational&rdquo;): when half the
network heartbeat timeout has been reached.</li><li><code>HA_STATEFILE_APPROACHING_TIMEOUT</code> (priority 5 &ldquo;informational&rdquo;): when half the
storage heartbeat timeout has been reached.</li><li><code>HA_NETWORK_BONDING_ERROR</code> (priority 3 &ldquo;service degraded&rdquo;): when one of the bond
links have failed.</li><li><code>HA_STATEFILE_LOST</code> (priority 2 &ldquo;service loss imminent&rdquo;): when the storage heartbeat
has completely failed and only the network heartbeat is left.</li><li>MULTIPATH_PERIODIC_ALERT (priority 3 &ldquo;service degrated&rdquo;): when one of the multipath
links have failed.</li></ul><p>Unfortunately alerts are triggered on &ldquo;edges&rdquo; i.e. when state changes, and not on &ldquo;levels&rdquo;
so it is difficult to see whether the link is currently broken.</p><p>We should define datasources suitable for use by xcp-rrdd to expose the current state
(and the history) of the I/O paths as follows:</p><ul><li><code>pif_&lt;name>_paths_failed</code>: the total number of paths which we know have failed.</li><li><code>pif_&lt;name>_paths_total</code>: the total number of paths which are configured.</li><li><code>sr_&lt;name>_paths_failed</code>: the total number of storage paths which we know have failed.</li><li><code>sr_&lt;name>_paths_total</code>: the total number of storage paths which are configured.</li></ul><p>The <code>pif</code> datasources should be generated by <code>xcp-networkd</code> which already has a
<a href=https://github.com/xapi-project/xcp-networkd/blob/bc0140feba19cf8dcced3bd66e54eeee112af819/networkd/network_monitor_thread.ml#L52 target=_blank>network bond monitoring thread</a>.
THe <code>sr</code> datasources should be generated by <code>xcp-rrdd</code> plugins since there is no
storage daemon to generate them.
We should create RRDs using the <code>MAX</code> consolidation function, otherwise information
about failures will be lost by averaging.</p><p>XenCenter (and any diagnostic tools) should warn when the system is at risk of fencing
in particular if any of the following are true:</p><ul><li><code>pif_&lt;name>_paths_failed</code> is non-zero</li><li><code>sr_&lt;name>_paths_failed</code> is non-zero</li><li><code>pif_&lt;name>_paths_total</code> is less than 2</li><li><code>sr_&lt;name>_paths_total</code> is less than 2</li></ul><p>XenCenter (and any diagnostic tools) should warn if any of the following <em>have been</em>
true over the past 7 days:</p><ul><li><code>pif_&lt;name>_paths_failed</code> is non-zero</li><li><code>sr_&lt;name>_paths_failed</code> is non-zero</li></ul><h2 id=heartbeat-qos>Heartbeat &ldquo;QoS&rdquo;</h2><p>The network and storage paths used by heartbeats <em>must</em> remain responsive otherwise
the host will fence (i.e. the host and all VMs will crash).</p><p>Outstanding issue: how slow can <code>multipathd</code> get? How does it scale with the number of
LUNs.</p><h1 id=post-crash-diagnostics>Post-crash diagnostics</h1><p>When a host crashes the effect on the user is severe: all the VMs will also
crash. In cases where the host crashed for a bad reason (such as a single failure
after a configuration error) we must help the user understand how they can
avoid the same situation happening again.</p><p>We must make sure the crash kernel runs reliably when <code>xhad</code> and <code>o2cb</code>
fence the host.</p><p>Xcp-rrdd will be modified to store RRDs in an <code>mmap(2)</code>d file sin the dom0
filesystem (rather than in-memory). Xcp-rrdd will call <code>msync(2)</code> every 5s
to ensure the historical records have hit the disk. We should use the same
on-disk format as RRDtool (or as close to it as makes sense) because it has
already been optimised to minimise the amount of I/O.</p><p>Xapi will be modified to run a crash-dump analyser program <code>xen-crash-analyse</code>.</p><p><code>xen-crash-analyse</code> will:</p><ul><li>parse the Xen and dom0 stacks and diagnose whether<ul><li>the dom0 kernel was panic&rsquo;ed by <code>o2cb</code></li><li>the Xen watchdog was fired by <code>xhad</code></li><li>anything else: this would indicate a bug that should be reported</li></ul></li><li>in cases where the system was fenced by <code>o2cb</code> or <code>xhad</code> then the analyser<ul><li>will read the archived RRDs and look for recent evidence of a path failure
or of a bad configuration (i.e. one where the total number of paths is 1)</li><li>will parse the <code>xhad.log</code> and look for evidence of heartbeats &ldquo;approaching
timeout&rdquo;</li></ul></li></ul><p>TODO: depending on what information we can determine from the analyser, we
will want to record some of it in the <code>Host_crash_dump</code> database table.</p><p>XenCenter will be modified to explain why the host crashed and explain what
the user should do to fix it, specifically:</p><ul><li>if the host crashed for no obvious reason then consider this a software
bug and recommend a bugtool/system-status-report is taken and uploaded somewhere</li><li>if the host crashed because of <code>o2cb</code> or <code>xhad</code> then either<ul><li>if there is evidence of path failures in the RRDs: recommend the user
increase the number of paths or investigate whether some of the equipment
(NICs or switches or HBAs or SANs) is unreliable</li><li>if there is evidence of insufficient paths: recommend the user add more
paths</li></ul></li></ul><h1 id=network-configuration>Network configuration</h1><p>The documentation should strongly recommend</p><ul><li>the management network is bonded</li><li>the management network is dedicated i.e. used only for management traffic
(including heartbeats)</li><li>the OCFS2 storage is multipathed</li></ul><p><code>xcp-networkd</code> will be modified to change the behaviour of the DHCP client.
Currently the <code>dhclient</code> will wait for a response and eventually background
itself. This is a big problem since DHCP can reset the hostname, and this can
break <code>o2cb</code>. Therefore we must insist that <code>PIF.reconfigure_ip</code> becomes
fully synchronous, supporting timeout and cancellation. Once the call returns
&ndash; whether through success or failure &ndash; there must not be anything in the
background which will change the system&rsquo;s hostname.</p><p>TODO: figure out whether we need to request &ldquo;maintenance mode&rdquo; for hostname
changes.</p><h1 id=maintenance-mode>Maintenance mode</h1><p>The purpose of &ldquo;maintenance mode&rdquo; is to take a host out of service and leave
it in a state where it&rsquo;s safe to fiddle with it without affecting services
in VMs.</p><p>XenCenter currently does the following:</p><ul><li><code>Host.disable</code>: prevents new VMs starting here</li><li>makes a list of all the VMs running on the host</li><li><code>Host.evacuate</code>: move the running VMs somewhere else</li></ul><p>The problems with maintenance mode are:</p><ul><li>it&rsquo;s not safe to fiddle with the host network configuration with storage
still attached. For NFS this risks deadlocking the SR. For OCFS2 this
risks fencing the host.</li><li>it&rsquo;s not safe to fiddle with the storage or network configuration if HA
is running because the host will be fenced. It&rsquo;s not safe to disable fencing
unless we guarantee to reboot the host on exit from maintenance mode.</li></ul><p>We should also</p><ul><li><code>PBD.unplug</code>: all storage. This allows the network to be safely reconfigured.
If the network is configured when NFS storage is plugged then the SR can
permanently deadlock; if the network is configured when OCFS2 storage is
plugged then the host can crash.</li></ul><p>TODO: should we add a <code>Host.prepare_for_maintenance</code> (better name TBD)
to take care of all this without XenCenter having to script it. This would also
help CLI and powershell users do the right thing.</p><p>TODO: should we insist that the host is rebooted to leave maintenance
mode? This would make maintenance mode more reliable and allow us to integrate
maintenance mode with xHA (where maintenance mode is a &ldquo;staged reboot&rdquo;)</p><p>TODO: should we leave all clusters as part of maintenance mode? We
probably need to do this to avoid fencing.</p><h1 id=walk-through-adding-ocfs2-storage>Walk-through: adding OCFS2 storage</h1><p>Assume you have an existing Pool of 2 hosts. First the client will set up
the O2CB cluster, choosing where to put the global heartbeat volume. The
client should check that the I/O paths have all been setup correctly with
bonding and multipath and prompt the user to fix any obvious problems.</p><p><a href=#image-4e40f6031a2f7c03a2fe4cc20eba98fe class=lightbox-link><img src=/new-docs/design/ocfs2/o2cb-enable-external.svg alt="The client enables O2CB and then creates an SR" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-4e40f6031a2f7c03a2fe4cc20eba98fe><img src=/new-docs/design/ocfs2/o2cb-enable-external.svg alt="The client enables O2CB and then creates an SR" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Internally within <code>Pool.enable_o2cb</code> Xapi will set up the cluster metadata
on every host in the pool:</p><p><a href=#image-5969f0681e865df4c4ec190de632463f class=lightbox-link><img src=/new-docs/design/ocfs2/o2cb-enable-internal1.svg alt="Xapi creates the cluster configuration and each host updates its metadata" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-5969f0681e865df4c4ec190de632463f><img src=/new-docs/design/ocfs2/o2cb-enable-internal1.svg alt="Xapi creates the cluster configuration and each host updates its metadata" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>At this point all hosts have in-sync <code>cluster.conf</code> files but all cluster
services are disabled. We also have <code>requires_mainenance=true</code> on all
<code>Membership</code> entries and the global <code>Cluster</code> has <code>enabled=false</code>.
The client will now try to enable the cluster with <code>Cluster.enable</code>:</p><p><a href=#image-8c80996b497a168ec79135c4db5bde97 class=lightbox-link><img src=/new-docs/design/ocfs2/o2cb-enable-internal2.svg alt="Xapi enables the cluster software on all hosts" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-8c80996b497a168ec79135c4db5bde97><img src=/new-docs/design/ocfs2/o2cb-enable-internal2.svg alt="Xapi enables the cluster software on all hosts" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Now all hosts are in the cluster and the SR can be created using the standard
SM APIs.</p><h1 id=walk-through-remove-a-host>Walk-through: remove a host</h1><p>Assume you have an existing Pool of 2 hosts with <code>o2cb</code> clustering enabled
and at least one <code>ocfs2</code> filesystem mounted. If the host is online then
<code>XenAPI:Pool.eject</code> will:</p><p><a href=#image-100a86e62439033d2d60f07775952762 class=lightbox-link><img src=/new-docs/design/ocfs2/pool-eject.svg alt="Xapi ejects a host from the pool" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-100a86e62439033d2d60f07775952762><img src=/new-docs/design/ocfs2/pool-eject.svg alt="Xapi ejects a host from the pool" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Note that:</p><ul><li>All hosts will have modified their <code>o2cb</code> <code>cluster.conf</code> to comment out
the former host</li><li>The <code>Membership</code> table still remembers the node number of the ejected host&ndash;
this cannot be re-used until the SR is taken down for maintenance.</li><li>All hosts can see the difference between their current <code>cluster.conf</code>
and the one they would use if they restarted the cluster service, so all
hosts report that the cluster must be taken offline i.e. <code>requires_maintence=true</code>.</li></ul><h1 id=summary-of-the-impact-on-the-admin>Summary of the impact on the admin</h1><p>OCFS2 is fundamentally a different type of storage to all existing storage
types supported by xapi. OCFS2 relies upon O2CB, which provides
<a href=/new-docs/design/ocfs2/../../../features/HA/HA.html>Host-level High Availability</a>. All HA implementations
(including O2CB and <code>xhad</code>) impose restrictions on the server admin to
prevent unnecessary host &ldquo;fencing&rdquo; (i.e. crashing). Once we have OCFS2 as
a feature, we will have to live with these restrictions which previously only
applied when HA was explicitly enabled. To reduce complexity we will not try
to enforce restrictions only when OCFS2 is being used or is likely to be used.</p><h2 id=impact-even-if-not-using-ocfs2>Impact even if not using OCFS2</h2><ul><li>&ldquo;Maintenance mode&rdquo; now includes detaching all storage.</li><li>Host network reconfiguration can only be done in maintenance mode</li><li>XenServer HA enable takes longer</li><li>XenServer HA failure detection takes longer</li><li>Network configuration with DHCP must be fully synchronous i.e. it wil block
until the DHCP server responds. On a timeout, the change will not be made.</li></ul><h2 id=impact-when-using-ocfs2>Impact when using OCFS2</h2><ul><li>Sometimes a host will not be able to join the pool without taking the
pool into maintenance mode</li><li>Every VM will have to be XSM&rsquo;ed (is that a verb?) to the new OCFS2 storage.
This means that VMs with more than 2 snapshots will have their snapshots
deleted; it means you need to provision another storage target, temporarily
doubling your storage needs; and it will take a long time.</li><li>There will now be 2 different reasons why a host has fenced which the
admin needs to understand.</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=patches-in-vdis>patches in VDIs</h1><p>&ldquo;Patches&rdquo; are signed binary blobs which can be queried and applied.
They are stored in the dom0 filesystem under <code>/var/patch</code>. Unfortunately
the patches can be quite large &ndash; imagine a repo full of RPMs &ndash; and
the dom0 filesystem is usually quite small, so it can be difficult
to upload and apply some patches.</p><p>Instead of writing patches to the dom0 filesystem, we shall write them
to disk images (VDIs) instead. We can then take advantage of features like</p><ul><li>shared storage</li><li>cross-host <code>VDI.copy</code></li></ul><p>to manage the patches.</p><h1 id=xenapi-changes>XenAPI changes</h1><ol><li><p>Add a field <code>pool_patch.VDI</code> of type <code>Ref(VDI)</code>. When a new patch is
stored in a VDI, it will be referenced here. Older patches and cleaned
patches will have invalid references here.</p></li><li><p>The HTTP handler for uploading patches will choose an SR to stream the
patch into. It will prefer to use the <code>pool.default_SR</code> and fall back
to choosing an SR on the master whose driver supports the <code>VDI_CLONE</code>
capability: we want the ability to fast clone patches, one per host
concurrently installing them. A VDI will be created whose size is 4x
the apparent size of the patch, defaulting to 4GiB if we have no size
information (i.e. no <code>content-length</code> header)</p></li><li><p><code>pool_patch.clean_on_host</code> will be deprecated. It will still try to
clean a patch <em>from the local filesystem</em> but this is pointless for
the new VDI patch uploads.</p></li><li><p><code>pool_patch.clean</code> will be deprecated. It will still try to clean a patch
from the <em>local filesystem</em> of the master but this is pointless for the
new VDI patch uploads.</p></li><li><p><code>pool_patch.pool_clean</code> will be deprecated. It will destroy any associated
patch VDI. Users will be encouraged to call <code>VDI.destroy</code> instead.</p></li></ol><h1 id=changes-beneath-the-xenapi>Changes beneath the XenAPI</h1><ol><li><p><code>pool_patch</code> records will only be deleted if both the <code>filename</code> field
refers to a missing file on the master <em>and</em> the <code>VDI</code> field is a dangling
reference</p></li><li><p>Patches stored in VDIs will be stored within a filesystem, like we used
to do with suspend images. This is needed because (a) we want to execute
the patches and block devices cannot be executed; and (b) we can use
spare space in the VDI as temporary scratch space during the patch
application process. Within the VDI we will call patches <code>patch</code> rather
than using a complicated filename.</p></li><li><p>When a host wishes to apply a patch it will call <code>VDI.copy</code> to duplicate
the VDI to a locally-accessible SR, mount the filesystem and execute it.
If the patch is still in the master&rsquo;s dom0 filesystem then it will fall
back to the HTTP handler.</p></li></ol><h1 id=summary-of-the-impact-on-the-admin>Summary of the impact on the admin</h1><ul><li>There will nolonger be a size limit on hotfixes imposed by the mechanism
itself.</li><li>There must be enough free space in an SR connected to the host to be able
to apply a patch on that host.</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=pci-passthrough-support>PCI passthrough support</h1><h2 id=introduction>Introduction</h2><p>GPU passthrough is already available in XAPI, this document proposes to also
offer passthrough for all PCI devices through XAPI.</p><h2 id=design-proposal>Design proposal</h2><p>New methods for PCI object:</p><ul><li><p><code>PCI.enable_dom0_access</code></p></li><li><p><code>PCI.disable_dom0_access</code></p></li><li><p><code>PCI.get_dom0_access_status</code>: compares the outputs of <code>/opt/xensource/libexec/xen-cmdline</code>
and <code>/proc/cmdline</code> to produce one of the four values that can be currently contained
in the <code>PGPU.dom0_access</code> field:</p><ul><li>disabled</li><li>disabled_on_reboot</li><li>enabled</li><li>enabled_on_reboot</li></ul><p>How do determine the expected dom0 access state:
If the device id is present in both <code>pciback.hide</code> of <code>/proc/cmdline</code> and <code>xen-cmdline</code>: <code>enabled</code>
If the device id is present not in both <code>pciback.hide</code> of <code>/proc/cmdline</code> and <code>xen-cmdline</code>: <code>disabled</code>
If the device id is present in the <code>pciback.hide</code> of <code>/proc/cmdline</code> but not in the one of <code>xen-cmdline</code>: <code>disabled_on_reboot</code>
If the device id is not present in the <code>pciback.hide</code> of <code>/proc/cmdline</code> but is in the one of <code>xen-cmdline</code>: <code>enabled_on_reboot</code></p><p>A function rather than a field makes the data always accurate and even accounts for
changes made by users outside XAPI, directly through <code>/opt/xensource/libexec/xen-cmdline</code></p></li></ul><p>With these generic methods available, the following field and methods will be <em>deprecated</em>:</p><ul><li><code>PGPU.enable_dom0_access</code></li><li><code>PGPU.disable_dom0_access</code></li><li><code>PGPU.dom0_access</code> (DB field)</li></ul><p>They would still be usable and up to date with the same info as for the PCI methods.</p><h2 id=test-cases>Test cases</h2><ul><li><p>hide a PCI:</p><ul><li>call <code>PCI.disable_dom0_access</code> on an <code>enabled</code> PCI</li><li>check the PCI goes in state <code>disabled_on_reboot</code></li><li>reboot the host</li><li>check the PCI goes in state <code>disabled</code></li></ul></li><li><p>unhide a PCI:</p><ul><li>call <code>PCI.enable_dom0_access</code> on an <code>disabled</code> PCI</li><li>check the PCI goes in state <code>enabled_on_reboot</code></li><li>reboot the host</li><li>check the PCI goes in state <code>enabled</code></li></ul></li><li><p>get a PCI dom0 access state:</p><ul><li>on a <code>enabled</code> PCI, make sure the <code>get_dom0_access_status</code> returns <code>enabled</code></li><li>hide the PCI</li><li>make sure the <code>get_dom0_access_status</code> returns <code>disabled_on_reboot</code></li><li>reboot</li><li>make sure the <code>get_dom0_access_status</code> returns <code>disabled</code></li><li>unhide the PCI</li><li>make sure the <code>get_dom0_access_status</code> returns <code>enabled_on_reboot</code></li><li>reboot</li><li>make sure the <code>get_dom0_access_status</code> returns <code>enabled</code></li></ul></li><li><p>Check PCI/PGPU dom0 access coherence:</p><ul><li>hide a PCI belonging to a PGPU and make sure both states remains coherent at every step</li><li>unhide a PCI belonging to a PGPU and make sure both states remains coherent at every step</li><li>hide a PGPU and make sure its and its PCI&rsquo;s states remains coherent at every step</li><li>unhide a PGPU and make sure its and its PCI&rsquo;s states remains coherent at every step</li></ul></li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=pool-wide-ssh>Pool-wide SSH</h1><h2 id=background>Background</h2><p>The SMAPIv3 plugin architecture requires that storage plugins are able to work
in the absence of xapi. Amongst other benefits, this allows them to be tested
in isolation, are able to be shared more widely than just within the XenServer
community and will cause less load on xapi&rsquo;s database.</p><p>However, many of the currently existing SMAPIv1 backends require inter-host
operations to be performed. This is achieved via the use of the Xen-API call
&lsquo;host.call_plugin&rsquo;, which allows an API user to execute a pre-installed plugin
on any pool member. This is important for operations such as coalesce / snapshot
where the active data path for a VM somewhere in the pool needs to be refreshed
in order to complete the operation. In order to use this, the RPM in which the
SM backend lives is used to deliver a plugin script into /etc/xapi.d/plugins,
and this executes the required function when the API call is made.</p><p>In order to support these use-cases without xapi running, a new mechanism needs
to be provided to allow the execution of required functionality on remote hosts.
The canonical method for remotely executing scripts is ssh - the secure shell.
This design proposal is setting out how xapi might manage the public and
private keys to enable passwordless authentication of ssh sessions between all
hosts in a pool.</p><h2 id=modifications-to-the-host>Modifications to the host</h2><p>On firstboot (and after being ejected), the host should generate a
host key (already done I believe), and an authentication key for the
user (root/xapi?).</p><h2 id=modifications-to-xapi>Modifications to xapi</h2><p>Three new fields will be added to the host object:</p><ul><li><p><code>host.ssh_public_host_key : string</code>: This is the host key that identifies the host
during the initial ssh key exchange protocol. This should be added to the
&lsquo;known_hosts&rsquo; field of any other host wishing to ssh to this host.</p></li><li><p><code>host.ssh_public_authentication_key : string</code>: This field is the public
key used for authentication when sshing from the root account on that host -
host A. This can be added to host B&rsquo;s <code>authorized_keys</code> file in order to
allow passwordless logins from host A to host B.</p></li><li><p><code>host.ssh_ready : bool</code>: A boolean flag indicating that the configuration
files in use by the ssh server/client on the host are up to date.</p></li></ul><p>One new field will be added to the pool record:</p><ul><li><code>pool.revoked_authentication_keys : string list</code>: This field records all
authentication keys that have been used by hosts in the past. It is updated
when a host is ejected from the pool.</li></ul><h3 id=pool-join>Pool Join</h3><p>On pool join, the master creates the record for the new host and populates the
two public key fields with values supplied by the joining host. It then sets
the <code>ssh_ready</code> field on all other hosts to <code>false</code>.</p><p>On each host in the pool, a thread is watching for updates to the
<code>ssh_ready</code> value for the local host. When this is set to false, the host
then adds the keys from xapi&rsquo;s database to the appropriate places in the ssh
configuration files and restarts sshd. Once this is done, the host sets the
<code>ssh_ready</code> field to &rsquo;true&rsquo;</p><h3 id=pool-eject>Pool Eject</h3><p>On pool eject, the host&rsquo;s ssh_public_host_key is lost, but the authetication key is added to a list of revoked keys on the pool object. This allows all other hosts to remove the key from the authorized_keys list when they next sync, which in the usual case is immediately the database is modified due to the event watch thread. If the host is offline though, the authorized_keys file will be updated the next time the host comes online.</p><h2 id=questions>Questions</h2><ul><li>Do we want a new user? e.g. &lsquo;xapi&rsquo; - how would we then use this user to execute privileged things? setuid binaries?</li><li>Is keeping the revoked_keys list useful? If we &lsquo;control the world&rsquo; of the authorized_keys file, we could just remove anything that&rsquo;s currently in there that xapi doesn&rsquo;t know about</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=process-events-from-xenopsd-in-a-timely-manner>Process events from xenopsd in a timely manner</h1><h1 id=background>Background</h1><p>There is a significant delay between the VM being unpaused and XAPI reporting it
as started during a bootstorm.
It can happen that the VM is able to send UDP packets already, but XAPI still reports it as not started for minutes.</p><p>XAPI currently processes all events from xenopsd in a single thread, the unpause
events get queued up behind a lot of other events generated by the already
running VMs.</p><p>We need to ensure that unpause events from xenopsd get processed in a timely
manner, even if XAPI is busy processing other events.</p><h1 id=timely-processing-of-events>Timely processing of events</h1><p>If we process the events in a Round-Robin fashion then <code>unpause</code> events are reported in a timely fashion.
We need to ensure that events operating on the same VM are not processed in parallel.</p><p>Xenopsd already has code that does exactly this, the purpose of the <a href=https://github.com/xapi-project/xenopsd/pull/337 target=_blank>xapi-work-queues refactoring PR</a> is to
reuse this code in XAPI by creating a shared package between xenopsd and xapi: <code>xapi-work-queues</code>.</p><h1 id=xapi-work-queues>xapi-work-queues</h1><p>From the documentation of the new <a href=https://edwintorok.github.io/xapi-work-queues/Xapi_work_queues.html target=_blank>Worker Pool interface</a>:</p><p>A worker pool has a limited number of worker threads.
Each worker pops one tagged item from the queue in a round-robin fashion.
While the item is executed the tag temporarily doesn&rsquo;t participate in round-robin scheduling.
If during execution more items get queued with the same tag they get redirected to a private queue.
Once the item finishes execution the tag will participate in RR scheduling again.</p><p>This ensures that items with the same tag do not get executed in parallel,
and that a tag with a lot of items does not starve the execution of other tags.</p><p>The XAPI side of the changes will <a href="https://github.com/edwintorok/xen-api/commit/b367bf86d3af4f773db9bf5d1500a4dec0f99bfa?diff=unified#diff-344dc1d17c4663add7fe5500813feef2" target=_blank>look like this</a></p><p>Known limitations: The active per-VM events should be a small number, this is already ensured in the <code>push_with_coalesce</code> / <code>should_keep</code> code on the <a href=https://github.com/xapi-project/xenopsd/blob/master/lib/xenops_server.ml#L441 target=_blank>xenopsd side</a>. Events to XAPI from xenopsd should already arrive coalesced.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (xenserver 6.5 sp1)</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/12>#12</a></td></tr></table></header><h1 id=rdp-control>RDP control</h1><h3 id=purpose>Purpose</h3><p>To administer guest VMs it can be useful to connect to them over Remote Desktop Protocol (RDP). XenCenter supports this; it has an integrated RDP client.</p><p>First it is necessary to turn on the RDP service in the guest.</p><p>This can be controlled from XenCenter. Several layers are involved. This description starts in the guest and works up the stack to XenCenter.</p><p>This feature was completed in the first quarter of 2015, and released in Service Pack 1 for XenServer 6.5.</p><h3 id=the-guest-agent>The guest agent</h3><p>The XenServer guest agent installed in Windows VMs can turn the RDP service on and off, and can report whether it is running.</p><p>The guest agent is at <a href=https://github.com/xenserver/win-xenguestagent target=_blank>https://github.com/xenserver/win-xenguestagent</a></p><p>Interaction with the agent is done through some Xenstore keys:</p><p>The guest agent running in domain N writes two xenstore nodes when it starts up:</p><ul><li><code>/local/domain/N/control/feature-ts = 1</code></li><li><code>/local/domain/N/control/feature-ts2 = 1</code></li></ul><p>This indicates support for the rest of the functionality described below.</p><p>(The &ldquo;&mldr;ts2&rdquo; flag is new for this feature; older versions of the guest agent wrote the &ldquo;&mldr;ts&rdquo; flag and had support for only a subset of the functionality (no firewall modification), and had a bug in updating <code>.../data/ts</code>.)</p><p>To indicate whether RDP is running, the guest agent writes the string &ldquo;1&rdquo; (running) or &ldquo;0&rdquo; (disabled) to xenstore node</p><p><code>/local/domain/N/data/ts</code>.</p><p>It does this on start-up, and also in response to the deletion of that node.</p><p>The guest agent also watches xenstore node <code>/local/domain/N/control/ts</code> and it turns RDP on and off in response to &ldquo;1&rdquo; or &ldquo;0&rdquo; (respectively) being written to that node. The agent acknowledges the request by deleting the node, and afterwards it deletes <code>local/domain/N/data/ts</code>, thus triggering itself to update that node as described above.</p><p>When the guest agent turns the RDP service on/off, it also modifies the standard Windows firewall to allow/forbid incoming connections to the RDP port. This is the same as the firewall change that happens automatically when the RDP service is turned on/off through the standard Windows GUI.</p><h3 id=xapi-etc>XAPI etc.</h3><p>xenopsd sets up watches on xenstore nodes including the <code>control</code> tree and <code>data/ts</code>, and prompts xapi to react by updating the relevant VM guest metrics record, which is available through a XenAPI call.</p><p>XenAPI includes a new message (function call) which can be used to ask the guest agent to turn RDP on and off.</p><p>This is <code>VM.call_plugin</code> (analogous to <code>Host.call_plugin</code>) in the hope that it can be used for other purposes in the future, even though for now it does not really call a plugin.</p><p>To use it, supply <code>plugin="guest-agent-operation"</code> and either <code>fn="request_rdp_on"</code> or <code>fn="request_rdp_off"</code>.</p><p>See <a href=http://xapi-project.github.io/xen-api/classes/vm.html target=_blank>http://xapi-project.github.io/xen-api/classes/vm.html</a></p><p>The function strings are named with &ldquo;request&rdquo; (rather than, say, &ldquo;enable_rdp&rdquo; or &ldquo;turn_rdp_on&rdquo;) to make it clear that xapi only makes a request of the guest: when one of these calls returns successfully this means only that the appropriate string (1 or 0) was written to the <code>control/ts</code> node and it is up to the guest whether it responds.</p><h3 id=xencenter>XenCenter</h3><h4 id=behaviour-on-older-xenserver-versions-that-do-not-support-rdp-control>Behaviour on older XenServer versions that do not support RDP control</h4><p>Note that the current behaviour depends on some global options: &ldquo;Enable Remote Desktop console scanning&rdquo; and &ldquo;Automatically switch to the Remote Desktop console when it becomes available&rdquo;.</p><ol><li>When tools are not installed:<ul><li>As of XenCenter 6.5, the RDP button is absent.</li></ul></li><li>When tools are installed but RDP is not switched on in the guest:<ol><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is on:<ul><li>The RDP button is present but greyed out. (It seems to sometimes read &ldquo;Switch to Remote Desktop&rdquo; and sometimes read &ldquo;Looking for guest console&mldr;&rdquo;: I haven&rsquo;t yet worked out the difference).</li><li>We scan the RDP port to detect when RDP is turned on</li></ul></li><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is off:<ul><li>The RDP button is enabled and reads &ldquo;Switch to Remote Desktop&rdquo;</li></ul></li></ol></li><li>When tools are installed and RDP is switched on in the guest:<ol><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is on:<ul><li>The RDP button is enabled and reads &ldquo;Switch to Remote Desktop&rdquo;</li><li>If &ldquo;Automatically switch&rdquo; is on, we switch to RDP immediately we detect it</li></ul></li><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is off:<ul><li>As above, the RDP button is enabled and reads &ldquo;Switch to Remote Desktop&rdquo;</li></ul></li></ol></li></ol><h4 id=new-behaviour-on-xenserver-versions-that-support-rdp-control>New behaviour on XenServer versions that support RDP control</h4><ol><li>This new XenCenter behaviour is only for XenServer versions that support RDP control, with guests with the new guest agent: behaviour must be unchanged if the server or guest-agent is older.</li><li>There should be no change in the behaviour for Linux guests, either PV or HVM varieties: this must be tested.</li><li>We should never scan the RDP port; instead we should watch for a change in the relevant variable in guest_metrics.</li><li>The XenCenter option &ldquo;Enable Remote Desktop console scanning&rdquo; should change to read &ldquo;Enable Remote Desktop console scanning (XenServer 6.5 and earlier)&rdquo;</li><li>The XenCenter option &ldquo;Automatically switch to the Remote Desktop console when it becomes available&rdquo; should be enabled even when &ldquo;Enable Remote Desktop console scanning&rdquo; is off.</li><li>When tools are not installed:<ul><li>As above, the RDP button should be absent.</li></ul></li><li>When tools are installed but RDP is not switched on in the guest:<ul><li>The RDP button should be enabled and read &ldquo;Turn on Remote Desktop&rdquo;</li><li>If pressed, it should launch a dialog with the following wording: &ldquo;Would you like to turn on Remote Desktop in this VM, and then connect to it over Remote Desktop? [Yes] [No]&rdquo;</li><li>That button should turn on RDP, wait for RDP to become enabled, and switch to an RDP connection. It should do this even if &ldquo;Automatically switch&rdquo; is off.</li></ul></li><li>When tools are installed and RDP is switched on in the guest:<ul><li>The RDP button should be enabled and read &ldquo;Switch to Remote Desktop&rdquo;</li><li>If &ldquo;Automatically switch&rdquo; is on, we should switch to RDP immediately</li><li>There is no need for us to provide UI to switch RDP off again</li></ul></li><li>We should also test the case where RDP has been switched on in the guest before the tools are installed.</li></ol><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7,0)</span></td></tr></table></header><h1 id=rrdd-archival-redesign>RRDD archival redesign</h1><h2 id=introduction>Introduction</h2><p>Current problems with rrdd:</p><ul><li>rrdd stores knowledge about whether it is running on a master or a slave</li></ul><p>This determines the host to which rrdd will archive a VM&rsquo;s rrd when the VM&rsquo;s
domain disappears - rrdd will always try to archive to the master. However,
when a host joins a pool as a slave rrdd is not restarted so this knowledge is
out of date. When a VM shuts down on the slave rrdd will archive the rrd
locally. When starting this VM again the master xapi will attempt to push any
locally-existing rrd to the host on which the VM is being started, but since
no rrd archive exists on the master the slave rrdd will end up creating a new
rrd and the previous rrd will be lost.</p><ul><li>rrdd handles rebooting VMs unpredictably</li></ul><p>When rebooting a VM, there is a chance rrdd will attempt to update that VM&rsquo;s rrd
during the brief period when there is no domain for that VM. If this happens,
rrdd will archive the VM&rsquo;s rrd to the master, and then create a new rrd for the
VM when it sees the new domain. If rrdd doesn&rsquo;t attempt to update that VM&rsquo;s rrd
during this period, rrdd will continue to add data for the new domain to the old
rrd.</p><h2 id=proposal>Proposal</h2><p>To solve these problems, we will remove some of the intelligence from rrdd and
make it into more of a slave process of xapi. This will entail removing all
knowledge from rrdd of whether it is running on a master or a slave, and also
modifying rrdd to only start monitoring a VM when it is told to, and only
archiving an rrd (to a specified address) when it is told to. This matches the
way xenopsd only manages domains which it has been told to manage.</p><h2 id=design>Design</h2><p>For most VM lifecycle operations, xapi and rrdd processes (sometimes across more
than one host) cooperate to start or stop recording a VM&rsquo;s metrics and/or to
restore or backup the VM&rsquo;s archived metrics. Below we will describe, for each
relevant VM operation, how the VM&rsquo;s rrd is currently handled, and how we propose
it will be handled after the redesign.</p><h4 id=vmdestroy>VM.destroy</h4><p>The master xapi makes a remove_rrd call to the local rrdd, which causes rrdd to
to delete the VM&rsquo;s archived rrd from disk. This behaviour will remain unchanged.</p><h4 id=vmstart_on-and-vmresume_on>VM.start(_on) and VM.resume(_on)</h4><p>The master xapi makes a push_rrd call to the local rrdd, which causes rrdd to
send any locally-archived rrd for the VM in question to the rrdd of the host on
which the VM is starting. This behaviour will remain unchanged.</p><h4 id=vmshutdown-and-vmsuspend>VM.shutdown and VM.suspend</h4><p>Every update cycle rrdd compares its list of registered VMs to the list of
domains actually running on the host. Any registered VMs which do not have a
corresponding domain have their rrds archived to the rrdd running on the host
believed to be the master. We will change this behaviour by stopping rrdd from
doing the archiving itself; instead we will expose a new function in rrdd&rsquo;s
interface:</p><div class="wrap-code highlight"><pre tabindex=0><code>val archive_rrd : vm_uuid:string -&gt; remote_address:string -&gt; unit</code></pre></div><p>This will cause rrdd to remove the specified rrd from its table of registered
VMs, and archive the rrd to the specified host. When a VM has finished shutting
down or suspending, the xapi process on the host on which the VM was running
will call archive_rrd to ask the local rrdd to archive back to the master rrdd.</p><h4 id=vmreboot>VM.reboot</h4><p>Removing rrdd&rsquo;s ability to automatically archive the rrds for disappeared
domains will have the bonus effect of fixing how the rrds of rebooting VMs are
handled, as we don&rsquo;t want the rrds of rebooting VMs to be archived at all.</p><h4 id=vmcheckpoint>VM.checkpoint</h4><p>This will be handled automatically, as internally VM.checkpoint carries out a
VM.suspend followed by a VM.resume.</p><h4 id=vmpool_migrate-and-vmmigrate_send>VM.pool_migrate and VM.migrate_send</h4><p>The source host&rsquo;s xapi makes a migrate_rrd call to the local rrd, with a
destination address and an optional session ID. The session ID is only required
for cross-pool migration. The local rrdd sends the rrd for that VM to the
destination host&rsquo;s rrdd as an HTTP PUT. This behaviour will remain unchanged.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr></table></header><h1 id=rrdd-plugin-protocol-v2>RRDD plugin protocol v2</h1><h2 id=motivation>Motivation</h2><p>rrdd plugins currently report datasources via a shared-memory file, using the
following format:</p><div class="wrap-code highlight"><pre tabindex=0><code>DATASOURCES
000001e4
dba4bf7a84b6d11d565d19ef91f7906e
{
  &#34;timestamp&#34;: 1339685573,
  &#34;data_sources&#34;: {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;64.33&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;62.14&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    }
  }
}</code></pre></div><p>This format contains four main components:</p><ul><li>A constant header string</li></ul><p><code>DATASOURCES</code></p><p>This should always be present.</p><ul><li>The JSON data length, encoded as hexadecimal</li></ul><p><code>000001e4</code></p><ul><li>The md5sum of the JSON data</li></ul><p><code>dba4bf7a84b6d11d565d19ef91f7906e</code></p><ul><li>The JSON data itself, encoding the values and metadata associated with the
reported datasources.</li></ul><h3 id=example>Example</h3><div class="wrap-code highlight"><pre tabindex=0><code>{
  &#34;timestamp&#34;: 1339685573,
  &#34;data_sources&#34;: {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;64.33&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;62.14&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    }
  }
}</code></pre></div><p>The disadvantage of this protocol is that rrdd has to parse the entire JSON
structure each tick, even though most of the time only the values will change.</p><p>For this reason a new protocol is proposed.</p><h2 id=protocol-v2>Protocol V2</h2><table><thead><tr><th>value</th><th>bits</th><th>format</th><th>notes</th></tr></thead><tbody><tr><td>header string</td><td>(string length)*8</td><td>string</td><td>&ldquo;DATASOURCES&rdquo; as in the V1 protocol</td></tr><tr><td>data checksum</td><td>32</td><td>int32</td><td>binary-encoded crc32 of the concatenation of the encoded timestamp and datasource values</td></tr><tr><td>metadata checksum</td><td>32</td><td>int32</td><td>binary-encoded crc32 of the metadata string (see below)</td></tr><tr><td>number of datasources</td><td>32</td><td>int32</td><td>only needed if the metadata has changed - otherwise RRDD can use a cached value</td></tr><tr><td>timestamp</td><td>64</td><td>int64</td><td>Unix epoch</td></tr><tr><td>datasource values</td><td>n * 64</td><td>int64 | double</td><td>n is the number of datasources exported by the plugin, type dependent on the setting in the metadata for value_type [int64|float]</td></tr><tr><td>metadata length</td><td>32</td><td>int32</td><td></td></tr><tr><td>metadata</td><td>(string length)*8</td><td>string</td><td></td></tr></tbody></table><p>All integers/double are bigendian. The metadata will have the same JSON-based format as
in the V1 protocol, minus the timestamp and <code>value</code> key-value pair for each
datasource.</p><table><thead><tr><th>field</th><th>values</th><th>notes</th><th>required</th></tr></thead><tbody><tr><td>description</td><td>string</td><td>Description of the datasource</td><td>no</td></tr><tr><td>owner</td><td>host | vm | sr</td><td>The object to which the data relates</td><td>no, default host</td></tr><tr><td>value_type</td><td>int64 | float</td><td>The type of the datasource</td><td>yes</td></tr><tr><td>type</td><td>absolute | derive | gauge</td><td>The type of measurement being sent. Absolute for counters which are reset on reading, derive stores the derivative of the recorded values (useful for metrics which continually increase like amount of data written since start), gauge for things like temperature</td><td>no, default absolute</td></tr><tr><td>default</td><td>true | false</td><td>Whether the source is default enabled or not</td><td>no, default false</td></tr><tr><td>units</td><td></td><td>The units the data should be displayed in</td><td>no</td></tr><tr><td>min</td><td></td><td>The minimum value for the datasource</td><td>no, default -infinity</td></tr><tr><td>max</td><td></td><td>The maximum value for the datasource</td><td>no, default +infinity</td></tr></tbody></table><h3 id=example-1>Example</h3><div class="wrap-code highlight"><pre tabindex=0><code>{
  &#34;datasources&#34;: {
    &#34;memory_reclaimed&#34;: {
      &#34;description&#34;:&#34;Host memory reclaimed by squeezed&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;:&#34;int64&#34;,
      &#34;type&#34;:&#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;:&#34;B&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    },
    &#34;memory_reclaimed_max&#34;: {
      &#34;description&#34;:&#34;Host memory that could be reclaimed by squeezed&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;:&#34;int64&#34;,
      &#34;type&#34;:&#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;:&#34;B&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    },
    {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;: &#34;float&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;: &#34;float&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    }
  }
}</code></pre></div><p>The above formatting is not required, but added here for readability.</p><h2 id=reading-algorithm>Reading algorithm</h2><div class="wrap-code highlight"><pre tabindex=0><code>if header != expected_header:
    raise InvalidHeader()
if data_checksum == last_data_checksum:
    raise NoUpdate()
if data_checksum != crc32(encoded_timestamp_and_values):
    raise InvalidChecksum()
if metadata_checksum == last_metadata_checksum:
    for datasource, value in cached_datasources, values:
        update(datasource, value)
else:
    if metadata_checksum != crc32(metadata):
        raise InvalidChecksum()
    cached_datasources = create_datasources(metadata)
    for datasource, value in cached_datasources, values:
        update(datasource, value)</code></pre></div><p>This means that for a normal update, RRDD will only have to read the header plus
the first (16 + 16 + 4 + 8 + 8*n) bytes of data, where n is the number of
datasources exported by the plugin. If the metadata changes RRDD will have to
read all the data (and parse the metadata).</p><p>n.b. the timestamp reported by plugins is not currently used by RRDD - it uses
its own global timestamp.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr></table></header><h1 id=rrdd-plugin-protocol-v3>RRDD plugin protocol v3</h1><h2 id=motivation>Motivation</h2><p>rrdd plugins protocol v2 report datasources via shared-memory file, however it
has various limitations :</p><ul><li>metrics are unique by their names, thus it is not possible cannot have
several metrics that shares a same name (e.g vCPU usage per vm)</li><li>only number metrics are supported, for example we can&rsquo;t expose string
metrics (e.g CPU Model)</li></ul><p>Therefore, it implies various limitations on plugins and limits
<a href=https://openmetrics.io/ target=_blank>OpenMetrics</a> support for the metrics daemon.</p><p>Moreover, it may not be practical for plugin developpers and parser implementations :</p><ul><li>json implementations may not keep insersion order on maps, which can cause
issues to expose datasource values as it is sensitive to the order of the metadata map</li><li>header length is not constant and depends on datasource count, which complicates parsing</li><li>it still requires a quite advanced parser to convert between bytes and numbers according to metadata</li></ul><p>A simpler protocol is proposed, based on OpenMetrics binary format to ease plugin and parser implementations.</p><h2 id=protocol-v3>Protocol V3</h2><p>For this protocol, we still use a shared-memory file, but significantly change the structure of the file.</p><table><thead><tr><th>value</th><th>bits</th><th>format</th><th>notes</th></tr></thead><tbody><tr><td>header string</td><td>12*8=96</td><td>string</td><td>&ldquo;OPENMETRICS1&rdquo; which is one byte longer than &ldquo;DATASOURCES&rdquo;, intentionally made at 12 bytes for alignment purposes</td></tr><tr><td>data checksum</td><td>32</td><td>uint32</td><td>Checksum of the concatenation of the rest of the header (from timestamp) and the payload data</td></tr><tr><td>timestamp</td><td>64</td><td>uint64</td><td>Unix epoch</td></tr><tr><td>payload length</td><td>32</td><td>uint32</td><td>Payload length</td></tr><tr><td>payload data</td><td>8*(payload length)</td><td>binary</td><td>OpenMetrics encoded metrics data (protocol-buffers format)</td></tr></tbody></table><p>All values are big-endian.</p><p>The header size is constant (28 bytes) that implementation can rely on (read
the entire header in one go, simplify usage of memory mapping).</p><p>As opposed to protocol v2 but alike protocol v1, metadata is included along
metrics in OpenMetrics format.</p><p><code>owner</code> attribute for metric should be exposed using a OpenMetrics label instead (named <code>owner</code>).</p><p>Multiple metrics that shares the same name should be exposed under the same
Metric Family and be differenciated by labels (e.g <code>owner</code>).</p><h2 id=reading-algorithm>Reading algorithm</h2><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> header <span style=color:#f92672>!=</span> expected_header:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> InvalidHeader()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> data_checksum <span style=color:#f92672>==</span> last_data_checksum:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> NoUpdate()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> timestamp <span style=color:#f92672>==</span> last_timestamp:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> NoUpdate()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> data_checksum <span style=color:#f92672>!=</span> crc32(concat_header_end_payload):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> InvalidChecksum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>metrics <span style=color:#f92672>=</span> parse_openmetrics(payload_data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> family <span style=color:#f92672>in</span> metrics:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> family_exists(family):
</span></span><span style=display:flex><span>        update_family(family)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>        create_family(family)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>track_removed_families(metrics)</span></span></code></pre></div><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/186>#186</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Renaming VMSS fields and APIs. API message_create superseeds vmss_create_alerts.</td></tr><tr><td><span class="label label-default">v3</span></td><td>Remove VMSS alarm_config details and use existing pool wide alarm config</td></tr><tr><td><span class="label label-default">v4</span></td><td>Renaming field from retention-value to retained-snapshots and schedule-snapshot to scheduled-snapshot</td></tr><tr><td><span class="label label-default">v5</span></td><td>Add new API task_set_status</td></tr></table></header><h1 id=schedule-snapshot-design>Schedule Snapshot Design</h1><p>The scheduled snapshot feature will utilize the existing architecture of VMPR. In terms of functionality, scheduled snapshot is basically VMPR without its archiving capability.</p><h2 id=introduction>Introduction</h2><ul><li>Schedule snapshot will be a new object in xapi as VMSS.</li><li>A pool can have multiple VMSS.</li><li>Multiple VMs can be a part of VMSS but a VM cannot be a part of multiple VMSS.</li><li>A VMSS takes VMs snapshot with type [<code>snapshot</code>, <code>checkpoint</code>, <code>snapshot_with_quiesce</code>].</li><li>VMSS takes snapshot of VMs on configured intervals:<ul><li><code>hourly</code> -> On everyday, Each hour, Mins [0;15;30;45]</li><li><code>daily</code> -> On everyday, Hour [0 to 23], Mins [0;15;30;45]</li><li><code>weekly</code> -> Days [<code>Monday</code>,<code>Tuesday</code>,<code>Wednesday</code>,<code>Thursday</code>,<code>Friday</code>,<code>Saturday</code>,<code>Sunday</code>], Hour[0 to 23], Mins [0;15;30;45]</li></ul></li><li>VMSS will have a limit on retaining number of VM snapshots in range [1 to 10].</li></ul><h2 id=datapath-design>Datapath Design</h2><ul><li>There will be a cron job for VMSS.</li><li>VMSS plugin will go through all the scheduled snapshot policies in the pool and check if any of them are due.</li><li>If a snapshot is due then : Go through all the VM objects in XAPI associated with this scheduled snapshot policy and create a new snapshot.</li><li>If the snapshot operation fails, create a notification alert for the event and move to the next VM.</li><li>Check if an older snapshot now needs to be deleted to comply with the retained snapshots defined in the scheduled policy.</li><li>If we need to delete any existing snapshots, delete the oldest snapshot created via scheduled policy.</li><li>Set the last-run timestamp in the scheduled policy.</li></ul><h2 id=xapi-changes>Xapi Changes</h2><p>There is a new record for VM Scheduled Snapshot with new fields.</p><p>New fields:</p><ul><li><code>name-label</code> type <code>String</code> : Name label for VMSS.</li><li><code>name-description</code> type <code>String</code> : Name description for VMSS.</li><li><code>enabled</code> type <code>Bool</code> : Enable/Disable VMSS to take snapshot.</li><li><code>type</code> type <code>Enum</code> [<code>snapshot</code>; <code>checkpoint</code>; <code>snapshot_with_quiesce</code>] : Type of snapshot VMSS takes.</li><li><code>retained-snapshots</code> type <code>Int64</code> : Number of snapshots limit for a VM, max limit is 10 and default is 7.</li><li><code>frequency</code> type <code>Enum</code> [<code>hourly</code>; <code>daily</code>; <code>weekly</code>] : Frequency of taking snapshot of VMs.</li><li><code>schedule</code> type <code>Map(String,String)</code> with (key, value) pair:<ul><li>hour : 0 to 23</li><li>min : [0;15;30;45]</li><li>days : [<code>Monday</code>,<code>Tuesday</code>,<code>Wednesday</code>,<code>Thursday</code>,<code>Friday</code>,<code>Saturday</code>,<code>Sunday</code>]</li></ul></li><li><code>last-run-time</code> type Date : DateTime of last execution of VMSS.</li><li><code>VMs</code> type VM refs : List of VMs part of VMSS.</li></ul><p>New fields to VM record:</p><ul><li><code>scheduled-snapshot</code> type VMSS ref : VM part of VMSS.</li><li><code>is-vmss-snapshot</code> type Bool : If snapshot created from VMSS.</li></ul><h2 id=new-apis>New APIs</h2><ul><li>vmss_snapshot_now (Ref vmss, Pool_Operater) -> String : This call executes the scheduled snapshot immediately.</li><li>vmss_set_retained_snapshots (Ref vmss, Int value, Pool_Operater) -> unit : Set the value of vmss retained snapshots, max is 10.</li><li>vmss_set_frequency (Ref vmss, String &ldquo;value&rdquo;, Pool_Operater) -> unit : Set the value of the vmss frequency field.</li><li>vmss_set_type (Ref vmss, String &ldquo;value&rdquo;, Pool_Operater) -> unit : Set the snapshot type of the vmss type field.</li><li>vmss_set_scheduled (Ref vmss, Map(String,String) &ldquo;value&rdquo;, Pool_Operater) -> unit : Set the vmss scheduled to take snapshot.</li><li>vmss_add_to_schedule (Ref vmss, String &ldquo;key&rdquo;, String &ldquo;value&rdquo;, Pool_Operater) -> unit : Add key value pair to VMSS schedule.</li><li>vmss_remove_from_schedule (Ref vmss, String &ldquo;key&rdquo;, Pool_Operater) -> unit : Remove key from VMSS schedule.</li><li>vmss_set_last_run_time (Ref vmss, DateTime &ldquo;value&rdquo;, Local_Root) -> unit : Set the last run time for VMSS.</li><li>task_set_status (Ref task, status_type &ldquo;value&rdquo;, READ_ONLY) -> unit : Set the status of task owned by same user, Pool_Operator can set status for any tasks.</li></ul><h2 id=new-clis>New CLIs</h2><ul><li>vmss-create (required : &ldquo;name-label&rdquo;;&ldquo;type&rdquo;;&ldquo;frequency&rdquo;, optional : &ldquo;name-description&rdquo;;&ldquo;enabled&rdquo;;&ldquo;schedule:&rdquo;;&ldquo;retained-snapshots&rdquo;) -> unit : Creates VM scheduled snapshot.</li><li>vmss-destroy (required : uuid) -> unit : Destroys a VM scheduled snapshot.</li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.6)</span></td></tr></table></header><h1 id=smapiv3>SMAPIv3</h1><p>Xapi accesses storage through &ldquo;plugins&rdquo; which currently use a protocol
called &ldquo;SMAPIv1&rdquo;. This protocol has a number of problems:</p><ol><li><p>the protocol has many missing features, and this leads to people
using the XenAPI from within a plugin, which is racy, difficult to
get right, unscalable and makes component testing impossible.</p></li><li><p>the protocol expects plugin authors to have a deep knowledge of the
Xen storage datapath (<code>tapdisk</code>, <code>blkback</code> etc) <em>and</em> the storage.</p></li><li><p>the protocol is undocumented.</p></li></ol><p>We shall create a new revision of the protocol (&ldquo;SMAPIv3&rdquo;) to address these
problems.</p><p>The following diagram shows the new control plane:</p><p><a href=#image-4d54e7f335b66fb6861e18d181ddd717 class=lightbox-link><img src=/new-docs/design/smapiv3/smapiv3.png alt="Storage control plane" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-4d54e7f335b66fb6861e18d181ddd717><img src=/new-docs/design/smapiv3/smapiv3.png alt="Storage control plane" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>Requests from xapi are filtered through the existing <code>storage_access</code>
layer which is responsible for managing the mapping between VM VBDs and
VDIs.</p><p>Each plugin is represented by a named queue, with APIs for</p><ul><li>querying the state of each queue</li><li>explicitly cancelling or replying to messages</li></ul><p>Legacy SMAPIv1 plugins will be processed via the existing <code>storage_access.SMAPIv1</code>
module. Newer SMAPIv3 plugins will be handled by a new <code>xapi-storage-script</code>
service.</p><p>The SMAPIv3 APIs will be defined in an IDL format in a separate repo.</p><h1 id=xapi-storage-script>xapi-storage-script</h1><p>The <code>xapi-storage-script</code> will run as a service and will</p><ul><li>use <code>inotify</code> to monitor a well-known path in dom0</li><li>when a directory is created, check whether it contains storage plugins by
executing a <code>Plugin.query</code></li><li>assuming the directory contains plugins, it will register the queue name
and start listening for messages</li><li>when messages from <code>xapi</code> or the CLI are received, it will generate the SMAPIv3
.json message and fork the relevant script.</li></ul><h1 id=smapiv3-idl>SMAPIv3 IDL</h1><p>The IDL will support</p><ul><li>documentation for all functions, parameters and results<ul><li>this will be extended to be a XenAPI-style versioning scheme in future</li></ul></li><li>generating hyperlinked HTML documentation, published on github</li><li>generating libraries for python and OCaml<ul><li>the libraries will include marshalling, unmarshalling, type-checking
and command-line parsing and help generation</li></ul></li></ul><h1 id=diagnostic-tools>Diagnostic tools</h1><p>It will be possible to view the contents of the queue associated with any
plugin, and see whether</p><ul><li>the queue is being served or not (perhaps the <code>xapi-storage-script</code> has
crashed)</li><li>there are unanswered messages (perhaps one of the messages has caused
a deadlock in the implementation?)</li></ul><p>It will be possible to</p><ul><li>delete/clear queues/messages</li><li>download a message-sequence chart of the last N messages for inclusion in
bugtools.</li></ul><h1 id=anatomy-of-a-plugin>Anatomy of a plugin</h1><p>The following diagram shows what a plugin would look like:</p><p><a href=#image-faad6f51ad0875689a13cdb54776c3ac class=lightbox-link><img src=/new-docs/design/smapiv3/plugin.png alt="Anatomy of a plugin" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-faad6f51ad0875689a13cdb54776c3ac><img src=/new-docs/design/smapiv3/plugin.png alt="Anatomy of a plugin" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><h1 id=the-smapiv3>The SMAPIv3</h1><p>Please read <a href=https://xapi-project.github.io/xapi-storage target=_blank>the current SMAPIv3 documentation</a>.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=specifying-emulated-pci-devices>Specifying Emulated PCI Devices</h1><h3 id=background-and-goals>Background and goals</h3><p>At present (early March 2015) the datamodel defines a VM as having a &ldquo;platform&rdquo; string-string map, in which two keys are interpreted as specifying a PCI device which should be emulated for the VM. Those keys are &ldquo;device_id&rdquo; and &ldquo;revision&rdquo; (with int values represented as decimal strings).</p><p>Limitations:</p><ul><li>Hardcoded defaults are used for the the vendor ID and all other parameters except device_id and revision.</li><li>Only one emulated PCI device can be specified.</li></ul><p>When instructing qemu to emulate PCI devices, qemu accepts twelve parameters for each device.</p><p>Future guest-agent features rely on additional emulated PCI devices. We cannot know in advance the full details of all the devices that will be needed, but we can predict some.</p><p>We need a way to configure VMs such that they will be given additional emulated PCI devices.</p><h3 id=design>Design</h3><p>In the datamodel, there will be a new type of object for emulated PCI devices.</p><p>Tentative name: &ldquo;emulated_pci_device&rdquo;</p><p>Fields to be passed through to qemu are the following, all static read-only, and all ints except devicename:</p><ul><li>devicename (string)</li><li>vendorid</li><li>deviceid</li><li>command</li><li>status</li><li>revision</li><li>classcode</li><li>headertype</li><li>subvendorid</li><li>subsystemid</li><li>interruptline</li><li>interruptpin</li></ul><p>We also need a &ldquo;built_in&rdquo; flag: see below.</p><p>Allow creation of these objects through the API (and CLI).</p><p>(It would be nice, but by no means essential, to be able to create one by specifying an existing one as a basis, along with one or more altered fields, e.g. &ldquo;Make a new one just like that existing one except with interruptpin=9.&rdquo;)</p><p>Create some of these devices to be defined as standard in XenServer, along the same lines as the VM templates. Those ones should have built_in=true.</p><p>Allow destruction of these objects through the API (and CLI), but not if they are in use or if they have built_in=true.</p><p>A VM will have a list of zero or more of these emulated-pci-device objects. (OPEN QUESTION: Should we forbid having more than one of a given device?)</p><p>Provide API (and CLI) commands to add and remove one of these devices from a VM (identifying the VM and device by uuid or other identifier such as name).</p><p>The CLI should allow performing this on multiple VMs in one go, based on a selector or filter for the VMs. We have this concept already in the CLI in commands such as vm-start.</p><p>In the function that adds an emulated PCI device to a VM, we must check if this is the first device to be added, and must refuse if the VM&rsquo;s Virtual Hardware Platform Version is too low. (Or should we just raise the version automatically if needed?)</p><p>When starting a VM, check its list of emulated pci devices and pass the details through to qemu (via xenopsd).</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v11</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/139>#139</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Added details about the VDI's binary format and size, and the SR capability name.</td></tr><tr><td><span class="label label-default">v3</span></td><td>Tar was not needed after all!</td></tr><tr><td><span class="label label-default">v4</span></td><td>Add details about discovering the VDI using a new vdi_type.</td></tr><tr><td><span class="label label-default">v5</span></td><td>Add details about the http handlers and interaction with xapi's database</td></tr><tr><td><span class="label label-default">v6</span></td><td>Add details about the framing of the data within the VDI</td></tr><tr><td><span class="label label-default">v7</span></td><td>Redesign semantics of the rrd_updates handler</td></tr><tr><td><span class="label label-default">v8</span></td><td>Redesign semantics of the rrd_updates handler (again)</td></tr><tr><td><span class="label label-default">v9</span></td><td>Magic number change in framing format of vdi</td></tr><tr><td><span class="label label-default">v10</span></td><td>Add details of new APIs added to xapi and xcp-rrdd</td></tr><tr><td><span class="label label-default">v11</span></td><td>Remove unneeded API calls</td></tr></table></header><h1 id=sr-level-rrds>SR-Level RRDs</h1><h2 id=introduction>Introduction</h2><p>Xapi has RRDs to track VM- and host-level metrics. There is a desire to have SR-level RRDs as a new category, because SR stats are not specific to a certain VM or host. Examples are size and free space on the SR. While recording SR metrics is relatively straightforward within the current RRD system, the main question is where to archive them, which is what this design aims to address.</p><h2 id=stats-collection>Stats Collection</h2><p>All SR types, including the existing ones, should be able to have RRDs defined for them. Some RRDs, such as a &ldquo;free space&rdquo; one, may make sense for multiple (if not all) SR types. However, the way to measure something like free space will be SR specific. Furthermore, it should be possible for each type of SR to have its own specialised RRDs.</p><p>It follows that each SR will need its own <code>xcp-rrdd</code> plugin, which runs on the SR master and defines and collects the stats. For the new thin-lvhd SR this could be <code>xenvmd</code> itself. The plugin registers itself with <code>xcp-rrdd</code>, so that the latter records the live stats from the plugin into RRDs.</p><h2 id=archiving>Archiving</h2><p>SR-level RRDs will be archived in the SR itself, in a VDI, rather than in the local filesystem of the SR master. This way, we don&rsquo;t need to worry about master failover.</p><p>The VDI will be 4MB in size. This is a little more space than we would need for the RRDs we have in mind at the moment, but will give us enough headroom for the foreseeable future. It will not have a filesystem on it for simplicity and performance. There will only be one RRD archive file for each SR (possibly containing data for multiple metrics), which is gzipped by <code>xcp-rrdd</code>, and can be copied onto the VDI.</p><p>There will be a simple framing format for the data on the VDI. This will be as follows:</p><table><thead><tr><th>Offset</th><th>Type</th><th>Name</th><th>Comment</th></tr></thead><tbody><tr><td>0</td><td>32 bit network-order int</td><td>magic</td><td>Magic number = 0x7ada7ada</td></tr><tr><td>4</td><td>32 bit network-order int</td><td>version</td><td>1</td></tr><tr><td>8</td><td>32 bit network-order int</td><td>length</td><td>length of payload</td></tr><tr><td>12</td><td>gzipped data</td><td>data</td><td></td></tr></tbody></table><p>Xapi will be in charge of the lifecycle of this VDI, not the plugin or <code>xcp-rrdd</code>, which will make it a little easier to manage them. Only xapi will attach/detach and read from/write to this VDI. We will keep <code>xcp-rrdd</code> as simple as possible, and have it archive to its standard path in the local file system. Xapi will then copy the RRDs in and out of the VDI.</p><p>A new value <code>"rrd"</code> in the <code>vdi_type</code> enum of the datamodel will be defined, and the <code>VDI.type</code> of the VDI will be set to that value. The storage backend will write the VDI type to the LVM metadata of the VDI, so that xapi can discover the VDI containing the SR-level RRDs when attaching an SR to a new pool. This means that SR-level RRDs are currently restricted to LVM SRs.</p><p>Because we will not write plugins for all SRs at once, and therefore do not need xapi to set up the VDI for all SRs, we will add an SR &ldquo;capability&rdquo; for the backends to be able to tell xapi whether it has the ability to record stats and will need storage for them. The capability name will be: <code>SR_STATS</code>.</p><h2 id=management-of-the-sr-stats-vdi>Management of the SR-stats VDI</h2><p>The SR-stats VDI will be attached/detached on <code>PBD.plug</code>/<code>unplug</code> on the SR master.</p><ul><li><p>On <code>PBD.plug</code> on the SR master, if the SR has the stats capability, xapi:</p><ul><li>Creates a stats VDI if not already there (search for an existing one based on the VDI type).</li><li>Attaches the stats VDI if it did already exist, and copies the RRDs to the local file system (standard location in the filesystem; asks <code>xcp-rrdd</code> where to put them).</li><li>Informs <code>xcp-rrdd</code> about the RRDs so that it will load the RRDs and add newly recorded data to them (needs a function like <code>push_rrd_local</code> for VM-level RRDs).</li><li>Detaches stats VDI.</li></ul></li><li><p>On <code>PBD.unplug</code> on the SR master, if the SR has the stats capability xapi:</p><ul><li>Tells <code>xcp-rrdd</code> to archive the RRDs for the SR, which it will do to the local filesystem.</li><li>Attaches the stats VDI, copies the RRDs into it, detaches VDI.</li></ul></li></ul><h2 id=periodic-archiving>Periodic Archiving</h2><p>Xapi&rsquo;s periodic scheduler regularly triggers <code>xcp-rrdd</code> to archive the host and VM RRDs. It will need to do this for the SR ones as well. Furthermore, xapi will need to attach the stats VDI and copy the RRD archives into it (as on <code>PBD.unplug</code>).</p><h2 id=exporting>Exporting</h2><p>There will be a new handler for downloading an SR RRD:</p><pre><code>http://&lt;server&gt;/sr_rrd?session_id=&lt;SESSION HANDLE&gt;&amp;uuid=&lt;SR UUID&gt;
</code></pre><p>RRD updates are handled via a single handler for the host, VM and SR UUIDs
RRD updates for the host, VMs and SRs are handled by a a single handler at
<code>/rrd_updates</code>. Exactly what is returned will be determined by the parameters
passed to this handler.</p><p>Whether the host RRD updates are returned is governed by the presence of
<code>host=true</code> in the parameters. <code>host=&lt;anything else></code> or the absence of the
<code>host</code> key will mean the host RRD is not returned.</p><p>Whether the VM RRD updates are returned is governed by the <code>vm_uuid</code> key in the
URL parameters. <code>vm_uuid=all</code> will return RRD updates for all VM RRDs.
<code>vm_uuid=xxx</code> will return the RRD updates for the VM with uuid <code>xxx</code> only.
If <code>vm_uuid</code> is <code>none</code> (or any other string which is not a valid VM UUID) then
the handler will return no VM RRD updates. If the <code>vm_uuid</code> key is absent, RRD
updates for all VMs will be returned.</p><p>Whether the SR RRD updates are returned is governed by the <code>sr_uuid</code> key in the
URL parameters. <code>sr_uuid=all</code> will return RRD updates for all SR RRDs.
<code>sr_uuid=xxx</code> will return the RRD updates for the SR with uuid <code>xxx</code> only.
If <code>sr_uuid</code> is <code>none</code> (or any other string which is not a valid SR UUID) then
the handler will return no SR RRD updates. If the <code>sr_uuid</code> key is absent, no
SR RRD updates will be returned.</p><p>It will be possible to mix and match these parameters; for example to return
RRD updates for the host and all VMs, the URL to use would be:</p><pre><code>http://&lt;server&gt;/rrd_updates?session_id=&lt;SESSION HANDLE&gt;&amp;start=10258122541&amp;host=true&amp;vm_uuid=all&amp;sr_uuid=none
</code></pre><p>Or, to return RRD updates for all SRs but nothing else, the URL to use would be:</p><pre><code>http://&lt;server&gt;/rrd_updates?session_id=&lt;SESSION HANDLE&gt;&amp;start=10258122541&amp;host=false&amp;vm_uuid=none&amp;sr_uuid=all
</code></pre><p>While behaviour is defined if any of the keys <code>host</code>, <code>vm_uuid</code> and <code>sr_uuid</code> is
missing, this is for backwards compatibility and it is recommended that clients
specify each parameter explicitly.</p><h2 id=database-updating>Database updating.</h2><p>If the SR is presenting a data source called &lsquo;physical_utilisation&rsquo;,
xapi will record this periodically in its database. In order to do
this, xapi will fork a thread that, every n minutes (2 suggested, but
open to suggestions here), will query the attached SRs, then query
RRDD for the latest data source for these, and update the database.</p><p>The utilisation of VDIs will <em>not</em> be updated in this way until
scalability worries for RRDs are addressed.</p><p>Xapi will cache whether it is SR master for every attached SR and only
attempt to update if it is the SR master.</p><h2 id=new-apis>New APIs.</h2><h4 id=xcp-rrdd>xcp-rrdd:</h4><ul><li><p>Get the filesystem location where sr rrds are archived: <code>val sr_rrds_path : uid:string -> string</code></p></li><li><p>Archive the sr rrds to the filesystem: <code>val archive_sr_rrd : sr_uuid:string -> unit</code></p></li><li><p>Load the sr rrds from the filesystem: <code>val push_sr_rrd : sr_uuid:string -> unit</code></p></li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=thin-lvhd-storage>thin LVHD storage</h1><p>LVHD is a block-based storage system built on top of Xapi and LVM. LVHD
disks are represented as LVM LVs with vhd-format data inside. When a
disk is snapshotted, the LVM LV is &ldquo;deflated&rdquo; to the minimum-possible
size, just big enough to store the current vhd data. All other disks are
stored &ldquo;inflated&rdquo; i.e. consuming the maximum amount of storage space.
This proposal describes how we could add dynamic thin-provisioning to
LVHD such that</p><ul><li>disks only consume the space they need (plus an adjustable small
overhead)</li><li>when a disk needs more space, the allocation can be done <em>locally</em>
in the common-case; in particular there is no network RPC needed</li><li>when the resource pool master host has failed, allocations can still
continue, up to some limit, allowing time for the master host to be
recovered; in particular there is no need for very low HA timeouts.</li><li>we can (in future) support in-kernel block allocation through the
device mapper dm-thin target.</li></ul><p>The following diagram shows the &ldquo;Allocation plane&rdquo;:</p><p><a href=#image-f21b380947da1c59e68dcad6626233ba class=lightbox-link><img src=/new-docs/design/thin-lvhd/allocation-plane.png alt="Allocation plane" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-f21b380947da1c59e68dcad6626233ba><img src=/new-docs/design/thin-lvhd/allocation-plane.png alt="Allocation plane" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>All VM disk writes are channelled through <code>tapdisk</code> which keeps track
of the remaining reserved space within the device mapper device. When
the free space drops below a &ldquo;low-water mark&rdquo;, tapdisk sends a message
to a local per-SR daemon called <code>local-allocator</code> and requests more
space.</p><p>The <code>local-allocator</code> maintains a free pool of blocks available for
allocation locally (hence the name). It will pick some blocks and
transactionally send the update to the <code>xenvmd</code> process running
on the SRmaster via the shared ring (labelled <code>ToLVM queue</code> in the diagram)
and update the device mapper tables locally.</p><p>There is one <code>xenvmd</code> process per SR on the SRmaster. <code>xenvmd</code> receives
local allocations from all the host shared rings (labelled <code>ToLVM queue</code>
in the diagram) and combines them together, appending them to a redo-log
also on shared storage. When <code>xenvmd</code> notices that a host&rsquo;s free space
(represented in the metadata as another LV) is low it allocates new free blocks
and pushes these to the host via another shared ring (labelled <code>FromLVM queue</code>
in the diagram).</p><p>The <code>xenvmd</code> process maintains a cache of the current VG metadata for
fast query and update. All updates are appended to the redo-log to ensure
they operate in O(1) time. The redo log updates are periodically flushed
to the primary LVM metadata.</p><p>Since the operations are stored in the redo-log and will only be removed
after the real metadata has been written, the implication is that it is
possible for the operations to be performed more than once. This will
occur if the xenvmd process exits between flushing to the real metadata
and acknowledging the operations as completed. For this to work as expected,
every individual operation stored in the redo-log <em>must</em> be idempotent.</p><h2 id=note-on-running-out-of-blocks>Note on running out of blocks</h2><p>Note that, while the host has plenty of free blocks, local allocations should
be fast. If the master fails and the local free pool starts running out
and <code>tapdisk</code> asks for more blocks, then the local allocator won&rsquo;t be able
to provide them.
<code>tapdisk</code> should start to slow
I/O in order to provide the local allocator more time.
Eventually if <code>tapdisk</code> runs
out of space before the local allocator can satisfy the request then
guest I/O will block. Note Windows VMs will start to crash if guest
I/O blocks for more than 70s. Linux VMs, no matter PV or HVM, may suffer
from &ldquo;block for more than 120 seconds&rdquo; issue due to slow I/O. This
known issue is that, slow I/O during dirty pages writeback/flush may
cause memory starvation, then other userland process or kernel threads
would be blocked.</p><p>The following diagram shows the control-plane:</p><p><a href=#image-2ec075ba8de8c42452fe0e19cc5db140 class=lightbox-link><img src=/new-docs/design/thin-lvhd/control-plane.png alt="control plane" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-2ec075ba8de8c42452fe0e19cc5db140><img src=/new-docs/design/thin-lvhd/control-plane.png alt="control plane" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>When thin-provisioning is enabled we will be modifying the LVM metadata at
an increased rate. We will cache the current metadata in the <code>xenvmd</code> process
and funnel all queries through it, rather than &ldquo;peeking&rdquo; at the metadata
on-disk. Note it will still be possible to peek at the on-disk metadata but it
will be out-of-date. Peeking can still be used to query the PV state of the volume
group.</p><p>The <code>xenvm</code> CLI uses a simple
RPC interface to query the <code>xenvmd</code> process, tunnelled through <code>xapi</code> over
the management network. The RPC interface can be used for</p><ul><li>activating volumes locally: <code>xenvm</code> will query the LV segments and program
device mapper</li><li>deactivating volumes locally</li><li>listing LVs, PVs etc</li></ul><p>Note that current LVHD requires the management network for these control-plane
functions.</p><p>When the SM backend wishes to query or update volume group metadata it should use the
<code>xenvm</code> CLI while thin-provisioning is enabled.</p><p>The <code>xenvmd</code> process shall use a redo-log to ensure that metadata updates are
persisted in constant time and flushed lazily to the regular metadata area.</p><p>Tunnelling through xapi will be done by POSTing to the localhost URI</p><pre><code>/services/xenvmd/&lt;SR uuid&gt;
</code></pre><p>Xapi will the either proxy the request transparently to the SRmaster, or issue an
http level redirect that the xenvm CLI would need to follow.</p><p>If the xenvmd process is not running on the host on which it should
be, xapi will start it.</p><h1 id=components-roles-and-responsibilities>Components: roles and responsibilities</h1><p><code>xenvmd</code>:</p><ul><li>one per plugged SRmaster PBD</li><li>owns the LVM metadata</li><li>provides a fast query/update API so we can (for example) create lots of LVs very fast</li><li>allocates free blocks to hosts when they are running low</li><li>receives block allocations from hosts and incorporates them in the LVM metadata</li><li>can safely flush all updates and downgrade to regular LVM</li></ul><p><code>xenvm</code>:</p><ul><li>a CLI which talks the <code>xenvmd</code> protocol to query / update LVs</li><li>can be run on any host, calls (except &ldquo;format&rdquo; and &ldquo;upgrade&rdquo;) are forwarded by <code>xapi</code></li><li>can &ldquo;format&rdquo; a LUN to prepare it for <code>xenvmd</code></li><li>can &ldquo;upgrade&rdquo; a LUN to prepare it for <code>xenvmd</code></li></ul><p><code>local_allocator</code>:</p><ul><li>one per plugged PBD</li><li>exposes a simple interface to <code>tapdisk</code> for requesting more space</li><li>receives free block allocations via a queue on the shared disk from <code>xenvmd</code></li><li>sends block allocations to <code>xenvmd</code> and updates the device mapper target locally</li></ul><p><code>tapdisk</code>:</p><ul><li>monitors the free space inside LVs and requests more space when running out</li><li>slows down I/O when nearly out of space</li></ul><p><code>xapi</code>:</p><ul><li>provides authenticated communication tunnels</li><li>ensures the xenvmd daemons are only running on the correct hosts.</li></ul><p><code>SM</code>:</p><ul><li>writes the configuration file for xenvmd (though doesn&rsquo;t start it)</li><li>has an on/off switch for thin-provisioning</li><li>can use either normal LVM or the <code>xenvm</code> CLI</li></ul><p><code>membership_monitor</code></p><ul><li>configures and manages the connections between <code>xenvmd</code> and the <code>local_allocator</code></li></ul><h1 id=queues-on-the-shared-disk>Queues on the shared disk</h1><p>The <code>local_allocator</code> communicates with <code>xenvmd</code> via a pair
of queues on the shared disk. Using the disk rather than the network means
that VMs will continue to run even if the management network is not working.
In particular</p><ul><li>if the (management) network fails, VMs continue to run on SAN storage</li><li>if a host changes IP address, nothing needs to be reconfigured</li><li>if xapi fails, VMs continue to run.</li></ul><h2 id=logical-messages-in-the-queues>Logical messages in the queues</h2><p>The <code>local_allocator</code> needs to tell the <code>xenvmd</code> which blocks have
been allocated to which guest LV. <code>xenvmd</code> needs to tell the
<code>local_allocator</code> which blocks have become free. Since we are based on
LVM, a &ldquo;block&rdquo; is an extent, and an &ldquo;allocation&rdquo; is a segment i.e. the
placing of a physical extent at a logical extent in the logical volume.</p><p>The <code>local_allocator</code> needs to send a message with logical contents:</p><ul><li><code>volume</code>: a human-readable name of the LV</li><li><code>segments</code>: a list of LVM segments which says
&ldquo;place physical extent x at logical extent y using a linear mapping&rdquo;.</li></ul><p>Note this message is idempotent.</p><p>The <code>xenvmd</code> needs to send a message with logical contents:</p><ul><li><code>extents</code>: a list of physical extents which are free for the host to use</li></ul><p>Although
for internal housekeeping <code>xenvmd</code> will want to assign these
physical extents to logical extents within the host&rsquo;s free LV, the
<code>local_allocator</code>
doesn&rsquo;t need to know the logical extents. It only needs to know
the set of blocks which it is free to allocate.</p><h2 id=starting-up-the-local_allocator>Starting up the local_allocator</h2><p>What happens when a <code>local_allocator</code> (re)starts, after a</p><ul><li>process crash, respawn</li><li>host crash, reboot?</li></ul><p>When the <code>local_allocator</code> starts up, there are 2 cases:</p><ol><li>the host has just rebooted, there are no attached disks and no running VMs</li><li>the process has just crashed, there are attached disks and running VMs</li></ol><p>Case 1 is uninteresting. In case 2 there may have been an allocation in
progress when the process crashed and this must be completed. Therefore
the operation is journalled in a local filesystem in a directory which
is deliberately deleted on host reboot (Case 1). The allocation operation
consists of:</p><ol><li><code>push</code>ing the allocation to <code>xenvmd</code> on the SRmaster</li><li>updating the device mapper</li></ol><p>Note that both parts of the allocation operation are idempotent and hence
the whole operation is idempotent. The journalling will guarantee it executes
at-least-once.</p><p>When the <code>local_allocator</code> starts up it needs to discover the list of
free blocks. Rather than have 2 code paths, it&rsquo;s best to treat everything
as if it is a cold start (i.e. no local caches already populated) and to
ask the master to resync the free block list. The resync is performed by
executing a &ldquo;suspend&rdquo; and &ldquo;resume&rdquo; of the free block queue, and requiring
the remote allocator to:</p><ul><li><code>pop</code> all block allocations and incorporate these updates</li><li>send the complete set of free blocks &ldquo;now&rdquo; (i.e. while the queue is
suspended) to the local allocator.</li></ul><h2 id=starting-xenvmd>Starting xenvmd</h2><p><code>xenvmd</code> needs to know</p><ul><li>the device containing the volume group</li><li>the hosts to &ldquo;connect&rdquo; to via the shared queues</li></ul><p>The device containing the volume group should be written to a config
file when the SR is plugged.</p><p><code>xenvmd</code> does not remember which hosts it is listening to across crashes,
restarts or master failovers. The <code>membership_monitor</code> will keep the
<code>xenvmd</code> list in sync with the <code>PBD.currently_attached</code> fields.</p><h2 id=shutting-down-the-local_allocator>Shutting down the local_allocator</h2><p>The <code>local_allocator</code> should be able to crash at any time and recover
afterwards. If the user requests a <code>PBD.unplug</code> we can perform a
clean shutdown by:</p><ul><li>signalling <code>xenvmd</code> to suspend the block allocation queue</li><li>arranging for the <code>local_allocator</code> to acknowledge the suspension and exit</li><li>when the <code>xenvmd</code> sees the acknowlegement, we know that the
<code>local_allocator</code> is offline and it doesn&rsquo;t need to poll the queue any more</li></ul><h2 id=downgrading-metadata>Downgrading metadata</h2><p><code>xenvmd</code> can be terminated at any time and restarted, since all compound
operations are journalled.</p><p>Downgrade is a special case of shutdown.
To downgrade, we need to stop all hosts allocating and ensure all updates
are flushed to the global LVM metadata. <code>xenvmd</code> can shutdown
by:</p><ul><li>shutting down all <code>local_allocator</code>s (see previous section)</li><li>flushing all outstanding block allocations to the LVM redo log</li><li>flushing the LVM redo log to the global LVM metadata</li></ul><h2 id=queues-as-rings>Queues as rings</h2><p>We can use a simple ring protocol to represent the queues on the disk.
Each queue will have a single consumer and single producer and reside within
a single logical volume.</p><p>To make diagnostics simpler, we can require the ring to only support <code>push</code>
and <code>pop</code> of <em>whole</em> messages i.e. there can be no partial reads or partial
writes. This means that the <code>producer</code> and <code>consumer</code> pointers will always
point to valid message boundaries.</p><p>One possible format used by the <a href=https://github.com/mirage/shared-block-ring/blob/master/lib/ring.ml target=_blank>prototype</a> is as follows:</p><ul><li>sector 0: a magic string</li><li>sector 1: producer state</li><li>sector 2: consumer state</li><li>sector 3&mldr;: data</li></ul><p>Within the producer state sector we can have:</p><ul><li>octets 0-7: producer offset: a little-endian 64-bit integer</li><li>octet 8: 1 means &ldquo;suspend acknowledged&rdquo;; 0 otherwise</li></ul><p>Within the consumer state sector we can have:</p><ul><li>octets 0-7: consumer offset: a little-endian 64-bit integer</li><li>octet 8: 1 means &ldquo;suspend requested&rdquo;; 0 otherwise</li></ul><p>The consumer and producer pointers point to message boundaries. Each
message is prefixed with a 4 byte length and padded to the next 4-byte
boundary.</p><p>To push a message onto the ring we need to</p><ul><li>check whether the message is too big to ever fit: this is a permanent
error</li><li>check whether the message is too big to fit given the current free
space: this is a transient error</li><li>write the message into the ring</li><li>advance the producer pointer</li></ul><p>To pop a message from the ring we need to</p><ul><li>check whether there is unconsumed space: if not this is a transient
error</li><li>read the message from the ring and process it</li><li>advance the consumer pointer</li></ul><h2 id=journals-as-queues>Journals as queues</h2><p>When we journal an operation we want to guarantee to execute it never
<em>or</em> at-least-once. We can re-use the queue implementation by <code>push</code>ing
a description of the work item to the queue and waiting for the
item to be <code>pop</code>ped, processed and finally consumed by advancing the
consumer pointer. The journal code needs to check for unconsumed data
during startup, and to process it before continuing.</p><h2 id=suspending-and-resuming-queues>Suspending and resuming queues</h2><p>During startup (resync the free blocks) and shutdown (flush the allocations)
we need to suspend and resume queues. The ring protocol can be extended
to allow the <em>consumer</em> to suspend the ring by:</p><ul><li>the consumer asserts the &ldquo;suspend requested&rdquo; bit</li><li>the producer <code>push</code> function checks the bit and writes &ldquo;suspend acknowledged&rdquo;</li><li>the producer also periodically polls the queue state and writes
&ldquo;suspend acknowledged&rdquo; (to catch the case where no items are to be pushed)</li><li>after the producer has acknowledged it will guarantee to <code>push</code> no more
items</li><li>when the consumer polls the producer&rsquo;s state and spots the &ldquo;suspend acknowledged&rdquo;,
it concludes that the queue is now suspended.</li></ul><p>The key detail is that the handshake on the ring causes the two sides
to synchronise and both agree that the ring is now suspended/ resumed.</p><h2 id=modelling-the-suspendresume-protocol>Modelling the suspend/resume protocol</h2><p>To check that the suspend/resume protocol works well enough to be used
to resynchronise the free blocks list on a slave, a simple
<a href=/new-docs/design/thin-lvhd/queue.pml>promela model</a> was created. We model the queue state as
2 boolean flags:</p><div class="wrap-code highlight"><pre tabindex=0><code>bool suspend /* suspend requested */
bool suspend_ack /* suspend acknowledged *./</code></pre></div><p>and an abstract representation of the data within the ring:</p><div class="wrap-code highlight"><pre tabindex=0><code>/* the queue may have no data (none); a delta or a full sync.
   the full sync is performed immediately on resume. */
mtype = { sync delta none }
mtype inflight_data = none</code></pre></div><p>There is a &ldquo;producer&rdquo; and a &ldquo;consumer&rdquo; process which run forever,
exchanging data and suspending and resuming whenever they want.
The special data item <code>sync</code> is only sent immediately after a resume
and we check that we never desynchronise with asserts:</p><div class="wrap-code highlight"><pre tabindex=0><code>  :: (inflight_data != none) -&gt;
    /* In steady state we receive deltas */
    assert (suspend_ack == false);
    assert (inflight_data == delta);
    inflight_data = none</code></pre></div><p>i.e. when we are receiving data normally (outside of the suspend/resume
code) we aren&rsquo;t suspended and we expect deltas, not full syncs.</p><p>The model-checker <a href=http://spinroot.com/spin/whatispin.html target=_blank>spin</a>
verifies this property holds.</p><h1 id=interaction-with-ha>Interaction with HA</h1><p>Consider what will happen if a host fails when HA is disabled:</p><ul><li>if the host is a slave: the VMs running on the host will crash but
no other host is affected.</li><li>if the host is a master: allocation requests from running VMs will
continue provided enough free blocks are cached on the hosts. If a
host eventually runs out of free blocks, then guest I/O will start to
block and VMs may eventually crash.</li></ul><p>Therefore we <em>recommend</em> that users enable HA and only disable it
for short periods of time. Note that, unlike other thin-provisioning
implementations, we will allow HA to be disabled.</p><h1 id=host-local-lvs>Host-local LVs</h1><p>When a host calls SMAPI <code>sr_attach</code>, it will use <code>xenvm</code> to tell <code>xenvmd</code> on the
SRmaster to connect to the <code>local_allocator</code> on the host. The <code>xenvmd</code>
daemon will create the volumes for queues and a volume to represent the
&ldquo;free blocks&rdquo; which a host is allowed to allocate.</p><h1 id=monitoring>Monitoring</h1><p>The <code>xenvmd</code> process should export RRD datasources over shared
memory named</p><ul><li><code>sr_&lt;SR uuid>_&lt;host uuid>_free</code>: the number of free blocks in
the local cache. It&rsquo;s useful to look at this and verify that it doesn&rsquo;t
usually hit zero, since that&rsquo;s when allocations will start to block.
For this reason we should use the <code>MIN</code> consolidation function.</li><li><code>sr_&lt;SR uuid>_&lt;host uuid>_requests</code>: a counter of the number
of satisfied allocation requests. If this number is too high then the quantum
of allocation should be increased. For this reason we should use the
<code>MAX</code> consolidation function.</li><li><code>sr_&lt;SR uuid>_&lt;host uuid>_allocations</code>: a counter of the number of
bytes being allocated. If the allocation rate is too high compared with
the number of free blocks divided by the HA timeout period then the
<code>SRmaster-allocator</code> should be reconfigured to supply more blocks with the host.</li></ul><h1 id=modifications-to-tapdisk>Modifications to tapdisk</h1><p>TODO: to be updated by Germano</p><p><code>tapdisk</code> will be modified to</p><ul><li>on open: discover the current maximum size of the file/LV (for a file
we assume there is no limit for now)</li><li>read a low-water mark value from a config file <code>/etc/tapdisk3.conf</code></li><li>read a very-low-water mark value from a config file <code>/etc/tapdisk3.conf</code></li><li>read a Unix domain socket path from a config file <code>/etc/tapdisk3.conf</code></li><li>when there is less free space available than the low-water mark: connect
to Unix domain socket and write an &ldquo;extend&rdquo; request</li><li>upon receiving the &ldquo;extend&rdquo; response, re-read the maximum size of the
file/LV</li><li>when there is less free space available than the very-low-water mark:
start to slow I/O responses and write a single &rsquo;error&rsquo; line to the log.</li></ul><h2 id=the-extend-request>The extend request</h2><p>TODO: to be updated by Germano</p><p>The request has the following format:</p><table><thead><tr><th>Octet offsets</th><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>0,1</td><td>tl</td><td>Total length (including this field) of message (in network byte order)</td></tr><tr><td>2</td><td>type</td><td>The value &lsquo;0&rsquo; indicating an extend request</td></tr><tr><td>3</td><td>nl</td><td>The length of the LV name in octets, including NULL terminator</td></tr><tr><td>4,..,4+nl-1</td><td>name</td><td>The LV name</td></tr><tr><td>4+nl,..,12+nl-1</td><td>vdi_size</td><td>The virtual size of the logical VDI (in network byte order)</td></tr><tr><td>12+nl,..,20+nl-1</td><td>lv_size</td><td>The current size of the LV (in network byte order)</td></tr><tr><td>20+nl,..,28+nl-1</td><td>cur_size</td><td>The current size of the vhd metadata (in network byte order)</td></tr></tbody></table><h2 id=the-extend-response>The extend response</h2><p>The response is a single byte value &ldquo;0&rdquo; which is a signal to re-examime
the LV size. The request will block indefinitely until it succeeds. The
request will block for a long time if</p><ul><li>the SR has genuinely run out of space. The admin should observe the
existing free space graphs/alerts and perform an SR resize.</li><li>the master has failed and HA is disabled. The admin should re-enable
HA or fix the problem manually.</li></ul><h1 id=the-local_allocator>The local_allocator</h1><p>There is one <code>local_allocator</code> process per plugged PBD.
The process will be
spawned by the SM <code>sr_attach</code> call, and shutdown from the <code>sr_detach</code> call.</p><p>The <code>local_allocator</code> accepts the following configuration (via a config file):</p><ul><li><code>socket</code>: path to a local Unix domain socket. This is where the <code>local_allocator</code>
listens for requests from <code>tapdisk</code></li><li><code>allocation_quantum</code>: number of megabytes to allocate to each tapdisk on request</li><li><code>local_journal</code>: path to a block device or file used for local journalling. This
should be deleted on reboot.</li><li><code>free_pool</code>: name of the LV used to store the host&rsquo;s free blocks</li><li><code>devices</code>: list of local block devices containing the PVs</li><li><code>to_LVM</code>: name of the LV containing the queue of block allocations sent to <code>xenvmd</code></li><li><code>from_LVM</code>: name of the LV containing the queue of messages sent from <code>xenvmd</code>.
There are two types of messages:<ol><li>Free blocks to put into the free pool</li><li>Cap requests to remove blocks from the free pool.</li></ol></li></ul><p>When the <code>local_allocator</code> process starts up it will read the host local
journal and</p><ul><li>re-execute any pending allocation requests from tapdisk</li><li>suspend and resume the <code>from_LVM</code> queue to trigger a full retransmit
of free blocks from <code>xenvmd</code></li></ul><p>The procedure for handling an allocation request from tapdisk is:</p><ol><li>if there aren&rsquo;t enough free blocks in the free pool, wait polling the
<code>from_LVM</code> queue</li><li>choose a range of blocks to assign to the tapdisk LV from the free LV</li><li>write the operation (i.e. exactly what we are about to do) to the journal.
This ensures that it will be repeated if the allocator crashes and restarts.
Note that, since the operation may be repeated multiple times, it must be
idempotent.</li><li>push the block assignment to the <code>toLVM</code> queue</li><li>suspend the device mapper device</li><li>add/modify the device mapper target</li><li>resume the device mapper device</li><li>remove the operation from the local journal (i.e. there&rsquo;s no need to repeat
it now)</li><li>reply to tapdisk</li></ol><h2 id=shutting-down-the-local-allocator>Shutting down the local-allocator</h2><p>The SM <code>sr_detach</code> called from <code>PBD.unplug</code> will use the <code>xenvm</code> CLI to request
that <code>xenvmd</code> disconnects from a host. The procedure is:</p><ol><li>SM calls <code>xenvm disconnect host</code></li><li><code>xenvm</code> sends an RPC to <code>xenvmd</code> tunnelled through <code>xapi</code></li><li><code>xenvmd</code> suspends the <code>to_LVM</code> queue</li><li>the <code>local_allocator</code> acknowledges the suspend and exits</li><li><code>xenvmd</code> flushes all updates from the <code>to_LVM</code> queue and stops listening</li></ol><h1 id=xenvmd>xenvmd</h1><p><code>xenvmd</code> is a daemon running per SRmaster PBD, started in <code>sr_attach</code> and
terminated in <code>sr_detach</code>. <code>xenvmd</code> has a config file containing:</p><ul><li><code>socket</code>: Unix domain socket where <code>xenvmd</code> listens for requests from
<code>xenvm</code> tunnelled by <code>xapi</code></li><li><code>host_allocation_quantum</code>: number of megabytes to hand to a host at a time</li><li><code>host_low_water_mark</code>: threshold below which we will hand blocks to a host</li><li><code>devices</code>: local devices containing the PVs</li></ul><p><code>xenvmd</code> continually</p><ul><li>peeks updates from all the <code>to_LVM</code> queues</li><li>calculates how much free space each host still has</li><li>if the size of a host&rsquo;s free pool drops below some threshold:<ul><li>choose some free blocks</li></ul></li><li>if the size of a host&rsquo;s free pool goes above some threshold:<ul><li>request a cap of the host&rsquo;s free pool</li></ul></li><li>writes the change it is going to make to a journal stored in an LV</li><li>pops the updates from the <code>to_LVM</code> queues</li><li>pushes the updates to the <code>from_LVM</code> queues</li><li>pushes updates to the LVM redo-log</li><li>periodically flush the LVM redo-log to the LVM metadata area</li></ul><h1 id=the-membership-monitor>The membership monitor</h1><p>The role of the membership monitor is to keep the list of <code>xenvmd</code> connections
in sync with the <code>PBD.currently_attached</code> fields.</p><p>We shall</p><ul><li>install a <code>host-pre-declare-dead</code> script to use <code>xenvm</code> to send an RPC
to <code>xenvmd</code> to forcibly flush (without acknowledgement) the <code>to_LVM</code> queue
and destroy the LVs.</li><li>modify XenAPI <code>Host.declare_dead</code> to call <code>host-pre-declare-dead</code> before
the VMs are unlocked</li><li>add a <code>host-pre-forget</code> hook type which will be called just before a Host
is forgotten</li><li>install a <code>host-pre-forget</code> script to use <code>xenvm</code> to call <code>xenvmd</code> to
destroy the host&rsquo;s local LVs</li></ul><h1 id=modifications-to-lvhd-sr>Modifications to LVHD SR</h1><ul><li><code>sr_attach</code> should:<ul><li>if an SRmaster, update the <code>MGT</code> major version number to prevent</li><li>Write the xenvmd configuration file (on <em>all</em> hosts, not just SRmaster)</li><li>spawn <code>local_allocator</code></li></ul></li><li><code>sr_detach</code> should:<ul><li>call <code>xenvm</code> to request the shutdown of <code>local_allocator</code></li></ul></li><li><code>vdi_deactivate</code> should:<ul><li>call <code>xenvm</code> to request the flushing of all the <code>to_LVM</code> queues to the
redo log</li></ul></li><li><code>vdi_activate</code> should:<ul><li>if necessary, call <code>xenvm</code> to deflate the LV to the minimum size (with some slack)</li></ul></li></ul><p>Note that it is possible to attach and detach the individual hosts in any order
but when the SRmaster is unplugged then there will be no &ldquo;refilling&rdquo; of the host
local free LVs; it will behave as if the master host has failed.</p><h1 id=modifications-to-xapi>Modifications to xapi</h1><ul><li>Xapi needs to learn how to forward xenvm connections to the SR master.</li><li>Xapi needs to start and stop xenvmd at the appropriate times</li><li>We must disable unplugging the PBDs for shared SRs on the pool master
if any other slave has its PBD plugging. This is actually fixing an
issue that exists today - LVHD SRs require the master PBD to be
plugged to do many operations.</li><li>Xapi should provide a mechanism by which the xenvmd process can be killed
once the last PBD for an SR has been unplugged.</li></ul><h1 id=enabling-thin-provisioning>Enabling thin provisioning</h1><p>Thin provisioning will be automatically enabled on upgrade. When the SRmaster
plugs in <code>PBD</code> the <code>MGT</code> major version number will be bumped to prevent old
hosts from plugging in the SR and getting confused.
When a VDI is activated, it will be deflated to the new low size.</p><h1 id=disabling-thin-provisioning>Disabling thin provisioning</h1><p>We shall make a tool which will</p><ul><li>allow someone to downgrade their pool after enabling thin provisioning</li><li>allow developers to test the upgrade logic without fully downgrading their
hosts</li></ul><p>The tool will</p><ul><li>check if there is enough space to fully inflate all non-snapshot leaves</li><li>unplug all the non-SRmaster <code>PBD</code>s</li><li>unplug the SRmaster <code>PBD</code>. As a side-effect all pending LVM updates will be
written to the LVM metadata.</li><li>modify the <code>MGT</code> volume to have the lower metadata version</li><li>fully inflate all non-snapshot leaves</li></ul><h1 id=walk-through-upgrade>Walk-through: upgrade</h1><p>Rolling upgrade should work in the usual way. As soon as the pool master has been
upgraded, hosts will be able to use thin provisioning when new VDIs are attached.
A VM suspend/resume/reboot or migrate will be needed to turn on thin provisioning
for existing running VMs.</p><h1 id=walk-through-downgrade>Walk-through: downgrade</h1><p>A pool may be safely downgraded to a previous version without thin provisioning
provided that the downgrade tool is run. If the tool hasn&rsquo;t run then the old
pool will refuse to attach the SR because the metadata has been upgraded.</p><h1 id=walk-through-after-a-host-failure>Walk-through: after a host failure</h1><p>If HA is enabled:</p><ul><li><code>xhad</code> elects a new master if necessary</li><li><code>Xapi</code> on the master will start xenvmd processes for shared thin-lvhd SRs</li><li>the <code>xhad</code> tells <code>Xapi</code> which hosts are alive and which have failed.</li><li><code>Xapi</code> runs the <code>host-pre-declare-dead</code> scripts for every failed host</li><li>the <code>host-pre-declare-dead</code> tells <code>xenvmd</code> to flush the <code>to_LVM</code> updates</li><li><code>Xapi</code> unlocks the VMs and restarts them on new hosts.</li></ul><p>If HA is not enabled:</p><ul><li>The admin should verify the host is definitely dead</li><li>If the dead host was the master, a new master must be designated. This will
start the xenvmd processes for the shared thin-lvhd SRs.</li><li>the admin must tell <code>Xapi</code> which hosts have failed with <code>xe host-declare-dead</code></li><li><code>Xapi</code> runs the <code>host-pre-declare-dead</code> scripts for every failed host</li><li>the <code>host-pre-declare-dead</code> tells <code>xenvmd</code> to flush the <code>to_LVM</code> updates</li><li><code>Xapi</code> unlocks the VMs</li><li>the admin may now restart the VMs on new hosts.</li></ul><h1 id=walk-through-co-operative-master-transition>Walk-through: co-operative master transition</h1><p>The admin calls Pool.designate_new_master. This initiates a two-phase
commit of the new master. As part of this, the slaves will restart,
and on restart each host&rsquo;s xapi will kill any xenvmd that should only
run on the pool master. The new designated master will then restart itself
and start up the xenvmd process on itself.</p><h1 id=future-use-of-dm-thin>Future use of dm-thin?</h1><p>Dm-thin also uses 2 local LVs: one for the &ldquo;thin pool&rdquo; and one for the metadata.
After replaying our journal we could potentially delete our host local LVs and
switch over to dm-thin.</p><h1 id=summary-of-the-impact-on-the-admin>Summary of the impact on the admin</h1><ul><li>If the VM workload performs a lot of disk allocation, then the admin <em>should</em>
enable HA.</li><li>The admin <em>must</em> not downgrade the pool without first cleanly detaching the
storage.</li><li>Extra metadata is needed to track thin provisioing, reducing the amount of
space available for user volumes.</li><li>If an SR is completely full then it will not be possible to enable thin
provisioning.</li><li>There will be more fragmentation, but the extent size is large (4MiB) so it
shouldn&rsquo;t be too bad.</li></ul><h1 id=ring-protocols>Ring protocols</h1><p>Each ring consists of 3 sectors of metadata followed by the data area. The
contents of the first 3 sectors are:</p><table><thead><tr><th>Sector, Octet offsets</th><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>0,0-30</td><td>signature</td><td>string</td><td>Signature (&ldquo;mirage shared-block-device 1.0&rdquo;)</td></tr><tr><td>1,0-7</td><td>producer</td><td>uint64</td><td>Pointer to the end of data written by the producer</td></tr><tr><td>1,8</td><td>suspend_ack</td><td>uint8</td><td>Suspend acknowledgement byte</td></tr><tr><td>2,0-7</td><td>consumer</td><td>uint64</td><td>Pointer to the end of data read by the consumer</td></tr><tr><td>2,8</td><td>suspend</td><td>uint8</td><td>Suspend request byte</td></tr></tbody></table><p>Note. producer and consumer pointers are stored in little endian
format.</p><p>The pointers are free running byte offsets rounded up to the next
4-byte boundary, and the position of the actual data is found by
finding the remainder when dividing by the size of the data area. The
producer pointer points to the first free byte, and the consumer
pointer points to the byte after the last data consumed. The actual
payload is preceded by a 4-byte length field, stored in little endian
format. When writing a 1 byte payload, the next value of the producer
pointer will therefore be 8 bytes on from the previous - 4 for the
length (which will contain [0x01,0x00,0x00,0x00]), 1 byte for the
payload, and 3 bytes padding.</p><p>A ring is suspended and resumed by the consumer. To suspend, the
consumer first checks that the producer and consumer agree on the
current suspend status. If they do not, the ring cannot be
suspended. The consumer then writes the byte 0x02 into byte 8 of
sector 2. The consumer must then wait for the producer to acknowledge
the suspend, which it will do by writing 0x02 into byte 8 of sector 1.</p><h2 id=the-fromlvm-ring>The FromLVM ring</h2><p>Two different types of message can be sent on the FromLVM ring.</p><p>The FreeAllocation message contains the blocks for the free pool.
Example message:</p><pre><code>(FreeAllocation((blocks((pv0(12326 12249))(pv0(11 1))))(generation 2)))
</code></pre><p>Pretty-printed:</p><pre><code>(FreeAllocation
    (
        (blocks
            (
                (pv0(12326 12249))
                (pv0(11 1))
            )
        )
        (generation 2)
    )
)
</code></pre><p>This is a message to add two new sets of extents to the free pool. A
span of length 12249 extents starting at extent 12326, and a span of
length 1 starting from extent 11, both within the physical volume
&lsquo;pv0&rsquo;. The generation count of this message is &lsquo;2&rsquo;. The semantics of
the generation is that the local allocator must record the generation
of the last message it received since the FromLVM ring was resumed,
and ignore any message with a generated less than or equal to the last
message received.</p><p>The CapRequest message contains a request to cap the free pool at
a maximum size.
Example message:</p><pre><code>(CapRequest((cap 6127)(name host1-freeme)))
</code></pre><p>Pretty-printed:</p><pre><code>(CapRequest
    (
        (cap 6127)
        (name host1-freeme)
    )
)
</code></pre><p>This is a request to cap the free pool at a maximum size of 6127
extents. The &rsquo;name&rsquo; parameter reflects the name of the LV into which
the extents should be transferred.</p><h2 id=the-tolvm-ring>The ToLVM Ring</h2><p>The ToLVM ring only contains 1 type of message. Example:</p><pre><code>((volume test5)(segments(((start_extent 1)(extent_count 32)(cls(Linear((name pv0)(start_extent 12328))))))))
</code></pre><p>Pretty-printed:</p><pre><code>(
    (volume test5)
    (segments
        (
            (
                (start_extent 1)
                (extent_count 32)
                (cls
                    (Linear
                        (
                            (name pv0)
                            (start_extent 12328)
                        )
                    )
                )
            )
        )
    )
)
</code></pre><p>This message is extending an LV named &rsquo;test5&rsquo; by giving it 32 extents
starting at extent 1, coming from PV &lsquo;pv0&rsquo; starting at extent
12328. The &lsquo;cls&rsquo; field should always be &lsquo;Linear&rsquo; - this is the only
acceptable value.</p><h1 id=cap-requests>Cap requests</h1><p>Xenvmd will try to keep the free pools of the hosts within a range
set as a fraction of free space. There are 3 parameters adjustable
via the config file:</p><ul><li>low_water_mark_factor</li><li>medium_water_mark_factor</li><li>high_water_mark_factor</li></ul><p>These three are all numbers between 0 and 1. Xenvmd will sum the free
size and the sizes of all hosts&rsquo; free pools to find the total
effective free size in the VG, <code>F</code>. It will then subtract the sizes of
any pending desired space from in-flight create or resize calls <code>s</code>. This
will then be divided by the number of hosts connected, <code>n</code>, and
multiplied by the three factors above to find the 3 absolute values
for the high, medium and low watermarks.</p><pre><code>{high, medium, low} * (F - s) / n
</code></pre><p>When xenvmd notices that a host&rsquo;s free pool size has dropped below
the low watermark, it will be topped up such that the size is equal
to the medium watermark. If xenvmd notices that a host&rsquo;s free pool
size is above the high watermark, it will issue a &lsquo;cap request&rsquo; to
the host&rsquo;s local allocator, which will then respond by allocating
from its free pool into the fake LV, which xenvmd will then delete
as soon as it gets the update.</p><p>Xenvmd keeps track of the last update it has sent to the local
allocator, and will not resend the same request twice, unless it
is restarted.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (22.6.0)</span></td></tr></table></header><h1 id=tls-vertification-for-intra-pool-communications>TLS vertification for intra-pool communications</h1><h2 id=overview>Overview</h2><p>Xenserver has used TLS-encrypted communications between xapi daemons in a pool since its first release.
However it does not use TLS certificates to authenticate the servers it connects to.
This allows possible attackers opportunities to impersonate servers when the pools’ management network is compromised.</p><p>In order to enable certificate verification, certificate exchange as well as proper set up to trust them must be provided by xapi.
This is currently done by allowing users to generate, sign and install the certificates themselves; and then enable the Common Criteria mode.
This requires a CA and has a high barrier of entry.</p><p>Using the same certificates for intra-host communication creates friction between what the user needs and what the host needs.
Instead of trying to reconcile these two uses with one set of certificates, host will serve two certificates: one for API calls from external clients, which is the one that can be changed by the users; and one that is use for intra-pool communications.
The TLS server in the host can select which certificate to serve depending on the service name the client requests when opening a TLS connection.
This mechanism is called Server Name Identification or SNI in short.</p><p>Last but not least the update bearing these changes must not disrupt pool operations while or after being applied.</p><h2 id=glossary>Glossary</h2><table><thead><tr><th>Term</th><th>Meaning</th></tr></thead><tbody><tr><td>SNI</td><td>Server Name Identification. This TLS protocol extension allows a server to select a certificate during the initial TLS handshake depending on a client-provided name. This usually allows a single reverse-proxy to serve several HTTPS websites.</td></tr><tr><td>Host certificate</td><td>Certificate that a host sends clients when the latter initiate a connection with the former. The clients may close the connection depending on the properties of this certificate and whether they have decided to trust it previously.</td></tr><tr><td>Trusted certificate</td><td>Certificate that a computer uses to verify whether a host certificate is valid. If the host certificate&rsquo;s chain of trust does not include a trusted certificate it will be considered invalid.</td></tr><tr><td>Default Certificate</td><td>Xenserver hosts present this certificate to clients which do not request an SNI. Users are allowed to install their own custom certificate.</td></tr><tr><td>Pool Certificate</td><td>Xenserver hosts present this certificate to clients which request <code>xapi:pool</code>as the SNI. They are used for host-to-host communications.</td></tr><tr><td>Common Criteria</td><td>Common Criteria for Information Technology Security Evaluation is a certification on computer security.</td></tr></tbody></table><h1 id=certificates-and-identity-management>Certificates and Identity management</h1><p>Currently Xenserver hosts generate self-signed certificates with the IP or FQDN as their subjects, users may also choose to install certificates.
When installing these certificates only the cryptographic algorithms used to generate the certificates (private key and hash) are validated and no properties about them are required.</p><p>This means that using user-installed certificates for intra-pool communication may prove difficult as restrictions regarding FQDN and chain validation need to be ensured before enabling TLS certificate checking or the pool communications will break down.</p><p>Instead a different certificate is used only for pool communication.
This allows to decouple whatever requirements users might have for the certificates they install to the requirements needed for secure pool communication.
This has several benefits:</p><ul><li>Frees the pool from ensuring a sound hostname resolution on the internal communications.</li><li>Allows the pool to rotate the certificates when it deems necessary. (in particular expiration, or forced invalidation)</li><li>Hosts never share a host certificate, and their private keys never get transmitted.</li></ul><p>In general, the project is able to more safely change the parameters of intra-pool communication without disrupting how users use custom certificates.</p><p>To be able to establish trust in a pool, hosts must distribute the certificates to the rest of the pool members.
Once that is done servers can verify whether they are connecting to another host in the pool by comparing the server certificate with the certificates in the trust root.
Certificate pinning is available and would allow more stringent checks, but it doesn&rsquo;t seem a necessity: hosts in a pool already share secret that allows them to have full control of the pool.</p><p>To be able to select a host certificate depending whether the connections is intra-pool or comes from API clients SNI will be used.
This allows clients to ask for a service when establishing a TLS connection.
This allows the server to choose the certificate they want to offer when negotiating the connection with the client.
The hosts will exploit this to request a particular service when they establish a connection with other hosts in the pool.
When initiating a connection to another host in the pool, a server will create requests for TLS connections with the server_name <code>xapi:pool</code> with the <code>name_type</code> <code>DNS</code>, this goes against RFC-6066 as this <code>server_name</code> is not resolvable.
This still works because we control the implementation in both peers of the connection and can follow the same convention.</p><p>In addition connections to the WLB appliance will continue to be validated using the current scheme of user-installed CA certificates.
This means that hosts connecting to the appliance will need a special case to only trust user-installed certificated when establishing the connection.
Conversely pool connections will ignore these certificates.</p><table><thead><tr><th>Name</th><th>Filesystem location</th><th>User-configurable</th><th>Used for</th></tr></thead><tbody><tr><td>Host Default</td><td>/etc/xensource/xapi-ssl.pem</td><td>yes (using API)</td><td>Hosts serve it to normal API clients</td></tr><tr><td>Host Pool</td><td>/etc/xensource/xapi-pool-tls.pem</td><td>no</td><td>Hosts serve to clients requesting &ldquo;xapi:pool&rdquo; as the SNI</td></tr><tr><td>Trusted Default</td><td>/etc/stunnel/certs/</td><td>yes (using API)</td><td>Certificates that users can install for trusting appliances</td></tr><tr><td>Trusted Pool</td><td>/etc/stunnel/certs-pool/</td><td>no</td><td>Certificates that are managed by the pool for host-to-host communications</td></tr><tr><td>Default Bundle</td><td>/etc/stunnel/xapi-stunnel-ca-bundle.pem</td><td>no</td><td>Bundle of certificates that hosts use to verify appliances (in particular WLB), this is kept in sync with &ldquo;Trusted Default&rdquo;</td></tr><tr><td>Pool Bundle</td><td>/etc/stunnel/xapi-pool-ca-bundle.pem</td><td>no</td><td>Bundle of certificates that hosts use to verify other hosts on pool communications, this is kept in sync with &ldquo;Trusted Pool&rdquo;</td></tr></tbody></table><h2 id=cryptography-of-certificates>Cryptography of certificates</h2><p>The certificates until now have been signed using sha256WithRSAEncryption:</p><ul><li>Pre-8.0 releases use 1024-bit RSA keys.</li><li>8.0, 8.1 and 8.2 use 2048-bit RSA keys.</li></ul><p>The Default Certificates served to API clients will continue to use sha256WithRSAEncryption with 2048-bit RSA keys. The Pool certificates will use the same algorithms for consistency.</p><p>The self-signed certificates until now have used a mix of IP and hostname claims:</p><ul><li>All released versions:<ul><li>Subject and issuer have CN FQDN if the hostname is different from localhost, or CN management IP</li><li>Subject Alternate Names extension contains all the domain names as DNS names</li></ul></li><li>Next release:<ul><li>Subject and issuer have CN management IP</li><li>SAN extension contains all domain names as DNS names and the management IP as IP</li></ul></li></ul><p>The Pool certificates do not contain claims about IPs nor hostnames as this may change during runtime and depending on their validity may make pool communication more brittle.
Instead the only claim they have is that their Issuer and their Subject are CN Host UUID, along with a serial number.</p><p>Self-signed certificates produced until now have had validity periods of 3650 days (~10 years).
The Pool certificates will have the same validity period.</p><h1 id=server-components>Server Components</h1><p>HTTPS Connections between hosts usually involve the xapi daemons and stunnel processes:</p><ul><li>When a xapi daemon needs to initiate a connection with another host it starts an HTTP connection with a local stunnel process.</li><li>The stunnel processes wrap http connections inside a TLS connection, allowing HTTPS to be used when hosts communicate</li></ul><p>This means that stunnel needs to be set up correctly to verify certificates when connecting to other hosts.
Some aspects like CA certificates are already managed, but certificate pinning is not.</p><h1 id=use-cases>Use Cases</h1><p>There are several use cases that need to be modified in order correctly manage trust between hosts.</p><h2 id=opening-a-connection-with-a-pool-host>Opening a connection with a pool host</h2><p>This is the main use case for the feature, the rest of use cases that need changes are modified to support this one.
Currently a Xenserver host connecting to another host within the pool does not try to authenticate the receiving server when opening a TLS connection.
(The receiving server authenticates the originating server by xapi authentication, see below)</p><p>Stunnel will be configured to verify the peer certificate against the CA certificates that are present in the host.
The CA certificates must be correctly set up when a host joins the pool to correctly establish trust.</p><p>The previous behaviour for WLB must be kept as the WLB connection <em>must</em> be checked against the user-friendly CA certificates.</p><h2 id=receiving-an-incoming-tls-connection>Receiving an incoming TLS connection</h2><p>All incoming connections authenticate the client using credentials, this does not need the addition of certificates.
(username and password, pool secret)</p><p>The hosts must present the certificate file to incoming connections so the client can authenticate them.
This is already managed by xapi, it configures stunnel to present the configured host certificate.
The configuration has to be changed so stunnel responds to SNI requests containing the string <code>xapi:pool</code> to serve the internal certificate instead of the client-installed one.</p><h2 id=u1-host-installation>U1. Host Installation</h2><p>On xapi startup an additional certificate is created now for pool operations.
It&rsquo;s added to the trusted pool certificates.
The certificate&rsquo;s only claim is the host&rsquo;s UUID.
No IP nor hostname information is kept as the clients only check for the certificate presence in the trust root.</p><h2 id=u2-pool-join>U2. Pool Join</h2><p>This use-case is delicate as it is the point where trust is established between hosts.
This is done with a call from the joiner to the pool coordinator where the certificate of the coordinator is not verified.
In this call the joiner transmits its certificate to the coordinator and the coordinator returns a list of the pool members&rsquo; UUIDs and certificates.
This means that in the policy used is trust on first use.</p><p>To deal with parallel pool joins, hosts download all the Pool certificates in the pool from the coordinator after all restarts.</p><p>The connection is initiated by a client, just like before, there is no change in the API as all the information needed to start the join is already provided (pool username and password, IP of coordinator)</p><div class="mermaid align-center">sequenceDiagram
participant clnt as Client
participant join as Joiner
participant coor as Coordinator
participant memb as Member
clnt->>join: pool.join coordinator_ip coordinator_username coordinator_password
join->>coor:login_with_password coordinator_ip coordinator_username coordinator_password
Note over join: pre_join_checks
join->>join: remote_pool_has_tls_enabled = self_pool_has_tls_enabled
alt are different
Note over join: interrupt join, raise error
end
Note right of join: certificate distribution
coor-->>join:
join->>coor: pool.internal_certificate_list_content
coor-->>join:
join->>coor: pool.upload_identity_host_certificate joiner_certificate uuid
coor->>memb: pool.internal_certificates_sync
memb-->>coor:
loop for every &lt;user CA certificate> in Joiner
join->>coor: Pool.install_ca_certitificate &lt;user CA certificate>
coor-->>join:
end
loop for every &lt;user CRL> in Joiner
join->>coor: Pool.install_crl &lt;user CRL>
coor-->>join:
end
join->>coor: host.add joiner
coor-->>join:
join->>join: restart_as_slave
join->>coor: pool.user_certificates_sync
join->>coor: host.copy_primary_host_certs</div><h2 id=u3-pool-eject>U3. Pool Eject</h2><p>During pool eject the pool must remove the host certificate of the ejected member from the internal trust root, this must be done by the xapi daemon of the coordinator.</p><p>The ejected member will recreate both server certificates to replicate a new installation.
This can be triggered by deleting the certificates and their private keys in the host before rebooting, the current boot scripts automatically generates a new self-signed certificate if the file is not present.
Additionally, both the user and the internal trust roots will be cleared before rebooting as well.</p><h2 id=u4-pool-upgrade>U4. Pool Upgrade</h2><p>When a pool has finished upgrading to the version with certificate checking the database reflects that the feature is turned off, this is done as part of the database upgrade procedure in xen-api.
The internal certificate is created on restart.
It is added to the internal trusted certificates directory.
The distribution of certificate will happens when the tls verification is turned on, afterwards.</p><h2 id=u5-host-certificate-state-inspection>U5. Host certificate state inspection</h2><p>In order to give information about the validity and useful information of installed user-facing certificates to API clients as well as the certificates used for internal purposes, 2 fields are added to certificate records in xapi&rsquo;s datamodel and database:</p><ul><li>type: indicates which of the 3 kind of certificates is the certificate. If it&rsquo;s a user-installed trusted CA certificate, a server certificate served to clients that do not use SNI, and a server certificate served when the SNI xapi:pool is used. The exact values are ca, host and host-internal, respectively.</li><li>name: the human-readable name given by the user. This fields is only present on trusted CA certificates and allows the pool operators to better recognise the certificates.</li></ul><p>Additionally, now the _host field contains a null reference if the certificate is a corporate CA (a ca certificate).</p><p>The fields will get exposed in the CLI whenever a certificate record is listed, this needs a xapi-cli-server to be modified to show the new field.</p><h2 id=u6-migrating-a-vm-to-another-pool>U6. Migrating a VM to another pool</h2><p>To enable a frictionless migration when pools have tls verification enabled, the host certificate of the host receiving the vm is sent to the sender.
This is done by adding the certificate of the receiving host as well as its pool coordinator to the return value of the function migrate_receive function.
The sender can then add the certificate to the folder of CA certificates that stunnel uses to verify the server in a TLS connection.
When the transaction finishes, whether it fails or succeeds the CA certificate is deleted.</p><p>The certificate is stored in a temporary location so xapi can clean up the file when it starts up, in case after the host fences or power cycles while the migration is in progress.</p><p>Xapi invokes sparse_dd with the filename correct trusted bundle as a parameter so it can verify the vhd-server running on the other host.</p><p>Xapi also invokes xcp-rrdd to migrate the VM metrics.
xcp-rrdd is passed the 2 certificates to verify the remote hosts when sending the metrics.</p><p>Clients should not be aware of this change and require no change.</p><p>Xapi-cli-server, the server of xe embedded into xapi, connects to the remote coordinator using TLS to be able to initiate the migration.
Currently no verification is done. A certificate is required to initiate the connection to verify the remote server.</p><p>In u6.3 and u6.4 no changes seem necessary.</p><h2 id=u7-change-a-hosts-name>U7. Change a host&rsquo;s name</h2><p>The Pool certificates do not depend on hostnames.
Changing the hostnames does not affect TLS certificate verification in a pool.</p><h2 id=u8-installing-a-certificate-corporate-ca>U8. Installing a certificate (corporate CA)</h2><p>Installation of corporate CA can be done with current API.
Certificates are added to the database as CA certificates.</p><h2 id=u9-resetting-a-certificate-to-self-signed-certificate>U9. Resetting a certificate (to self-signed certificate)</h2><p>This needs a reimplementation of the current API to reset host certificate, this time allowing the operation to happen when the host is not on emergency node and to be able to do it remotely.</p><h2 id=u10-enabling-certificate-verification>U10. Enabling certificate verification</h2><p>A new API call is introduced to enable tls certificate verification: Pool.enable_tls_verification.
This is used by the CLI command pool-enable-tls-verification.
The call causes the coordinator of the pool to install the Pool certificates of all the members in its internal trust root.
Then calls the api for each member to install all of these certificates.
After this public key exchange is done, TLS certificate verification is enabled on the members, with the coordinator being the last to enable it.</p><p>When there are issues that block enabling the feature, the call returns an error specific to that problem:</p><ul><li>HA must not be enabled, as it can interrupt the procedure when certificates are distributed</li><li>Pool operations that can disrupt the certificate exchange block this operation: These operations are listed in here</li><li>There was an issue with the certificate exchange in the pool.</li></ul><p>The coordinator enabling verification last is done to ensure that if there is any issue enabling the coordinator host can still connect to members and rollback the setting.</p><p>A new field is added to the pool: tls_verification_enabled. This enables clients to query whether TLS verification is enabled.</p><h2 id=u11-disabling-certificate-verification>U11. Disabling certificate verification</h2><p>A new emergency command is added emergency-host-disable-tls-verification.
This command disables tls-verification for the xapi daemon in a host.
This allows the host to communicate with other hosts in the pool.</p><p>After that, the admin can regenerate the certificates using the new host-refresh-server-certificate in the hosts with invalid certificates, finally they can reenable tls certificate checking using the call emergency-host-reenable-tls-verification.</p><p>The documentation will include instructions for administrators on how to reset certificates and manually installing the host certificates as CA certificates to recover pools.</p><p>This means they will not have to disable TLS and compromise on security.</p><h2 id=u12-being-aware-of-certificate-expiry>U12. Being aware of certificate expiry</h2><p>Stockholm hosts provide alerts 30 days before hosts certificates expire, it must be changed to alert about users&rsquo; CA certificates expiring.</p><p>Pool certificates need to be cycled when the certificate expiry is approaching.
Alerts are introduced to warn the administrator this task must be done, or risk the operation of the pool.
A new API is introduced to create certificates for all members in a pool and replace the existing internal certificates with these.
This call imposes the same requirements in a pool as the pool secret rotation: It cannot be run in a pool unless all the host are online, it can only be started by the coordinator, the coordinator is in a valid state, HA is disabled, no RPU is in progress, and no pool operations are in progress.
The API call is Pool.rotate_internal_certificates.
It is exposed by xe as pool-rotate-internal-certificates.</p><h1 id=changes>Changes</h1><p>Xapi startup has to account for host changes that affect this feature and modify the filesystem and pool database accordingly.</p><ul><li>Public certificate changed: On first boot, after a pool join and when doing emergency repairs the server certificate record of the host may not match to the contents in the filesystem. A check is to be introduced that detects if the database does not associate a certificate with the host or if the certificate&rsquo;s public key in the database and the filesystem are different. If that&rsquo;s the case the database is updated with the certificate in the filesystem.</li><li>Pool certificate not present: In the same way the public certificate served is generated on startup, the internal certificate must be generated if the certificate is not present in the filesystem.</li><li>Pool certificate changed: On first boot, after a pool join and after having done emergency repairs the internal server certificate record may not match the contents of the filesystem. A check is to be introduced that detects if the database does not associate a certificate with the host or if the certificate&rsquo;s public key in the database and the filesystem are different. This check is made aware whether the host is joining a pool or is on first-boot, it does this by counting the amount of hosts in the pool from the database. In the case where it&rsquo;s joining a pool it simply updated the database record with the correct information from the filesystem as the filesystem contents have been put in place before the restart. In the case of first boot the public part of the certificate is copied to the directory and the bundle for internally-trusted certificates: /etc/stunnel/certs-pool/ and /etc/stunnel/xapi-pool-ca-bundle.pem.</li></ul><p>The xapi database records for certificates must be changed according with the additions explained before.</p><h3 id=api>API</h3><p>Additions</p><ul><li>Pool.tls_verification_enabled: this is a field that indicates whether TLS verification is enabled.</li><li>Pool.enable_tls_verification: this call is allowed for role _R_POOL_ADMIN. It&rsquo;s not allowed to run if HA is enabled nor pool operations are in progress. All the hosts in the pool transmit their certificate to the coordinator and the coordinator then distributes the certificates to all members of the pool. Once that is done the coordinator tries to initiate a session with all the pool members with TLS verification enabled. If it&rsquo;s successful TLS verification is enabled for the whole pool, otherwise the error COULD_NOT_VERIFY_HOST [member UUID] is emmited.</li><li>TLS_VERIFICATION_ENABLE_IN_PROGRESS is a new error that is produced when trying to do other pool operations while enabling TLS verification is in progress</li><li>Host.emergency_disable_tls_verification: this called is allowed for role _R_LOCAL_ROOT_ONLY: it&rsquo;s an emergency command and acts locally. It forces connections in xapi to stop verifying the peers on outgoing connections. It generates an alert to warn the administrators of this uncommon state.</li><li>Host.emergency_reenable_tls_verification: this call is allowed for role _R_LOCAL_ROOT_ONLY: it&rsquo;s an emergency command and acts locally. It changes the configuration so xapi verifies connections by default after being switched off with the previous command.</li><li>Pool.install_ca_certificate: rename of Pool.certificate_install, add the ca certificate to the database.</li><li>Pool.uninstall_ca_certificate: rename of Pool.certificate_uninstall, removes the certificate from the database.</li><li>Host.reset_server_certificate: replaces Host.emergency_reset_server_certificate, now it&rsquo;s allowed for role _R_POOL_ADMIN. It adds a record for the generated Default Certificate to the database while removing the previous record, if any.</li><li>Pool.rotate_internal_certificates: This call generates new Pool certificates, and substitutes the previous certificates with these. See the certificate expiry section for more details.</li></ul><p>Modifications:</p><ul><li>Pool.join: certificates must be correctly distributed. API Error POOL_JOINING_HOST_TLS_VERIFICATION_MISMATCH is returned if the tls_verification of the two pools doesn&rsquo;t match.</li><li>Pool.eject: all certificates must be deleted from the ejected host&rsquo;s filesystem and the ejected host&rsquo;s certificate must be deleted from the pool&rsquo;s trust root.</li><li>Host.install_server_certificate: the certificate type host for the record must be added to denote it&rsquo;s a Standard Certificate.</li></ul><p>Deprecations:</p><ul><li>pool.certificate_install</li><li>pool.certificate_uninstall</li><li>pool.certificate_list</li><li>pool.wlb_verify_cert: This setting is superseeded by pool.enable_tls_verification. It cannot be removed, however. When updating from a previous version when this setting is on, TLS connections to WLB must still verify the external host. When the global setting is enabled this setting is ignored.</li><li>host.emergency_reset_server_certificate: host.reset_server_certificate should be used instead as this call does not modify the database.</li></ul><h3 id=cli>CLI</h3><p>Following API additions:</p><ul><li>pool-enable-tls-verification</li><li>pool-install-ca-certificate</li><li>pool-uninstall-ca-certificate</li><li>pool-internal-certificates-rotation</li><li>host-reset-server-certificate</li><li>host-emergency-disable-tls-verification (emits a warning when verification is off and the pool-level is on)</li><li>host-emergency-reenable-tls-verification</li></ul><p>And removals:</p><ul><li>host-emergency-server-certificate</li></ul><h3 id=feature-flags>Feature Flags</h3><p>This feature needs clients to behave differently when initiating pool joins, to allow them to choose behaviour the toolstack will expose a new feature flag &lsquo;Certificate_verification&rsquo;. This flag will be part of the express edition as it&rsquo;s meant to aid detection of a feature and not block access to it.</p><h3 id=alerts>Alerts</h3><p>Several alerts are introduced:</p><ul><li><p>POOL_CA_CERTIFICATE_EXPIRING_30, POOL_CA_CERTIFICATE_EXPIRING_14, POOL_CA_CERTIFICATE_EXPIRING_07, POOL_CA_CERTIFICATE_EXPIRED: Similar to host certificates, now the user-installable pool&rsquo;s CA certificates are monitored for expiry dates and alerts are generated about them. The body for this type of message is:</p></li><li><p>HOST_INTERNAL_CERTIFICATE_EXPIRING_30, HOST_INTERNAL_CERTIFICATE_EXPIRING_14, HOST_INTERNAL_CERTIFICATE_EXPIRING_07, HOST_INTERNAL_CERTIFICATE_EXPIRED: Similar to host certificates, the newly-introduced hosts&rsquo; internal server certificates are monitored for expiry dates and alerts are generated about them. The body for this type of message is:</p></li><li><p>TLS_VERIFICATION_EMERGENCY_DISABLED: The host is in emergency mode and is not enforcing tls verification anymore, the situation that forced the disabling must be fixed and the verification enabled ASAP.</p></li><li><p>FAILED_LOGIN_ATTEMPTS: An hourly alert that contains the number of failed attempts and the 3 most common origins for these failed alerts. The body for this type of message is:</p></li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (5.6 fp1)</span></td></tr></table></header><h1 id=tunnelling-api-design>Tunnelling API design</h1><p>To isolate network traffic between VMs (e.g. for security reasons) one can use
VLANs. The number of possible VLANs on a network, however, is limited, and
setting up a VLAN requires configuring the physical switches in the network.
GRE tunnels provide a similar, though more flexible solution. This document
proposes a design that integrates the use of tunnelling in the XenAPI. The
design relies on the recent introduction of the Open vSwitch, and
requires an Open vSwitch
(<a href=https://www.opennetworking.org/sdn-resources/openflow target=_blank>OpenFlow</a>) controller
(further referred to as
<em>the controller</em>) to set up and maintain the actual GRE tunnels.</p><p>We suggest following the way VLANs are modelled in the datamodel. Introducing a
VLAN involves creating a Network object for the VLAN, that VIFs can connect to.
The <code>VLAN.create</code> API call takes references to a PIF and Network to use and a
VLAN tag, and creates a VLAN object and a PIF object. We propose something
similar for tunnels; the resulting objects and relations for two hosts would
look like this:</p><pre><code>PIF (transport) -- Tunnel -- PIF (access) \          / VIF
                                            Network -- VIF
PIF (transport) -- Tunnel -- PIF (access) /          \ VIF
</code></pre><h2 id=xenapi-changes>XenAPI changes</h2><h3 id=new-tunnel-class>New tunnel class</h3><h4 id=fields>Fields</h4><ul><li><code>string uuid</code> (read-only)</li><li><code>PIF ref access_PIF</code> (read-only)</li><li><code>PIF ref transport_PIF</code> (read-only)</li><li><code>(string -> string) map status</code> (read/write); owned by the controller, containing at least the
key <code>active</code>, and <code>key</code> and <code>error</code> when appropriate (see below)</li><li><code>(string -> string) map other_config</code> (read/write)</li></ul><p>New fields in PIF class (automatically linked to the corresponding <code>tunnel</code>
fields):</p><ul><li><code>PIF ref set tunnel_access_PIF_of</code> (read-only)</li><li><code>PIF ref set tunnel_transport_PIF_of</code> (read-only)</li></ul><h4 id=messages>Messages</h4><ul><li><code>tunnel ref create (PIF ref, network ref)</code></li><li><code>void destroy (tunnel ref)</code></li></ul><h3 id=backends>Backends</h3><p>For clients to determine which network backend is in use (to decide whether
tunnelling functionality is enabled) a key <code>network_backend</code> is added to the
<code>Host.software_version</code> map on each host. The value of this key can be:</p><ul><li><code>bridge</code>: the Linux bridging backend is in use;</li><li><code>openvswitch</code>: the [Open vSwitch] backend is in use.</li></ul><h3 id=notes>Notes</h3><ul><li><p>The user is responsible for creating tunnel and network objects, associating
VIFs with the right networks, and configuring the physical PIFs, all using
the XenAPI/CLI/XC.</p></li><li><p>The <code>tunnel.status</code> field is owned by the controller. It
may be possible to define an RBAC role for the controller, such that only the
controller is able to write to it.</p></li><li><p>The <code>tunnel.create</code> message does not take
a tunnel identifier (GRE key). The controller is responsible for assigning
the right keys transparently. When a tunnel has been set up, the controller
will write its key to <code>tunnel.status:key</code>, and it will set
<code>tunnel.status:active</code> to <code>"true"</code> in the same field.</p></li><li><p>In case a tunnel could
not be set up, an error code (to be defined) will be written to
<code>tunnel.status:error</code>, and <code>tunnel.status:active</code> will be <code>"false"</code>.</p></li></ul><h2 id=xapi>Xapi</h2><h3 id=tunnelcreate>tunnel.create</h3><ul><li>Fails with <code>OPENVSWITCH_NOT_ACTIVE</code> if the Open vSwitch networking sub-system
is not active (the host uses linux bridging).</li><li>Fails with <code>IS_TUNNEL_ACCESS_PIF</code> if the specified transport PIF is a tunnel access PIF.</li><li>Takes care of creating and connecting the new tunnel and PIF objects.<ul><li>Sets a random MAC on the access PIF.</li><li>IP configuration of the tunnel
access PIF is left blank. (The IP configuration on a PIF is normally used for
the interface in dom0. In this case, there is no tunnel interface for dom0 to
use. Such functionality may be added in future.)</li><li>The <code>tunnel.status:active</code>
field is initialised to <code>"false"</code>, indicating that no actual tunnelling
infrastructure has been set up yet.</li></ul></li><li>Calls <code>PIF.plug</code> on the new tunnel access PIF.</li></ul><h3 id=tunneldestroy>tunnel.destroy</h3><ul><li>Calls <code>PIF.unplug</code> on the tunnel access PIF. Destroys the <code>tunnel</code> and
tunnel access PIF objects.</li></ul><h3 id=pifplug-on-a-tunnel-access-pif>PIF.plug on a tunnel access PIF</h3><ul><li>Fails with <code>TRANSPORT_PIF_NOT_CONFIGURED</code> if the underlying transport PIF has
<code>PIF.ip_configuration_mode = None</code>, as this interface needs to be configured
for the tunnelling to work. Otherwise, the transport PIF will be plugged.</li><li>Xapi requests <code>interface-reconfigure</code> to &ldquo;bring up&rdquo; the tunnel access PIF,
which causes it to create a local bridge.</li><li>No link will be made between the
new bridge and the physical interface by <code>interface-reconfigure</code>. The
controller is responsible for setting up these links. If the controller is
not available, no links can be created, and the tunnel network degrades to an
internal network (only intra-host connectivity).</li><li><code>PIF.currently_attached</code> is set to <code>true</code>.</li></ul><h3 id=pifunplug-on-a-tunnel-access-pif>PIF.unplug on a tunnel access PIF</h3><ul><li>Xapi requests <code>interface-reconfigure</code> to &ldquo;bring down&rdquo; the tunnel PIF, which
causes it to destroy the local bridge.</li><li><code>PIF.currently_attached</code> is set to <code>false</code>.</li></ul><h3 id=pifunplug-on-a-tunnel-transport-pif>PIF.unplug on a tunnel transport PIF</h3><ul><li>Calls <code>PIF.unplug</code> on the associated tunnel access PIF(s).</li></ul><h3 id=pifforget-on-a-tunnel-access-of-transport-pif>PIF.forget on a tunnel access of transport PIF</h3><ul><li>Fails with <code>PIF_TUNNEL_STILL_EXISTS</code>.</li></ul><h3 id=vlancreate>VLAN.create</h3><ul><li>Tunnels can only exist on top of physical/VLAN/Bond PIFs, and not the other
way around. <code>VLAN.create</code> fails with <code>IS_TUNNEL_ACCESS_PIF</code> if given an
underlying PIF that is a tunnel access PIF.</li></ul><h3 id=pool-join>Pool join</h3><ul><li>As for VLANs, when a host joins a pool, it will inherit the tunnels that are
present on the pool master.</li><li>Any tunnels (tunnel and access PIF objects)
configured on the host are removed, which will leave their networks
disconnected (the networks become internal networks). As a joining host is
always a single host, there is no real use for having had tunnels on it, so
this probably will never be an issue.</li></ul><h2 id=the-controller>The controller</h2><ul><li>The controller tracks the <code>tunnel</code> class to determine which bridges/networks
require GRE tunnelling.<ul><li>On start-up, it calls <code>tunnel.get_all</code> to obtain the information about all
tunnels.</li><li>Registers for events on the <code>tunnel</code> class to stay up-to-date.</li></ul></li><li>A tunnel network is organised as a star topology. The controller is free to
decide which host will be the central host (&ldquo;switching host&rdquo;).</li><li>If the
current switching host goes down, a new one will be selected, and GRE tunnels
will be reconstructed.</li><li>The controller creates GRE tunnels connecting each
existing Open vSwitch bridge that is associated with the same tunnel network,
after assigning the network a unique GRE key.</li><li>The controller destroys GRE
tunnels if associated Open vSwitch bridges are destroyed. If the destroyed
bridge was on the switching host, and other hosts are still using the same
tunnel network, a new switching host will be selected, and GRE tunnels will
be reconstructed.</li><li>The controller sets <code>tunnel.status:active</code> to <code>"true"</code> for
all tunnel links that have been set up, and <code>"false"</code> if links are broken.</li><li>The controller writes an appropriate error code (to be defined) to
<code>tunnel.status:error</code> in case something went wrong.</li><li>When an access PIF is
plugged, and the controller succeeds to set up the tunnelling infrastructure,
it writes the GRE key to <code>tunnel.status:key</code> on the associated tunnel object
(at the same time <code>tunnel.status:active</code> will be set to <code>"true"</code>).</li><li>When the
tunnel infrastructure is not up and running, the controller may remove the
key <code>tunnel.status:key</code> (optional; the key should anyway be disregarded if
<code>tunnel.status:active</code> is <code>"false"</code>).</li></ul><h2 id=cli>CLI</h2><p>New <code>xe</code> commands (analogous to <code>xe vlan-</code>):</p><ul><li><code>tunnel-create</code></li><li><code>tunnel-destroy</code></li><li><code>tunnel-list</code></li><li><code>tunnel-param-get</code></li><li><code>tunnel-param-list</code></li></ul><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (8.2)</span></td></tr></table></header><h1 id=user-installable-host-certificates>User-installable host certificates</h1><h2 id=introduction>Introduction</h2><p>It is often necessary to replace the TLS certificate used to secure
communications to Xenservers hosts, for example to allow a XenAPI user such as
Citrix Virtual Apps and Desktops (CVAD) to validate that the host is genuine
and not impersonating the actual host.</p><p>Historically there has not been a supported mechanism to do this, and as a
result users have had to rely on guides written by third parties that show how
to manually replace the xapi-ssl.pem file on a host. This process is
error-prone, and if a mistake is made, can result in an unuseable system.
This design provides a fully supported mechanism to allow replacing the
certificates.</p><h2 id=design-proposal>Design proposal</h2><p>It is expected that an API caller will provide, in a single API call, a private
key, and one or more certificates for use on the host. The key will be provided
in PKCS #8 format, and the certificates in X509 format, both in
base-64-encoded PEM containers.</p><p>Multiple certificates can be provided to cater for the case where an
intermediate certificate or certificates are required for the caller to be able
to verify the certificate back to a trusted root (best practice for Certificate
Authorities is to have an &lsquo;offline&rsquo; root, and issue certificates from an
intermediate Certificate Authority). In this situation, it is expected (and
common practice among other tools) that the first certificate provided in the
chain is the host&rsquo;s unique server certificate, and subsequent certificates form
the chain.</p><p>To detect mistakes a user may make, certain checks will be carried out on the
provided key and certificate(s) before they are used on the host. If all checks
pass, the key and certificate(s) will be written to the host, at which stage a
signal will be sent to stunnel that will cause it to start serving the new
certificate.</p><h2 id=certificate-installation>Certificate Installation</h2><h3 id=api-additions>API Additions</h3><p>Xapi must provide an API call through Host RPC API to install host
certificates:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> install_server_certificate <span style=color:#f92672>=</span> call
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>lifecycle<span style=color:#f92672>:[</span><span style=color:#a6e22e>Published</span><span style=color:#f92672>,</span> rel_stockholm<span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>name<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;install_server_certificate&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>doc<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;Install the TLS server certificate.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>versioned_params<span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>[{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>Ref</span> <span style=color:#f92672>_</span>host<span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;host&#34;</span><span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The host&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>None</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>;{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>String</span><span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;certificate&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The server certificate, in PEM form&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>None</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>;{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>String</span><span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;private_key&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The unencrypted private key used to sign the certificate, \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    in PKCS#8 form&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>None</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>;{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>String</span><span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;certificate_chain&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The certificate chain, in PEM form&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>Some</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>VString</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>)}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>allowed_roles<span style=color:#f92672>:_</span>R_POOL_ADMIN
</span></span><span style=display:flex><span>    ()</span></span></code></pre></div><p>This call should be implemented within xapi, using the already-existing crypto
libraries available to it.</p><p>Analogous to the API call, a new CLI call <code>host-server-certificate-install</code>
must be introduced, which takes the parameters <code>certificate</code>, <code>key</code> and
<code>certificate-chain</code> - these parameters are expected to be filenames, from which
the key and certificate(s) must be read, and passed to the
<code>install_server_certificate</code> RPC call.</p><p>The CLI will be defined as:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#e6db74>&#34;host-server-certificate-install&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  reqd<span style=color:#f92672>=[</span><span style=color:#e6db74>&#34;certificate&#34;</span><span style=color:#f92672>;</span> <span style=color:#e6db74>&#34;private-key&#34;</span><span style=color:#f92672>];</span>
</span></span><span style=display:flex><span>  optn<span style=color:#f92672>=[</span><span style=color:#e6db74>&#34;certificate-chain&#34;</span><span style=color:#f92672>];</span>
</span></span><span style=display:flex><span>  help<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Install a server TLS certificate on a host&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  implementation<span style=color:#f92672>=</span><span style=color:#a6e22e>With_fd</span> Cli_operations.host_install_server_certificate<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  flags<span style=color:#f92672>=[</span> <span style=color:#a6e22e>Host_selectors</span> <span style=color:#f92672>];</span>
</span></span><span style=display:flex><span><span style=color:#f92672>};</span></span></span></code></pre></div><h3 id=validation>Validation</h3><p>Xapi must perform the following validation steps on the provided key and
certificate. If any validation step fails, the API call must return an error
with the specified error code, providing any associated text:</p><h3 id=private-key>Private Key</h3><ul><li><p>Validate that it is a pem-encoded PKCS#8 key, use error
<code>SERVER_CERTIFICATE_KEY_INVALID []</code> and exposed as
&ldquo;The provided key is not in a pem-encoded PKCS#8 format.&rdquo;</p></li><li><p>Validate that the algorithm of the key is RSA, use error
<code>SERVER_CERTIFICATE_KEY_ALGORITHM_NOT_SUPPORTED, [&lt;algorithms's ASN.1 OID>]</code>
and exposed as &ldquo;The provided key uses an unsupported algorithm.&rdquo;</p></li><li><p>Validate that the key length is ≥ 2048, and ≤ 4096 bits, use error
<code>SERVER_CERTIFICATE_KEY_RSA_LENGTH_NOT_SUPPORTED, [length]</code> and exposed as
&ldquo;The provided RSA key does not have a length between 2048 and 4096.&rdquo;</p></li><li><p>The library used does not support multi-prime RSA keys, when it&rsquo;s
encountered use error <code>SERVER_CERTIFICATE_KEY_RSA_MULTI_NOT_SUPPORTED []</code> and
exposed as &ldquo;The provided RSA key is using more than 2 primes, expecting only
2&rdquo;</p></li></ul><h4 id=server-certificate>Server Certificate</h4><ul><li><p>Validate that it is a pem-encoded X509 certificate, use error
<code>SERVER_CERTIFICATE_INVALID []</code> and exposed as &ldquo;The provided certificate is not
in a pem-encoded X509.&rdquo;</p></li><li><p>Validate that the public key of the certificate matches the public key from
the private key, using error <code>SERVER_CERTIFICATE_KEY_MISMATCH []</code> and exposing
it as &ldquo;The provided key does not match the provided certificate&rsquo;s public key.&rdquo;</p></li><li><p>Validate that the certificate is currently valid. (ensure all time
comparisons are done using UTC, and any times presented in errors are using
ISO8601 format):</p><ul><li><p>Ensure the certificate&rsquo;s <code>not_before</code> date is ≤ NOW
<code>SERVER_CERTIFICATE_NOT_VALID_YET, [&lt;NOW>; &lt;not_before>]</code> and exposed as
&ldquo;The provided certificate certificate is not valid yet.&rdquo;</p></li><li><p>Ensure the certificate&rsquo;s <code>not_after</code> date is > NOW
<code>SERVER_CERTIFICATE_EXPIRED, [&lt;NOW>; &lt;not_after>]</code> and exposed as &ldquo;The
provided certificate has expired.&rdquo;</p></li></ul></li><li><p>Validate that the certificate signature algorithm is SHA-256
<code>SERVER_CERTIFICATE_SIGNATURE_NOT_SUPPORTED []</code> and exposed as
&ldquo;The provided certificate is not using the SHA256 (SHA2) signature algorithm.&rdquo;</p></li></ul><h4 id=intermediate-certificates>Intermediate Certificates</h4><ul><li>Validate that it is an X509 certificate, use
<code>SERVER_CERTIFICATE_CHAIN_INVALID []</code> and exposed as &ldquo;The provided
intermediate certificates are not in a pem-encoded X509.&rdquo;</li></ul><h3 id=filesystem-interaction>Filesystem Interaction</h3><p>If validation has been completed successfully, a temporary file must be created
with permissions 0x400 containing the key and certificate(s), in that order,
separated by an empty line.</p><p>This file must then be atomically moved to /etc/xensource/xapi-ssl.pem in
order to ensure the integrity of the contents. This may be done using rename
with the origin and destination in the same mount-point.</p><h2 id=alerting>Alerting</h2><p>A daily task must be added. This task must check the expiry date of the first
certificate present in /etc/xensource/xapi-ssl.pem, and if it is within 30
days of expiry, generate a <code>message</code> to alert the administrator that the
certificate is due to expire shortly.</p><p>The body of the message should contain:</p><div class="wrap-code highlight"><pre tabindex=0><code>&lt;body&gt;
  &lt;message&gt;
    The TLS server certificate is expiring soon
  &lt;/message&gt;
  &lt;date&gt;
    &lt;expiry date in ISO8601 &#39;YYYY-MM-DDThh:mm:ssZ&#39; format&gt;`
  &lt;/date&gt;
&lt;/body&gt;</code></pre></div><p>The priority of the message should be based on the number of days to expiry as
follows:</p><table><thead><tr><th>Number of days</th><th>Priority</th></tr></thead><tbody><tr><td>0-7</td><td>1</td></tr><tr><td>8-14</td><td>2</td></tr><tr><td>14+</td><td>3</td></tr></tbody></table><p>The other fields of the message should be:</p><table><thead><tr><th>Field</th><th>Value</th></tr></thead><tbody><tr><td>name</td><td>HOST_SERVER_CERTIFICATE_EXPIRING</td></tr><tr><td>class</td><td>Host</td></tr><tr><td>obj-uuid</td><td>&lt; Host UUID ></td></tr></tbody></table><p>Any existing <code>HOST_SERVER_CERTIFICATE_EXPIRING</code> messages with this host&rsquo;s UUID
should be removed to avoid a build-up of messages.</p><p>Additionally, the task may also produce messages for expired server
certificates which must use the name <code>HOST_SERVER_CERTIFICATE_EXPIRED</code>.
These kind of message must contain the message &ldquo;The TLS server certificate has
expired.&rdquo; as well as the expiry date, like the expiring messages.
They also may replace the existing expiring messages in a host.</p><h2 id=expose-certificate-metadata>Expose Certificate metadata</h2><p>Currently xapi exposes a CLI command to print the certificate being used to
verify external hosts. We would like to also expose through the API and the
CLI useful metadata about the certificates in use by each host.</p><p>The new class is meant to cover server certificates and trusted certificates.</p><h3 id=schema>Schema</h3><p>A new class, Certificate, will be added with the following schema:</p><table><thead><tr><th>Field</th><th>Type</th><th>Notes</th></tr></thead><tbody><tr><td>uuid</td><td></td><td></td></tr><tr><td>type</td><td>CA</td><td>Certificate trusted by all hosts</td></tr><tr><td></td><td>Host</td><td>Certificate that the host present sto normal clients</td></tr><tr><td>name</td><td>String</td><td>Name, only present for trusted certificates</td></tr><tr><td>host</td><td>Ref _host</td><td>Host where the certificate is installed</td></tr><tr><td>not_before</td><td>DateTime</td><td>Date after which the certificate is valid</td></tr><tr><td>not_after</td><td>DateTime</td><td>Date before which the certificate is valid</td></tr><tr><td>fingerprint_sha256</td><td>String</td><td>The certificate&rsquo;s SHA256 fingerprint / hash</td></tr><tr><td>fingerprint_sha1</td><td>String</td><td>The certificate&rsquo;s SHA1 fingerprint / hash</td></tr></tbody></table><h3 id=cli--api>CLI / API</h3><p>There are currently-existing CLI parameters for certificates:
<code>pool-certificate-{install,uninstall,list,sync}</code>,
<code>pool-crl-{install,uninstall,list}</code> and <code>host-get-server-certificate</code>.</p><p>The new command must show the metadata of installed server certificates in
the pool.
It must be able to show all of them in the same call, and be able to filter
the certificates per-host.</p><p>To make it easy to separate it from the previous calls and to reflect that
certificates are a class type in xapi the call will be named <code>certificate-list</code>
and it will accept the parameter <code>host-uuid=&lt;uuid></code>.</p><h2 id=recovery-mechanism>Recovery mechanism</h2><p>In the case a certificate is let to expire TLS clients connecting to the host
will refuse establish the connection.
This means that the host is going to be unable to be managed using the xapi
API (Xencenter, or a CVAD control plane)</p><p>There needs to be a mechanism to recover from this situation.
A CLI command must be provided to install a self-signed certificate, in the
same way it is generated during the setup process at the moment.
The command will be <code>host-emergency-reset-server-certificate</code>.
This command is never to be forwarded to another host and will call openssl to
create a new RSA private key</p><p>The command must notify stunnel to make sure stunnel uses the newly-created
certificate.</p><h1 id=miscellaneous>Miscellaneous</h1><p>The auto-generated <code>xapi-ssl.pem</code> currently contains Diffie-Hellman (DH)
Parameters, specifically 512 bits worth. We no longer support any ciphers which
require DH parameters, so these are no longer needed, and it is acceptable for
them to be lost as part of installing a new certificate/key pair.</p><p>The generation should also be modified to avoid creating these for new
installations.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/156>#156</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr></table></header><h1 id=vgpu-type-identifiers>VGPU type identifiers</h1><h2 id=introduction>Introduction</h2><p>When xapi starts, it may create a number of VGPU_type objects. These act as
VGPU presets, and exactly which VGPU_type objects are created depends on the
installed hardware and in certain cases the presence of certain files in dom0.</p><p>When deciding which VGPU_type objects need to be created, xapi needs to
determine whether a suitable VGPU_type object already exists, as there should
never be duplicates. At the moment the combination of vendor name and model name
is used as a primary key, but this is not ideal as these values are subject to
change. We therefore need a way of creating a primary key to uniquely identify
VGPU_type objects.</p><h2 id=identifier>Identifier</h2><p>We will add a new read-only field to the database:</p><ul><li><code>VGPU_type.identifier (string)</code></li></ul><p>This field will contain a string representation of the parameters required to
uniquely identify a VGPU_type. The parameters required can be summed up with the
following OCaml type:</p><div class="wrap-code highlight"><pre tabindex=0><code>type nvidia_id = {
  pdev_id : int;
  psubdev_id : int option;
  vdev_id : int;
  vsubdev_id : int;
}

type gvt_g_id = {
  pdev_id : int;
  low_gm_sz : int64;
  high_gm_sz : int64;
  fence_sz : int64;
  monitor_config_file : string option;
}

type t =
  | Passthrough
  | Nvidia of nvidia_id
  | GVT_g of gvt_g_id</code></pre></div><p>When converting this type to a string, the string will always be prefixed with
<code>0001:</code> enabling future versioning of the serialisation format.</p><p>For passthrough, the string will simply be:</p><p><code>0001:passthrough</code></p><p>For NVIDIA, the string will be <code>nvidia</code> followed by the four device IDs
serialised as four-digit hex values, separated by commas. If <code>psubdev_id</code> is
<code>None</code>, the empty string will be used e.g.</p><div class="wrap-code highlight"><pre tabindex=0><code>Nvidia {
  pdev_id = 0x11bf;
  psubdev_id = None;
  vdev_id = 0x11b0;
  vsubdev_id = 0x109d;
}</code></pre></div><p>would map to</p><p><code>0001:nvidia,11bf,,11b0,109d</code></p><p>For GVT-g, the string will be <code>gvt-g</code> followed by the physical device ID encoded
as four-digit hex, followed by <code>low_gm_sz</code>, <code>high_gm_sz</code> and <code>fence_sz</code> encoded
as hex, followed by <code>monitor_config_file</code> (or the empty string if it is <code>None</code>)
e.g.</p><div class="wrap-code highlight"><pre tabindex=0><code>GVT_g {
  pdev_id = 0x162a;
  low_gm_sz = 128L;
  high_gm_sz = 384L;
  fence_sz = 4L;
  monitor_config_file = None;
}</code></pre></div><p>would map to</p><p><code>0001:gvt-g,162a,80,180,4,,</code></p><p>Having this string in the database will allow us to do a simple lookup to test
whether a certain VGPU_type already exists. Although it is not currently
required, this string can also be converted back to the type from which it was
generated.</p><p>When deciding whether to create VGPU_type objects, xapi will generate the
identifier string and use it to look for existing VGPU_type objects in the
database. If none are found, xapi will look for existing VGPU_type objects with
the tuple of model name and vendor name. If still none are found, xapi will
create a new VGPU_type object.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr></table></header><h1 id=virtual-hardware-platform-version>Virtual Hardware Platform Version</h1><h3 id=background-and-goal>Background and goal</h3><p>Some VMs can only be run on hosts of sufficiently recent versions.</p><p>We want a clean way to ensure that xapi only tries to run a guest VM on a host that supports the &ldquo;virtual hardware platform&rdquo; required by the VM.</p><h3 id=suggested-design>Suggested design</h3><ul><li>In the datamodel, VM has a new integer field &ldquo;hardware_platform_version&rdquo; which defaults to zero.</li><li>In the datamodel, Host has a corresponding new integer-list field &ldquo;virtual_hardware_platform_versions&rdquo; which defaults to list containing a single zero element (i.e. <code>[0]</code> or <code>[0L]</code> in OCaml notation). The zero represents the implicit version supported by older hosts that lack the code to handle the Virtual Hardware Platform Version concept.</li><li>When a host boots it populates its own entry from a hardcoded value, currently <code>[0; 1]</code> i.e. a list containing the two integer elements <code>0</code> and <code>1</code>. (Alternatively this could come from a config file.)<ul><li>If this new version-handling functionality is introduced in a hotfix, at some point the pool master will have the new functionality while at least one slave does not. An old slave-host that does not yet have software to handle this feature will not set its DB entry, which will therefore remain as <code>[0]</code> (maintained in the DB by the master).</li></ul></li><li>The existing test for whether a VM can run on (or migrate to) a host must include a check that the VM&rsquo;s virtual hardware platform version is in the host&rsquo;s list of supported versions.</li><li>When a VM is made to start using a feature that is available only in a certain virtual hardware platform version, xapi must set the VM&rsquo;s hardware_platform_version to the maximum of that version-number and its current value (i.e. raise if needed).</li></ul><p>For the version we could consider some type other than integer, but a strict ordering is needed.</p><h3 id=first-use-case>First use-case</h3><p>Version 1 denotes support for a certain feature:</p><blockquote><p>When a VM starts, if a certain flag is set in VM.platform then XenServer will provide an emulated PCI device which will trigger the guest Windows OS to seek drivers for the device, or updates for those drivers. Thus updated drivers can be obtained through the standard Windows Update mechanism.</p></blockquote><p>If the PCI device is removed, the guest OS will fail to boot. A VM using this feature must not be migrated to or started on a XenServer that lacks support for the feature.</p><p>Therefore at VM start, we can look at whether this feature is being used; if it is, then if the VM&rsquo;s Virtual Hardware Platform Version is less than 1 we should raise it to 1.</p><h3 id=limitation>Limitation</h3><p>Consider a VM that requires version 1 or higher. Suppose it is exported, then imported into an old host that does not support this feature. Then the host will not check the versions but will attempt to run the VM, which will then have difficulties.</p><p>The only way to prevent this would be to make a backwards-incompatible change to the VM metadata (e.g. a new item in an enum) so that the old hosts cannot read it, but that seems like a bad idea.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=xenprep>XenPrep</h1><h3 id=background>Background</h3><p>Windows guests should have XenServer-specific drivers installed. As of mid-2015 these have been always been installed and upgraded by an essentially manual process involving an ISO carrying the drivers. We have a plan to enable automation through the standard Windows Update mechanism. This will involve a new additional virtual PCI device being provided to the VM, to trigger Windows Update to fetch drivers for the device.</p><p>There are many existing Windows guests that have drivers installed already. These drivers must be uninstalled before the new drivers are installed (and ideally before the new PCI device is added). To make this easier, we are planning a XenAPI call that will cause the removal of the old drivers and the addition of the new PCI device.</p><p>Since this is only to help with updating old guests, the call may well be removed at some point in the future.</p><h3 id=brief-high-level-design>Brief high-level design</h3><p>The XenAPI call will be called <code>VM.xenprep_start</code>. It will update the VM record to note that the process has started, and will insert a special ISO into the VM&rsquo;s virtual CD drive.</p><p>That ISO will contain a tool which will be set up to auto-run (if auto-run is enabled in the guest). The tool will:</p><ol><li>Lock the CD drive so other Windows programs cannot eject the disc.</li><li>Uninstall the old drivers.</li><li>Eject the CD to signal success.</li><li>Shut down the VM.</li></ol><p>XenServer will interpret the ejection of the CD as a success signal, and when the VM shuts down without the special ISO in the drive, XenServer will:</p><ol><li>Update the VM record:</li></ol><ul><li>Remove the mark that shows that the xenprep process is in progress</li><li>Give it the new PCI device: set <code>VM.auto_update_drivers</code> to <code>true</code>.</li><li>If <code>VM.virtual_hardware_platform_version</code> is less than 2, then set it to 2.</li></ul><ol start=2><li>Start the VM.</li></ol><h3 id=more-details-of-the-xapi-project-parts>More details of the xapi-project parts</h3><p>(The tool that runs in the guest is out of scope for this document.)</p><h4 id=start>Start</h4><p>The XenAPI call <code>VM.xenprep_start</code> will throw a power-state error if the VM is not running.
For RBAC roles, it will be available to &ldquo;VM Operator&rdquo; and above.</p><p>It will:</p><ol><li>Insert the xenprep ISO into the VM&rsquo;s virtual CD drive.</li><li>Write <code>VM.other_config</code> key <code>xenprep_progress=ISO_inserted</code> to record the fact that the xenprep process has been initiated.</li></ol><p>If <code>xenprep_start</code> is called on a VM already undergoing xenprep, the call will return successfully but will not do anything.</p><p>If the VM does not have an empty virtual CD drive, the call will fail with a suitable error.</p><h4 id=cancellation>Cancellation</h4><p>While xenprep is in progress, any request to eject the xenprep ISO (except from inside the guest) will be rejected with a new error &ldquo;VBD_XENPREP_CD_IN_USE&rdquo;.</p><p>There will be a new XenAPI call <code>VM.xenprep_abort</code> which will:</p><ol><li>Remove the <code>xenprep_progress</code> entry from <code>VM.other_config</code>.</li><li>Make a best-effort attempt to eject the CD. (The guest might prevent ejection.)</li></ol><p>This is not intended for cancellation while the xenprep tool is running, but rather for use before it starts, for example if auto-run is disabled or if the VM has a non-Windows OS.</p><h4 id=completion>Completion</h4><p>Aim: when the guest shuts down after ejecting the CD, XenServer will start the guest again with the new PCI device.</p><p>Xapi works through the queue of events it receives from xenopsd. It is possible that by the time xapi processes the cd-eject event, the guest might have shut down already.</p><p>When the shutdown (not reboot) event is handled, we shall check whether we need to do anything xenprep-related. If</p><ul><li>The VM <code>other_config</code> map has <code>xenprep_progress</code> as either of <code>ISO_inserted</code> or <code>shutdown</code>, and</li><li>The xenprep ISO is no longer in the drive</li></ul><p>then we must (in the specified order)</p><ol><li>Update the VM record:</li><li>In <code>VM.other_config</code> set <code>xenprep_progress=shutdown</code></li><li>If <code>VM.virtual_hardware_platform_version</code> is less than 2, then set it to 2.</li><li>Give it the new PCI device: set <code>VM.auto_update_drivers</code> to <code>true</code>.</li><li>Initiate VM start.</li><li>Remove <code>xenprep_progress</code> from <code>VM.other_config</code></li></ol><p>The most relevant code is probably the <code>update_vm</code> function in <code>ocaml/xapi/xapi_xenops.ml</code> in the <code>xen-api</code> repo (or in some function called from there).</p><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=rrdd>RRDD</h1><p>The <code>xcp-rrdd</code> daemon (hereafter simply called “rrdd”) is a component in the
xapi toolstack that is responsible for collecting metrics, storing them as
&ldquo;Round-Robin Databases&rdquo; (RRDs) and exposing these to clients.</p><p>The code is in ocaml/xcp-rrdd.</p><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of RRDD</h1><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7,0)</span></td></tr></table></header><h1 id=rrdd-archival-redesign>RRDD archival redesign</h1><h2 id=introduction>Introduction</h2><p>Current problems with rrdd:</p><ul><li>rrdd stores knowledge about whether it is running on a master or a slave</li></ul><p>This determines the host to which rrdd will archive a VM&rsquo;s rrd when the VM&rsquo;s
domain disappears - rrdd will always try to archive to the master. However,
when a host joins a pool as a slave rrdd is not restarted so this knowledge is
out of date. When a VM shuts down on the slave rrdd will archive the rrd
locally. When starting this VM again the master xapi will attempt to push any
locally-existing rrd to the host on which the VM is being started, but since
no rrd archive exists on the master the slave rrdd will end up creating a new
rrd and the previous rrd will be lost.</p><ul><li>rrdd handles rebooting VMs unpredictably</li></ul><p>When rebooting a VM, there is a chance rrdd will attempt to update that VM&rsquo;s rrd
during the brief period when there is no domain for that VM. If this happens,
rrdd will archive the VM&rsquo;s rrd to the master, and then create a new rrd for the
VM when it sees the new domain. If rrdd doesn&rsquo;t attempt to update that VM&rsquo;s rrd
during this period, rrdd will continue to add data for the new domain to the old
rrd.</p><h2 id=proposal>Proposal</h2><p>To solve these problems, we will remove some of the intelligence from rrdd and
make it into more of a slave process of xapi. This will entail removing all
knowledge from rrdd of whether it is running on a master or a slave, and also
modifying rrdd to only start monitoring a VM when it is told to, and only
archiving an rrd (to a specified address) when it is told to. This matches the
way xenopsd only manages domains which it has been told to manage.</p><h2 id=design>Design</h2><p>For most VM lifecycle operations, xapi and rrdd processes (sometimes across more
than one host) cooperate to start or stop recording a VM&rsquo;s metrics and/or to
restore or backup the VM&rsquo;s archived metrics. Below we will describe, for each
relevant VM operation, how the VM&rsquo;s rrd is currently handled, and how we propose
it will be handled after the redesign.</p><h4 id=vmdestroy>VM.destroy</h4><p>The master xapi makes a remove_rrd call to the local rrdd, which causes rrdd to
to delete the VM&rsquo;s archived rrd from disk. This behaviour will remain unchanged.</p><h4 id=vmstart_on-and-vmresume_on>VM.start(_on) and VM.resume(_on)</h4><p>The master xapi makes a push_rrd call to the local rrdd, which causes rrdd to
send any locally-archived rrd for the VM in question to the rrdd of the host on
which the VM is starting. This behaviour will remain unchanged.</p><h4 id=vmshutdown-and-vmsuspend>VM.shutdown and VM.suspend</h4><p>Every update cycle rrdd compares its list of registered VMs to the list of
domains actually running on the host. Any registered VMs which do not have a
corresponding domain have their rrds archived to the rrdd running on the host
believed to be the master. We will change this behaviour by stopping rrdd from
doing the archiving itself; instead we will expose a new function in rrdd&rsquo;s
interface:</p><div class="wrap-code highlight"><pre tabindex=0><code>val archive_rrd : vm_uuid:string -&gt; remote_address:string -&gt; unit</code></pre></div><p>This will cause rrdd to remove the specified rrd from its table of registered
VMs, and archive the rrd to the specified host. When a VM has finished shutting
down or suspending, the xapi process on the host on which the VM was running
will call archive_rrd to ask the local rrdd to archive back to the master rrdd.</p><h4 id=vmreboot>VM.reboot</h4><p>Removing rrdd&rsquo;s ability to automatically archive the rrds for disappeared
domains will have the bonus effect of fixing how the rrds of rebooting VMs are
handled, as we don&rsquo;t want the rrds of rebooting VMs to be archived at all.</p><h4 id=vmcheckpoint>VM.checkpoint</h4><p>This will be handled automatically, as internally VM.checkpoint carries out a
VM.suspend followed by a VM.resume.</p><h4 id=vmpool_migrate-and-vmmigrate_send>VM.pool_migrate and VM.migrate_send</h4><p>The source host&rsquo;s xapi makes a migrate_rrd call to the local rrd, with a
destination address and an optional session ID. The session ID is only required
for cross-pool migration. The local rrdd sends the rrd for that VM to the
destination host&rsquo;s rrdd as an HTTP PUT. This behaviour will remain unchanged.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr></table></header><h1 id=rrdd-plugin-protocol-v2>RRDD plugin protocol v2</h1><h2 id=motivation>Motivation</h2><p>rrdd plugins currently report datasources via a shared-memory file, using the
following format:</p><div class="wrap-code highlight"><pre tabindex=0><code>DATASOURCES
000001e4
dba4bf7a84b6d11d565d19ef91f7906e
{
  &#34;timestamp&#34;: 1339685573,
  &#34;data_sources&#34;: {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;64.33&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;62.14&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    }
  }
}</code></pre></div><p>This format contains four main components:</p><ul><li>A constant header string</li></ul><p><code>DATASOURCES</code></p><p>This should always be present.</p><ul><li>The JSON data length, encoded as hexadecimal</li></ul><p><code>000001e4</code></p><ul><li>The md5sum of the JSON data</li></ul><p><code>dba4bf7a84b6d11d565d19ef91f7906e</code></p><ul><li>The JSON data itself, encoding the values and metadata associated with the
reported datasources.</li></ul><div class="wrap-code highlight"><pre tabindex=0><code>{
  &#34;timestamp&#34;: 1339685573,
  &#34;data_sources&#34;: {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;64.33&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;62.14&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    }
  }
}</code></pre></div><p>The disadvantage of this protocol is that rrdd has to parse the entire JSON
structure each tick, even though most of the time only the values will change.</p><p>For this reason a new protocol is proposed.</p><h2 id=protocol-v2>Protocol V2</h2><table><thead><tr><th>value</th><th>bits</th><th>format</th><th>notes</th></tr></thead><tbody><tr><td>header string</td><td>(string length)*8</td><td>string</td><td>&ldquo;Datasources&rdquo; as in the V1 protocol</td></tr><tr><td>data checksum</td><td>32</td><td>int32</td><td>binary-encoded crc32 of the concatenation of the encoded timestamp and datasource values</td></tr><tr><td>metadata checksum</td><td>32</td><td>int32</td><td>binary-encoded crc32 of the metadata string (see below)</td></tr><tr><td>number of datasources</td><td>32</td><td>int32</td><td>only needed if the metadata has changed - otherwise RRDD can use a cached value</td></tr><tr><td>timestamp</td><td>64</td><td>int64</td><td>Unix epoch</td></tr><tr><td>datasource values</td><td>n * 64</td><td>int64</td><td>n is the number of datasources exported by the plugin</td></tr><tr><td>metadata length</td><td>32</td><td>int32</td><td></td></tr><tr><td>metadata</td><td>(string length)*8</td><td>string</td><td></td></tr></tbody></table><p>All integers are bigendian. The metadata will have the same JSON-based format as
in the V1 protocol, minus the timestamp and <code>value</code> key-value pair for each
datasource, for example:</p><div class="wrap-code highlight"><pre tabindex=0><code>{
  &#34;datasources&#34;: {
    &#34;memory_reclaimed&#34;: {
      &#34;description&#34;:&#34;Host memory reclaimed by squeezed&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;:&#34;int64&#34;,
      &#34;type&#34;:&#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;:&#34;B&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    },
    &#34;memory_reclaimed_max&#34;: {
      &#34;description&#34;:&#34;Host memory that could be reclaimed by squeezed&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;:&#34;int64&#34;,
      &#34;type&#34;:&#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;:&#34;B&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    }
  }
}</code></pre></div><p>The above formatting is not required, but added here for readability.</p><h2 id=reading-algorithm>Reading algorithm</h2><div class="wrap-code highlight"><pre tabindex=0><code>if header != expected_header:
    raise InvalidHeader()
if data_checksum == last_data_checksum:
    raise NoUpdate()
if data_checksum != md5sum(encoded_timestamp_and_values):
    raise InvalidChecksum()
if metadata_checksum == last_metadata_checksum:
    for datasource, value in cached_datasources, values:
        update(datasource, value)
else:
    if metadata_checksum != md5sum(metadata):
        raise InvalidChecksum()
    cached_datasources = create_datasources(metadata)
    for datasource, value in cached_datasources, values:
        update(datasource, value)</code></pre></div><p>This means that for a normal update, RRDD will only have to read the header plus
the first (16 + 16 + 4 + 8 + 8*n) bytes of data, where n is the number of
datasources exported by the plugin. If the metadata changes RRDD will have to
read all the data (and parse the metadata).</p><p>n.b. the timestamp reported by plugins is not currently used by RRDD - it uses
its own global timestamp.</p><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v11</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/139>#139</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Added details about the VDI's binary format and size, and the SR capability name.</td></tr><tr><td><span class="label label-default">v3</span></td><td>Tar was not needed after all!</td></tr><tr><td><span class="label label-default">v4</span></td><td>Add details about discovering the VDI using a new vdi_type.</td></tr><tr><td><span class="label label-default">v5</span></td><td>Add details about the http handlers and interaction with xapi's database</td></tr><tr><td><span class="label label-default">v6</span></td><td>Add details about the framing of the data within the VDI</td></tr><tr><td><span class="label label-default">v7</span></td><td>Redesign semantics of the rrd_updates handler</td></tr><tr><td><span class="label label-default">v8</span></td><td>Redesign semantics of the rrd_updates handler (again)</td></tr><tr><td><span class="label label-default">v9</span></td><td>Magic number change in framing format of vdi</td></tr><tr><td><span class="label label-default">v10</span></td><td>Add details of new APIs added to xapi and xcp-rrdd</td></tr><tr><td><span class="label label-default">v11</span></td><td>Remove unneeded API calls</td></tr></table></header><h1 id=sr-level-rrds>SR-Level RRDs</h1><h2 id=introduction>Introduction</h2><p>Xapi has RRDs to track VM- and host-level metrics. There is a desire to have SR-level RRDs as a new category, because SR stats are not specific to a certain VM or host. Examples are size and free space on the SR. While recording SR metrics is relatively straightforward within the current RRD system, the main question is where to archive them, which is what this design aims to address.</p><h2 id=stats-collection>Stats Collection</h2><p>All SR types, including the existing ones, should be able to have RRDs defined for them. Some RRDs, such as a &ldquo;free space&rdquo; one, may make sense for multiple (if not all) SR types. However, the way to measure something like free space will be SR specific. Furthermore, it should be possible for each type of SR to have its own specialised RRDs.</p><p>It follows that each SR will need its own <code>xcp-rrdd</code> plugin, which runs on the SR master and defines and collects the stats. For the new thin-lvhd SR this could be <code>xenvmd</code> itself. The plugin registers itself with <code>xcp-rrdd</code>, so that the latter records the live stats from the plugin into RRDs.</p><h2 id=archiving>Archiving</h2><p>SR-level RRDs will be archived in the SR itself, in a VDI, rather than in the local filesystem of the SR master. This way, we don&rsquo;t need to worry about master failover.</p><p>The VDI will be 4MB in size. This is a little more space than we would need for the RRDs we have in mind at the moment, but will give us enough headroom for the foreseeable future. It will not have a filesystem on it for simplicity and performance. There will only be one RRD archive file for each SR (possibly containing data for multiple metrics), which is gzipped by <code>xcp-rrdd</code>, and can be copied onto the VDI.</p><p>There will be a simple framing format for the data on the VDI. This will be as follows:</p><table><thead><tr><th>Offset</th><th>Type</th><th>Name</th><th>Comment</th></tr></thead><tbody><tr><td>0</td><td>32 bit network-order int</td><td>magic</td><td>Magic number = 0x7ada7ada</td></tr><tr><td>4</td><td>32 bit network-order int</td><td>version</td><td>1</td></tr><tr><td>8</td><td>32 bit network-order int</td><td>length</td><td>length of payload</td></tr><tr><td>12</td><td>gzipped data</td><td>data</td><td></td></tr></tbody></table><p>Xapi will be in charge of the lifecycle of this VDI, not the plugin or <code>xcp-rrdd</code>, which will make it a little easier to manage them. Only xapi will attach/detach and read from/write to this VDI. We will keep <code>xcp-rrdd</code> as simple as possible, and have it archive to its standard path in the local file system. Xapi will then copy the RRDs in and out of the VDI.</p><p>A new value <code>"rrd"</code> in the <code>vdi_type</code> enum of the datamodel will be defined, and the <code>VDI.type</code> of the VDI will be set to that value. The storage backend will write the VDI type to the LVM metadata of the VDI, so that xapi can discover the VDI containing the SR-level RRDs when attaching an SR to a new pool. This means that SR-level RRDs are currently restricted to LVM SRs.</p><p>Because we will not write plugins for all SRs at once, and therefore do not need xapi to set up the VDI for all SRs, we will add an SR &ldquo;capability&rdquo; for the backends to be able to tell xapi whether it has the ability to record stats and will need storage for them. The capability name will be: <code>SR_STATS</code>.</p><h2 id=management-of-the-sr-stats-vdi>Management of the SR-stats VDI</h2><p>The SR-stats VDI will be attached/detached on <code>PBD.plug</code>/<code>unplug</code> on the SR master.</p><ul><li><p>On <code>PBD.plug</code> on the SR master, if the SR has the stats capability, xapi:</p><ul><li>Creates a stats VDI if not already there (search for an existing one based on the VDI type).</li><li>Attaches the stats VDI if it did already exist, and copies the RRDs to the local file system (standard location in the filesystem; asks <code>xcp-rrdd</code> where to put them).</li><li>Informs <code>xcp-rrdd</code> about the RRDs so that it will load the RRDs and add newly recorded data to them (needs a function like <code>push_rrd_local</code> for VM-level RRDs).</li><li>Detaches stats VDI.</li></ul></li><li><p>On <code>PBD.unplug</code> on the SR master, if the SR has the stats capability xapi:</p><ul><li>Tells <code>xcp-rrdd</code> to archive the RRDs for the SR, which it will do to the local filesystem.</li><li>Attaches the stats VDI, copies the RRDs into it, detaches VDI.</li></ul></li></ul><h2 id=periodic-archiving>Periodic Archiving</h2><p>Xapi&rsquo;s periodic scheduler regularly triggers <code>xcp-rrdd</code> to archive the host and VM RRDs. It will need to do this for the SR ones as well. Furthermore, xapi will need to attach the stats VDI and copy the RRD archives into it (as on <code>PBD.unplug</code>).</p><h2 id=exporting>Exporting</h2><p>There will be a new handler for downloading an SR RRD:</p><pre><code>http://&lt;server&gt;/sr_rrd?session_id=&lt;SESSION HANDLE&gt;&amp;uuid=&lt;SR UUID&gt;
</code></pre><p>RRD updates are handled via a single handler for the host, VM and SR UUIDs
RRD updates for the host, VMs and SRs are handled by a a single handler at
<code>/rrd_updates</code>. Exactly what is returned will be determined by the parameters
passed to this handler.</p><p>Whether the host RRD updates are returned is governed by the presence of
<code>host=true</code> in the parameters. <code>host=&lt;anything else></code> or the absence of the
<code>host</code> key will mean the host RRD is not returned.</p><p>Whether the VM RRD updates are returned is governed by the <code>vm_uuid</code> key in the
URL parameters. <code>vm_uuid=all</code> will return RRD updates for all VM RRDs.
<code>vm_uuid=xxx</code> will return the RRD updates for the VM with uuid <code>xxx</code> only.
If <code>vm_uuid</code> is <code>none</code> (or any other string which is not a valid VM UUID) then
the handler will return no VM RRD updates. If the <code>vm_uuid</code> key is absent, RRD
updates for all VMs will be returned.</p><p>Whether the SR RRD updates are returned is governed by the <code>sr_uuid</code> key in the
URL parameters. <code>sr_uuid=all</code> will return RRD updates for all SR RRDs.
<code>sr_uuid=xxx</code> will return the RRD updates for the SR with uuid <code>xxx</code> only.
If <code>sr_uuid</code> is <code>none</code> (or any other string which is not a valid SR UUID) then
the handler will return no SR RRD updates. If the <code>sr_uuid</code> key is absent, no
SR RRD updates will be returned.</p><p>It will be possible to mix and match these parameters; for example to return
RRD updates for the host and all VMs, the URL to use would be:</p><pre><code>http://&lt;server&gt;/rrd_updates?session_id=&lt;SESSION HANDLE&gt;&amp;start=10258122541&amp;host=true&amp;vm_uuid=all&amp;sr_uuid=none
</code></pre><p>Or, to return RRD updates for all SRs but nothing else, the URL to use would be:</p><pre><code>http://&lt;server&gt;/rrd_updates?session_id=&lt;SESSION HANDLE&gt;&amp;start=10258122541&amp;host=false&amp;vm_uuid=none&amp;sr_uuid=all
</code></pre><p>While behaviour is defined if any of the keys <code>host</code>, <code>vm_uuid</code> and <code>sr_uuid</code> is
missing, this is for backwards compatibility and it is recommended that clients
specify each parameter explicitly.</p><h2 id=database-updating>Database updating.</h2><p>If the SR is presenting a data source called &lsquo;physical_utilisation&rsquo;,
xapi will record this periodically in its database. In order to do
this, xapi will fork a thread that, every n minutes (2 suggested, but
open to suggestions here), will query the attached SRs, then query
RRDD for the latest data source for these, and update the database.</p><p>The utilisation of VDIs will <em>not</em> be updated in this way until
scalability worries for RRDs are addressed.</p><p>Xapi will cache whether it is SR master for every attached SR and only
attempt to update if it is the SR master.</p><h2 id=new-apis>New APIs.</h2><h4 id=xcp-rrdd>xcp-rrdd:</h4><ul><li><p>Get the filesystem location where sr rrds are archived: <code>val sr_rrds_path : uid:string -> string</code></p></li><li><p>Archive the sr rrds to the filesystem: <code>val archive_sr_rrd : sr_uuid:string -> unit</code></p></li><li><p>Load the sr rrds from the filesystem: <code>val push_sr_rrd : sr_uuid:string -> unit</code></p></li></ul><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=xenapi>XenAPI</h1><ul class="children children-li children-sort-"><li><a href=/new-docs/xen-api/basics/index.html>XenAPI Basics</a></li><li><a href=/new-docs/xen-api/wire-protocol/index.html>Wire Protocol</a></li><li><a href=/new-docs/xen-api/overview/index.html>Overview of the XenAPI</a></li><li><a href=/new-docs/xen-api/evolution/index.html>API evolution</a></li><li><a href=/new-docs/xen-api/usage/index.html>Using the API</a></li><li><a href=/new-docs/xen-api/classes/index.html>XenAPI Reference</a></li><li><a href=/new-docs/xen-api/releases/index.html>XenAPI Releases</a></li><li><a href=/new-docs/xen-api/topics/index.html>Topics</a></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of XenAPI</h1><article class=default><header class=headline></header><h1 id=xenapi-basics>XenAPI Basics</h1><p>This document contains a description of the Xen Management API – an interface for
remotely configuring and controlling virtualised guests running on a
Xen-enabled host.</p><p>The XenAPI is presented here as a set of Remote Procedure Calls, with a wire
format based upon <a href=http://xmlrpc.scripting.com target=_blank>XML-RPC</a>.
No specific language bindings are prescribed,
although examples will be given in the python programming language.</p><p>Although we adopt some terminology from object-oriented programming,
future client language bindings may or may not be object oriented.
The API reference uses the terminology <em>classes</em> and <em>objects</em>.
For our purposes a <em>class</em> is simply a hierarchical namespace;
an <em>object</em> is an instance of a class with its fields set to
specific values. Objects are persistent and exist on the server-side.
Clients may obtain opaque references to these server-side objects and then
access their fields via get/set RPCs.</p><p>For each class we specify a list of fields along with their <em>types</em> and <em>qualifiers</em>. A
qualifier is one of:</p><ul><li><em>RO/runtime</em>: the field is Read
Only. Furthermore, its value is automatically computed at runtime.
For example: current CPU load and disk IO throughput.</li><li><em>RO/constructor</em>: the field must be manually set
when a new object is created, but is then Read Only for
the duration of the object&rsquo;s life.
For example, the maximum memory addressable by a guest is set
before the guest boots.</li><li><em>RW</em>: the field is Read/Write. For example, the name of a VM.</li></ul><h2 id=types>Types</h2><p>The following types are used to specify methods and fields in the API Reference:</p><ul><li><code>string</code>: Text strings.</li><li><code>int</code>: 64-bit integers.</li><li><code>float</code>: IEEE double-precision floating-point numbers.</li><li><code>bool</code>: Boolean.</li><li><code>datetime</code>: Date and timestamp.</li><li><code>c ref</code>: Reference to an object of class <code>c</code>.</li><li><code>t set</code>: Arbitrary-length set of values of type <code>t</code>.</li><li><code>(k → v) map</code>: Mapping from values of type <code>k</code> to values of type <code>v</code>.</li><li><code>e enum</code>: Enumeration type with name <code>e</code>. Enums are defined in the API Reference together with classes that use them.</li></ul><p>Note that there are a number of cases where <code>ref</code>s are <em>doubly
linked</em> – e.g. a VM has a field called <code>VIFs</code> of type
<code>VIF ref set</code>; this field lists
the network interfaces attached to a particular VM. Similarly, the VIF
class has a field called <code>VM</code> of type <code>VM ref</code> which references the VM to which the interface is connected.
These two fields are <em>bound together</em>, in the sense that
creating a new VIF causes the <code>VIFs</code> field of the corresponding
VM object to be updated automatically.</p><p>The API reference explicitly lists the fields that are
bound together in this way. It also contains a diagram that shows
relationships between classes. In this diagram an edge signifies the
existance of a pair of fields that are bound together, using standard
crows-foot notation to signify the type of relationship (e.g.
one-many, many-many).</p><h2 id=rpcs-associated-with-fields>RPCs associated with fields</h2><p>Each field, <code>f</code>, has an RPC accessor associated with it
that returns <code>f</code>&rsquo;s value:</p><ul><li><code>get_f (r)</code>: Takes a <code>ref</code>, <code>r</code>, that refers to an object and returns the value of <code>f</code>.</li></ul><p>Each field, <code>f</code>, with attribute <code>RW</code> and whose outermost type is <code>set</code> has the following
additional RPCs associated with it:</p><ul><li><p><code>add_f (r, v)</code>: Adds a new element <code>v</code> to the set. Since sets cannot contain duplicate values this operation has no action in the case
that <code>v</code> was already in the set.</p></li><li><p><code>remove_f (r, v)</code>: Removes element <code>v</code> from the set.</p></li></ul><p>Each field, <code>f</code>, with attribute RW and whose outermost type is <code>map</code> has the following
additional RPCs associated with it:</p><ul><li><code>add_to_f (r, k, v)</code>: Adds new pair <code>(k → v)</code> to the mapping stored in <code>f</code> in object <code>r</code>. Attempting to add a new pair for duplicate
key, <code>k</code>, fails with an <code>MAP_DUPLICATE_KEY</code> error.</li><li><code>remove_from_f (r, k)</code>: Removes the pair with key <code>k</code> from the mapping stored in <code>f</code> in object <code>r</code>.</li></ul><p>Each field whose outermost type is neither <code>set</code> nor <code>map</code>,
but whose attribute is RW has an RPC accessor associated with it
that sets its value:</p><ul><li><code>set_f (r, v)</code>: Sets field <code>f</code> on object <code>r</code> to value <code>v</code>.</li></ul><h2 id=rpcs-associated-with-classes>RPCs associated with classes</h2><ul><li>Most classes have a <em>constructor</em> RPC named <code>create</code> that
takes as parameters all fields marked RW and
RO/constructor. The result of this RPC is that a new <em>persistent</em> object is
created on the server-side with the specified field values.</li><li>Each class has a <code>get_by_uuid (uuid)</code> RPC that returns the object
of that class that has the specified UUID.</li><li>Each class that has a <code>name_label</code> field has a <code>get_by_name_label (name_label)</code> RPC that returns a set of objects of that
class that have the specified <code>name_label</code>.</li><li>Most classes have a <code>destroy (r)</code> RPC that explicitly deletes the persistent object specified by <code>r</code> from the system. This is a
non-cascading delete – if the object being removed is referenced by another
object then the <code>destroy</code> call will fail.</li></ul><h2 id=additional-rpcs>Additional RPCs</h2><p>As well as the RPCs enumerated above, most classes have additional RPCs
associated with them. For example, the VM class has RPCs for cloning,
suspending, starting etc. Such additional RPCs are described explicitly
in the API reference.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=wire-protocol>Wire Protocol</h1><p>API calls are sent over a network to a Xen-enabled host using the
<a href=http://xmlrpc.scripting.com/spec.html target=_blank>XML-RPC</a> protocol. On this page, we describe how the higher-level types
used in our API Reference are mapped to primitive XML-RPC types.</p><p>We specify the signatures of API functions in the following style:</p><pre><code>(VM ref set)  VM.get_all ()
</code></pre><p>This specifies that the function with name <code>VM.get_all</code>
takes no parameters and returns a <code>set</code> of <code>VM ref</code>s. These
types are mapped onto XML-RPC types in a straight-forward manner:</p><ul><li><p><code>float</code>s, <code>bool</code>s, <code>datetime</code>s and <code>string</code>s map directly to the XML-RPC
<code>&lt;double></code>, <code>&lt;boolean></code>, <code>&lt;dateTime.iso8601></code>, and <code>&lt;string></code> elements.</p></li><li><p>all <code>ref</code> types are opaque references, encoded as the
XML-RPC’s <code>&lt;string></code> type. Users of the API should not make
assumptions about the concrete form of these strings and should not
expect them to remain valid after the client’s session with the
server has terminated.</p></li><li><p>fields named <code>uuid</code> of type <code>string</code> are
mapped to the XML-RPC <code>&lt;string></code> type. The string itself is
the OSF DCE UUID presentation format (as output by
<code>uuidgen</code>, etc).</p></li><li><p><code>int</code>s are all assumed to be 64-bit in our API and are encoded as a
string of decimal digits (rather than using XML-RPC’s built-in
32-bit <code>&lt;i4></code> type).</p></li><li><p>values of <code>enum</code> types are encoded as strings. For example, a value of
<code>destroy</code> of type <code>enum on_normal_exit</code>, would be
conveyed as:</p><pre><code>&lt;value&gt;&lt;string&gt;destroy&lt;/string&gt;&lt;/value&gt;
</code></pre></li><li><p>for all our types, <code>t</code>, our type <code>t set</code>
simply maps to XML-RPC’s <code>&lt;array></code> type, so for example a
value of type <code>string set</code> would be transmitted like
this:</p><pre><code>&lt;value&gt;
  &lt;array&gt;
    &lt;data&gt;
      &lt;value&gt;&lt;string&gt;CX8&lt;/string&gt;&lt;/value&gt;
      &lt;value&gt;&lt;string&gt;PSE36&lt;/string&gt;&lt;/value&gt;
      &lt;value&gt;&lt;string&gt;FPU&lt;/string&gt;&lt;/value&gt;
    &lt;/data&gt;
  &lt;/array&gt; 
&lt;/value&gt;
</code></pre></li><li><p>for types <code>k</code> and <code>v</code>, our type <code>(k → v) map</code> maps onto an XML-RPC <code>&lt;struct></code>, with the key as the name of
the struct. Note that the <code>(k → v) map</code> type is only valid
when <code>k</code> is a <code>string</code>, <code>ref</code>, or <code>int</code>, and in each case the keys of the maps are
stringified as above. For example, the <code>(string → double) map</code> containing a the mappings <code>"Mike" → 2.3</code> and
<code>"John" → 1.2</code> would be represented as:</p><pre><code>&lt;value&gt;
  &lt;struct&gt;
    &lt;member&gt;
      &lt;name&gt;Mike&lt;/name&gt;
      &lt;value&gt;&lt;double&gt;2.3&lt;/double&gt;&lt;/value&gt;
    &lt;/member&gt;
    &lt;member&gt;
      &lt;name&gt;John&lt;/name&gt;
      &lt;value&gt;&lt;double&gt;1.2&lt;/double&gt;&lt;/value&gt;
    &lt;/member&gt;
  &lt;/struct&gt;
&lt;/value&gt;
</code></pre></li><li><p>our <code>void</code> type is transmitted as an empty string.</p></li></ul><h2 id=note-on-references-vs-uuids>Note on References vs UUIDs</h2><p>References are opaque types — encoded as XML-RPC strings on the wire —
understood only by the particular server which generated them. Servers
are free to choose any concrete representation they find convenient;
clients should not make any assumptions or attempt to parse the string
contents. References are not guaranteed to be permanent identifiers for
objects; clients should not assume that references generated during one
session are valid for any future session. References do not allow
objects to be compared for equality. Two references to the same object
are not guaranteed to be textually identical.</p><p>UUIDs are intended to be permanent names for objects. They are
guaranteed to be in the OSF DCE UUID presentation format (as output by
<code>uuidgen</code>. Clients may store UUIDs on disk and use them to
lookup objects in subsequent sessions with the server. Clients may also
test equality on objects by comparing UUID strings.</p><p>The API provides mechanisms for translating between UUIDs and opaque
references. Each class that contains a UUID field provides:</p><ul><li><p>A <code>get_by_uuid</code> method that takes a UUID, and
returns an opaque reference to the server-side object that has that
UUID;</p></li><li><p>A <code>get_uuid</code> function (a regular “field getter” RPC)
that takes an opaque reference and returns the UUID of the
server-side object that is referenced by it.</p></li></ul><h2 id=return-valuesstatus-codes>Return Values/Status Codes</h2><p>The return value of an RPC call is an XML-RPC <code>&lt;struct></code>.</p><ul><li>The first element of the struct is named <code>"Status"</code>; it
contains a string value indicating whether the result of the call
was a <code>"Success"</code> or a <code>"Failure"</code>.</li></ul><p>If <code>"Status"</code> was set to <code>"Success"</code> then the Struct
contains a second element named <code>"Value"</code>:</p><ul><li>The element of the struct named <code>"Value"</code> contains the
function’s return value.</li></ul><p>In the case where <code>"Status"</code> is set to <code>"Failure"</code>
then the struct contains a second element named
<code>"ErrorDescription"</code>:</p><ul><li>The element of the struct named <code>"ErrorDescription"</code>
contains an array of string values. The first element of the array
is an error code; the remainder of the array are strings
representing error parameters relating to that code.</li></ul><p>For example, an XML-RPC return value from the
<code>host.get_resident_VMs</code> function above may look like this:</p><pre><code>&lt;struct&gt;
   &lt;member&gt;
     &lt;name&gt;Status&lt;/name&gt;
     &lt;value&gt;Success&lt;/value&gt;
   &lt;/member&gt;
   &lt;member&gt;
      &lt;name&gt;Value&lt;/name&gt;
      &lt;value&gt;
        &lt;array&gt;
           &lt;data&gt;
             &lt;value&gt;81547a35-205c-a551-c577-00b982c5fe00&lt;/value&gt;
             &lt;value&gt;61c85a22-05da-b8a2-2e55-06b0847da503&lt;/value&gt;
             &lt;value&gt;1d401ec4-3c17-35a6-fc79-cee6bd9811fe&lt;/value&gt;
           &lt;/data&gt;
        &lt;/array&gt;
     &lt;/value&gt;
   &lt;/member&gt;
&lt;/struct&gt;
</code></pre><h1 id=making-xml-rpc-calls>Making XML-RPC Calls</h1><h2 id=transport-layer>Transport Layer</h2><p>The following transport layers are currently supported:</p><ul><li><p>HTTPS for remote administration</p></li><li><p>HTTP over Unix domain sockets for local administration</p></li></ul><h2 id=session-layer>Session Layer</h2><p>The XML-RPC interface is session-based; before you can make arbitrary
RPC calls you must login and initiate a session. For example:</p><pre><code>(session ref)  session.login_with_password(string  uname, string  pwd, string  version, string  originator)
</code></pre><p>Where <code>uname</code> and <code>password</code> refer to your
username and password respectively, as defined by the Xen administrator.
The <code>session ref</code> returned by <code>session.login_with_password</code> is passed to subequent RPC
calls as an authentication token.</p><p>A session can be terminated with the <code>session.logout</code> function:</p><pre><code>(void)  session.logout (session ref)
</code></pre><h2 id=synchronous-and-asynchronous-invocation>Synchronous and Asynchronous invocation</h2><p>Each method call (apart from methods on <code>session</code> and <code>task</code> objects and
“getters” and “setters” derived from fields) can be made either
synchronously or asynchronously. A synchronous RPC call blocks until the
return value is received; the return value of a synchronous RPC call is
exactly as specified above.</p><p>Only synchronous API calls are listed explicitly in this document. All
asynchronous versions are in the special <code>Async</code> namespace.
For example, synchronous call <code>VM.clone (...)</code> has an asynchronous counterpart,
<code>Async.VM.clone (...)</code>, that is non-blocking.</p><p>Instead of returning its result directly, an asynchronous RPC call
returns a task ID (of type <code>task ref</code>); this identifier is subsequently used to
track the status of a running asynchronous RPC. Note that an asychronous
call may fail immediately, before a task has even been
created. To represent this eventuality, the returned <code>task ref</code>
is wrapped in an XML-RPC struct with a <code>Status</code>,
<code>ErrorDescription</code> and <code>Value</code> fields, exactly as
specified above.</p><p>The <code>task ref</code> is provided in the <code>Value</code> field if
<code>Status</code> is set to <code>Success</code>.</p><p>The RPC call</p><pre><code>(task ref set)  task.get_all (session ref)
</code></pre><p>returns a set of all task IDs known to the system. The status (including
any returned result and error codes) of these tasks can then be queried
by accessing the fields of the Task object in the usual way. Note that,
in order to get a consistent snapshot of a task’s state, it is advisable
to call the <code>get_record</code> function.</p><h1 id=example-interactive-session>Example interactive session</h1><p>This section describes how an interactive session might look, using the
python XML-RPC client library.</p><p>First, initialise python and import the library <code>xmlrpc.client</code>:</p><pre><code>$ python
...
&gt;&gt;&gt; import xmlrpc.client
</code></pre><p>Create a python object referencing the remote server:</p><pre><code>&gt;&gt;&gt; xen = xmlrpc.client.Server(&quot;https://localhost:443&quot;)
</code></pre><p>Acquire a session reference by logging in with a username and password
(error-handling ommitted for brevity; the session reference is returned
under the key <code>'Value'</code> in the resulting dictionary)</p><pre><code>&gt;&gt;&gt; session = xen.session.login_with_password(&quot;user&quot;, &quot;passwd&quot;)['Value']
</code></pre><p>When serialised, this call looks like the following:</p><pre><code>&lt;?xml version='1.0'?&gt;
&lt;methodCall&gt;
  &lt;methodName&gt;session.login_with_password&lt;/methodName&gt;
  &lt;params&gt;
    &lt;param&gt;
      &lt;value&gt;&lt;string&gt;user&lt;/string&gt;&lt;/value&gt;
    &lt;/param&gt;
    &lt;param&gt;
      &lt;value&gt;&lt;string&gt;passwd&lt;/string&gt;&lt;/value&gt;
    &lt;/param&gt;
  &lt;/params&gt;
&lt;/methodCall&gt;
</code></pre><p>Next, the user may acquire a list of all the VMs known to the system:
(Note the call takes the session reference as the only parameter)</p><pre><code>&gt;&gt;&gt; all_vms = xen.VM.get_all(session)['Value']
&gt;&gt;&gt; all_vms
['OpaqueRef:1', 'OpaqueRef:2', 'OpaqueRef:3', 'OpaqueRef:4']
</code></pre><p>The VM references here have the form <code>OpaqueRef:X</code>, though
they may not be that simple in the future, and you should treat them as
opaque strings. <em>Templates</em> are VMs with the
<code>is_a_template</code> field set to <code>true</code>. We can find the subset
of template VMs using a command like the following:</p><pre><code>&gt;&gt;&gt; all_templates = filter(lambda x: xen.VM.get_is_a_template(session, x)['Value'], all_vms)
</code></pre><p>Once a reference to a VM has been acquired a lifecycle operation may be
invoked:</p><pre><code>&gt;&gt;&gt; xen.VM.start(session, all_templates[0], False, False)
{'Status': 'Failure', 'ErrorDescription': ['VM_IS_TEMPLATE', 'OpaqueRef:X']}
</code></pre><p>In this case the <code>start</code> message has been rejected, because
the VM is a template, and so an error response has been returned. These
high-level errors are returned as structured data (rather than as
XML-RPC faults), allowing them to be internationalised.</p><p>Rather than querying fields individually, whole <em>records</em>
may be returned at once. To retrieve the record of a single object as a
python dictionary:</p><pre><code>&gt;&gt;&gt; record = xen.VM.get_record(session, all_templates[0])['Value']
&gt;&gt;&gt; record['power_state']
'Halted'
&gt;&gt;&gt; record['name_label']
'XenSource P2V Server'
</code></pre><p>To retrieve all the VM records in a single call:</p><pre><code>&gt;&gt;&gt; records = xen.VM.get_all_records(session)['Value']
&gt;&gt;&gt; records.keys()
['OpaqueRef:1', 'OpaqueRef:2', 'OpaqueRef:3', 'OpaqueRef:4' ]
&gt;&gt;&gt; records['OpaqueRef:1']['name_label']
'RHEL 4.1 Autoinstall Template'
</code></pre><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=overview-of-the-xenapi>Overview of the XenAPI</h1><p>This chapter introduces the XenAPI and its associated object model. The API has the following key features:</p><ul><li><p><em>Management of all aspects of the XenServer Host</em>.
The API allows you to manage VMs, storage, networking, host configuration and pools. Performance and status metrics can also be queried from the API.</p></li><li><p><em>Persistent Object Model</em>.
The results of all side-effecting operations (e.g. object creation, deletion and parameter modifications) are persisted in a server-side database that is managed by the XenServer installation.</p></li><li><p><em>An event mechanism</em>.
Through the API, clients can register to be notified when persistent (server-side) objects are modified. This enables applications to keep track of datamodel modifications performed by concurrently executing clients.</p></li><li><p><em>Synchronous and asynchronous invocation</em>.
All API calls can be invoked synchronously (that is, block until completion); any API call that may be long-running can also be invoked <em>asynchronously</em>. Asynchronous calls return immediately with a reference to a <em>task</em> object. This task object can be queried (through the API) for progress and status information. When an asynchronously invoked operation completes, the result (or error code) is available from the task object.</p></li><li><p><em>Remotable and Cross-Platform</em>.
The client issuing the API calls does not have to be resident on the host being managed; nor does it have to be connected to the host over ssh in order to execute the API. API calls make use of the XML-RPC protocol to transmit requests and responses over the network.</p></li><li><p><em>Secure and Authenticated Access</em>.
The XML-RPC API server executing on the host accepts secure socket connections. This allows a client to execute the APIs over the https protocol. Further, all the API calls execute in the context of a login session generated through username and password validation at the server. This provides secure and authenticated access to the XenServer installation.</p></li></ul><h2 id=getting-started-with-the-api>Getting Started with the API</h2><p>We will start our tour of the API by describing the calls required to create a new VM on a XenServer installation, and take it through a start/suspend/resume/stop cycle. This is done without reference to code in any specific language; at this stage we just describe the informal sequence of RPC invocations that accomplish our &ldquo;install and start&rdquo; task.</p><h3 id=authentication-acquiring-a-session-reference>Authentication: acquiring a session reference</h3><p>The first step is to call <code>Session.login_with_password(, , , )</code>. The API is session based, so before you can make other calls you will need to authenticate with the server. Assuming the username and password are authenticated correctly, the result of this call is a <em>session reference</em>. Subsequent API calls take the session reference as a parameter. In this way we ensure that only API users who are suitably authorized can perform operations on a XenServer installation. You can continue to use the same session for any number of API calls. When you have finished the session, Citrix recommends that you call <code>Session.logout(session)</code> to clean up: see later.</p><h3 id=acquiring-a-list-of-templates-to-base-a-new-vm-installation-on>Acquiring a list of templates to base a new VM installation on</h3><p>The next step is to query the list of &ldquo;templates&rdquo; on the host. Templates are specially-marked VM objects that specify suitable default parameters for a variety of supported guest types. (If you want to see a quick enumeration of the templates on a XenServer installation for yourself then you can execute the <code>xe template-list</code> CLI command.) To get a list of templates from the API, we need to find the VM objects on the server that have their <code>is_a_template</code> field set to true. One way to do this by calling <code>VM.get_all_records(session)</code> where the session parameter is the reference we acquired from our <code>Session.login_with_password</code> call earlier. This call queries the server, returning a snapshot (taken at the time of the call) containing all the VM object references and their field values.</p><p>(Remember that at this stage we are not concerned about the particular mechanisms by which the returned object references and field values can be manipulated in any particular client language: that detail is dealt with by our language-specific API bindings and described concretely in the following chapter. For now it suffices just to assume the existence of an abstract mechanism for reading and manipulating objects and field values returned by API calls.)</p><p>Now that we have a snapshot of all the VM objects&rsquo; field values in the memory of our client application we can simply iterate through them and find the ones that have their &ldquo;<code>is_a_template</code>&rdquo; set to true. At this stage let&rsquo;s assume that our example application further iterates through the template objects and remembers the reference corresponding to the one that has its &ldquo;<code>name_label</code>&rdquo; set to &ldquo;Debian Etch 4.0&rdquo; (one of the default Linux templates supplied with XenServer).</p><h3 id=installing-the-vm-based-on-a-template>Installing the VM based on a template</h3><p>Continuing through our example, we must now install a new VM based on the template we selected. The installation process requires 4 API calls:</p><ul><li><p>First we must now invoke the API call <code>VM.clone(session, t_ref, "my first VM")</code>. This tells the server to clone the VM object referenced by <code>t_ref</code> in order to make a new VM object. The return value of this call is the VM reference corresponding to the newly-created VM. Let&rsquo;s call this <code>new_vm_ref</code>.</p></li><li><p>Next, we need to specify the UUID of the Storage Repository where the VM&rsquo;s
disks will be instantiated. We have to put this in the <code>sr</code> attribute in
the disk provisioning XML stored under the &ldquo;<code>disks</code>&rdquo; key in the
<code>other_config</code> map of the newly-created VM. This field can be updated by
calling its getter (<code>other_config &lt;- VM.get_other_config(session, new_vm_ref)</code>) and then its setter (<code>VM.set_other_config(session, new_vm_ref, other_config)</code>) with the modified <code>other_config</code> map.</p></li><li><p>At this stage the object referred to by <code>new_vm_ref</code> is still a template (just like the VM object referred to by <code>t_ref</code>, from which it was cloned). To make <code>new_vm_ref</code> into a VM object we need to call <code>VM.provision(session, new_vm_ref)</code>. When this call returns the <code>new_vm_ref</code> object will have had its <code>is_a_template</code> field set to false, indicating that <code>new_vm_ref</code> now refers to a regular VM ready for starting.</p></li></ul><blockquote><p><strong>Note</strong></p><p>The provision operation may take a few minutes, as it is as during this call that the template&rsquo;s disk images are created. In the case of the Debian template, the newly created disks are also at this stage populated with a Debian root filesystem.</p></blockquote><h3 id=taking-the-vm-through-a-startsuspendresumestop-cycle>Taking the VM through a start/suspend/resume/stop cycle</h3><p>Now we have an object reference representing our newly-installed VM, it is trivial to take it through a few lifecycle operations:</p><ul><li><p>To start our VM we can just call <code>VM.start(session, new_vm_ref)</code></p></li><li><p>After it&rsquo;s running, we can suspend it by calling <code>VM.suspend(session, new_vm_ref)</code>,</p></li><li><p>and then resume it by calling <code>VM.resume(session, new_vm_ref)</code>.</p></li><li><p>We can call <code>VM.shutdown(session, new_vm_ref)</code> to shutdown the VM cleanly.</p></li></ul><h3 id=logging-out>Logging out</h3><p>Once an application is finished interacting with a XenServer Host it is good practice to call <code>Session.logout(session)</code>. This invalidates the session reference (so it cannot be used in subsequent API calls) and simultaneously deallocates server-side memory used to store the session object.</p><p>Although inactive sessions will eventually timeout, the server has a hardcoded limit of 500 concurrent sessions for each <code>username</code> or <code>originator</code>. Once this limit has been reached fresh logins will evict the session objects that have been used least recently, causing their associated session references to become invalid. For successful interoperability with other applications, concurrently accessing the server, the best policy is:</p><ul><li><p>Choose a string that identifies your application and its version.</p></li><li><p>Create a single session at start-of-day, using that identifying string for the <code>originator</code> parameter to <code>Session.login_with_password</code>.</p></li><li><p>Use this session throughout the application (note that sessions can be used across multiple separate client-server <em>network connections</em>) and then explicitly logout when possible.</p></li></ul><p>If a poorly written client leaks sessions or otherwise exceeds the limit, then as long as the client uses an appropriate <code>originator</code> argument, it will be easily identifiable from the XenServer logs and XenServer will destroy the longest-idle sessions of the rogue client only; this may cause problems for that client but not for other clients. If the misbehaving client did not specify an <code>originator</code>, it would be harder to identify and would cause the premature destruction of sessions of any clients that also did not specify an <code>originator</code></p><h3 id=install-and-start-example-summary>Install and start example: summary</h3><p>We have seen how the API can be used to install a VM from a XenServer template and perform a number of lifecycle operations on it. You will note that the number of calls we had to make in order to affect these operations was small:</p><ul><li><p>One call to acquire a session: <code>Session.login_with_password()</code></p></li><li><p>One call to query the VM (and template) objects present on the XenServer installation: <code>VM.get_all_records()</code>. Recall that we used the information returned from this call to select a suitable template to install from.</p></li><li><p>Four calls to install a VM from our chosen template: <code>VM.clone()</code>, followed
by the getter and setter of the <code>other_config</code> field to specify where to
create the disk images of the template, and then <code>VM.provision()</code>.</p></li><li><p>One call to start the resultant VM: <code>VM.start()</code> (and similarly other single calls to suspend, resume and shutdown accordingly)</p></li><li><p>And then one call to logout <code>Session.logout()</code></p></li></ul><p>The take-home message here is that, although the API as a whole is complex and fully featured, common tasks (such as creating and performing lifecycle operations on VMs) are very straightforward to perform, requiring only a small number of simple API calls. Keep this in mind while you study the next section which may, on first reading, appear a little daunting!</p><h2 id=object-model-overview>Object Model Overview</h2><p>This section gives a high-level overview of the object model of the API. A more detailed description of the parameters and methods of each class outlined here can be found in the XenServer API Reference document.</p><p>We start by giving a brief outline of some of the core classes that make up the API. (Don&rsquo;t worry if these definitions seem somewhat abstract in their initial presentation; the textual description in subsequent sections, and the code-sample walk through in the next Chapter will help make these concepts concrete.)</p><table><thead><tr><th>Class</th><th>Description</th></tr></thead><tbody><tr><td>VM</td><td>A VM object represents a particular virtual machine instance on a XenServer Host or Resource Pool. Example methods include <code>start</code>, <code>suspend</code>, <code>pool_migrate</code>; example parameters include <code>power_state</code>, <code>memory_static_max</code>, and <code>name_label</code>. (In the previous section we saw how the VM class is used to represent both templates and regular VMs)</td></tr><tr><td>Host</td><td>A host object represents a physical host in a XenServer pool. Example methods include <code>reboot</code> and <code>shutdown</code>. Example parameters include <code>software_version</code>, <code>hostname</code>, and [IP] <code>address</code>.</td></tr><tr><td>VDI</td><td>A VDI object represents a <em>Virtual Disk Image</em>. Virtual Disk Images can be attached to VMs, in which case a block device appears inside the VM through which the bits encapsulated by the Virtual Disk Image can be read and written. Example methods of the VDI class include &ldquo;resize&rdquo; and &ldquo;clone&rdquo;. Example fields include &ldquo;virtual_size&rdquo; and &ldquo;sharable&rdquo;. (When we called <code>VM.provision</code> on the VM template in our previous example, some VDI objects were automatically created to represent the newly created disks, and attached to the VM object.)</td></tr><tr><td>SR</td><td>An SR (<em>Storage Repository</em>) aggregates a collection of VDIs and encapsulates the properties of physical storage on which the VDIs&rsquo; bits reside. Example parameters include <code>type</code> (which determines the storage-specific driver a XenServer installation uses to read/write the SR&rsquo;s VDIs) and <code>physical_utilisation</code>; example methods include <code>scan</code> (which invokes the storage-specific driver to acquire a list of the VDIs contained with the SR and the properties of these VDIs) and <code>create</code> (which initializes a block of physical storage so it is ready to store VDIs).</td></tr><tr><td>Network</td><td>A network object represents a layer-2 network that exists in the environment in which the XenServer Host instance lives. Since XenServer does not manage networks directly this is a lightweight class that serves merely to model physical and virtual network topology. VM and Host objects that are <em>attached</em> to a particular Network object (by virtue of VIF and PIF instances &ndash; see below) can send network packets to each other.</td></tr></tbody></table><p>At this point, readers who are finding this enumeration of classes rather terse may wish to skip to the code walk-throughs of the next chapter: there are plenty of useful applications that can be written using only a subset of the classes already described! For those who wish to continue this description of classes in the abstract, read on.</p><p>On top of the classes listed above, there are 4 more that act as <em>connectors</em>, specifying relationships between VMs and Hosts, and Storage and Networks. The first 2 of these classes that we will consider, <em>VBD</em> and <em>VIF</em>, determine how VMs are attached to virtual disks and network objects respectively:</p><table><thead><tr><th>Class</th><th>Description</th></tr></thead><tbody><tr><td>VBD</td><td>A VBD (<em>Virtual Block Device</em>) object represents an attachment between a VM and a VDI. When a VM is booted its VBD objects are queried to determine which disk images (VDIs) should be attached. Example methods of the VBD class include &ldquo;plug&rdquo; (which <em>hot plugs</em> a disk device into a running VM, making the specified VDI accessible therein) and &ldquo;unplug&rdquo; (which <em>hot unplugs</em> a disk device from a running guest); example fields include &ldquo;device&rdquo; (which determines the device name inside the guest under which the specified VDI will be made accessible).</td></tr><tr><td>VIF</td><td>A VIF (<em>Virtual network InterFace</em>) object represents an attachment between a VM and a Network object. When a VM is booted its VIF objects are queried to determine which network devices should be created. Example methods of the VIF class include &ldquo;plug&rdquo; (which <em>hot plugs</em> a network device into a running VM) and &ldquo;unplug&rdquo; (which <em>hot unplugs</em> a network device from a running guest).</td></tr></tbody></table><p>The second set of &ldquo;connector classes&rdquo; that we will consider determine how Hosts are attached to Networks and Storage.</p><table><thead><tr><th>Class</th><th>Description</th></tr></thead><tbody><tr><td>PIF</td><td>A PIF (<em>Physical InterFace</em>) object represents an attachment between a Host and a Network object. If a host is connected to a Network (over a PIF) then packets from the specified host can be transmitted/received by the corresponding host. Example fields of the PIF class include &ldquo;device&rdquo; (which specifies the device name to which the PIF corresponds &ndash; e.g. <em>eth0</em>) and &ldquo;MAC&rdquo; (which specifies the MAC address of the underlying NIC that a PIF represents). Note that PIFs abstract both physical interfaces and VLANs (the latter distinguished by the existence of a positive integer in the &ldquo;VLAN&rdquo; field).</td></tr><tr><td>PBD</td><td>A PBD (<em>Physical Block Device</em>) object represents an attachment between a Host and a SR (Storage Repository) object. Fields include &ldquo;currently-attached&rdquo; (which specifies whether the chunk of storage represented by the specified SR object) is currently available to the host; and &ldquo;device_config&rdquo; (which specifies storage-driver specific parameters that determines how the low-level storage devices are configured on the specified host &ndash; e.g. in the case of an SR rendered on an NFS filer, device_config may specify the host-name of the filer and the path on the filer in which the SR files live.).</td></tr></tbody></table><p><a href=#image-cf644c2b7873049f383cb2b6107f9e36 class=lightbox-link><img src=/new-docs/xen-api/overview/dia_class_overview.svg alt="Graphical overview of API classes for managing VMs, Hosts, Storage and Networking" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-cf644c2b7873049f383cb2b6107f9e36><img src=/new-docs/xen-api/overview/dia_class_overview.svg alt="Graphical overview of API classes for managing VMs, Hosts, Storage and Networking" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>The figure above presents a graphical overview of the API classes involved in managing VMs, Hosts, Storage and Networking. From this diagram, the symmetry between storage and network configuration, and also the symmetry between virtual machine and host configuration is plain to see.</p><h2 id=working-with-vifs-and-vbds>Working with VIFs and VBDs</h2><p>In this section we walk through a few more complex scenarios, describing informally how various tasks involving virtual storage and network devices can be accomplished using the API.</p><h3 id=creating-disks-and-attaching-them-to-vms>Creating disks and attaching them to VMs</h3><p>Let&rsquo;s start by considering how to make a new blank disk image and attach it to a running VM. We will assume that we already have ourselves a running VM, and we know its corresponding API object reference (e.g. we may have created this VM using the procedure described in the previous section, and had the server return its reference to us.) We will also assume that we have authenticated with the XenServer installation and have a corresponding <code>session reference</code>. Indeed in the rest of this chapter, for the sake of brevity, we will stop mentioning sessions altogether.</p><h4 id=creating-a-new-blank-disk-image>Creating a new blank disk image</h4><p>The first step is to instantiate the disk image on physical storage. We do this by calling <code>VDI.create()</code>. The <code>VDI.create</code> call takes a number of parameters, including:</p><ul><li><p><code>name_label</code> and <code>name_description</code>: a human-readable name/description for the disk (e.g. for convenient display in the UI etc.). These fields can be left blank if desired.</p></li><li><p><code>SR</code>: the object reference of the Storage Repository representing the physical storage in which the VDI&rsquo;s bits will be placed.</p></li><li><p><code>read_only</code>: setting this field to true indicates that the VDI can <em>only</em> be attached to VMs in a read-only fashion. (Attempting to attach a VDI with its <code>read_only</code> field set to true in a read/write fashion results in error.)</p></li></ul><p>Invoking the <code>VDI.create</code> call causes the XenServer installation to create a blank disk image on physical storage, create an associated VDI object (the datamodel instance that refers to the disk image on physical storage) and return a reference to this newly created VDI object.</p><p>The way in which the disk image is represented on physical storage depends on the type of the SR in which the created VDI resides. For example, if the SR is of type &ldquo;lvm&rdquo; then the new disk image will be rendered as an LVM volume; if the SR is of type &ldquo;nfs&rdquo; then the new disk image will be a sparse VHD file created on an NFS filer. (You can query the SR type through the API using the <code>SR.get_type()</code> call.)</p><blockquote><p><strong>Note</strong></p><p>Some SR types might round up the <code>virtual-size</code> value to make it divisible by a configured block size.</p></blockquote><h4 id=attaching-the-disk-image-to-a-vm>Attaching the disk image to a VM</h4><p>So far we have a running VM (that we assumed the existence of at the start of this example) and a fresh VDI that we just created. Right now, these are both independent objects that exist on the XenServer Host, but there is nothing linking them together. So our next step is to create such a link, associating the VDI with our VM.</p><p>The attachment is formed by creating a new &ldquo;connector&rdquo; object called a VBD (<em>Virtual Block Device</em>). To create our VBD we invoke the <code>VBD.create()</code> call. The <code>VBD.create()</code> call takes a number of parameters including:</p><ul><li><p><code>VM</code> - the object reference of the VM to which the VDI is to be attached</p></li><li><p><code>VDI</code> - the object reference of the VDI that is to be attached</p></li><li><p><code>mode</code> - specifies whether the VDI is to be attached in a read-only or a read-write fashion</p></li><li><p><code>userdevice</code> - specifies the block device inside the guest through which applications running inside the VM will be able to read/write the VDI&rsquo;s bits.</p></li><li><p><code>type</code> - specifies whether the VDI should be presented inside the VM as a regular disk or as a CD. (Note that this particular field has more meaning for Windows VMs than it does for Linux VMs, but we will not explore this level of detail in this chapter.)</p></li></ul><p>Invoking <code>VBD.create</code> makes a VBD object on the XenServer installation and returns its object reference. However, this call in itself does not have any side-effects on the running VM (that is, if you go and look inside the running VM you will see that the block device has not been created). The fact that the VBD object exists but that the block device in the guest is not active, is reflected by the fact that the VBD object&rsquo;s <code>currently_attached</code> field is set to false.</p><p><a href=#image-d1abb3d720e0df187c900c2fcf7ff95f class=lightbox-link><img src=/new-docs/xen-api/overview/dia_vm_sr.svg alt="A VM object with 2 associated VDIs" class="figure-image noborder lightbox noshadow" style=height:auto;width:auto loading=lazy></a>
<a href=javascript:history.back(); class=lightbox-back id=image-d1abb3d720e0df187c900c2fcf7ff95f><img src=/new-docs/xen-api/overview/dia_vm_sr.svg alt="A VM object with 2 associated VDIs" class="lightbox-image noborder lightbox noshadow" loading=lazy></a></p><p>For expository purposes, the figure above presents a graphical example that shows the relationship between VMs, VBDs, VDIs and SRs. In this instance a VM object has 2 attached VDIs: there are 2 VBD objects that form the connections between the VM object and its VDIs; and the VDIs reside within the same SR.</p><h4 id=hotplugging-the-vbd>Hotplugging the VBD</h4><p>If we rebooted the VM at this stage then, after rebooting, the block device corresponding to the VBD would appear: on boot, XenServer queries all VBDs of a VM and actively attaches each of the corresponding VDIs.</p><p>Rebooting the VM is all very well, but recall that we wanted to attach a newly created blank disk to a <em>running</em> VM. This can be achieved by invoking the <code>plug</code> method on the newly created VBD object. When the <code>plug</code> call returns successfully, the block device to which the VBD relates will have appeared inside the running VM &ndash; i.e. from the perspective of the running VM, the guest operating system is led to believe that a new disk device has just been <em>hot plugged</em>. Mirroring this fact in the managed world of the API, the <code>currently_attached</code> field of the VBD is set to true.</p><p>Unsurprisingly, the VBD <code>plug</code> method has a dual called &ldquo;<code>unplug</code>&rdquo;. Invoking the <code>unplug</code> method on a VBD object causes the associated block device to be <em>hot unplugged</em> from a running VM, setting the <code>currently_attached</code> field of the VBD object to false accordingly.</p><h3 id=creating-and-attaching-network-devices-to-vms>Creating and attaching Network Devices to VMs</h3><p>The API calls involved in configuring virtual network interfaces in VMs are similar in many respects to the calls involved in configuring virtual disk devices. For this reason we will not run through a full example of how one can create network interfaces using the API object-model; instead we will use this section just to outline briefly the symmetry between virtual <em>networking device</em> and virtual <em>storage device</em> configuration.</p><p>The networking analogue of the VBD class is the VIF class. Just as a VBD is the API representation of a block device inside a VM, a VIF (<em>Virtual network InterFace</em>) is the API representation of a network device inside a VM. Whereas VBDs associate VM objects with VDI objects, VIFs associate VM objects with Network objects. Just like VBDs, VIFs have a <code>currently_attached</code> field that determines whether or not the network device (inside the guest) associated with the VIF is currently active or not. And as we saw with VBDs, at VM boot-time the VIFs of the VM are queried and a corresponding network device for each created inside the booting VM. Similarly, VIFs also have <code>plug</code> and <code>unplug</code> methods for hot plugging/unplugging network devices in/out of running VMs.</p><h3 id=host-configuration-for-networking-and-storage>Host configuration for networking and storage</h3><p>We have seen that the VBD and VIF classes are used to manage configuration of block devices and network devices (respectively) inside VMs. To manage host configuration of storage and networking there are two analogous classes: PBD (<em>Physical Block Device</em>) and PIF (<em>Physical [network] InterFace</em>).</p><h4 id=host-storage-configuration-pbds>Host storage configuration: PBDs</h4><p>Let us start by considering the PBD class. A <code>PBD_create()</code> call takes a number of parameters including:</p><table><thead><tr><th>Parameter</th><th>Description</th></tr></thead><tbody><tr><td>host</td><td>physical machine on which the PBD is available</td></tr><tr><td>SR</td><td>the Storage Repository that the PBD connects to</td></tr><tr><td>device_config</td><td>a string-to-string map that is provided to the host&rsquo;s SR-backend-driver, containing the low-level parameters required to configure the physical storage device(s) on which the SR is to be realized. The specific contents of the <code>device_config</code> field depend on the type of the SR to which the PBD is connected. (Executing <code>xe sm-list</code> will show a list of possible SR types; the <em>configuration</em> field in this enumeration specifies the <code>device_config</code> parameters that each SR type expects.)</td></tr></tbody></table><p>For example, imagine we have an SR object <em>s</em> of type &ldquo;nfs&rdquo; (representing a directory on an NFS filer within which VDIs are stored as VHD files); and let&rsquo;s say that we want a host, <em>h</em>, to be able to access <em>s</em>. In this case we invoke <code>PBD.create()</code> specifying host <em>h</em>, SR <em>s</em>, and a value for the <em>device_config</em> parameter that is the following map:</p><p><code>("server", "my_nfs_server.example.com"), ("serverpath", "/scratch/mysrs/sr1")</code></p><p>This tells the XenServer Host that SR <em>s</em> is accessible on host <em>h</em>, and further that to access SR <em>s</em>, the host needs to mount the directory <code>/scratch/mysrs/sr1</code> on the NFS server named <code>my_nfs_server.example.com</code>.</p><p>Like VBD objects, PBD objects also have a field called <code>currently_attached</code>. Storage repositories can be attached and detached from a given host by invoking <code>PBD.plug</code> and <code>PBD.unplug</code> methods respectively.</p><h4 id=host-networking-configuration-pifs>Host networking configuration: PIFs</h4><p>Host network configuration is specified by virtue of PIF objects. If a PIF object connects a network object, <em>n</em>, to a host object <em>h</em>, then the network corresponding to <em>n</em> is bridged onto a physical interface (or a physical interface plus a VLAN tag) specified by the fields of the PIF object.</p><p>For example, imagine a PIF object exists connecting host <em>h</em> to a network <em>n</em>, and that <code>device</code> field of the PIF object is set to <code>eth0</code>. This means that all packets on network <em>n</em> are bridged to the NIC in the host corresponding to host network device <code>eth0</code>.</p><h2 id=xml-rpc-notes>XML-RPC notes</h2><h3 id=datetimes>Datetimes</h3><p>The API deviates from the XML-RPC specification in handling of datetimes. The API appends a &ldquo;Z&rdquo; to the end of datetime strings, which is meant to indicate that the time is expressed in UTC.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=api-evolution>API evolution</h1><p>All APIs evolve as bugs are fixed, new features added and features are removed</p><ul><li>the XenAPI is no exception. This document lists policies describing how the
XenAPI evolves over time.</li></ul><p>The goals of XenAPI evolution are:</p><ul><li>to allow bugs to be fixed efficiently;</li><li>to allow new, innovative features to be added easily;</li><li>to keep old, unmodified clients working as much as possible; and</li><li>where backwards-incompatible changes are to be made, publish this
information early to enable affected parties to give timely feedback.</li></ul><h2 id=background>Background</h2><p>In this document, the term <em>XenAPI</em> refers to the XMLRPC-derived wire protocol
used by xapi. The XenAPI has <em>objects</em> which each have <em>fields</em> and
<em>messages</em>. The XenAPI is described in detail elsewhere.</p><h2 id=xenapi-lifecycle>XenAPI Lifecycle</h2><div class="mermaid align-center">graph LR
Prototype -->|1| Published -->|4| Deprecated -->|5| Removed
Published -->|2,3| Published</div><p>Each element of the XenAPI (objects, messages and fields) follows the lifecycle
diagram above. When an element is newly created and being still in development,
it is in the <em>Prototype</em> state. Elements in this state may be stubs: the
interface is there and can be used by clients for prototyping their new
features, but the actual implementation is not yet ready.</p><p>When the element subsequently becomes ready for use (the stub is replaced by a
real implementation), it transitions to the <em>Published</em> state. This is the only
state in which the object, message or field should be used. From this point
onwards, the element needs to have clearly defined semantics that are available
for reference in the XenAPI documentation.</p><p>If the XenAPI element becomes <em>Deprecated</em>, it will still function as it did
before, but its use is discouraged. The final stage of the lifecycle is the
<em>Removed</em> state, in which the element is not available anymore.</p><p>The numbered state changes in the diagram have the following meaning:</p><ol><li>Publish: declare that the XenAPI element is ready for people to use.</li><li>Extend: a <em>backwards-compatible</em> extension of the XenAPI, for example an
additional parameter in a message with an appropriate default value. If the
API is used as before, it still has the same effect.</li><li>Change: a <em>backwards-incompatible</em> change. That is, the message now behaves
differently, or the field has different semantics. Such changes are
discouraged and should only be considered in special cases (always consider
whether deprecation is a better solution). The use of a message can for
example be restricted for security or efficiency reasons, or the behaviour
can be changed simply to fix a bug.</li><li>Deprecate: declare that the use of this XenAPI element should be avoided from
now on. Reasons for doing this include: the element is redundant (it
duplicates functionality elsewhere), it is inconsistent with other parts of
the XenAPI, it is insecure or inefficient (for examples of deprecation
policies of other projects, see
<a href=http://developer.symbian.org/wiki/index.php/Public_API_Change_Control_Process target=_blank>symbian</a>
<a href=http://wiki.eclipse.org/Eclipse/API_Central/Deprecation_Policy target=_blank>eclipse</a>
<a href=http://oval.mitre.org/language/about/deprecation.html target=_blank>oval</a>.</li><li>Remove: the element is taken out of the public API and can no longer be used.</li></ol><p>Each lifecycle transition must be accompanied by an explanation describing the
change and the reason for the change. This message should be enough to
understand the semantics of the XenAPI element after the change, and in the case
of backwards-incompatible changes or deprecation, it should give directions
about how to modify a client to deal with the change (for example, how to avoid
using the deprecated field or message).</p><h2 id=releases>Releases</h2><p>Every release must be accompanied by <em>release notes</em> listing all objects, fields
and messages that are newly prototyped, published, extended, changed, deprecated
or removed in the release. Each item should have an explanation as implied
above, documenting the new or changed XenAPI element. The release notes for
every release shall be prominently displayed in the XenAPI HTML documentation.</p><h2 id=documentation>Documentation</h2><p>The XenAPI documentation will contain its complete lifecycle history for each
XenAPI element. Only the elements described in the documentation are
&ldquo;official&rdquo; and supported.</p><p>Each object, message and field in <code>datamodel.ml</code> will have lifecycle
metadata attached to it, which is a list of transitions (transition type *
release * explanation string) as described above. Release notes are automatically generated from this data.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=using-the-api>Using the API</h1><p>This chapter describes how to use the XenServer Management API from real programs to manage XenServer Hosts and VMs. The chapter begins with a walk-through of a typical client application and demonstrates how the API can be used to perform common tasks. Example code fragments are given in python syntax but equivalent code in the other programming languages would look very similar. The language bindings themselves are discussed afterwards and the chapter finishes with walk-throughs of two complete examples.</p><h2 id=anatomy-of-a-typical-application>Anatomy of a typical application</h2><p>This section describes the structure of a typical application using the XenServer Management API. Most client applications begin by connecting to a XenServer Host and authenticating (e.g. with a username and password). Assuming the authentication succeeds, the server will create a &ldquo;session&rdquo; object and return a reference to the client. This reference will be passed as an argument to all future API calls. Once authenticated, the client may search for references to other useful objects (e.g. XenServer Hosts, VMs, etc.) and invoke operations on them. Operations may be invoked either synchronously or asynchronously; special task objects represent the state and progress of asynchronous operations. These application elements are all described in detail in the following sections.</p><h3 id=choosing-a-low-level-transport>Choosing a low-level transport</h3><p>API calls can be issued over two transports:</p><ul><li><p>SSL-encrypted TCP on port 443 (https) over an IP network</p></li><li><p>plaintext over a local Unix domain socket: <code>/var/xapi/xapi</code></p></li></ul><p>The SSL-encrypted TCP transport is used for all off-host traffic while the Unix domain socket can be used from services running directly on the XenServer Host itself. In the SSL-encrypted TCP transport, all API calls should be directed at the Resource Pool master; failure to do so will result in the error <code>HOST_IS_SLAVE</code>, which includes the IP address of the master as an error parameter.</p><p>Because the master host of a pool can change, especially if HA is enabled on a pool, clients must implement the following steps to detect a master host change and connect to the new master as required:</p><p>Subscribe to updates in the list of hosts servers, and maintain a current list of hosts in the pool</p><p>If the connection to the pool master fails to respond, attempt to connect to all hosts in the list until one responds</p><p>The first host to respond will return the <code>HOST_IS_SLAVE</code> error message, which contains the identity of the new pool master (unless of course the host is the new master)</p><p>Connect to the new master</p><blockquote><p><strong>Note</strong></p><p>As a special-case, all messages sent through the Unix domain socket are transparently forwarded to the correct node.</p></blockquote><h3 id=authentication-and-session-handling>Authentication and session handling</h3><p>The vast majority of API calls take a session reference as their first parameter; failure to supply a valid reference will result in a <code>SESSION_INVALID</code> error being returned. Acquire a session reference by supplying a username and password to the <code>login_with_password</code> function.</p><blockquote><p><strong>Note</strong></p><p>As a special-case, if this call is executed over the local Unix domain socket then the username and password are ignored and the call always succeeds.</p></blockquote><p>Every session has an associated &ldquo;last active&rdquo; timestamp which is updated on every API call. The server software currently has a built-in limit of 500 active sessions and will remove those with the oldest &ldquo;last active&rdquo; field if this limit is exceeded for a given <code>username</code> or <code>originator</code>. In addition all sessions whose &ldquo;last active&rdquo; field is older than 24 hours are also removed. Therefore it is important to:</p><ul><li><p>Specify an appropriate <code>originator</code> when logging in; and</p></li><li><p>Remember to log out of active sessions to avoid leaking them; and</p></li><li><p>Be prepared to log in again to the server if a <code>SESSION_INVALID</code> error is caught.</p></li></ul><p>In the following Python fragment a connection is established over the Unix domain socket and a session is created:</p><pre><code>import XenAPI

    session = XenAPI.xapi_local()
    try:
        session.xenapi.login_with_password(&quot;root&quot;, &quot;&quot;, &quot;2.3&quot;, &quot;My Widget v0.1&quot;)
        ...
    finally:
        session.xenapi.session.logout()
</code></pre><h3 id=finding-references-to-useful-objects>Finding references to useful objects</h3><p>Once an application has authenticated the next step is to acquire references to objects in order to query their state or invoke operations on them. All objects have a set of &ldquo;implicit&rdquo; messages which include the following:</p><ul><li><p><code>get_by_name_label</code> : return a list of all objects of a particular class with a particular label;</p></li><li><p><code>get_by_uuid</code> : return a single object named by its UUID;</p></li><li><p><code>get_all</code> : return a set of references to all objects of a particular class; and</p></li><li><p><code>get_all_records</code> : return a map of reference to records for each object of a particular class.</p></li></ul><p>For example, to list all hosts:</p><pre><code>hosts = session.xenapi.host.get_all()
</code></pre><p>To find all VMs with the name &ldquo;my first VM&rdquo;:</p><pre><code>vms = session.xenapi.VM.get_by_name_label('my first VM')
</code></pre><blockquote><p><strong>Note</strong></p><p>Object <code>name_label</code> fields are not guaranteed to be unique and so the <code>get_by_name_label</code> API call returns a set of references rather than a single reference.</p></blockquote><p>In addition to the methods of finding objects described above, most objects also contain references to other objects within fields. For example it is possible to find the set of VMs running on a particular host by calling:</p><pre><code>vms = session.xenapi.host.get_resident_VMs(host)
</code></pre><h3 id=invoking-synchronous-operations-on-objects>Invoking synchronous operations on objects</h3><p>Once object references have been acquired, operations may be invoked on them. For example to start a VM:</p><pre><code>session.xenapi.VM.start(vm, False, False)
</code></pre><p>All API calls are by default synchronous and will not return until the operation has completed or failed. For example in the case of <code>VM.start</code> the call does not return until the VM has started booting.</p><blockquote><p><strong>Note</strong></p><p>When the <code>VM.start</code> call returns the VM will be booting. To determine when the booting has finished, wait for the in-guest agent to report internal statistics through the <code>VM_guest_metrics</code> object.</p></blockquote><h3 id=using-tasks-to-manage-asynchronous-operations>Using Tasks to manage asynchronous operations</h3><p>To simplify managing operations which take quite a long time (e.g. <code>VM.clone</code> and <code>VM.copy</code>) functions are available in two forms: synchronous (the default) and asynchronous. Each asynchronous function returns a reference to a task object which contains information about the in-progress operation including:</p><ul><li><p>whether it is pending</p></li><li><p>whether it is has succeeded or failed</p></li><li><p>progress (in the range 0-1)</p></li><li><p>the result or error code returned by the operation</p></li></ul><p>An application which wanted to track the progress of a <code>VM.clone</code> operation and display a progress bar would have code like the following:</p><pre><code>vm = session.xenapi.VM.get_by_name_label('my vm')
task = session.xenapi.Async.VM.clone(vm)
while session.xenapi.task.get_status(task) == &quot;pending&quot;:
        progress = session.xenapi.task.get_progress(task)
        update_progress_bar(progress)
        time.sleep(1)
session.xenapi.task.destroy(task)
</code></pre><blockquote><p><strong>Note</strong></p><p>Note that a well-behaved client should remember to delete tasks created by asynchronous operations when it has finished reading the result or error. If the number of tasks exceeds a built-in threshold then the server will delete the oldest of the completed tasks.</p></blockquote><h3 id=subscribing-to-and-listening-for-events>Subscribing to and listening for events</h3><p>With the exception of the task and metrics classes, whenever an object is modified the server generates an event. Clients can subscribe to this event stream on a per-class basis and receive updates rather than resorting to frequent polling. Events come in three types:</p><ul><li><p><code>add</code> - generated when an object has been created;</p></li><li><p><code>del</code> - generated immediately before an object is destroyed; and</p></li><li><p><code>mod</code> - generated when an object&rsquo;s field has changed.</p></li></ul><p>Events also contain a monotonically increasing ID, the name of the class of object and a snapshot of the object state equivalent to the result of a <code>get_record()</code>.</p><p>Clients register for events by calling <code>event.register()</code> with a list of class names or the special string &ldquo;*&rdquo;. Clients receive events by executing <code>event.next()</code> which blocks until events are available and returns the new events.</p><blockquote><p><strong>Note</strong></p><p>Since the queue of generated events on the server is of finite length a very slow client might fail to read the events fast enough; if this happens an <code>EVENTS_LOST</code> error is returned. Clients should be prepared to handle this by re-registering for events and checking that the condition they are waiting for hasn&rsquo;t become true while they were unregistered.</p></blockquote><p>The following python code fragment demonstrates how to print a summary of every event generated by a system: (similar code exists in <code>Xenserver-SDK/XenServerPython/samples/watch-all-events.py</code>)</p><pre><code>fmt = &quot;%8s  %20s  %5s  %s&quot;
session.xenapi.event.register([&quot;*&quot;])
while True:
    try:
        for event in session.xenapi.event.next():
            name = &quot;(unknown)&quot;
            if &quot;snapshot&quot; in event.keys():
                snapshot = event[&quot;snapshot&quot;]
                if &quot;name_label&quot; in snapshot.keys():
                    name = snapshot[&quot;name_label&quot;]
            print fmt % (event['id'], event['class'], event['operation'], name)           
    except XenAPI.Failure, e:
        if e.details == [ &quot;EVENTS_LOST&quot; ]:
            print &quot;Caught EVENTS_LOST; should reregister&quot;
</code></pre><h2 id=language-bindings>Language bindings</h2><h3 id=c>C</h3><p>The SDK includes the source to the C language binding in the directory <code>XenServer-SDK/libxenserver/src</code> together with a Makefile which compiles the binding into a library. Every API object is associated with a header file which contains declarations for all that object&rsquo;s API functions; for example the type definitions and functions required to invoke VM operations are all contained in <code>xen_vm.h</code>.</p><p><strong>C binding dependencies</strong></p><p>The following simple examples are included with the C bindings:</p><ul><li><p><code>test_vm_async_migrate</code>: demonstrates how to use asynchronous API calls to migrate running VMs from a slave host to the pool master.</p></li><li><p><code>test_vm_ops</code>: demonstrates how to query the capabilities of a host, create a VM, attach a fresh blank disk image to the VM and then perform various powercycle operations;</p></li><li><p><code>test_failures</code>: demonstrates how to translate error strings into enum_xen_api_failure, and vice versa;</p></li><li><p><code>test_event_handling</code>: demonstrates how to listen for events on a connection.</p></li><li><p><code>test_enumerate</code>: demonstrates how to enumerate the various API objects.</p></li></ul><h3 id=c35>C#</h3><p>The C# bindings are contained within the directory <code>XenServer-SDK/XenServer.NET</code> and include project files suitable for building under Microsoft Visual Studio. Every API object is associated with one C# file; for example the functions implementing the VM operations are contained within the file <code>VM.cs</code>.</p><p><strong>C# binding dependencies</strong></p><p>Three examples are included with the C# bindings in the directory <code>XenServer-SDK/XenServer.NET/samples</code> as separate projects of the <code>XenSdkSample.sln</code> solution:</p><ul><li><p><code>GetVariousRecords</code>: logs into a XenServer Host and displays information about hosts, storage and virtual machines;</p></li><li><p><code>GetVmRecords</code>: logs into a XenServer Host and lists all the VM records;</p></li><li><p><code>VmPowerStates</code>: logs into a XenServer Host, finds a VM and takes it through the various power states. Requires a shut-down VM to be already installed.</p></li></ul><h3 id=java>Java</h3><p>The Java bindings are contained within the directory <code>XenServer-SDK/XenServerJava</code> and include project files suitable for building under Microsoft Visual Studio. Every API object is associated with one Java file; for example the functions implementing the VM operations are contained within the file <code>VM.java</code>.</p><p><strong>Java binding dependencies</strong></p><p>Running the main file <code>XenServer-SDK/XenServerJava/samples/RunTests.java</code> will run a series of examples included in the same directory:</p><ul><li><p><code>AddNetwork</code>: Adds a new internal network not attached to any NICs;</p></li><li><p><code>SessionReuse</code>: Demonstrates how a Session object can be shared between multiple Connections;</p></li><li><p><code>AsyncVMCreate</code>: Makes asynchronously a new VM from a built-in template, starts and stops it;</p></li><li><p><code>VdiAndSrOps</code>: Performs various SR and VDI tests, including creating a dummy SR;</p></li><li><p><code>CreateVM</code>: Creates a VM on the default SR with a network and DVD drive;</p></li><li><p><code>DeprecatedMethod</code>: Tests a warning is displayed wehn a deprecated API method is called;</p></li><li><p><code>GetAllRecordsOfAllTypes</code>: Retrieves all the records for all types of objects;</p></li><li><p><code>SharedStorage</code>: Creates a shared NFS SR;</p></li><li><p><code>StartAllVMs</code>: Connects to a host and tries to start each VM on it.</p></li></ul><h3 id=powershell>PowerShell</h3><p>The PowerShell bindings are contained within the directory <code>XenServer-SDK/XenServerPowerShell</code>. We provide the PowerShell module <code>XenServerPSModule</code> and source code exposing the XenServer API as Windows PowerShell cmdlets.</p><p><strong>PowerShell binding dependencies</strong></p><p>These example scripts are included with the PowerShell bindings in the directory <code>XenServer-SDK/XenServerPowerShell/samples</code>:</p><ul><li><p><code>AutomatedTestCore.ps1</code>: demonstrates how to log into a XenServer host, create a storage repository and a VM, and then perform various powercycle operations;</p></li><li><p><code>HttpTest.ps1</code>: demonstrates how to log into a XenServer host, create a VM, and then perform operations such as VM importing and exporting, patch upload, and retrieval of performance statistics.</p></li></ul><h3 id=python>Python</h3><p>The python bindings are contained within a single file: <code>XenServer-SDK/XenServerPython/XenAPI.py</code>.</p><p><strong>Python binding dependencies</strong></p><p>|:&ndash;|:&ndash;|
|Platform supported:|Linux|
|Library:|XenAPI.py|
|Dependencies:|None|</p><p>The SDK includes 7 python examples:</p><ul><li><p><code>fixpbds.py</code> - reconfigures the settings used to access shared storage;</p></li><li><p><code>install.py</code> - installs a Debian VM, connects it to a network, starts it up and waits for it to report its IP address;</p></li><li><p><code>license.py</code> - uploads a fresh license to a XenServer Host;</p></li><li><p><code>permute.py</code> - selects a set of VMs and uses XenMotion to move them simultaneously between hosts;</p></li><li><p><code>powercycle.py</code> - selects a set of VMs and powercycles them;</p></li><li><p><code>shell.py</code> - a simple interactive shell for testing;</p></li><li><p><code>vm_start_async.py</code> - demonstrates how to invoke operations asynchronously;</p></li><li><p><code>watch-all-events.py</code> - registers for all events and prints details when they occur.</p></li></ul><h3 id=command-line-interface-cli>Command Line Interface (CLI)</h3><p>Besides using raw XML-RPC or one of the supplied language bindings, third-party software developers may integrate with XenServer Hosts by using the XE command line interface <code>xe</code>. The xe CLI is installed by default on XenServer hosts; a stand-alone remote CLI is also available for Linux. On Windows, the <code>xe.exe</code> CLI executable is installed along with XenCenter.</p><p><strong>CLI dependencies</strong></p><p>|:&ndash;|:&ndash;|
|Platform supported:|Linux and Windows|
|Library:|None|
|Binary:|xe (xe.exe on Windows)|
|Dependencies:|None|</p><p>The CLI allows almost every API call to be directly invoked from a script or other program, silently taking care of the required session management.
The XE CLI syntax and capabilities are described in detail in the <a href=https://docs.citrix.com/en-us/citrix-hypervisor/command-line-interface.html target=_blank>XenServer Administrator&rsquo;s Guide</a>. For additional resources and examples, visit the <a href=http://support.citrix.com target=_blank>Citrix Knowledge Center</a>.</p><blockquote><p><strong>Note</strong></p><p>When running the CLI from a XenServer Host console, tab-completion of both command names and arguments is available.</p></blockquote><h2 id=complete-application-examples>Complete application examples</h2><p>This section describes two complete examples of real programs using the API.</p><h3 id=simultaneously-migrating-vms-using-xenmotion>Simultaneously migrating VMs using XenMotion</h3><p>This python example (contained in <code>XenServer-SDK/XenServerPython/samples/permute.py</code>) demonstrates how to use XenMotion to move VMs simultaneously between hosts in a Resource Pool. The example makes use of asynchronous API calls and shows how to wait for a set of tasks to complete.</p><p>The program begins with some standard boilerplate and imports the API bindings module</p><pre><code>import sys, time
import XenAPI
</code></pre><p>Next the commandline arguments containing a server URL, username, password and a number of iterations are parsed. The username and password are used to establish a session which is passed to the function <code>main</code>, which is called multiple times in a loop. Note the use of <code>try: finally:</code> to make sure the program logs out of its session at the end.</p><pre><code>if __name__ == &quot;__main__&quot;:
    if len(sys.argv) &lt;&gt; 5:
        print &quot;Usage:&quot;
        print sys.argv[0], &quot; &lt;url&gt; &lt;username&gt; &lt;password&gt; &lt;iterations&gt;&quot;
        sys.exit(1)
    url = sys.argv[1]
    username = sys.argv[2]
    password = sys.argv[3]
    iterations = int(sys.argv[4])
    # First acquire a valid session by logging in:
    session = XenAPI.Session(url)
    session.xenapi.login_with_password(username, password, &quot;2.3&quot;,
                                       &quot;Example migration-demo v0.1&quot;)
    try:
        for i in range(iterations):
            main(session, i)
    finally:
        session.xenapi.session.logout()
</code></pre><p>The <code>main</code> function examines each running VM in the system, taking care to filter out <em>control domains</em> (which are part of the system and not controllable by the user). A list of running VMs and their current hosts is constructed.</p><pre><code>def main(session, iteration):
    # Find a non-template VM object
    all = session.xenapi.VM.get_all()
    vms = []
    hosts = []
    for vm in all:
        record = session.xenapi.VM.get_record(vm)
        if not(record[&quot;is_a_template&quot;]) and \
           not(record[&quot;is_control_domain&quot;]) and \
           record[&quot;power_state&quot;] == &quot;Running&quot;:
            vms.append(vm)
            hosts.append(record[&quot;resident_on&quot;])
    print &quot;%d: Found %d suitable running VMs&quot; % (iteration, len(vms))
</code></pre><p>Next the list of hosts is rotated:</p><pre><code># use a rotation as a permutation
    hosts = [hosts[-1]] + hosts[:(len(hosts)-1)]
</code></pre><p>Each VM is then moved using XenMotion to the new host under this rotation (i.e. a VM running on host at position 2 in the list will be moved to the host at position 1 in the list etc.) In order to execute each of the movements in parallel, the asynchronous version of the <code>VM.pool_migrate</code> is used and a list of task references constructed. Note the <code>live</code> flag passed to the <code>VM.pool_migrate</code>; this causes the VMs to be moved while they are still running.</p><pre><code>tasks = []
    for i in range(0, len(vms)):
        vm = vms[i]
        host = hosts[i]
        task = session.xenapi.Async.VM.pool_migrate(vm, host, { &quot;live&quot;: &quot;true&quot; })
        tasks.append(task)
</code></pre><p>The list of tasks is then polled for completion:</p><pre><code>finished = False
    records = {}
    while not(finished):
        finished = True
        for task in tasks:
            record = session.xenapi.task.get_record(task)
            records[task] = record
            if record[&quot;status&quot;] == &quot;pending&quot;:
                finished = False
        time.sleep(1)
</code></pre><p>Once all tasks have left the <em>pending</em> state (i.e. they have successfully completed, failed or been cancelled) the tasks are polled once more to see if they all succeeded:</p><pre><code>allok = True
    for task in tasks:
        record = records[task]
        if record[&quot;status&quot;] &lt;&gt; &quot;success&quot;:
            allok = False
</code></pre><p>If any one of the tasks failed then details are printed, an exception is raised and the task objects left around for further inspection. If all tasks succeeded then the task objects are destroyed and the function returns.</p><pre><code>if not(allok):
        print &quot;One of the tasks didn't succeed at&quot;, \
            time.strftime(&quot;%F:%HT%M:%SZ&quot;, time.gmtime())
        idx = 0
        for task in tasks:
            record = records[task]
            vm_name = session.xenapi.VM.get_name_label(vms[idx])
            host_name = session.xenapi.host.get_name_label(hosts[idx])
            print &quot;%s : %12s %s -&gt; %s [ status: %s; result = %s; error = %s ]&quot; % \
                  (record[&quot;uuid&quot;], record[&quot;name_label&quot;], vm_name, host_name,      \
                   record[&quot;status&quot;], record[&quot;result&quot;], repr(record[&quot;error_info&quot;]))
            idx = idx + 1
        raise &quot;Task failed&quot;
    else:
        for task in tasks:
            session.xenapi.task.destroy(task)
</code></pre><h3 id=cloning-a-vm-using-the-xe-cli>Cloning a VM using the XE CLI</h3><p>This example is a <code>bash</code> script which uses the XE CLI to clone a VM taking care to shut it down first if it is powered on.</p><p>The example begins with some boilerplate which first checks if the environment variable <code>XE</code> has been set: if it has it assumes that it points to the full path of the CLI, else it is assumed that the XE CLI is on the current path. Next the script prompts the user for a server name, username and password:</p><pre><code># Allow the path to the 'xe' binary to be overridden by the XE environment variable
if [ -z &quot;${XE}&quot; ]; then
  XE=xe
fi

if [ ! -e &quot;${HOME}/.xe&quot; ]; then
  read -p &quot;Server name: &quot; SERVER
  read -p &quot;Username: &quot; USERNAME
  read -p &quot;Password: &quot; PASSWORD
  XE=&quot;${XE} -s ${SERVER} -u ${USERNAME} -pw ${PASSWORD}&quot;
fi
</code></pre><p>Next the script checks its commandline arguments. It requires exactly one: the UUID of the VM which is to be cloned:</p><pre><code># Check if there's a VM by the uuid specified
${XE} vm-list params=uuid | grep -q &quot; ${vmuuid}$&quot;
if [ $? -ne 0 ]; then
        echo &quot;error: no vm uuid \&quot;${vmuuid}\&quot; found&quot;
        exit 2
fi
</code></pre><p>The script then checks the power state of the VM and if it is running, it attempts a clean shutdown. The event system is used to wait for the VM to enter state &ldquo;Halted&rdquo;.</p><blockquote><p><strong>Note</strong></p><p>The XE CLI supports a command-line argument <code>--minimal</code> which causes it to print its output without excess whitespace or formatting, ideal for use from scripts. If multiple values are returned they are comma-separated.</p></blockquote><pre><code># Check the power state of the vm
name=$(${XE} vm-list uuid=${vmuuid} params=name-label --minimal)
state=$(${XE} vm-list uuid=${vmuuid} params=power-state --minimal)
wasrunning=0

# If the VM state is running, we shutdown the vm first
if [ &quot;${state}&quot; = &quot;running&quot; ]; then
        ${XE} vm-shutdown uuid=${vmuuid}
        ${XE} event-wait class=vm power-state=halted uuid=${vmuuid}
        wasrunning=1
fi
</code></pre><p>The VM is then cloned and the new VM has its <code>name_label</code> set to <code>cloned_vm</code>.</p><pre><code># Clone the VM
newuuid=$(${XE} vm-clone uuid=${vmuuid} new-name-label=cloned_vm)
</code></pre><p>Finally, if the original VM had been running and was shutdown, both it and the new VM are started.</p><pre><code># If the VM state was running before cloning, we start it again
# along with the new VM.
if [ &quot;$wasrunning&quot; -eq 1 ]; then
        ${XE} vm-start uuid=${vmuuid}
        ${XE} vm-start uuid=${newuuid}
fi
</code></pre><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenapi-reference>XenAPI Reference</h1><div><h2 class=title>XenAPI Classes</h2><p>Click on a class to view the associated fields and messages.</p><img src=/new-docs/xen-api/classes/classes.png alt="Xen-api class diagram" usemap=#graph border=0>
<map id=graph name=graph><area shape=poly href=/new-docs/xen-api/classes/task title="an asynchronous server-side task" alt coords="408,233 406,227 402,222 395,217 387,214 378,213 369,214 360,217 354,222 349,227 348,233 349,240 354,245 360,250 369,252 378,253 387,252 395,250 402,245 406,240"><area shape=poly href=/new-docs/xen-api/classes/event title="allows event registration and reading" alt coords="408,296 406,289 402,284 395,279 387,277 378,276 369,277 360,279 354,284 349,289 348,296 349,302 354,307 360,312 369,315 378,316 387,315 395,312 402,307 406,302"><area shape=poly href=/new-docs/xen-api/classes/pool title="a Resource Pool" alt coords="470,78 469,72 464,66 458,62 449,59 440,58 431,59 422,62 416,66 411,72 410,78 411,84 416,90 422,94 431,97 440,98 449,97 458,94 464,90 469,84"><area shape=rect href=/new-docs/xen-api/classes/session title="an authenticated session" alt coords=92,211,152,251><area shape=rect href=/new-docs/xen-api/classes/user title="a user" alt coords=6,201,66,241><area shape=rect href=/new-docs/xen-api/classes/host title="a physical host" alt coords=187,223,247,263><area shape=rect href=/new-docs/xen-api/classes/vm title="a Virtual Machine" alt coords=508,279,568,319><area shape=poly href=/new-docs/xen-api/classes/vm_metrics title="dynamic VM configuration information" alt coords="673,301 671,295 665,289 656,285 644,282 631,281 618,282 607,285 597,289 591,295 589,301 591,307 597,313 607,317 618,320 631,321 644,320 656,317 665,313 671,307"><area shape=poly href=/new-docs/xen-api/classes/vm_guest_metrics title="dynamic information from inside the guest" alt coords="670,360 667,354 658,348 645,344 628,341 610,340 592,341 575,344 562,348 553,354 550,360 553,366 562,372 575,376 592,379 610,380 628,379 645,376 658,372 667,366"><area shape=poly href=/new-docs/xen-api/classes/crashdump title="VM crashdump" alt coords="650,246 648,239 642,234 633,229 622,227 609,226 596,227 585,229 576,234 570,239 568,246 570,252 576,257 585,262 596,265 609,266 622,265 633,262 642,257 648,252"><area shape=poly href=/new-docs/xen-api/classes/console title="location information for a guest's console" alt coords="590,387 589,380 584,375 577,370 569,368 559,367 549,368 540,370 533,375 529,380 527,387 529,393 533,398 540,403 549,406 559,407 569,406 577,403 584,398 589,393"><area shape=rect href=/new-docs/xen-api/classes/pbd title="the connection between an SR and a host" alt coords=244,147,304,187><area shape=poly href=/new-docs/xen-api/classes/host_metrics title="dynamic host information" alt coords="202,307 199,300 193,295 183,290 170,288 156,287 141,288 128,290 118,295 112,300 109,307 112,313 118,318 128,323 141,326 156,327 170,326 183,323 193,318 199,313"><area shape=poly href=/new-docs/xen-api/classes/host_cpu title="a physical CPU on a host" alt coords="206,164 204,158 199,153 191,148 182,145 171,144 160,145 151,148 143,153 138,158 136,164 138,171 143,176 151,181 160,183 171,184 182,183 191,181 199,176 204,171"><area shape=rect href=/new-docs/xen-api/classes/network title="an ethernet network" alt coords=331,331,391,371><area shape=rect href=/new-docs/xen-api/classes/vif title="a network interface for a Virtual Machine" alt coords=423,334,483,374><area shape=poly href=/new-docs/xen-api/classes/vif_metrics title="IO stats and configuration information for a virtual interface" alt coords="498,451 496,445 490,439 481,435 470,432 457,431 444,432 432,435 423,439 417,445 415,451 417,457 423,463 432,467 444,470 457,471 470,470 481,467 490,463 496,457"><area shape=rect href=/new-docs/xen-api/classes/pif title="a network interface for a physical host" alt coords=241,306,301,346><area shape=poly href=/new-docs/xen-api/classes/pif_metrics title="IO stats and configuration information for a physical interface" alt coords="274,412 272,406 266,400 257,396 245,393 232,392 219,393 208,396 198,400 193,406 191,412 193,418 198,424 208,428 219,431 232,432 245,431 257,428 266,424 272,418"><area shape=rect href=/new-docs/xen-api/classes/sr title="Storage Repository, a container for virtual disk images (VDIs)" alt coords=323,97,383,137><area shape=rect href=/new-docs/xen-api/classes/vdi title="a virtual disk image" alt coords=412,124,472,164><area shape=poly href=/new-docs/xen-api/classes/sm title="storage manager plugin module" alt coords="377,26 375,19 371,14 364,9 356,7 347,6 337,7 329,9 322,14 318,19 317,26 318,32 322,37 329,42 337,45 347,46 356,45 364,42 371,37 375,32"><area shape=rect href=/new-docs/xen-api/classes/vbd title="a virtual block device" alt coords=486,181,546,221><area shape=poly href=/new-docs/xen-api/classes/vbd_metrics title="IO stats and configuration information for a virtual block device" alt coords="623,128 620,122 614,116 604,112 592,109 578,108 564,109 551,112 541,116 535,122 533,128 535,134 541,140 551,144 564,147 578,148 592,147 604,144 614,140 620,134"></map><h2>Classes, Fields and Messages</h2><p>Classes have both <i>fields</i> and <i>messages.</i> Messages are either <i>implicit</i> or <i>explicit</i> where an implicit message is one of:</p><ul><li>a constructor (usually called "create");</li><li>a destructor (usually called "destroy");</li><li>"get_by_name_label";</li><li>"get_by_uuid";</li><li>"get_record";</li><li>"get_all"; and</li><li>"get_all_records".</li></ul><p>Explicit messages include all the rest, more class-specific messages (e.g. "VM.start", "VM.clone")</p><p>Every field has at least one <i>accessor</i> depending both on its type and whether it is read-only or read-write. Accessors for a field named "X" would be a proper subset of:</p><ul><li>set_X: change the value of field X (only if it is read-write);</li><li>get_X: retrieve the value of field X;</li><li>add_X: add a key/value pair (for fields of type set);</li><li>remove_X: remove a key (for fields of type set);</li><li>add_to_X: add a key/value pair (for fields of type map); and</li><li>remove_from_X: remove a key (for fields of type map).</li></ul></div><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of XenAPI Reference</h1><article class=default><header class=headline></header><h1 id=auth>auth</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=blob>blob</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=bond>Bond</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=certificate>Certificate</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=cluster>Cluster</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=cluster_host>Cluster_host</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=console>console</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=crashdump>crashdump</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=data_source>data_source</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=dr_task>DR_task</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=event>event</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=feature>Feature</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=gpu_group>GPU_group</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=host>host</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=host_cpu>host_cpu</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=host_crashdump>host_crashdump</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=host_metrics>host_metrics</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=host_patch>host_patch</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lvhd>LVHD</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=message>message</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=network>network</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=network_sriov>network_sriov</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=observer>Observer</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pbd>PBD</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pci>PCI</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pgpu>PGPU</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pif>PIF</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pif_metrics>PIF_metrics</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pool>pool</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pool_patch>pool_patch</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pool_update>pool_update</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=probe_result>probe_result</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pusb>PUSB</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pvs_cache_storage>PVS_cache_storage</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pvs_proxy>PVS_proxy</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pvs_server>PVS_server</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pvs_site>PVS_site</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=repository>Repository</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=role>role</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=sdn_controller>SDN_controller</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=secret>secret</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=session>session</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=sm>SM</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=sr>SR</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=sr_stat>sr_stat</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=subject>subject</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=task>task</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=tunnel>tunnel</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=usb_group>USB_group</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=user>user</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vbd>VBD</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vbd_metrics>VBD_metrics</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vdi>VDI</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vdi_nbd_server_info>vdi_nbd_server_info</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vgpu>VGPU</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vgpu_type>VGPU_type</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vif>VIF</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vif_metrics>VIF_metrics</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vlan>VLAN</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vm>VM</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vm_appliance>VM_appliance</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vm_guest_metrics>VM_guest_metrics</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vm_metrics>VM_metrics</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vmpp>VMPP</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vmss>VMSS</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vtpm>VTPM</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vusb>VUSB</h1><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=xenapi-releases>XenAPI Releases</h1><ul class="children children-li children-sort-"><li><a href=/new-docs/xen-api/releases/24.16.0/index.html>XAPI 24.16.0</a></li><li><a href=/new-docs/xen-api/releases/24.14.0/index.html>XAPI 24.14.0</a></li><li><a href=/new-docs/xen-api/releases/24.10.0/index.html>XAPI 24.10.0</a></li><li><a href=/new-docs/xen-api/releases/24.3.0/index.html>XAPI 24.3.0</a></li><li><a href=/new-docs/xen-api/releases/24.0.0/index.html>XAPI 24.0.0</a></li><li><a href=/new-docs/xen-api/releases/23.30.0/index.html>XAPI 23.30.0</a></li><li><a href=/new-docs/xen-api/releases/23.27.0/index.html>XAPI 23.27.0</a></li><li><a href=/new-docs/xen-api/releases/23.25.0/index.html>XAPI 23.25.0</a></li><li><a href=/new-docs/xen-api/releases/23.18.0/index.html>XAPI 23.18.0</a></li><li><a href=/new-docs/xen-api/releases/23.14.0/index.html>XAPI 23.14.0</a></li><li><a href=/new-docs/xen-api/releases/23.9.0/index.html>XAPI 23.9.0</a></li><li><a href=/new-docs/xen-api/releases/23.1.0/index.html>XAPI 23.1.0</a></li><li><a href=/new-docs/xen-api/releases/22.37.0/index.html>XAPI 22.37.0</a></li><li><a href=/new-docs/xen-api/releases/22.33.0/index.html>XAPI 22.33.0</a></li><li><a href=/new-docs/xen-api/releases/22.27.0/index.html>XAPI 22.27.0</a></li><li><a href=/new-docs/xen-api/releases/22.26.0/index.html>XAPI 22.26.0</a></li><li><a href=/new-docs/xen-api/releases/22.20.0/index.html>XAPI 22.20.0</a></li><li><a href=/new-docs/xen-api/releases/22.19.0/index.html>XAPI 22.19.0</a></li><li><a href=/new-docs/xen-api/releases/22.16.0/index.html>XAPI 22.16.0</a></li><li><a href=/new-docs/xen-api/releases/22.12.0/index.html>XAPI 22.12.0</a></li><li><a href=/new-docs/xen-api/releases/22.5.0/index.html>XAPI 22.5.0</a></li><li><a href=/new-docs/xen-api/releases/21.4.0/index.html>XAPI 21.4.0</a></li><li><a href=/new-docs/xen-api/releases/21.3.0/index.html>XAPI 21.3.0</a></li><li><a href=/new-docs/xen-api/releases/21.2.0/index.html>XAPI 21.2.0</a></li><li><a href=/new-docs/xen-api/releases/1.329.0/index.html>XAPI 1.329.0</a></li><li><a href=/new-docs/xen-api/releases/1.318.0/index.html>XAPI 1.318.0</a></li><li><a href=/new-docs/xen-api/releases/1.313.0/index.html>XAPI 1.313.0</a></li><li><a href=/new-docs/xen-api/releases/1.307.0/index.html>XAPI 1.307.0</a></li><li><a href=/new-docs/xen-api/releases/1.304.0/index.html>XAPI 1.304.0</a></li><li><a href=/new-docs/xen-api/releases/1.303.0/index.html>XAPI 1.303.0</a></li><li><a href=/new-docs/xen-api/releases/1.301.0/index.html>XAPI 1.301.0</a></li><li><a href=/new-docs/xen-api/releases/1.298.0/index.html>XAPI 1.298.0</a></li><li><a href=/new-docs/xen-api/releases/1.297.0/index.html>XAPI 1.297.0</a></li><li><a href=/new-docs/xen-api/releases/1.294.0/index.html>XAPI 1.294.0</a></li><li><a href=/new-docs/xen-api/releases/1.290.0/index.html>XAPI 1.290.0</a></li><li><a href=/new-docs/xen-api/releases/1.271.0/index.html>XAPI 1.271.0</a></li><li><a href=/new-docs/xen-api/releases/1.257.0/index.html>XAPI 1.257.0</a></li><li><a href=/new-docs/xen-api/releases/1.250.0/index.html>XAPI 1.250.0</a></li><li><a href=/new-docs/xen-api/releases/nile-preview/index.html>XenServer 8 Preview</a></li><li><a href=/new-docs/xen-api/releases/stockholm_psr/index.html>Citrix Hypervisor 8.2 Hotfix 2</a></li><li><a href=/new-docs/xen-api/releases/stockholm/index.html>Citrix Hypervisor 8.2</a></li><li><a href=/new-docs/xen-api/releases/quebec/index.html>Citrix Hypervisor 8.1</a></li><li><a href=/new-docs/xen-api/releases/naples/index.html>Citrix Hypervisor 8.0</a></li><li><a href=/new-docs/xen-api/releases/lima/index.html>XenServer 7.6</a></li><li><a href=/new-docs/xen-api/releases/kolkata/index.html>XenServer 7.5</a></li><li><a href=/new-docs/xen-api/releases/jura/index.html>XenServer 7.4</a></li><li><a href=/new-docs/xen-api/releases/inverness/index.html>XenServer 7.3</a></li><li><a href=/new-docs/xen-api/releases/falcon/index.html>XenServer 7.2</a></li><li><a href=/new-docs/xen-api/releases/ely/index.html>XenServer 7.1</a></li><li><a href=/new-docs/xen-api/releases/dundee/index.html>XenServer 7.0</a></li><li><a href=/new-docs/xen-api/releases/indigo/index.html>XenServer 6.5 SP1 Hotfix 31</a></li><li><a href=/new-docs/xen-api/releases/cream/index.html>XenServer 6.5 SP1</a></li><li><a href=/new-docs/xen-api/releases/creedence/index.html>XenServer 6.5</a></li><li><a href=/new-docs/xen-api/releases/clearwater-whetstone/index.html>XenServer 6.2 SP1 Hotfix 11</a></li><li><a href=/new-docs/xen-api/releases/clearwater-felton/index.html>XenServer 6.2 SP1 Hotfix 4</a></li><li><a href=/new-docs/xen-api/releases/vgpu-productisation/index.html>XenServer 6.2 SP1</a></li><li><a href=/new-docs/xen-api/releases/vgpu-tech-preview/index.html>XenServer 6.2 SP1 Tech-Preview</a></li><li><a href=/new-docs/xen-api/releases/clearwater/index.html>XenServer 6.2</a></li><li><a href=/new-docs/xen-api/releases/tampa/index.html>XenServer 6.1</a></li><li><a href=/new-docs/xen-api/releases/boston/index.html>XenServer 6.0</a></li><li><a href=/new-docs/xen-api/releases/cowley/index.html>XenServer 5.6 FP1</a></li><li><a href=/new-docs/xen-api/releases/midnight-ride/index.html>XenServer 5.6</a></li><li><a href=/new-docs/xen-api/releases/george/index.html>XenServer 5.5</a></li><li><a href=/new-docs/xen-api/releases/orlando-update-1/index.html>XenServer 5.0 Update 1</a></li><li><a href=/new-docs/xen-api/releases/orlando/index.html>XenServer 5.0</a></li><li><a href=/new-docs/xen-api/releases/symc/index.html>XenServer 4.1.1</a></li><li><a href=/new-docs/xen-api/releases/miami/index.html>XenServer 4.1</a></li><li><a href=/new-docs/xen-api/releases/rio/index.html>XenServer 4.0</a></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of XenAPI Releases</h1><article class=default><header class=headline></header><h1 id=xapi-24160>XAPI 24.16.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-24140>XAPI 24.14.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-24100>XAPI 24.10.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2430>XAPI 24.3.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2400>XAPI 24.0.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-23300>XAPI 23.30.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-23270>XAPI 23.27.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-23250>XAPI 23.25.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-23180>XAPI 23.18.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-23140>XAPI 23.14.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2390>XAPI 23.9.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2310>XAPI 23.1.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22370>XAPI 22.37.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22330>XAPI 22.33.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22270>XAPI 22.27.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22260>XAPI 22.26.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22200>XAPI 22.20.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22190>XAPI 22.19.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22160>XAPI 22.16.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-22120>XAPI 22.12.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2250>XAPI 22.5.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2140>XAPI 21.4.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2130>XAPI 21.3.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-2120>XAPI 21.2.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-13290>XAPI 1.329.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-13180>XAPI 1.318.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-13130>XAPI 1.313.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-13070>XAPI 1.307.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-13040>XAPI 1.304.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-13030>XAPI 1.303.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-13010>XAPI 1.301.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-12980>XAPI 1.298.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-12970>XAPI 1.297.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-12940>XAPI 1.294.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-12900>XAPI 1.290.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-12710>XAPI 1.271.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-12570>XAPI 1.257.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xapi-12500>XAPI 1.250.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-8-preview>XenServer 8 Preview</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=citrix-hypervisor-82-hotfix-2>Citrix Hypervisor 8.2 Hotfix 2</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=citrix-hypervisor-82>Citrix Hypervisor 8.2</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=citrix-hypervisor-81>Citrix Hypervisor 8.1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=citrix-hypervisor-80>Citrix Hypervisor 8.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-76>XenServer 7.6</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-75>XenServer 7.5</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-74>XenServer 7.4</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-73>XenServer 7.3</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-72>XenServer 7.2</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-71>XenServer 7.1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-70>XenServer 7.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-65-sp1-hotfix-31>XenServer 6.5 SP1 Hotfix 31</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-65-sp1>XenServer 6.5 SP1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-65>XenServer 6.5</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-62-sp1-hotfix-11>XenServer 6.2 SP1 Hotfix 11</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-62-sp1-hotfix-4>XenServer 6.2 SP1 Hotfix 4</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-62-sp1>XenServer 6.2 SP1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-62-sp1-tech-preview>XenServer 6.2 SP1 Tech-Preview</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-62>XenServer 6.2</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-61>XenServer 6.1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-60>XenServer 6.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-56-fp1>XenServer 5.6 FP1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-56>XenServer 5.6</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-55>XenServer 5.5</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-50-update-1>XenServer 5.0 Update 1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-50>XenServer 5.0</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-411>XenServer 4.1.1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-41>XenServer 4.1</h1><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenserver-40>XenServer 4.0</h1><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=topics>Topics</h1><ul class="children children-li children-sort-"><li><a href=/new-docs/xen-api/topics/udhcp/index.html>API for configuring the udhcp server in Dom0</a></li><li><a href=/new-docs/xen-api/topics/guest-agents/index.html>Guest agents</a></li><li><a href=/new-docs/xen-api/topics/memory/index.html>Memory</a></li><li><a href=/new-docs/xen-api/topics/metrics/index.html>Metrics</a></li><li><a href=/new-docs/xen-api/topics/snapshots/index.html>Snapshots</a></li><li><a href=/new-docs/xen-api/topics/consoles/index.html>VM consoles</a></li><li><a href=/new-docs/xen-api/topics/importexport/index.html>VM import/export</a></li><li><a href=/new-docs/xen-api/topics/vm-lifecycle/index.html>VM Lifecycle</a></li><li><a href=/new-docs/xen-api/topics/xencenter/index.html>XenCenter</a></li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Topics</h1><article class=default><header class=headline></header><h1 id=api-for-configuring-the-udhcp-server-in-dom0>API for configuring the udhcp server in Dom0</h1><p>This API allows you to configure the DHCP service running on the Host
Internal Management Network (HIMN). The API configures a udhcp daemon
residing in Dom0 and alters the service configuration for any VM using
the network.</p><p>It should be noted that for this reason, that callers who modify the
default configuration should be aware that their changes may have an
adverse affect on other consumers of the HIMN.</p><h2 id=version-history>Version history</h2><pre><code>Date        State
----        ----
2013-3-15   Stable
</code></pre><p><em>Stable</em>: this API is considered stable and unlikely to change between
software version and between hotfixes.</p><h2 id=api-description>API description</h2><p>The API for configuring the network is based on a series of other_config
keys that can be set by the caller on the HIMN XAPI network object. Once
any of the keys below have been set, the caller must ensure that any VIFs
attached to the HIMN are removed, destroyed, created and plugged.</p><pre><code>ip_begin
</code></pre><p>The first IP address in the desired subnet that the caller wishes the
DHCP service to use.</p><pre><code>ip_end
</code></pre><p>The last IP address in the desired subnet that the caller wishes the
DHCP service to use.</p><pre><code>netmask
</code></pre><p>The subnet mask for each of the issues IP addresses.</p><pre><code>ip_disable_gw
</code></pre><p>A boolean key for disabling the DHCP server from returning a default
gateway for VMs on the network. To disable returning the gateway address
set the key to True.</p><p><em>Note</em>: By default, the DHCP server will issue a default gateway for
those requesting an address. Setting this key may disrupt applications
that require the default gateway for communicating with Dom0 and so
should be used with care.</p><h2 id=example-code>Example code</h2><p>An example python extract of setting the config for the network:</p><pre><code>def get_himn_ref():
    networks = session.xenapi.network.get_all_records()
    for ref, rec in networks.iteritems():
        if 'is_host_internal_management_network' \
                                        in rec['other_config']:                                            
            return ref

    raise Exception(&quot;Error: unable to find HIMN.&quot;)


himn_ref = get_himn_ref()
other_config = session.xenapi.network.get_other_config(himn_ref)

other_config['ip_begin'] = &quot;169.254.0.1&quot;
other_config['ip_end'] = &quot;169.254.255.254&quot;
other_config['netmask'] = &quot;255.255.0.0&quot;

session.xenapi.network.set_other_config(himn_ref, other_config)
</code></pre><p>An example for how to disable the server returning a default gateway:</p><pre><code>himn_ref = get_himn_ref()
other_config = session.xenapi.network.get_other_config(himn_ref)

other_config['ip_disable_gw'] = True

session.xenapi.network.set_other_config(himn_ref, other_config)
</code></pre><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=guest-agents>Guest agents</h1><p>&ldquo;Guest agents&rdquo; are special programs which run inside VMs which can be controlled
via the XenAPI.</p><p>One communication method between XenAPI clients is via Xenstore.</p><h2 id=adding-xenstore-entries-to-vms>Adding Xenstore entries to VMs</h2><p>Developers may wish to install guest agents into VMs which take special action based on the type of the VM. In order to communicate this information into the guest, a special Xenstore name-space known as <code>vm-data</code> is available which is populated at VM creation time. It is populated from the <code>xenstore-data</code> map in the VM record.</p><p>Set the <code>xenstore-data</code> parameter in the VM record:</p><pre><code>xe vm-param-set uuid= xenstore-data:vm-data/foo=bar
</code></pre><p>Start the VM.</p><p>If it is a Linux-based VM, install the COMPANY_TOOLS and use the <code>xenstore-read</code> to verify that the node exists in Xenstore.</p><blockquote><p><strong>Note</strong></p><p>Only prefixes beginning with <code>vm-data</code> are permitted, and anything not in this name-space will be silently ignored when starting the VM.</p></blockquote><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=memory>Memory</h1><p>Memory is used for many things:</p><ul><li>the hypervisor code: this is the Xen executable itself</li><li>the hypervisor heap: this is needed for per-domain structures and per-vCPU
structures</li><li>the crash kernel: this is needed to collect information after a host crash</li><li>domain RAM: this is the memory the VM believes it has</li><li>shadow memory: for HVM guests running on hosts without hardware assisted
paging (HAP) Xen uses shadow to optimise page table updates. For all guests
shadow is used during live migration for tracking the memory transfer.</li><li>video RAM for the virtual graphics card</li></ul><p>Some of these are constants (e.g. hypervisor code) while some depend on the VM
configuration (e.g. domain RAM). Xapi calls the constants &ldquo;host overhead&rdquo; and
the variables due to VM configuration as &ldquo;VM overhead&rdquo;.
These overheads are subtracted from free memory on the host when starting,
resuming and migrating VMs.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=metrics>Metrics</h1><p><a href=https://github.com/xapi-project/xen-api/ocaml/xcp-rrdd target=_blank>xcp-rrdd</a>
records statistics about the host and the VMs running on top.
The metrics are stored persistently for long-term access and analysis of
historical trends.
Statistics are stored in <a href=http://oss.oetiker.ch/rrdtool/ target=_blank>RRDs</a> (Round Robin
Databases).
RRDs are fixed-size structures that store time series with decreasing time
resolution: the older the data point is, the longer the timespan it represents.
&lsquo;Data sources&rsquo; are sampled every few seconds and points are added to
the highest resolution RRD. Periodically each high-frequency RRD is
&lsquo;consolidated&rsquo; (e.g. averaged) to produce a data point for a lower-frequency
RRD.</p><p>RRDs are resident on the host on which the VM is running, or the pool
coordinator when the VM is not running.
The RRDs are backed up every day.</p><h2 id=granularity>Granularity</h2><p>Statistics are persisted for a maximum of one year, and are stored at
different granularities.
The average and most recent values are stored at intervals of:</p><ul><li>five seconds for the past ten minutes</li><li>one minute for the past two hours</li><li>one hour for the past week</li><li>one day for the past year</li></ul><p>RRDs are saved to disk as uncompressed XML. The size of each RRD when
written to disk ranges from 200KiB to approximately 1.2MiB when the RRD
stores the full year of statistics.</p><p>By default each RRD contains only averaged data to save storage space.
To record minimum and maximum values in future RRDs, set the Pool-wide flag</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>xe pool-param-set uuid<span style=color:#f92672>=</span> other-config:create_min_max_in_new_VM_RRDs<span style=color:#f92672>=</span>true</span></span></code></pre></div><h1 id=downloading>Downloading</h1><p>Statistics can be downloaded over HTTP in XML or JSON format, for example
using <code>wget</code>.
See <a href=http://oss.oetiker.ch/rrdtool/doc/rrddump.en.html target=_blank>rrddump</a> and
<a href=http://oss.oetiker.ch/rrdtool/doc/rrdxport.en.html target=_blank>rrdxport</a> for information
about the XML format.
The JSON format has the same structure as the XML.
Parameters are appended to the URL following a question mark (?) and separated
by ampersands (&).
HTTP authentication can take the form of a username and password or a session
token in a URL parameter.</p><p>Statistics may be downloaded all at once, including all history, or as
deltas suitable for interactive graphing.</p><h2 id=downloading-statistics-all-at-once>Downloading statistics all at once</h2><p>To obtain a full dump of RRD data for a host use:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>wget  http://hostname/host_rrd?session_id<span style=color:#f92672>=</span>OpaqueRef:43df3204-9360-c6ab-923e-41a8d19389ba<span style=color:#e6db74>&#34;</span></span></span></code></pre></div><p>where the session token has been fetched from the server using the API.</p><p>For example, using Python&rsquo;s <a href=https://pypi.org/project/XenAPI/ target=_blank>XenAPI</a> library:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> XenAPI
</span></span><span style=display:flex><span>username <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;root&#34;</span>
</span></span><span style=display:flex><span>password <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;actual_password&#34;</span>
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;http://hostname&#34;</span>
</span></span><span style=display:flex><span>session <span style=color:#f92672>=</span> XenAPI<span style=color:#f92672>.</span>Session(url)
</span></span><span style=display:flex><span>session<span style=color:#f92672>.</span>xenapi<span style=color:#f92672>.</span>login_with_password(username, password, <span style=color:#e6db74>&#34;1.0&#34;</span>, <span style=color:#e6db74>&#34;session_getter&#34;</span>)
</span></span><span style=display:flex><span>session<span style=color:#f92672>.</span>_session</span></span></code></pre></div><p>A URL parameter is used to decide which format to return: XML is returned by
default, adding the parameter <code>json</code> makes the server return JSON.
Starting from xapi version 23.17.0, the server uses the HTTP header <code>Accept</code>
to decide which format to return.
When both formats are accepted, for example, using <code>*/*</code>; JSON is returned.
Of interest are the clients wget and curl which use this accept header value,
meaning that when using them the default behaviour will change and the accept
header needs to be overridden to make the server return XML.
The content type is provided in the reponse&rsquo;s headers in these newer versions.</p><p>The XML RRD data is in the format used by rrdtool and looks like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#75715e>&lt;?xml version=&#34;1.0&#34;?&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;rrd&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;version&gt;</span>0003<span style=color:#f92672>&lt;/version&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;step&gt;</span>5<span style=color:#f92672>&lt;/step&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;lastupdate&gt;</span>1213616574<span style=color:#f92672>&lt;/lastupdate&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;ds&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;name&gt;</span>memory_total_kib<span style=color:#f92672>&lt;/name&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;type&gt;</span>GAUGE<span style=color:#f92672>&lt;/type&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;minimal_heartbeat&gt;</span>300.0000<span style=color:#f92672>&lt;/minimal_heartbeat&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;min&gt;</span>0.0<span style=color:#f92672>&lt;/min&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;max&gt;</span>Infinity<span style=color:#f92672>&lt;/max&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;last_ds&gt;</span>2070172<span style=color:#f92672>&lt;/last_ds&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;value&gt;</span>9631315.6300<span style=color:#f92672>&lt;/value&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;unknown_sec&gt;</span>0<span style=color:#f92672>&lt;/unknown_sec&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;/ds&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;ds&gt;</span>
</span></span><span style=display:flex><span>   <span style=color:#75715e>&lt;!-- other dss - the order of the data sources is important
</span></span></span><span style=display:flex><span><span style=color:#75715e>        and defines the ordering of the columns in the archives below --&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;/ds&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;rra&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;cf&gt;</span>AVERAGE<span style=color:#f92672>&lt;/cf&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;pdp_per_row&gt;</span>1<span style=color:#f92672>&lt;/pdp_per_row&gt;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>&lt;params&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;xff&gt;</span>0.5000<span style=color:#f92672>&lt;/xff&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/params&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;cdp_prep&gt;</span> <span style=color:#75715e>&lt;!-- This is for internal use --&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;ds&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;primary_value&gt;</span>0.0<span style=color:#f92672>&lt;/primary_value&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;secondary_value&gt;</span>0.0<span style=color:#f92672>&lt;/secondary_value&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;value&gt;</span>0.0<span style=color:#f92672>&lt;/value&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;unknown_datapoints&gt;</span>0<span style=color:#f92672>&lt;/unknown_datapoints&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;/ds&gt;</span>
</span></span><span style=display:flex><span>      ...other dss - internal use only...
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/cdp_prep&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;database&gt;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>&lt;row&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>2070172.0000<span style=color:#f92672>&lt;/v&gt;</span>  <span style=color:#75715e>&lt;!-- columns correspond to the DSs defined above --&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>1756408.0000<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>732.2130<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>782.9186<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>647.0431<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0001<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0268<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0100<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>&lt;v&gt;</span>615.1072<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>&lt;/row&gt;</span>
</span></span><span style=display:flex><span>     ...
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;/rra&gt;</span>
</span></span><span style=display:flex><span>  ... other archives ...
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;/rrd&gt;</span></span></span></code></pre></div><p>To obtain a full dump of RRD data of a VM with uuid <code>x</code>:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>wget <span style=color:#e6db74>&#34;http://hostname/vm_rrd?session_id=&lt;token&gt;&amp;uuid=x&#34;</span></span></span></code></pre></div><p>Note that it is quite expensive to download full RRDs as they contain
lots of historical information. For interactive displays clients should
download deltas instead.</p><h2 id=downloading-deltas>Downloading deltas</h2><p>To obtain an update of all VM statistics on a host, the URL would be of
the form:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>wget <span style=color:#e6db74>&#34;https://hostname/rrd_updates?session_id=&lt;token&gt;&amp;start=&lt;secondsinceepoch&gt;&#34;</span></span></span></code></pre></div><p>This request returns data in an rrdtool <code>xport</code> style XML format, for every VM
resident on the particular host that is being queried.
To differentiate which column in the export is associated with which VM, the
<code>legend</code> field is prefixed with the UUID of the VM.</p><p>An example <code>rrd_updates</code> output:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;xport&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;meta&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;start&gt;</span>1213578000<span style=color:#f92672>&lt;/start&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;step&gt;</span>3600<span style=color:#f92672>&lt;/step&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;end&gt;</span>1213617600<span style=color:#f92672>&lt;/end&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;rows&gt;</span>12<span style=color:#f92672>&lt;/rows&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;columns&gt;</span>12<span style=color:#f92672>&lt;/columns&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;legend&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>AVERAGE:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu1<span style=color:#f92672>&lt;/entry&gt;</span> <span style=color:#75715e>&lt;!-- nb - each data source might have multiple entries for different consolidation functions --&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>AVERAGE:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu0<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>AVERAGE:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:memory<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>MIN:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu1<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>MIN:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu0<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>MIN:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:memory<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>MAX:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu1<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>MAX:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu0<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>MAX:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:memory<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>LAST:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu1<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>LAST:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:cpu0<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;entry&gt;</span>LAST:vm:ecd8d7a0-1be3-4d91-bd0e-4888c0e30ab3:memory<span style=color:#f92672>&lt;/entry&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/legend&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;/meta&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;data&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;row&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;t&gt;</span>1213617600<span style=color:#f92672>&lt;/t&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span> <span style=color:#75715e>&lt;!-- once again, the order or the columns is defined by the legend above --&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0282<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>209715200.0000<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0201<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>209715200.0000<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0445<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>209715200.0000<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>0.0243<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>&lt;v&gt;</span>209715200.0000<span style=color:#f92672>&lt;/v&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;/row&gt;</span>
</span></span><span style=display:flex><span>   ...
</span></span><span style=display:flex><span>  <span style=color:#f92672>&lt;/data&gt;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;/xport&gt;</span></span></span></code></pre></div><p>To obtain host updates too, use the query parameter <code>host=true</code>:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>wget <span style=color:#e6db74>&#34;http://hostname/rrd_updates?session_id=&lt;token&gt;&amp;start=&lt;secondssinceepoch&gt;&amp;host=true&#34;</span></span></span></code></pre></div><p>The step will decrease as the period decreases, which means that if you
request statistics for a shorter time period you will get more detailed
statistics.</p><p>To download updates containing only the averages, or minimums or maximums,
add the parameter <code>cf=AVERAGE|MIN|MAX</code> (note case is important) e.g.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>wget <span style=color:#e6db74>&#34;http://hostname/rrd_updates?session_id=&lt;token&gt;&amp;start=0&amp;cf=MAX&#34;</span></span></span></code></pre></div><p>To request a different update interval, add the parameter <code>interval=seconds</code> e.g.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>wget <span style=color:#e6db74>&#34;http://hostname/rrd_updates?session_id=&lt;token&gt;&amp;start=0&amp;interval=5&#34;</span></span></span></code></pre></div><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=snapshots>Snapshots</h1><p>Snapshots represent the state of a VM, or a disk (VDI) at a point in time.
They can be used for:</p><ul><li>backups (hourly, daily, weekly etc)</li><li>experiments (take snapshot, try something, revert back again)</li><li>golden images (install OS, get it just right, clone it 1000s of times)</li></ul><p>Read more about <a href=/new-docs/xen-api/topics/snapshots/../features/snapshots/snapshots.html>Snapshots: the High-Level Feature</a>.</p><h1 id=taking-a-vdi-snapshot>Taking a VDI snapshot</h1><p>To take a snapshot of a single disk (VDI):</p><div class="wrap-code highlight"><pre tabindex=0><code>snapshot_vdi &lt;- VDI.snapshot(session_id, vdi, driver_params)</code></pre></div><p>where <code>vdi</code> is the reference to the disk to be snapshotted, and <code>driver_params</code>
is a list of string pairs providing optional backend implementation-specific hints.
The snapshot operation should be quick (i.e. it should never be implemented as
a slow disk copy) and the resulting VDI will have</p><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td>is_a_snapshot</td><td>a flag, set to true, indicating the disk is a snapshot</td></tr><tr><td>snapshot_of</td><td>a reference to the disk the snapshot was created from</td></tr><tr><td>snapshot_time</td><td>the time the snapshot was taken</td></tr></tbody></table><p>The resulting snapshot should be considered read-only. Depending on the backend
implementation it may be technically possible to write to the snapshot, but clients
must not do this. To create a writable disk from a snapshot, see &ldquo;restoring from
a snapshot&rdquo; below.</p><p>Note that the storage backend is free to implement this in different ways. We
do not assume the presence of a .vhd-formatted storage repository. Clients
must never assume anything about the backend implementation without checking
first with the maintainers of the backend implementation.</p><h1 id=restoring-to-a-vdi-snapshot>Restoring to a VDI snapshot</h1><p>To restore from a VDI snapshot first</p><div class="wrap-code highlight"><pre tabindex=0><code>new_vdi &lt;- VDI.clone(session_id, snapshot_vdi, driver_params)</code></pre></div><p>where <code>snapshot_vdi</code> is a reference to the snapshot VDI, and <code>driver_params</code>
is a list of string pairs providing optional backend implementation-specific hints.
The clone operation should be quick (i.e. it should never be implemented as
a slow disk copy) and the resulting VDI will have</p><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td>is_a_snapshot</td><td>a flag, set to false, indicating the disk is not a snapshot</td></tr><tr><td>snapshot_of</td><td>an invalid reference</td></tr><tr><td>snapshot_time</td><td>an invalid time</td></tr></tbody></table><p>The resulting disk is writable and can be used by the client as normal.</p><p>Note that the &ldquo;restored&rdquo; VDI will have a different <code>VDI.uuid</code> and reference to
the original VDI.</p><h1 id=taking-a-vm-snapshot>Taking a VM snapshot</h1><p>A VM snapshot is a copy of the VM metadata and a snapshot of all the associated
VDIs at around the same point in time. To take a VM snapshot:</p><div class="wrap-code highlight"><pre tabindex=0><code>snapshot_vm &lt;- VM.snapshot(session_id, vm, new_name)</code></pre></div><p>where <code>vm</code> is a reference to the existing VM and <code>new_name</code> will be the <code>name_label</code>
of the resulting VM (snapshot) object. The resulting VM will have</p><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td>is_a_snapshot</td><td>a flag, set to true, indicating the VM is a snapshot</td></tr><tr><td>snapshot_of</td><td>a reference to the VM the snapshot was created from</td></tr><tr><td>snapshot_time</td><td>the time the snapshot was taken</td></tr></tbody></table><p>Note that each disk is snapshotted one-by-one and not at the same time.</p><h1 id=restoring-to-a-vm-snapshot>Restoring to a VM snapshot</h1><p>A VM snapshot can be reverted to a snapshot using</p><div class="wrap-code highlight"><pre tabindex=0><code>VM.revert(session_id, snapshot_ref)</code></pre></div><p>where <code>snapshot_ref</code> is a reference to the snapshot VM. Each VDI associated with
the VM before the snapshot will be destroyed and each VDI associated with the
snapshot will be cloned (see &ldquo;Reverting to a disk snapshot&rdquo; above) and associated
with the VM. The resulting VM will have</p><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td>is_a_snapshot</td><td>a flag, set to false, indicating the VM is not a snapshot</td></tr><tr><td>snapshot_of</td><td>an invalid reference</td></tr><tr><td>snapshot_time</td><td>an invalid time</td></tr></tbody></table><p>Note that the <code>VM.uuid</code> and reference are preserved, but the <code>VDI.uuid</code> and
VDI references are not.</p><h1 id=downloading-a-disk-or-snapshot>Downloading a disk or snapshot</h1><p>Disks can be downloaded in either raw or vhd format using an HTTP 1.0 GET
request as follows:</p><div class="wrap-code highlight"><pre tabindex=0><code>GET /export_raw_vdi?session_id=%s&amp;task_id=%s&amp;vdi=%s&amp;format=%s[&amp;base=%s] HTTP/1.0\r\n
Connection: close\r\n
\r\n
\r\n</code></pre></div><p>where</p><ul><li><code>session_id</code> is a currently logged-in session</li><li><code>task_id</code> is a <code>Task</code> reference which will be used to monitor the
progress of this task and receive errors from it</li><li><code>vdi</code> is the reference of the <code>VDI</code> into which the data will be
imported</li><li><code>format</code> is either <code>vhd</code> or <code>raw</code></li><li>(optional) <code>base</code> is the reference of a <code>VDI</code> which has already been
exported and this export should only contain the blocks which have changed
since then.</li></ul><p>Note that the vhd format allows the disk to be sparse i.e. only contain allocated
blocks. This helps reduce the size of the download.</p><p>The xapi-project/xen-api repo has a
<a href=https://github.com/xapi-project/xen-api/blob/19afd3dfe8883814e525ce7ce39c8c959ce3c924/scripts/examples/python/exportimport.py#L32 target=_blank>python download example</a></p><h1 id=uploading-a-disk-or-snapshot>Uploading a disk or snapshot</h1><p>Disks can be uploaded in either raw or vhd format using an HTTP 1.0 PUT
request as follows:</p><div class="wrap-code highlight"><pre tabindex=0><code>PUT /import_raw_vdi?session_id=%s&amp;task_id=%s&amp;vdi=%s&amp;format=%s HTTP/1.0\r\n
Connection: close\r\n
\r\n
\r\n</code></pre></div><p>where</p><ul><li><code>session_id</code> is a currently logged-in session</li><li><code>task_id</code> is a <code>Task</code> reference which will be used to monitor the
progress of this task and receive errors from it</li><li><code>vdi</code> is the reference of the <code>VDI</code> into which the data will be
imported</li><li><code>format</code> is either <code>vhd</code> or <code>raw</code></li></ul><p>Note that you must create the disk (with the correct size) before importing
data to it. The disk doesn&rsquo;t have to be empty, in fact if restoring from a
series of incremental downloads it makes sense to upload them all to the
same disk in order.</p><h1 id=example-incremental-backup-with-xe>Example: incremental backup with xe</h1><p>This section will show how easy it is to build an incremental backup
tool using these APIs. For simplicity we will use the <code>xe</code> commands
rather than raw XMLRPC and HTTP.</p><p>For a VDI with uuid $VDI, take a snapshot:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>FULL<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>xe vdi-snapshot uuid<span style=color:#f92672>=</span>$VDI<span style=color:#66d9ef>)</span></span></span></code></pre></div><p>Next perform a full backup into a file &ldquo;full.vhd&rdquo;, in vhd format:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>xe vdi-export uuid<span style=color:#f92672>=</span>$FULL filename<span style=color:#f92672>=</span>full.vhd format<span style=color:#f92672>=</span>vhd  --progress</span></span></code></pre></div><p>If the SR was using the vhd format internally (this is the default)
then the full backup will be sparse and will only contain blocks if they
have been written to.</p><p>After some time has passed and the VDI has been written to, take another
snapshot:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>DELTA<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>xe vdi-snapshot uuid<span style=color:#f92672>=</span>$VDI<span style=color:#66d9ef>)</span></span></span></code></pre></div><p>Now we can backup only the disk blocks which have changed between the original
snapshot $FULL and the next snapshot $DELTA into a file called &ldquo;delta.vhd&rdquo;:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>xe vdi-export uuid<span style=color:#f92672>=</span>$DELTA filename<span style=color:#f92672>=</span>delta.vhd format<span style=color:#f92672>=</span>vhd base<span style=color:#f92672>=</span>$FULL --progress</span></span></code></pre></div><p>We now have 2 files on the local system:</p><ul><li>&ldquo;full.vhd&rdquo;: a complete backup of the first snapshot</li><li>&ldquo;delta.vhd&rdquo;: an incremental backup of the second snapshot, relative to
the first</li></ul><p>For example:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>test $ ls -lh *.vhd
</span></span><span style=display:flex><span>-rw------- <span style=color:#ae81ff>1</span> dscott xendev 213M Aug <span style=color:#ae81ff>15</span> 10:39 delta.vhd
</span></span><span style=display:flex><span>-rw------- <span style=color:#ae81ff>1</span> dscott xendev 8.0G Aug <span style=color:#ae81ff>15</span> 10:39 full.vhd</span></span></code></pre></div><p>To restore the original snapshot you must create an empty disk with the
correct size. To find the size of a .vhd file use <code>qemu-img</code> as follows:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>test $ qemu-img info delta.vhd
</span></span><span style=display:flex><span>image: delta.vhd
</span></span><span style=display:flex><span>file format: vpc
</span></span><span style=display:flex><span>virtual size: 24G <span style=color:#f92672>(</span><span style=color:#ae81ff>25769705472</span> bytes<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>disk size: 212M</span></span></code></pre></div><p>Here the size is 25769705472 bytes.
Create a fresh VDI in SR $SR to restore the backup as follows:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>SIZE<span style=color:#f92672>=</span><span style=color:#ae81ff>25769705472</span>
</span></span><span style=display:flex><span>RESTORE<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>xe vdi-create name-label<span style=color:#f92672>=</span>restored virtual-size<span style=color:#f92672>=</span>$SIZE sr-uuid<span style=color:#f92672>=</span>$SR type<span style=color:#f92672>=</span>user<span style=color:#66d9ef>)</span></span></span></code></pre></div><p>then import &ldquo;full.vhd&rdquo; into it:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>xe vdi-import uuid<span style=color:#f92672>=</span>$RESTORE filename<span style=color:#f92672>=</span>full.vhd format<span style=color:#f92672>=</span>vhd --progress</span></span></code></pre></div><p>Once &ldquo;full.vhd&rdquo; has been imported, the incremental backup can be restored
on top:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>xe vdi-import uuid<span style=color:#f92672>=</span>$RESTORE filename<span style=color:#f92672>=</span>delta.vhd format<span style=color:#f92672>=</span>vhd --progress</span></span></code></pre></div><p>Note there is no need to supply a &ldquo;base&rdquo; parameter when importing; Xapi will
treat the &ldquo;vhd differencing disk&rdquo; as a set of blocks and import them. It
is up to you to check you are importing them to the right place.</p><p>Now the VDI $RESTORE should have the same contents as $DELTA.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vm-consoles>VM consoles</h1><p>Most XenAPI graphical interfaces will want to gain access to the VM consoles, in order to render them to the user as if they were physical machines. There are several types of consoles available, depending on the type of guest or if the physical host console is being accessed:</p><h1 id=types-of-consoles>Types of consoles</h1><table><thead><tr><th style=text-align:left>Operating System</th><th style=text-align:left>Text</th><th style=text-align:left>Graphical</th><th style=text-align:left>Optimized graphical</th></tr></thead><tbody><tr><td style=text-align:left>Windows</td><td style=text-align:left>No</td><td style=text-align:left>VNC, using an API call</td><td style=text-align:left>RDP, directly from guest</td></tr><tr><td style=text-align:left>Linux</td><td style=text-align:left>Yes, through VNC and an API call</td><td style=text-align:left>No</td><td style=text-align:left>VNC, directly from guest</td></tr><tr><td style=text-align:left>Physical Host</td><td style=text-align:left>Yes, through VNC and an API call</td><td style=text-align:left>No</td><td style=text-align:left>No</td></tr></tbody></table><p>Hardware-assisted VMs, such as Windows, directly provide a graphical console over VNC. There is no text-based console, and guest networking is not necessary to use the graphical console. Once guest networking has been established, it is more efficient to setup Remote Desktop Access and use an RDP client to connect directly (this must be done outside of the XenAPI).</p><p>Paravirtual VMs, such as Linux guests, provide a native text console directly. XenServer provides a utility (called <code>vncterm</code>) to convert this text-based console into a graphical VNC representation. Guest networking is not necessary for this console to function. As with Windows above, Linux distributions often configure VNC within the guest, and directly connect to it over a guest network interface.</p><p>The physical host console is only available as a <code>vt100</code> console, which is exposed through the XenAPI as a VNC console by using <code>vncterm</code> in the control domain.</p><p>RFB (Remote Framebuffer) is the protocol which underlies VNC, specified in <a href=http://www.realvnc.com/docs/rfbproto.pdf target=_blank>The RFB Protocol</a>. Third-party developers are expected to provide their own VNC viewers, and many freely available implementations can be adapted for this purpose. RFB 3.3 is the minimum version which viewers must support.</p><h1 id=retrieving-vnc-consoles-using-the-api>Retrieving VNC consoles using the API</h1><p>VNC consoles are retrieved using a special URL passed through to the host agent. The sequence of API calls is as follows:</p><ol><li><p>Client to Master/443: XML-RPC: <code>Session.login_with_password()</code>.</p></li><li><p>Master/443 to Client: Returns a session reference to be used with subsequent calls.</p></li><li><p>Client to Master/443: XML-RPC: <code>VM.get_by_name_label()</code>.</p></li><li><p>Master/443 to Client: Returns a reference to a particular VM (or the &ldquo;control domain&rdquo; if you want to retrieve the physical host console).</p></li><li><p>Client to Master/443: XML-RPC: <code>VM.get_consoles()</code>.</p></li><li><p>Master/443 to Client: Returns a list of console objects associated with the VM.</p></li><li><p>Client to Master/443: XML-RPC: <code>VM.get_location()</code>.</p></li><li><p>Returns a URI describing where the requested console is located. The URIs are of the form: <code>https://192.168.0.1/console?ref=OpaqueRef:c038533a-af99-a0ff-9095-c1159f2dc6a0</code>.</p></li><li><p>Client to 192.168.0.1: HTTP CONNECT &ldquo;/console?ref=(&mldr;)&rdquo;</p></li></ol><p>The final HTTP CONNECT is slightly non-standard since the HTTP/1.1 RFC specifies that it should only be a host and a port, rather than a URL. Once the HTTP connect is complete, the connection can subsequently directly be used as a VNC server without any further HTTP protocol action.</p><p>This scheme requires direct access from the client to the control domain&rsquo;s IP, and will not work correctly if there are Network Address Translation (NAT) devices blocking such connectivity. You can use the CLI to retrieve the console URI from the client and perform a connectivity check.</p><p>Retrieve the VM UUID by running:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ VM<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>xe vm-list params<span style=color:#f92672>=</span>uuid --minimal name-label<span style=color:#f92672>=</span>&lt;name&gt;<span style=color:#66d9ef>)</span></span></span></code></pre></div><p>Retrieve the console information:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ xe console-list vm-uuid<span style=color:#f92672>=</span>$VM
</span></span><span style=display:flex><span>uuid <span style=color:#f92672>(</span> RO<span style=color:#f92672>)</span>             : 8013b937-ff7e-60d1-ecd8-e52d66c5879e
</span></span><span style=display:flex><span>          vm-uuid <span style=color:#f92672>(</span> RO<span style=color:#f92672>)</span>: 2d7c558a-8f03-b1d0-e813-cbe7adfa534c
</span></span><span style=display:flex><span>    vm-name-label <span style=color:#f92672>(</span> RO<span style=color:#f92672>)</span>: <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>         protocol <span style=color:#f92672>(</span> RO<span style=color:#f92672>)</span>: RFB
</span></span><span style=display:flex><span>         location <span style=color:#f92672>(</span> RO<span style=color:#f92672>)</span>: https://10.80.228.30/console?uuid<span style=color:#f92672>=</span>8013b937-ff7e-60d1-ecd8-e52d66c5879e</span></span></code></pre></div><p>Use command-line utilities like <code>ping</code> to test connectivity to the IP address provided in the <code>location</code> field.</p><h1 id=disabling-vnc-forwarding-for-linux-vm>Disabling VNC forwarding for Linux VM</h1><p>When creating and destroying Linux VMs, the host agent automatically manages the <code>vncterm</code> processes which convert the text console into VNC. Advanced users who wish to directly access the text console can disable VNC forwarding for that VM. The text console can then only be accessed directly from the control domain directly, and graphical interfaces such as XenCenter will not be able to render a console for that VM.</p><p>Before starting the guest, set the following parameter on the VM record:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ xe vm-param-set uuid<span style=color:#f92672>=</span>$VM other-config:disable_pv_vnc<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span></span></span></code></pre></div><p>Start the VM.</p><p>Use the CLI to retrieve the underlying domain ID of the VM with:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ DOMID<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>xe vm-list params<span style=color:#f92672>=</span>dom-id uuid<span style=color:#f92672>=</span>$VM --minimal<span style=color:#66d9ef>)</span></span></span></code></pre></div><p>On the host console, connect to the text console directly by:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ /usr/lib/xen/bin/xenconsole $DOMID</span></span></code></pre></div><p>This configuration is an advanced procedure, and we do not recommend that the text console is directly used for heavy I/O operations. Instead, connect to the guest over SSH or some other network-based connection mechanism.</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vm-importexport>VM import/export</h1><p>VMs can be exported to a file and later imported to any Xapi host. The export
protocol is a simple HTTP(S) GET, which should be sent to the Pool master.
Authorization is either via a pre-created <code>session_id</code> or by HTTP basic
authentication (particularly useful on the command-line).
The VM to export is specified either by UUID or by reference. To keep track of
the export, a task can be created and passed in using its reference. Note that
Xapi may send an HTTP redirect if a different host has better access to the
disk data.</p><p>The following arguments are passed as URI query parameters or HTTP cookies:</p><table><thead><tr><th>Argument</th><th>Description</th></tr></thead><tbody><tr><td>session_id</td><td>the reference of the session being used to authenticate; required only when not using HTTP basic authentication</td></tr><tr><td>task_id</td><td>the reference of the task object with which to keep track of the operation; optional, required only if you have created a task object to keep track of the export</td></tr><tr><td>ref</td><td>the reference of the VM; required only if not using the UUID</td></tr><tr><td>uuid</td><td>the UUID of the VM; required only if not using the reference</td></tr><tr><td>use_compression</td><td>an optional boolean &ldquo;true&rdquo; or &ldquo;false&rdquo; (defaulting to &ldquo;false&rdquo;). If &ldquo;true&rdquo; then the output will be gzip-compressed before transmission.</td></tr></tbody></table><p>For example, using the Linux command line tool cURL:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ curl http://root:foo@myxenserver1/export?uuid<span style=color:#f92672>=</span>&lt;vm_uuid&gt; -o &lt;exportfile&gt;</span></span></code></pre></div><p>will export the specified VM to the file <code>exportfile</code>.</p><p>To export just the metadata, use the URI <code>http://server/export_metadata</code>.</p><p>The import protocol is similar, using HTTP(S) PUT. The <code>session_id</code> and <code>task_id</code> arguments are as for the export. The <code>ref</code> and <code>uuid</code> are not used; a new reference and uuid will be generated for the VM. There are some additional parameters:</p><table><thead><tr><th>Argument</th><th>Description</th></tr></thead><tbody><tr><td>restore</td><td>if <code>true</code>, the import is treated as replacing the original VM - the implication of this currently is that the MAC addresses on the VIFs are exactly as the export was, which will lead to conflicts if the original VM is still being run.</td></tr><tr><td>force</td><td>if <code>true</code>, any checksum failures will be ignored (the default is to destroy the VM if a checksum error is detected)</td></tr><tr><td>sr_id</td><td>the reference of an SR into which the VM should be imported. The default behavior is to import into the <code>Pool.default_SR</code></td></tr></tbody></table><p>Note there is no need to specify whether the export is compressed, as Xapi
will automatically detect and decompress gzip-encoded streams.</p><p>For example, again using cURL:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl -T &lt;exportfile&gt; http://root:foo@myxenserver2/import</span></span></code></pre></div><p>will import the VM to the default SR on the server.</p><blockquote><p><strong>Note</strong></p><p>Note that if no default SR has been set, and no <code>sr_uuid</code> is specified, the error message <code>DEFAULT_SR_NOT_FOUND</code> is returned.</p></blockquote><p>Another example:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl -T &lt;exportfile&gt; http://root:foo@myxenserver2/import?sr_id<span style=color:#f92672>=</span>&lt;ref_of_sr&gt;</span></span></code></pre></div><p>will import the VM to the specified SR on the server.</p><p>To import just the metadata, use the URI <code>http://server/import_metadata</code></p><h1 id=legacy-vm-import-format>Legacy VM Import Format</h1><p>This section describes the legacy VM import/export format and is for historical
interest only. It should be updated to describe the current format, see
<a href=https://github.com/xapi-project/xapi-project.github.io/issues/64 target=_blank>issue 64</a></p><p>Xapi supports a human-readable legacy VM input format called XVA. This section describes the syntax and structure of XVA.</p><p>An XVA consists of a directory containing XML metadata and a set of disk images. A VM represented by an XVA is not intended to be directly executable. Data within an XVA package is compressed and intended for either archiving on permanent storage or for being transmitted to a VM server - such as a XenServer host - where it can be decompressed and executed.</p><p>XVA is a hypervisor-neutral packaging format; it should be possible to create simple tools to instantiate an XVA VM on any other platform. XVA does not specify any particular runtime format; for example disks may be instantiated as file images, LVM volumes, QCoW images, VMDK or VHD images. An XVA VM may be instantiated any number of times, each instantiation may have a different runtime format.</p><p>XVA does not:</p><ul><li><p>specify any particular serialization or transport format</p></li><li><p>provide any mechanism for customizing VMs (or templates) on install</p></li><li><p>address how a VM may be upgraded post-install</p></li><li><p>define how multiple VMs, acting as an appliance, may communicate</p></li></ul><p>These issues are all addressed by the related Open Virtual Appliance specification.</p><p>An XVA is a directory containing, at a minimum, a file called <code>ova.xml</code>. This file describes the VM contained within the XVA and is described in Section 3.2. Disks are stored within sub-directories and are referenced from the ova.xml. The format of disk data is described later in Section 3.3.</p><p>The following terms will be used in the rest of the chapter:</p><ul><li><p>HVM: a mode in which unmodified OS kernels run with the help of virtualization support in the hardware.</p></li><li><p>PV: a mode in which specially modified &ldquo;paravirtualized&rdquo; kernels run explicitly on top of a hypervisor without requiring hardware support for virtualization.</p></li></ul><p>The &ldquo;ova.xml&rdquo; file contains the following elements:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;appliance</span> <span style=color:#a6e22e>version=</span><span style=color:#e6db74>&#34;0.1&#34;</span><span style=color:#f92672>&gt;</span></span></span></code></pre></div><p>The number in the attribute &ldquo;version&rdquo; indicates the version of this specification to which the XVA is constructed; in this case version 0.1. Inside the &lt;appliance> there is exactly one &lt;vm>: (in the OVA specification, multiple &lt;vm>s are permitted)</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;vm</span> <span style=color:#a6e22e>name=</span><span style=color:#e6db74>&#34;name&#34;</span><span style=color:#f92672>&gt;</span></span></span></code></pre></div><p>Each <code>&lt;vm></code> element describes one VM. The &ldquo;name&rdquo; attribute is for future internal use only and must be unique within the ova.xml file. The &ldquo;name&rdquo; attribute is permitted to be any valid UTF-8 string. Inside each &lt;vm> tag are the following compulsory elements:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;label&gt;</span>... text ... <span style=color:#f92672>&lt;/label&gt;</span></span></span></code></pre></div><p>A short name for the VM to be displayed in a UI.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;shortdesc&gt;</span> ... description ... <span style=color:#f92672>&lt;/shortdesc&gt;</span></span></span></code></pre></div><p>A description for the VM to be displayed in the UI. Note that for both <code>&lt;label></code> and <code>&lt;shortdesc></code> contents, leading and trailing whitespace will be ignored.</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;config</span> <span style=color:#a6e22e>mem_set=</span><span style=color:#e6db74>&#34;268435456&#34;</span> <span style=color:#a6e22e>vcpus=</span><span style=color:#e6db74>&#34;1&#34;</span><span style=color:#f92672>/&gt;</span></span></span></code></pre></div><p>The <code>&lt;config></code> element has attributes which describe the amount of memory in bytes (<code>mem_set</code>) and number of CPUs (VCPUs) the VM should have.</p><p>Each <code>&lt;vm></code> has zero or more <code>&lt;vbd></code> elements representing block devices which look like the following:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;vbd</span> <span style=color:#a6e22e>device=</span><span style=color:#e6db74>&#34;sda&#34;</span> <span style=color:#a6e22e>function=</span><span style=color:#e6db74>&#34;root&#34;</span> <span style=color:#a6e22e>mode=</span><span style=color:#e6db74>&#34;w&#34;</span> <span style=color:#a6e22e>vdi=</span><span style=color:#e6db74>&#34;vdi_sda&#34;</span><span style=color:#f92672>/&gt;</span></span></span></code></pre></div><p>The attributes have the following meanings:</p><ul><li><code>device</code>: name of the physical device to expose to the VM. For linux guests
we use &ldquo;sd[a-z]&rdquo; and for windows guests we use &ldquo;hd[a-d]&rdquo;.</li><li><code>function</code>: if marked as &ldquo;root&rdquo;, this disk will be used to boot the guest.
(NB this does not imply the existence of the Linux root i.e. / filesystem)
Only one device should be marked as &ldquo;root&rdquo;. See Section 3.4 describing VM
booting. Any other string is ignored.</li><li><code>mode</code>: either &ldquo;w&rdquo; or &ldquo;ro&rdquo; if the device is to be read/write or read-only</li><li><code>vdi</code>: the name of the disk image (represented by a <code>&lt;vdi></code> element) to which
this block device is connected</li></ul><p>Each <code>&lt;vm></code> may have an optional <code>&lt;hacks></code> section like the following:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;hacks</span> <span style=color:#a6e22e>is_hvm=</span><span style=color:#e6db74>&#34;false&#34;</span> <span style=color:#a6e22e>kernel_boot_cmdline=</span><span style=color:#e6db74>&#34;root=/dev/sda1 ro&#34;</span><span style=color:#f92672>/&gt;</span></span></span></code></pre></div><p>The <code>&lt;hacks></code> element will be removed in future. The attribute <code>is_hvm</code> is
either <code>true</code> or <code>false</code>, depending on whether the VM should be booted in HVM or not.
The <code>kernel_boot_cmdline</code> contains additional kernel commandline arguments when
booting a guest using pygrub.</p><p>In addition to a <code>&lt;vm></code> element, the <code>&lt;appliance></code> will contain zero or more
<code>&lt;vdi></code> elements like the following:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#f92672>&lt;vdi</span> <span style=color:#a6e22e>name=</span><span style=color:#e6db74>&#34;vdi_sda&#34;</span> <span style=color:#a6e22e>size=</span><span style=color:#e6db74>&#34;5368709120&#34;</span> <span style=color:#a6e22e>source=</span><span style=color:#e6db74>&#34;file://sda&#34;</span> <span style=color:#a6e22e>type=</span><span style=color:#e6db74>&#34;dir-gzipped-chunks&#34;</span><span style=color:#f92672>&gt;</span></span></span></code></pre></div><p>Each <code>&lt;vdi></code> corresponds to a disk image. The attributes have the following meanings:</p><ul><li><code>name</code>: name of the VDI, referenced by the vdi attribute of <code>&lt;vbd></code>elements.
Any valid UTF-8 string is permitted.</li><li><code>size</code>: size of the required image in bytes</li><li><code>source</code>: a URI describing where to find the data for the image, only
file:// URIs are currently permitted and must describe paths relative to the
directory containing the ova.xml</li><li><code>type</code>: describes the format of the disk data</li></ul><p>A single disk image encoding is specified in which has type &ldquo;dir-gzipped-chunks&rdquo;: Each image is represented by a directory containing a sequence of files as follows:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>458286013</span>    Sep <span style=color:#ae81ff>18</span> 09:51 chunk000000000.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>422271283</span>    Sep <span style=color:#ae81ff>18</span> 09:52 chunk000000001.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>395914244</span>    Sep <span style=color:#ae81ff>18</span> 09:53 chunk000000002.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>9452401</span>      Sep <span style=color:#ae81ff>18</span> 09:53 chunk000000003.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>1096066</span>      Sep <span style=color:#ae81ff>18</span> 09:53 chunk000000004.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>971976</span>       Sep <span style=color:#ae81ff>18</span> 09:53 chunk000000005.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>971976</span>       Sep <span style=color:#ae81ff>18</span> 09:53 chunk000000006.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>971976</span>       Sep <span style=color:#ae81ff>18</span> 09:53 chunk000000007.gz
</span></span><span style=display:flex><span>-rw-r--r-- <span style=color:#ae81ff>1</span> dscott xendev <span style=color:#ae81ff>573930</span>       Sep <span style=color:#ae81ff>18</span> 09:53 chunk000000008.gz</span></span></code></pre></div><p>Each file (named &ldquo;chunk-XXXXXXXXX.gz&rdquo;) is a gzipped file containing exactly 1e9 bytes (1GB, not 1GiB) of raw block data. The small size was chosen to be safely under the maximum file size limits of several filesystems. If the files are gunzipped and then concatenated together, the original image is recovered.</p><p>Because the import and export of VMs can take some time to complete, an
asynchronous HTTP interface to the import and export operations is
provided. To perform an export using the XenServer API, construct
an HTTP GET call providing a valid session ID, task ID and VM UUID, as
shown in the following pseudo code:</p><pre><code>task = Task.create()
result = HTTP.get(
  server, 80, &quot;/export?session_id=&amp;task_id=&amp;ref=&quot;);
</code></pre><p>For the import operation, use an HTTP PUT call as demonstrated in the
following pseudo code:</p><pre><code>task = Task.create()
result = HTTP.put(
  server, 80, &quot;/import?session_id=&amp;task_id=&amp;ref=&quot;);
</code></pre><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=vm-lifecycle>VM Lifecycle</h1><div class="mermaid align-center">graph
halted-- start(paused) -->paused
halted-- start(not paused) -->running
running-- suspend -->suspended
suspended-- resume(not paused) -->running
suspended-- resume(paused) -->paused
suspended-- hard shutdown -->halted
paused-- unpause -->running
paused-- hard shutdown -->halted
running-- clean shutdown\n hard shutdown -->halted
running-- pause -->paused
halted-- destroy -->destroyed</div><p>The figure above shows the states that a VM can be in and the
API calls that can be used to move the VM between these states.</p><ul class="children children-li children-sort-"></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xencenter>XenCenter</h1><p>XenCenter uses some conventions on top of the XenAPI:</p><h2 id=internationalization-for-sr-names>Internationalization for SR names</h2><p>The SRs created at install time now have an <code>other_config</code> key indicating how their names may be internationalized.</p><p><code>other_config["i18n-key"]</code> may be one of</p><ul><li><p><code>local-hotplug-cd</code></p></li><li><p><code>local-hotplug-disk</code></p></li><li><p><code>local-storage</code></p></li><li><p><code>xenserver-tools</code></p></li></ul><p>Additionally, <code>other_config["i18n-original-value-&lt;field name>"]</code> gives the value of that field when the SR was created. If XenCenter sees a record where <code>SR.name_label</code> equals <code>other_config["i18n-original-value-name_label"]</code> (that is, the record has not changed since it was created during XenServer installation), then internationalization will be applied. In other words, XenCenter will disregard the current contents of that field, and instead use a value appropriate to the user&rsquo;s own language.</p><p>If you change <code>SR.name_label</code> for your own purpose, then it no longer is the same as <code>other_config["i18n-original-value-name_label"]</code>. Therefore, XenCenter does not apply internationalization, and instead preserves your given name.</p><h2 id=hiding-objects-from-xencenter>Hiding objects from XenCenter</h2><p>Networks, PIFs, and VMs can be hidden from XenCenter by adding the key <code>HideFromXenCenter=true</code> to the <code>other_config</code> parameter for the object. This capability is intended for ISVs who know what they are doing, not general use by everyday users. For example, you might want to hide certain VMs because they are cloned VMs that shouldn&rsquo;t be used directly by general users in your environment.</p><p>In XenCenter, hidden Networks, PIFs, and VMs can be made visible, using the View menu.</p><footer class=footline></footer></article></section></section></section></div></main></div><script src=/new-docs/js/clipboard.min.js?1724316359 defer></script><script src=/new-docs/js/perfect-scrollbar.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-color.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-dispatch.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-drag.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-ease.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-interpolate.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-selection.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-timer.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-transition.min.js?1724316359 defer></script><script src=/new-docs/js/d3/d3-zoom.min.js?1724316359 defer></script><script src=/new-docs/js/js-yaml.min.js?1724316359 defer></script><script src=/new-docs/js/mermaid.min.js?1724316359 defer></script><script>window.themeUseMermaid=JSON.parse("{}")</script><script src=/new-docs/js/theme.js?1724316359 defer></script></body></html>