<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.127.0"><meta name=generator content="Relearn 5.20.0+tip"><meta name=description content><title>Operation Walk-Throughs :: XAPI Toolstack Developer Documentation</title>
<link href=https://xapi-project.github.io/new-docs/xenopsd/walkthroughs/index.html rel=canonical type=text/html title="Operation Walk-Throughs :: XAPI Toolstack Developer Documentation"><link href=/new-docs/xenopsd/walkthroughs/index.xml rel=alternate type=application/rss+xml title="Operation Walk-Throughs :: XAPI Toolstack Developer Documentation"><link href=/new-docs/images/favicon.png?1737129287 rel=icon type=image/png><link href=/new-docs/css/fontawesome-all.min.css?1737129292 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/fontawesome-all.min.css?1737129292 rel=stylesheet></noscript><link href=/new-docs/css/nucleus.css?1737129292 rel=stylesheet><link href=/new-docs/css/auto-complete.css?1737129292 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/auto-complete.css?1737129292 rel=stylesheet></noscript><link href=/new-docs/css/perfect-scrollbar.min.css?1737129292 rel=stylesheet><link href=/new-docs/css/fonts.css?1737129292 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/fonts.css?1737129292 rel=stylesheet></noscript><link href=/new-docs/css/theme.css?1737129292 rel=stylesheet><link href=/new-docs/css/theme-auto.css?1737129292 rel=stylesheet id=variant-style><link href=/new-docs/css/variant.css?1737129292 rel=stylesheet><link href=/new-docs/css/print.css?1737129292 rel=stylesheet media=print><link href=/new-docs/css/format-print.css?1737129292 rel=stylesheet><link href=/new-docs/css/ie.css?1737129292 rel=stylesheet><script src=/new-docs/js/url.js?1737129292></script><script src=/new-docs/js/variant.js?1737129292></script><script>window.index_js_url="/new-docs/index.search.js";var baseUriFull,root_url="/",baseUri=root_url.replace(/\/$/,"");window.T_Copy_to_clipboard="Copy to clipboard",window.T_Copied_to_clipboard="Copied to clipboard!",window.T_Copy_link_to_clipboard="Copy link to clipboard",window.T_Link_copied_to_clipboard="Copied link to clipboard!",window.T_No_results_found='No results found for "{0}"',window.T_N_results_found='{1} results found for "{0}"',baseUriFull="https://xapi-project.github.io/new-docs/",window.variants&&variants.init(["auto","zen-light","zen-dark","red","blue","green","learn","neon","relearn-light","relearn-bright","relearn-dark"])</script><link rel=stylesheet href=https://xapi-project.github.io/new-docs/css/misc.css></head><body class="mobile-support print" data-url=/new-docs/xenopsd/walkthroughs/index.html><div id=body class=default-animation><div id=sidebar-overlay></div><div id=toc-overlay></div><nav id=topbar class=highlightable><div><div id=breadcrumbs><span id=sidebar-toggle-span><a href=# id=sidebar-toggle class=topbar-link title='Menu (CTRL+ALT+n)'><i class="fas fa-bars fa-fw"></i></a></span><ol class=links itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/new-docs/index.html><span itemprop=name>XAPI Toolstack Developer Guide</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/new-docs/xenopsd/index.html><span itemprop=name>Xenopsd</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Operation Walk-Throughs</span><meta itemprop=position content="3"></li></ol></div></div></nav><main id=body-inner class="highlightable default" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=operation-walk-throughs>Operation Walk-Throughs</h1><p>Let&rsquo;s trace through interesting operations to see how the whole system
works.</p><ul><li><a href=/new-docs/xenopsd/walkthroughs/VM.start.md>Starting a VM</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate.md>Migrating a VM</a></li><li>Shutting down a VM and waiting for it to happen</li><li>A VM wants to reboot itself</li><li>A disk is hotplugged</li><li>A disk refuses to hotunplug</li><li>A VM is suspended</li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Operation Walk-Throughs</h1><article class=default><header class=headline></header><h1 id=live-migration-sequence-diagram>Live Migration Sequence Diagram</h1><div class="mermaid align-left">sequenceDiagram
autonumber
participant tx as sender
participant rx0 as receiver thread 0
participant rx1 as receiver thread 1
participant rx2 as receiver thread 2
activate tx
tx->>rx0: VM.import_metadata
tx->>tx: Squash memory to dynamic-min
tx->>rx1: HTTP /migrate/vm
activate rx1
rx1->>rx1: VM_receive_memory&lt;br/>VM_create (00000001)&lt;br/>VM_restore_vifs
rx1->>tx: handshake (control channel)&lt;br/>Synchronisation point 1
tx->>rx2: HTTP /migrate/mem
activate rx2
rx2->>tx: handshake (memory channel)&lt;br/>Synchronisation point 1-mem
tx->>rx1: handshake (control channel)&lt;br/>Synchronisation point 1-mem ACK
rx2->>rx1: memory fd
tx->>rx1: VM_save/VM_restore&lt;br/>Synchronisation point 2
tx->>tx: VM_rename
rx1->>rx2: exit
deactivate rx2
tx->>rx1: handshake (control channel)&lt;br/>Synchronisation point 3
rx1->>rx1: VM_rename&lt;br/>VM_restore_devices&lt;br/>VM_unpause&lt;br/>VM_set_domain_action_request
rx1->>tx: handshake (control channel)&lt;br/>Synchronisation point 4
deactivate rx1
tx->>tx: VM_shutdown&lt;br/>VM_remove
deactivate tx</div><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=walkthrough-migrating-a-vm>Walkthrough: Migrating a VM</h1><p>A XenAPI client wishes to migrate a VM from one host to another within
the same pool.</p><p>The client will issue a command to migrate the VM and it will be dispatched
by the autogenerated <code>dispatch_call</code> function from <strong>xapi/server.ml</strong>. For
more information about the generated functions you can have a look to
<a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/idl/ocaml_backend target=_blank>XAPI IDL model</a>.</p><p>The command will trigger the operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/lib/xenops_server.ml#L2572 target=_blank>VM_migrate</a>
that has low level operations performed by the backend. These atomics operations
that we will describe in the documentation are:</p><ul><li>VM.restore</li><li>VM.rename</li><li>VBD.set_active</li><li>VBD.plug</li><li>VIF.set_active</li><li>VGPU.set_active</li><li>VM.create_device_model</li><li>PCI.plug</li><li>VM.set_domain_action_request</li></ul><p>The command have serveral parameters such as: should it be ran asynchronously,
should it be forwared to another host, how arguments should be marshalled and
so on. A new thread is created by <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xapi/server_helpers.ml#L55 target=_blank>xapi/server_helpers.ml</a>
to handle the command asynchronously. At this point the helper also check if
the command should be passed to the <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi/message_forwarding.ml target=_blank>message forwarding</a>
layer in order to be executed on another host (the destination) or locally if
we are already at the right place.</p><p>It will finally reach <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xapi/api_server.ml#L242 target=_blank>xapi/api_server.ml</a> that will take the action
of posted a command to the message broker <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/message-switch target=_blank>message switch</a>.
It is a JSON-RPC HTTP request sends on a Unix socket to communicate between some
XAPI daemons. In the case of the migration this message sends by <strong>XAPI</strong> will be
consumed by the <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd target=_blank>xenopsd</a>
daemon that will do the job of migrating the VM.</p><h1 id=the-migration-of-the-vm>The migration of the VM</h1><p>The migration is an asynchronous task and a thread is created to handle this task.
The tasks&rsquo;s reference is returned to the client, which can then check
its status until completion.</p><p>As we see in the introduction the <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd target=_blank>xenopsd</a>
daemon will pop the operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/lib/xenops_server.ml#L2572 target=_blank>VM_migrate</a>
from the message broker.</p><p>Only one backend is know available that interacts with libxc, libxenguest
and xenstore. It is the <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd/xc target=_blank>xc backend</a>.</p><p>The entities that need to be migrated are: <em>VDI</em>, <em>VIF</em>, <em>VGPU</em> and <em>PCI</em> components.</p><p>During the migration process the destination domain will be built with the same
uuid than the original VM but the last part of the UUID will be
<code>XXXXXXXX-XXXX-XXXX-XXXX-000000000001</code>. The original domain will be removed using
<code>XXXXXXXX-XXXX-XXXX-XXXX-000000000000</code>.</p><p>There are some points called <em>hooks</em> at which <code>xenopsd</code> can execute some script.
Before starting a migration a command is send to the original domain to execute
a pre migrate script if it exists.</p><p>Before starting the migration a command is sent to Qemu using the Qemu Machine Protocol (QMP)
to check that the domain can be suspended (see <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/device_common.ml target=_blank>xenopsd/xc/device_common.ml</a>).
After checking with Qemu that the VM is suspendable we can start the migration.</p><h2 id=importing-metadata>Importing metadata</h2><p>As for <em>hooks</em>, commands to source domain are sent using <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/libs/stunnel target=_blank>stunnel</a> a daemon which
is used as a wrapper to manage SSL encryption communication between two hosts on the same
pool. To import metada an XML RPC command is sent to the original domain.</p><p>Once imported it will give us a reference id and will allow to build the new domain
on the destination using the temporary VM uuid <code>XXXXXXXX-XXXX-XXXX-XXXX-000000000001</code>
where <code>XXX...</code> is the reference id of the original VM.</p><h2 id=setting-memory>Setting memory</h2><p>One of the first thing to do is to setup the memory. The backend will check that there
is no ballooning operation in progress. At this point the migration can fail if a
ballooning operation is in progress and takes too much time.</p><p>Once memory checked the daemon will get the state of the VM (running, halted, &mldr;) and
information about the VM are retrieve by the backend like the maximum memory the domain
can consume but also information about quotas for example.
Information are retrieve by the backend from xenstore.</p><p>Once this is complete, we can restore VIF and create the domain.</p><p>The synchronisation of the memory is the first point of synchronisation and everythin
is ready for VM migration.</p><h2 id=vm-migration>VM Migration</h2><p>After receiving memory we can set up the destination domain. If we have a vGPU we need to kick
off its migration process. We will need to wait the acknowledge that indicates that the entry
for the GPU has been well initialized. before starting the main VM migration.</p><p>Their is a mechanism of handshake for synchronizing between the source and the
destination. Using the handshake protocol the receiver inform the sender of the
request that everything has been setup and ready to save/restore.</p><h3 id=vm-restore>VM restore</h3><p>VM restore is a low level atomic operation <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L2684 target=_blank>VM.restore</a>.
This operation is represented by a function call to <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/domain.ml#L1540 target=_blank>backend</a>.
It uses <strong>Xenguest</strong>, a low-level utility from XAPI toolstack, to interact with the Xen hypervisor
and libxc for sending a request of migration to the <strong>emu-manager</strong>.</p><p>After sending the request results coming from <strong>emu-manager</strong> are collected
by the main thread. It blocks until results are received.</p><p>During the live migration, <strong>emu-manager</strong> helps in ensuring the correct state
transitions for the devices and handling the message passing for the VM as
it&rsquo;s moved between hosts. This includes making sure that the state of the
VM&rsquo;s virtual devices, like disks or network interfaces, is correctly moved over.</p><h3 id=vm-renaming>VM renaming</h3><p>Once all operations are done we can rename the VM on the target from its temporary
name to its real UUID. This operation is another low level atomic one
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L1667 target=_blank>VM.rename</a>
that will take care of updating the xenstore on the destination.</p><p>The next step is the restauration of devices and unpause the domain.</p><h3 id=restoring-remaining-devices>Restoring remaining devices</h3><p>Restoring devices starts by activating VBD using the low level atomic operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3674 target=_blank>VBD.set_active</a>. It is an update of Xenstore. VBDs that are read-write must
be plugged before read-only ones. Once activated the low level atomic operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3721 target=_blank>VBD.plug</a>
is called. VDI are attached and activate.</p><p>Next devices are VIFs that are set as active <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L4296 target=_blank>VIF.set_active</a> and plug <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L4394 target=_blank>VIF.plug</a>.
If there are VGPUs we will set them as active now using the atomic <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3490 target=_blank>VGPU.set_active</a>.</p><p>We are almost done. The next step is to create the device model</p><h4 id=create-device-model>create device model</h4><p>Create device model is done by using the atomic operation <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L2375 target=_blank>VM.create_device_model</a>. This
will configure <strong>qemu-dm</strong> and started. This allow to manage PCI devices.</p><h4 id=pci-plug>PCI plug</h4><p><a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3399 target=_blank>PCI.plug</a>
is executed by the backend. It plugs a PCI device and advertise it to QEMU if this option is set. It is
the case for NVIDIA SR-IOV vGPUS.</p><p>At this point devices have been restored. The new domain is considered survivable. We can
unpause the domain and performs last actions</p><h3 id=unpause-and-done>Unpause and done</h3><p>Unpause is done by managing the state of the domain using bindings to <a href="https://xenbits.xen.org/gitweb/?p=xen.git;a=blob;f=tools/libs/ctrl/xc_domain.c;h=f2d9d14b4d9f24553fa766c5dcb289f88d684bb0;hb=HEAD#l76" target=_blank>xenctrl</a>.
Once hypervisor has unpaused the domain some actions can be requested using <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3172 target=_blank>VM.set_domain_action_request</a>.
It is a path in xenstore. By default no action is done but a reboot can be for example
initiated.</p><p>Previously we spoke about some points called <em>hooks</em> at which <code>xenopsd</code> can execute some script. There
is also a hook to run a post migrate script. After the execution of the script if there is one
the migration is almost done. The last step is a handskake to seal the success of the migration
and the old VM can now be cleaned.</p><h1 id=links>Links</h1><p>Some links are old but even if many changes occured they are relevant for a global understanding
of the XAPI toolstack.</p><ul><li><a href=https://xapi-project.github.io/xapi/architecture.html target=_blank>XAPI architecture</a></li><li><a href=https://wiki.xenproject.org/wiki/XAPI_Dispatch target=_blank>XAPI dispatcher</a></li><li><a href=https://xapi-project.github.io/xenopsd/architecture.html target=_blank>Xenopsd architecture</a></li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=walkthrough-starting-a-vm>Walkthrough: Starting a VM</h1><p>A Xenopsd client wishes to start a VM. They must first tell Xenopsd the VM
configuration to use. A VM configuration is broken down into objects:</p><ul><li>VM: A device-less Virtual Machine</li><li>VBD: A virtual block device for a VM</li><li>VIF: A virtual network interface for a VM</li><li>PCI: A virtual PCI device for a VM</li></ul><p>Treating devices as first-class objects is convenient because we wish to expose
operations on the devices such as hotplug, unplug, eject (for removable media),
carrier manipulation (for network interfaces) etc.</p><p>The &ldquo;add&rdquo; functions in the Xenopsd interface cause Xenopsd to create the
objects:</p><ul><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L420 target=_blank>VM.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L464 target=_blank>VBD.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L475 target=_blank>VIF.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L457 target=_blank>PCI.add</a></li></ul><p>In the case of <a href=https://github.com/xapi-project/xen-api target=_blank>xapi</a>, there are a set
of functions which
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L380 target=_blank>convert between the XenAPI objects and the Xenopsd objects</a>.
The two interfaces are slightly different because they have different expected
users:</p><ul><li>the XenAPI has many clients which are updated on long release cycles. The
main property needed is backwards compatibility, so that new release of xapi
remain compatible with these older clients. Quite often we will chose to
&ldquo;grandfather in&rdquo; some poorly designed interface simply because we wish to
avoid imposing churn on 3rd parties.</li><li>the Xenopsd API clients are all open-source and are part of the xapi-project.
These clients can be updated as the API is changed. The main property needed
is to keep the interface clean, so that it properly hides the complexity
of dealing with Xen from other components.</li></ul><p>The Xenopsd &ldquo;VM.add&rdquo; function has code like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>let</span> add&#39; x <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>		debug <span style=color:#e6db74>&#34;VM.add %s&#34;</span> <span style=color:#f92672>(</span>Jsonrpc.to_string <span style=color:#f92672>(</span>rpc_of_t x<span style=color:#f92672>));</span>
</span></span><span style=display:flex><span>		DB.write x<span style=color:#f92672>.</span>id x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>B</span> <span style=color:#f92672>=</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>val</span> get_backend () <span style=color:#f92672>:</span> <span style=color:#a6e22e>S</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>		B.VM.add x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		x<span style=color:#f92672>.</span>id</span></span></code></pre></div><p>This function does 2 things:</p><ul><li>it stores the VM configuration in the &ldquo;database&rdquo;</li><li>it tells the &ldquo;backend&rdquo; that the VM exists</li></ul><p>The Xenopsd database is really a set of config files in the filesystem. All
objects belonging to a VM (recall we only have VMs, VBDs, VIFs, PCIs and not
stand-alone entities like disks) and are placed into a subdirectory named after
the VM e.g.:</p><div class="wrap-code highlight"><pre tabindex=0><code># ls /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701
config	vbd.xvda  vbd.xvdb
# cat /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701/config
{&#34;id&#34;: &#34;7b719ce6-0b17-9733-e8ee-dbc1e6e7b701&#34;, &#34;name&#34;: &#34;fedora&#34;,
 ...
}</code></pre></div><p>Xenopsd doesn&rsquo;t have as persistent a notion of a VM as xapi, it is expected that
all objects are deleted when the host is rebooted. However the objects should
be persisted over a simple Xenopsd restart, which is why the objects are stored
in the filesystem.</p><p>Aside: it would probably be more appropriate to store the metadata in Xenstore
since this has the exact object lifetime we need. This will require a more
performant Xenstore to realise.</p><p>Every running Xenopsd process is linked with a single backend. Currently backends
exist for:</p><ul><li>Xen via libxc, libxenguest and xenstore</li><li>Xen via libxl, libxc and xenstore</li><li>Xen via libvirt</li><li>KVM by direct invocation of qemu</li><li>Simulation for testing</li></ul><p>From here we shall assume the use of the &ldquo;Xen via libxc, libxenguest and xenstore&rdquo; (a.k.a.
&ldquo;Xenopsd classic&rdquo;) backend.</p><p>The backend <a href=https://github.com/xapi-project/xenopsd/blob/2a476c132c0b5732f9b224316b851a1b4d57520b/xc/xenops_server_xen.ml#L719 target=_blank>VM.add</a>
function checks whether the VM we have to manage already exists &ndash; and if it does
then it ensures the Xenstore configuration is intact. This Xenstore configuration
is important because at any time a client can query the state of a VM with
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L438 target=_blank>VM.stat</a>
and this relies on certain Xenstore keys being present.</p><p>Once the VM metadata has been registered with Xenopsd, the client can call
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L443 target=_blank>VM.start</a>.
Like all potentially-blocking Xenopsd APIs, this function returns a Task id.
Please refer to the <a href=/new-docs/xenopsd/walkthroughs/VM.start/../design/Tasks.html>Task handling design</a> for a general
overview of how tasks are handled.</p><p>Clients can poll the state of a task by calling <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L404 target=_blank>TASK.stat</a>
but most clients will prefer to use the event system instead.
Please refer to the <a href=/new-docs/xenopsd/walkthroughs/VM.start/../design/Events.html>Event handling design</a> for a general
overview of how events are handled.</p><p>The event model is similar to the XenAPI: clients call a blocking
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L487 target=_blank>UPDATES.get</a>
passing in a token which represents the point in time when the last UPDATES.get
returned. The call blocks until some objects have changed state, and these object
ids are returned (NB in the XenAPI the current object states are returned)
The client must then call the relevant &ldquo;stat&rdquo; function, in this
case <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L404 target=_blank>TASK.stat</a></p><p>The client will be able to see the task make progress and use this to &ndash; for example &ndash;
populate a progress bar in a UI. If the client needs to cancel the task then it
can call the <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L405 target=_blank>TASK.cancel</a>;
again see the <a href=/new-docs/xenopsd/walkthroughs/VM.start/../design/Tasks.html>Task handling design</a> to understand how this is
implemented.</p><p>When the Task has completed successfully, then calls to *.stat will show:</p><ul><li>the power state is Paused</li><li>exactly one valid Xen domain id</li><li>all VBDs have active = plugged = true</li><li>all VIFs have active = plugged = true</li><li>all PCI devices have plugged = true</li><li>at least one active console</li><li>a valid start time</li><li>valid &ldquo;targets&rdquo; for memory and vCPU</li></ul><p>Note: before a Task completes, calls to *.stat will show partial updates e.g.
the power state may be Paused but none of the disks may have become plugged.
UI clients must choose whether they are happy displaying this in-between state
or whether they wish to hide it and pretend the whole operation has happened
transactionally. If a particular client wishes to perform side-effects in
response to Xenopsd state changes &ndash; for example to clean up an external resource
when a VIF becomes unplugged &ndash; then it must be very careful to avoid responding
to these in-between states. Generally it is safest to passively report these
values without driving things directly from them. Think of them as status lights
on the front panel of a PC: fine to look at but it&rsquo;s not a good idea to wire
them up to actuators which actually do things.</p><p>Note: the Xenopsd implementation guarantees that, if it is restarted at any point
during the start operation, on restart the VM state shall be &ldquo;fixed&rdquo; by either
(i) shutting down the VM; or (ii) ensuring the VM is intact and running.</p><p>In the case of <a href=https://github.com/xapi-project/xen-api target=_blank>xapi</a> every Xenopsd
Task id bound one-to-one with a XenAPI task by the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1831 target=_blank>sync_with_task</a>.
The function <a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1450 target=_blank>update_task</a>
is called when xapi receives a notification that a Xenopsd Task has changed state,
and updates the corresponding XenAPI task.
Xapi launches exactly one thread per Xenopsd instance (&ldquo;queue&rdquo;) to monitor for
background events via the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467 target=_blank>events_watch</a>
while each thread performing a XenAPI call waits for its specific Task to complete
via the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L30 target=_blank>event_wait</a>.</p><p>It is the responsibility of the client to call
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L406 target=_blank>TASK.destroy</a>
when the Task is nolonger needed. Xenopsd won&rsquo;t destroy the task because it contains
the success/failure result of the operation which is needed by the client.</p><p>What happens when a Xenopsd receives a VM.start request?</p><p>When Xenopsd receives the request it adds it to the appropriate per-VM queue
via the function
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1744 target=_blank>queue_operation</a>.
To understand this and other internal details of Xenopsd, consult the
<a href=/new-docs/xenopsd/walkthroughs/VM.start/../architecture.html>architecture description</a>.
The <a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1457 target=_blank>queue_operation_int</a>
function looks like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> queue_operation_int dbg id op <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>let</span> task <span style=color:#f92672>=</span> Xenops_task.add tasks dbg <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> t <span style=color:#f92672>-&gt;</span> perform op t<span style=color:#f92672>;</span> <span style=color:#a6e22e>None</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>	Redirector.push id <span style=color:#f92672>(</span>op<span style=color:#f92672>,</span> task<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>	task</span></span></code></pre></div><p>The &ldquo;task&rdquo; is a record containing Task metadata plus a &ldquo;do it now&rdquo; function
which will be executed by a thread from the thread pool. The
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L396 target=_blank>module Redirector</a>
takes care of:</p><ul><li>pushing operations to the right queue</li><li>ensuring at most one worker thread is working on a VM&rsquo;s operations</li><li>reducing the queue size by coalescing items together</li><li>providing a diagnostics interface</li></ul><p>Once a thread from the worker pool becomes free, it will execute the &ldquo;do it now&rdquo;
function. In the example above this is <code>perform op t</code> where <code>op</code> is
<code>VM_start vm</code> and <code>t</code> is the Task. The function
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1198 target=_blank>perform</a>
has fragments like this:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>VM_start</span> id <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>			debug <span style=color:#e6db74>&#34;VM.start %s&#34;</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>			perform_atomics <span style=color:#f92672>(</span>atomics_of_operation op<span style=color:#f92672>)</span> t<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>			VM_DB.signal id</span></span></code></pre></div><p>Each &ldquo;operation&rdquo; (e.g. <code>VM_start vm</code>) is decomposed into &ldquo;micro-ops&rdquo; by the
function
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L739 target=_blank>atomics_of_operation</a>
where the micro-ops are small building-block actions common to the higher-level
operations. Each operation corresponds to a list of &ldquo;micro-ops&rdquo;, where there is
no if/then/else. Some of the &ldquo;micro-ops&rdquo; may be a no-op depending on the VM
configuration (for example a PV domain may not need a qemu). In the case of
<code>VM_start vm</code> this decomposes into the sequence:</p><h2 id=1-run-the-vm_pre_start-scripts>1. run the &ldquo;VM_pre_start&rdquo; scripts</h2><p>The <code>VM_hook_script</code> micro-op runs the corresponding &ldquo;hook&rdquo; scripts. The
code is all in the
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/lib/xenops_hooks.ml target=_blank>Xenops_hooks</a>
module and looks for scripts in the hardcoded path <code>/etc/xapi.d</code>.</p><h2 id=2-create-a-xen-domain>2. create a Xen domain</h2><p>The <code>VM_create</code> micro-op calls the <code>VM.create</code> function in the backend.
In the classic Xenopsd backend the
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L633 target=_blank>VM.create_exn</a>
function must</p><ol><li>check if we&rsquo;re creating a domain for a fresh VM or resuming an existing one:
if it&rsquo;s a resume then the domain configuration stored in the VmExtra database
table must be used</li><li>ask <em>squeezed</em> to create a memory &ldquo;reservation&rdquo; big enough to hold the VM
memory. Unfortunately the domain cannot be created until the memory is free
because domain create often fails in low-memory conditions. This means the
&ldquo;reservation&rdquo; is associated with our &ldquo;session&rdquo; with squeezed; if Xenopsd
crashes and restarts the reservation will be freed automatically.</li><li>create the Domain via the libxc hypercall</li><li>&ldquo;transfer&rdquo; the squeezed reservation to the domain such that squeezed will
free the memory if the domain is destroyed later</li><li>compute and set an initial balloon target depending on the amount of memory
reserved (recall we ask for a range between <em>dynamic_min</em> and <em>dynamic_max</em>)</li><li>apply the &ldquo;suppress spurious page faults&rdquo; workaround if requested</li><li>set the &ldquo;machine address size&rdquo;</li><li>&ldquo;hotplug&rdquo; the vCPUs. This operates a lot like memory ballooning &ndash; Xen creates
lots of vCPUs and then the guest is asked to only use some of them. Every VM
therefore starts with the &ldquo;VCPUs_max&rdquo; setting and co-operative hotplug is
used to reduce the number. Note there is no enforcement mechanism: a VM which
cheats and uses too many vCPUs would have to be caught by looking at the
performance statistics.</li></ol><h2 id=3-build-the-domain>3. build the domain</h2><p>On a Xen system a domain is created empty, and memory is actually allocated
from the host in the &ldquo;build&rdquo; phase via functions in <em>libxenguest</em>. The
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L994 target=_blank>VM.build_domain_exn</a>
function must</p><ol><li>run pygrub (or eliloader) to extract the kernel and initrd, if necessary</li><li>invoke the <em>xenguest</em> binary to interact with libxenguest.</li><li>apply the <code>cpuid</code> configuration</li><li>store the current domain configuration on disk &ndash; it&rsquo;s important to know
the difference between the configuration you started with and the configuration
you would use after a reboot because some properties (such as maximum memory
and vCPUs) as fixed on create.</li></ol><p>The xenguest binary was originally
a separate binary for two reasons: (i) the libxenguest functions weren&rsquo;t
threadsafe since they used lots of global variables; and (ii) the libxenguest
functions used to have a different, incompatible license, which prevent us
linking. Both these problems have been resolved but we still shell out to
the xenguest binary.</p><p>The xenguest binary has also evolved to configure more of the initial domain
state. It also <a href=https://github.com/xapi-project/ocaml-xen-lowlevel-libs/blob/master/xenguest-4.4/xenguest_stubs.c#L42 target=_blank>reads Xenstore</a>
and configures</p><ul><li>the vCPU affinity</li><li>the vCPU credit2 weight/cap parameters</li><li>whether the NX bit is exposed</li><li>whether the viridian CPUID leaf is exposed</li><li>whether the system has PAE or not</li><li>whether the system has ACPI or not</li><li>whether the system has nested HVM or not</li><li>whether the system has an HPET or not</li></ul><h2 id=4-mark-each-vbd-as-active>4. mark each VBD as &ldquo;active&rdquo;</h2><p>VBDs and VIFs are said to be &ldquo;active&rdquo; when they are intended to be used by a
particular VM, even if the backend/frontend connection hasn&rsquo;t been established,
or has been closed. If someone calls <code>VBD.stat</code> or <code>VIF.stat</code> then
the result includes both &ldquo;active&rdquo; and &ldquo;plugged&rdquo;, where &ldquo;plugged&rdquo; is true if
the frontend/backend connection is established.
For example xapi will
set <a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1300 target=_blank>VBD.currently_attached</a>
to &ldquo;active || plugged&rdquo;. The &ldquo;active&rdquo; flag is conceptually very similar to the
traditional &ldquo;online&rdquo; flag (which is not documented in the upstream Xen tree
as of Oct/2014 but really should be) except that on unplug, one would set
the &ldquo;online&rdquo; key to &ldquo;0&rdquo; (false) <em>first</em> before initiating the hotunplug. By
contrast the &ldquo;active&rdquo; flag is set to false <em>after</em> the unplug i.e. &ldquo;set_active&rdquo;
calls bracket plug/unplug. If the &ldquo;active&rdquo; flag was set before the unplug
attempt then as soon as the frontend/backend connection is removed clients
would see the VBD as completely dissociated from the VM &ndash; this would be misleading
because Xenopsd will not have had time to use the storage API to release locks
on the disks. By doing all the cleanup before setting &ldquo;active&rdquo; to false, clients
can be assured that the disks are now free to be reassigned.</p><h2 id=5-handle-non-persistent-disks>5. handle non-persistent disks</h2><p>A non-persistent disk is one which is reset to a known-good state on every
VM start. The <code>VBD_epoch_begin</code> is the signal to perform any necessary reset.</p><h2 id=6-plug-vbds>6. plug VBDs</h2><p>The <code>VBD_plug</code> micro-op will plug the VBD into the VM. Every VBD is plugged
in a carefully-chosen order.
Generally, plug order is important for all types of devices. For VBDs, we must
work around the deficiency in the storage interface where a VDI, once attached
read/only, cannot be attached read/write. Since it is legal to attach the same
VDI with multiple VBDs, we must plug them in such that the read/write VBDs
come first. From the guest&rsquo;s point of view the order we plug them doesn&rsquo;t
matter because they are indexed by the Xenstore device id (e.g. 51712 = xvda).</p><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1631 target=_blank>VBD.plug</a>
will</p><ul><li>call <code>VDI.attach</code> and <code>VDI.activate</code> in the storage API to make the
devices ready (start the tapdisk processes etc)</li><li>add the Xenstore frontend/backend directories containing the block device
info</li><li>add the extra xenstore keys returned by the <code>VDI.attach</code> call that are
needed for SCSIid passthrough which is needed to support VSS</li><li>write the VBD information to the Xenopsd database so that future calls to
<em>VBD.stat</em> can be told about the associated disk (this is needed so clients
like xapi can cope with CD insert/eject etc)</li><li>if the qemu is going to be in a different domain to the storage, a frontend
device in the qemu domain is created.</li></ul><p>The Xenstore keys are written by the functions
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L486 target=_blank>Device.Vbd.add_async</a>
and
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L550 target=_blank>Device.Vbd.add_wait</a>.
In a Linux domain (such as dom0) when the backend directory is created, the kernel
creates a &ldquo;backend device&rdquo;. Creating any device will cause a kernel UEVENT to fire
which is picked up by udev. The udev rules run a script whose only job is to
stat(2) the device (from the &ldquo;params&rdquo; key in the backend) and write the major
and minor number to Xenstore for blkback to pick up. (Aside: FreeBSD doesn&rsquo;t do
any of this, instead the FreeBSD kernel module simply opens the device in the
&ldquo;params&rdquo; key). The script also writes the backend key &ldquo;hotplug-status=connected&rdquo;.
We currently wait for this key to be written so that later calls to <em>VBD.stat</em>
will return with &ldquo;plugged=true&rdquo;. If the call returns before this key is written
then sometimes we receive an event, call <em>VBD.stat</em> and conclude erroneously
that a spontaneous VBD unplug occurred.</p><h2 id=7-mark-each-vif-as-active>7. mark each VIF as &ldquo;active&rdquo;</h2><p>This is for the same reason as VBDs are marked &ldquo;active&rdquo;.</p><h2 id=8-plug-vifs>8. plug VIFs</h2><p>Again, the order matters. Unlike VBDs,
there is no read/write read/only constraint and the devices
have unique indices (0, 1, 2, &mldr;) <em>but</em> Linux kernels have often (always?)
ignored the actual index and instead relied on the order of results from the
<code>xenstore-ls</code> listing. The order that xenstored returns the items happens
to be the order the nodes were created so this means that (i) xenstored must
continue to store directories as ordered lists rather than maps (which would
be more efficient); and (ii) Xenopsd must make sure to plug the vifs in
the same order. Note that relying on ethX device numbering has always been a
bad idea but is still common. I bet if you change this lots of tests will
suddenly start to fail!</p><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1945 target=_blank>VIF.plug_exn</a>
will</p><ul><li>compute the port locking configuration required and write this to a well-known
location in the filesystem where it can be read from the udev scripts. This
really should be written to Xenstore instead, since this scheme doesn&rsquo;t work
with driver domains.</li><li>add the Xenstore frontend/backend directories containing the network device
info</li><li>write the VIF information to the Xenopsd database so that future calls to
<em>VIF.stat</em> can be told about the associated network</li><li>if the qemu is going to be in a different domain to the storage, a frontend
device in the qemu domain is created.</li></ul><p>Similarly to the VBD case, the function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L642 target=_blank>Device.Vif.add</a>
will write the Xenstore keys and wait for the &ldquo;hotplug-status=connected&rdquo; key.
We do this because we cannot apply the port locking rules until the backend
device has been created, and we cannot know the rules have been applied
until after the udev script has written the key. If we didn&rsquo;t wait for it then
the VM might execute without all the port locking properly configured.</p><h2 id=9-create-the-device-model>9. create the device model</h2><p>The <code>VM_create_device_model</code> micro-op will create a qemu device model if</p><ul><li>the VM is HVM; or</li><li>the VM uses a PV keyboard or mouse (since only qemu currently has backend
support for these devices).</li></ul><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1090 target=_blank>VM.create_device_model_exn</a>
will</p><ul><li>(if using a qemu stubdom) it will create and build the qemu domain</li><li>compute the necessary qemu arguments and launch it.</li></ul><p>Note that qemu (aka the &ldquo;device model&rdquo;) is created after the VIFs and VBDs have
been plugged but before the PCI devices have been plugged. Unfortunately qemu
traditional infers the needed emulated hardware by inspecting the Xenstore
VBD and VIF configuration and assuming that we want one emulated device per
PV device, up to the natural limits of the emulated buses (i.e. there can be
at most 4 IDE devices: {primary,secondary}{master,slave}). Not only does this
create an ordering dependency that needn&rsquo;t exist &ndash; and which impacts migration
downtime &ndash; but it also completely ignores the plain fact that, on a Xen system,
qemu can be in a different domain than the backend disk and network devices.
This hack only works because we currently run everything in the same domain.
There is an option (off by default) to list the emulated devices explicitly
on the qemu command-line. If we switch to this by default then we ought to be
able to start up qemu early, as soon as the domain has been created (qemu will
need to know the domain id so it can map the I/O request ring).</p><h2 id=10-plug-pci-devices>10. plug PCI devices</h2><p>PCI devices are treated differently to VBDs and VIFs.
If we are attaching the device to an
HVM guest then instead of relying on the traditional Xenstore frontend/backend
state machine we instead send RPCs to qemu requesting they be hotplugged. Note
the domain is paused at this point, but qemu still supports PCI hotplug/unplug.
The reasons why this doesn&rsquo;t follow the standard Xenstore model are known only
to the people who contributed this support to qemu.
Again the order matters because it determines the position of the virtual device
in the VM.</p><p>Note that Xenopsd doesn&rsquo;t know anything about the PCI devices; concepts such
as &ldquo;GPU groups&rdquo; belong to higher layers, such as xapi.</p><h2 id=11-mark-the-domain-as-alive>11. mark the domain as alive</h2><p>A design principle of Xenopsd is that it should tolerate failures such as being
suddenly restarted. It guarantees to always leave the system in a valid state,
in particular there should never be any &ldquo;half-created VMs&rdquo;. We achieve this for
VM start by exploiting the mechanism which is necessary for reboot. When a VM
wishes to reboot it causes the domain to exit (via SCHEDOP_shutdown) with a
&ldquo;reason code&rdquo; of &ldquo;reboot&rdquo;. When Xenopsd sees this event <code>VM_check_state</code>
operation is queued. This operation calls
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1443 target=_blank>VM.get_domain_action_request</a>
to ask the question, &ldquo;what needs to be done to make this VM happy now?&rdquo;. The
implementation checks the domain state for shutdown codes and also checks a
special Xenopsd Xenstore key. When Xenopsd creates a Xen domain it sets this
key to &ldquo;reboot&rdquo; (meaning &ldquo;please reboot me if you see me&rdquo;) and when Xenopsd
finishes starting the VM it clears this key. This means that if Xenopsd crashes
while starting a VM, the new Xenopsd will conclude that the VM needs to be rebooted
and will clean up the current domain and create a fresh one.</p><h2 id=12-unpause-the-domain>12. unpause the domain</h2><p>A Xenopsd VM.start will always leave the domain paused, so strictly speaking
this is a separate &ldquo;operation&rdquo; queued by the client (such as xapi) after the
VM.start has completed. The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L808 target=_blank>VM.unpause</a>
is reassuringly simple:</p><div class="wrap-code highlight"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>		<span style=color:#66d9ef>if</span> di<span style=color:#f92672>.</span>Xenctrl.total_memory_pages <span style=color:#f92672>=</span> 0n <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Domain_not_built</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>		Domain.unpause <span style=color:#f92672>~</span>xc di<span style=color:#f92672>.</span>Xenctrl.domid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		Opt.iter
</span></span><span style=display:flex><span>			<span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> stubdom_domid <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>				Domain.unpause <span style=color:#f92672>~</span>xc stubdom_domid
</span></span><span style=display:flex><span>			<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>get_stubdom <span style=color:#f92672>~</span>xs di<span style=color:#f92672>.</span>Xenctrl.domid<span style=color:#f92672>)</span></span></span></code></pre></div><footer class=footline></footer></article></section></div></main></div><script src=/new-docs/js/clipboard.min.js?1737129292 defer></script><script src=/new-docs/js/perfect-scrollbar.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-color.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-dispatch.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-drag.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-ease.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-interpolate.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-selection.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-timer.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-transition.min.js?1737129292 defer></script><script src=/new-docs/js/d3/d3-zoom.min.js?1737129292 defer></script><script src=/new-docs/js/js-yaml.min.js?1737129292 defer></script><script src=/new-docs/js/mermaid.min.js?1737129292 defer></script><script>window.themeUseMermaid=JSON.parse("{}")</script><script src=/new-docs/js/theme.js?1737129292 defer></script></body></html>