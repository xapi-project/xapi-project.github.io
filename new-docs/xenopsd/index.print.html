<!doctype html><html lang=en-us dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.127.0"><meta name=generator content="Relearn 7.3.2"><meta name=description content="Xenopsd is the VM manager of the XAPI Toolstack. Xenopsd is responsible for:
Starting, stopping, rebooting, suspending, resuming, migrating VMs. (Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices. Setting up VM consoles. Running bootloaders. Setting QoS parameters. Configuring SMBIOS tables. Handling crashes. etc. Check out the full features list.
The code is in ocaml/xenopsd.
Principles Do no harm: Xenopsd should never touch domains/VMs which it hasn’t been asked to manage."><meta name=author content><meta name=twitter:card content="summary"><meta name=twitter:title content="Xenopsd :: XAPI Toolstack Developer Documentation"><meta name=twitter:description content="Xenopsd is the VM manager of the XAPI Toolstack. Xenopsd is responsible for:
Starting, stopping, rebooting, suspending, resuming, migrating VMs. (Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices. Setting up VM consoles. Running bootloaders. Setting QoS parameters. Configuring SMBIOS tables. Handling crashes. etc. Check out the full features list.
The code is in ocaml/xenopsd.
Principles Do no harm: Xenopsd should never touch domains/VMs which it hasn’t been asked to manage."><meta property="og:url" content="https://xapi-project.github.io/new-docs/xenopsd/index.html"><meta property="og:site_name" content="XAPI Toolstack Developer Documentation"><meta property="og:title" content="Xenopsd :: XAPI Toolstack Developer Documentation"><meta property="og:description" content="Xenopsd is the VM manager of the XAPI Toolstack. Xenopsd is responsible for:
Starting, stopping, rebooting, suspending, resuming, migrating VMs. (Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices. Setting up VM consoles. Running bootloaders. Setting QoS parameters. Configuring SMBIOS tables. Handling crashes. etc. Check out the full features list.
The code is in ocaml/xenopsd.
Principles Do no harm: Xenopsd should never touch domains/VMs which it hasn’t been asked to manage."><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta itemprop=name content="Xenopsd :: XAPI Toolstack Developer Documentation"><meta itemprop=description content="Xenopsd is the VM manager of the XAPI Toolstack. Xenopsd is responsible for:
Starting, stopping, rebooting, suspending, resuming, migrating VMs. (Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices. Setting up VM consoles. Running bootloaders. Setting QoS parameters. Configuring SMBIOS tables. Handling crashes. etc. Check out the full features list.
The code is in ocaml/xenopsd.
Principles Do no harm: Xenopsd should never touch domains/VMs which it hasn’t been asked to manage."><meta itemprop=wordCount content="196"><title>Xenopsd :: XAPI Toolstack Developer Documentation</title>
<link href=https://xapi-project.github.io/new-docs/xenopsd/index.html rel=canonical type=text/html title="Xenopsd :: XAPI Toolstack Developer Documentation"><link href=/new-docs/xenopsd/index.xml rel=alternate type=application/rss+xml title="Xenopsd :: XAPI Toolstack Developer Documentation"><link href=/new-docs/images/favicon.png?1741781726 rel=icon type=image/png><link href=/new-docs/css/fontawesome-all.min.css?1741781726 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/fontawesome-all.min.css?1741781726 rel=stylesheet></noscript><link href=/new-docs/css/auto-complete.css?1741781726 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/auto-complete.css?1741781726 rel=stylesheet></noscript><link href=/new-docs/css/perfect-scrollbar.min.css?1741781726 rel=stylesheet><link href=/new-docs/css/theme.min.css?1741781726 rel=stylesheet><link href=/new-docs/css/format-print.min.css?1741781726 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://xapi-project.github.io/new-docs",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["auto","zen-light","zen-dark","red","blue","green","learn","neon","relearn-light","relearn-bright","relearn-dark"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script><link rel=stylesheet href=https://xapi-project.github.io/new-docs/css/misc.css></head><body class="mobile-support print" data-url=/new-docs/xenopsd/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button><div class=topbar-content><div class=topbar-content-wrapper><nav class=TableOfContents><ul><li><a href=#principles>Principles</a></li></ul></nav></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/new-docs/index.html><span itemprop=name>XAPI Toolstack Developer Guide</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Xenopsd</span><meta itemprop=position content="2"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-edit" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=https://github.com/xapi-project/xen-api/edit/master/doc/content/xenopsd/_index.md target=_blank title="Edit (CTRL+ALT+w)"><i class="fa-fw fas fa-pen"></i></a></div><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/xenopsd/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></div><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/xapi/walkthroughs/migration_overview/index.html title="From RPC migration request to xapi internals (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/xenopsd/architecture/index.html title="Xenopsd Architecture (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable xenopsd" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=xenopsd>Xenopsd</h1><p>Xenopsd is the VM manager of the XAPI Toolstack.
Xenopsd is responsible for:</p><ul><li>Starting, stopping, rebooting, suspending, resuming, migrating VMs.</li><li>(Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices.</li><li>Setting up VM consoles.</li><li>Running bootloaders.</li><li>Setting QoS parameters.</li><li>Configuring SMBIOS tables.</li><li>Handling crashes.</li><li>etc.</li></ul><p>Check out the <a href=/new-docs/xenopsd/features/index.html>full features list</a>.</p><p>The code is in <code>ocaml/xenopsd</code>.</p><h2 id=principles>Principles</h2><ol><li>Do no harm: Xenopsd should never touch domains/VMs which it hasn&rsquo;t been
asked to manage. This means that it can co-exist with other VM managers
such as &lsquo;xl&rsquo; and &rsquo;libvirt&rsquo;.</li><li>Be independent: Xenopsd should be able to work in isolation. In particular
the loss of some other component (e.g. the network) should not by itself
prevent VMs being managed locally (including shutdown and reboot).</li><li>Asynchronous by default: Xenopsd exposes task monitoring and offers
cancellation for all operations. Xenopsd ensures that the system is always
in a manageable state after an operation has been cancelled.</li><li>Avoid state duplication: where another component owns some state, Xenopsd
will always defer to it. We will avoid creating out-of-sync caches of
this state.</li><li>Be debuggable: Xenopsd will expose diagnostic APIs and tools to allow
its internal state to be inspected and modified.</li></ol><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Xenopsd</h1><article class=default><header class=headline></header><h1 id=xenopsd-architecture>Xenopsd Architecture</h1><p>Xenopsd instances run on a host and manage VMs on behalf of clients. This
picture shows 3 different Xenopsd instances: 2 named &ldquo;xenopsd-xc&rdquo; and 1 named
&ldquo;xenopsd-xenlight&rdquo;.</p><p><img alt="Where xenopsd fits on a host" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/xenopsd/architecture/host.svg style=height:auto;width:auto></p><p>Each instance is responsible for managing a disjoint set of VMs. Clients should
never ask more than one Xenopsd to manage the same VM.
Managing a VM means:</p><ul><li>handling start/shutdown/suspend/resume/migrate/reboot</li><li>allowing devices (disks, nics, PCI cards, vCPUs etc) to be manipulated</li><li>providing updates to clients when things change (reboots, console becomes
available, guest agent says something etc).</li></ul><p>For a full list of features, consult the <a href=/new-docs/xenopsd/features/index.html>feature list</a>.</p><p>Each Xenopsd instance has a unique name on the host. A typical name is</p><ul><li>org.xen.xcp.xenops.classic</li><li>org.xen.xcp.xenops.xenlight</li></ul><p>A higher-level tool, such as <a href=https://github.com/xapi-project/xen-api rel=external target=_blank>xapi</a>
will associate VMs with individual Xenopsd names.</p><p>Running multiple Xenopsds is necessary because</p><ul><li>The virtual hardware supported by different technologies (libxc, libxl, qemu)
is expected to be different. We can guarantee the virtual hardware is stable
across a rolling upgrade by running the VM on the old Xenopsd. We can then switch
Xenopsds later over a VM reboot when the VM admin is happy with it. If the
VM admin is unhappy then we can reboot back to the original Xenopsd again.</li><li>The suspend/resume/migrate image formats will differ across technologies
(again libxc vs libxl) and it will be more reliable to avoid switching
technology over a migrate.</li><li>In the future different security domains may have different Xenopsd instances
providing even stronger isolation guarantees between domains than is possible
today.</li></ul><p>Communication with Xenopsd is handled through a Xapi-global library:
<a href=https://github.com/xapi-project/xcp-idl rel=external target=_blank>xcp-idl</a>. This library supports</p><ul><li>message framing: by default using HTTP but a binary framing format is
available</li><li>message encoding: by default we use JSON but XML is also available</li><li>RPCs over Unix domain sockets and persistent queues.</li></ul><p>This library allows the communication details to be changed without having to
change all the Xapi clients and servers.</p><p>Xenopsd has a number of &ldquo;backends&rdquo; which perform the low-level VM operations
such as (on Xen) &ldquo;create domain&rdquo; &ldquo;hotplug disk&rdquo; &ldquo;destroy domain&rdquo;. These backends
contain all the hypervisor-specific code including</p><ul><li>connecting to Xenstore</li><li>opening the libxc /proc/xen/privcmd interface</li><li>initialising libxl contexts</li></ul><p>The following diagram shows the internal structure of Xenopsd:</p><p><img alt="Inside xenopsd" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/xenopsd/architecture/xenopsd.svg style=height:auto;width:auto></p><p>At the top of the diagram two client RPC have been sent: one to start a VM
and the other to fetch the latest events. The RPCs are all defined in
<a href=https://github.com/xapi-project/xcp-idl/blob/master/xen/xenops_interface.ml rel=external target=_blank>xcp-idl/xen/xenops_interface.ml</a>.
The RPCs are received by the Xenops_server module and decomposed into
&ldquo;micro-ops&rdquo; (labelled &ldquo;μ op&rdquo;). These micro ops represent actions like</p><ul><li>create a Xen domain (recall a Xen domain is an empty shell with no memory)</li><li>build a Xen domain: this is where the kernel or hvmloader is copied in</li><li>launch a device model: this is where a qemu instance is started (if one is
required)</li><li>hotplug a device: this involves writing the frontend and backend trees to
Xenstore</li><li>unpause a domain (recall a Xen domain is created in the paused state)</li></ul><p>Each of these micro-ops is represented by a function call in a &ldquo;backend plugin&rdquo;
interface. The micro-ops are enqueued in queues, one queue per VM. There is a
thread pool (whose size can be changed dynamically by the admin) which pulls
micro-ops from the VM queues and calls the corresponding backend function.</p><p>The active backend (there can only be one backend per Xenopsd instance)
executes the micro-ops. The Xenops_server_xen backend in the picture above
talks to libxc, libxl and qemu to create and destroy domains. The backend
also talks to other Xapi services, in particular</p><ul><li>it registers datasources with xcp-rrdd, telling xcp-rrdd to measure I/O
throughput and vCPU utilisation</li><li>it reserves memory for new domains by talking to squeezed</li><li>it makes disks available by calling SMAPIv2 VDI.{at,de}tach, VDI.{,de}activate</li><li>it launches subprocesses by talking to forkexecd (avoiding problems with
accidental fd capture)</li></ul><p>Xenopsd backends are also responsible for monitoring running VMs. In the
Xenops_server_xen backend this is done by watching Xenstore for</p><ul><li>@releaseDomain watch events</li><li>device hotplug status changes</li></ul><p>When such an event happens (for example: @releaseDomain sent when a domain
requests a reboot) the corresponding operation does not happen inline. Instead
the event is rebroadcast upwards to Xenops_server as a signal (for example:
&ldquo;VM <em>id</em> needs some attention&rdquo;) and a &ldquo;VM_stat&rdquo; micro-op is queued in the
appropriate queue. Xenopsd does not allow operations to run on the same VM
in parallel and enforces this by:</p><ul><li>pushing all operations pertaining to a VM to the same queue</li><li>associating each VM queue to at-most-one worker pool thread</li></ul><p>The event takes the form &ldquo;VM <em>id</em> needs some attention&rdquo; and not &ldquo;VM <em>id</em> needs
to be rebooted&rdquo; because, by the time the queue is flushed, the VM may well now
be in a different state. Perhaps rather than being rebooted it now needs to
be shutdown; or perhaps the domain is now in a good state because the reboot
has already happened. The signals sent by the backend to the Xenops_server are
a bit like event channel notifications in the Xen ring protocols: they are
requests to ask someone to perform work, they don&rsquo;t themselves describe the work
that needs to be done.</p><p>An implication of this design is that it should always be possible to answer
the question, &ldquo;what operation should be performed to get the VM into a valid state?&rdquo;.
If an operation is cancelled half-way through or if Xenopsd is suddenly restarted,
it will ask the question about all the VMs and perform the necessary operations.
The operations must be designed carefully to make this work. For example if Xenopsd
is restarted half-way through starting a VM, it must be obvious on restart that
the VM should either be forcibly shutdown or rebooted to make it a valid state
again. Note: we don&rsquo;t demand that operations are performed as transactions;
we only demand that the state they leave the system be &ldquo;sensible&rdquo; in the sense
that the admin will recognise it and be able to continue their work.</p><p>Sometimes this can be achieved through careful ordering of side-effects
within the operations, taking advantage of artifacts of the system such as:</p><ul><li>a domain which has not been fully created will have total vCPU time = 0 and
will be paused. If we see one of these we should reboot it because it may
not be fully intact.</li></ul><p>In the absense of &ldquo;tells&rdquo; from the system, operations are expected to journal
their intentions and support restart after failure.</p><p>There are three categories of metadata associated with VMs:</p><ol><li>system metadata: this is created as a side-effect of starting VMs. This
includes all the information about active disks and nics stored in Xenstore
and the list of running domains according to Xen.</li><li>VM: this is the configuration to use when the VM is started or rebooted.
This is like a &ldquo;config file&rdquo; for the VM.</li><li>VmExtra: this is the runtime configuration of the VM. When VM configuration
is changed it often cannot be applied immediately; instead the VM continues
to run with the previous configuration. We need to track the runtime
configuration of the VM in order for suspend/resume and migrate to work. It
is also useful to be able to tell a client, &ldquo;on next reboot this value will
be <em>x</em> but currently it is <em>x-1</em>&rdquo;.</li></ol><p>VM and VmExtra metadata is stored by Xenopsd in the domain 0 filesystem, in
a simple directory hierarchy.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=design>Design</h1><p>Design documents for <code>xenopsd</code>:<ul class="children children-li children-sort-"><li><a href=/new-docs/xenopsd/design/Events/index.html>Events</a></li><li><a href=/new-docs/xenopsd/design/hooks/index.html>Hooks</a></li><li><a href=/new-docs/xenopsd/design/pvs-proxy-ovs/index.html>PVS Proxy OVS Rules</a></li><li><a href=/new-docs/xenopsd/design/suspend-image-considerations/index.html>Requirements for suspend image framing</a></li><li><a href=/new-docs/xenopsd/design/suspend-image-framing-format/index.html>Suspend image framing format</a></li><li><a href=/new-docs/xenopsd/design/Tasks/index.html>Tasks</a></li></ul></p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Design</h1><article class=default><header class=headline></header><h1 id=events>Events</h1><ul><li>ids rather than data; inherently coalescable</li><li>blocking poll + async operations implies a client needs 2 connections</li><li>coarse granularity</li><li>similarity and differences with: XenAPI, event channels, xenstore watches</li></ul><p><a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467 rel=external target=_blank>https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467</a></p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=hooks>Hooks</h1><p>There are a number of hook points at which xenopsd may execute certain scripts. These scripts are found in hook-specific directories of the form <code>/etc/xapi.d/&lt;hookname>/</code>. All executable scripts in these directories are run with the following arguments:</p><pre><code>&lt;script.sh&gt; -reason &lt;reason&gt; -vmuuid &lt;uuid of VM&gt;
</code></pre><p>The scripts are executed in filename-order. By convention, the filenames are usually of the form <code>10resetvdis</code>.</p><p>The hook points are:</p><pre><code>vm-pre-shutdown
vm-pre-migrate
vm-post-migrate (Dundee only)
vm-pre-start
vm-pre-reboot
vm-pre-resume
vm-post-resume (Dundee only)
vm-post-destroy
</code></pre><p>and the reason codes are:</p><pre><code>clean-shutdown
hard-shutdown
clean-reboot
hard-reboot
suspend
source -- passed to pre-migrate hook on source host
destination -- passed to post-migrate hook on destination (Dundee only)
none
</code></pre><p>For example, in order to execute a script on VM shutdown, it would be sufficient to create the script in the post-destroy hook point:</p><pre><code>/etc/xapi.d/vm-post-destroy/01myscript.sh
</code></pre><p>containing</p><pre><code>#!/bin/bash
echo I was passed $@ &gt; /tmp/output
</code></pre><p>And when, for example, VM e30d0050-8f15-e10d-7613-cb2d045c8505 is shut-down, the script is executed:</p><pre><code>[vagrant@localhost ~]$ sudo xe vm-shutdown --force uuid=e30d0050-8f15-e10d-7613-cb2d045c8505
[vagrant@localhost ~]$ cat /tmp/output
I was passed -vmuuid e30d0050-8f15-e10d-7613-cb2d045c8505 -reason hard-shutdown
</code></pre><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=pvs-proxy-ovs-rules>PVS Proxy OVS Rules</h1><h1 id=rule-design>Rule Design</h1><p>The Open vSwitch (OVS) daemon implements a programmable switch.
XenServer uses it to re-direct traffic between three entities:</p><ul><li>PVS server - identified by its IP address</li><li>a local VM - identified by its MAC address</li><li>a local Proxy - identified by its MAC address</li></ul><p>VM and PVS server are unaware of the Proxy; xapi configures OVS to
redirect traffic between PVS and VM to pass through the proxy.</p><p>OVS uses rules that match packets. Rules are organised in sets called
tables. A rule can be used to match a packet and to inject it into
another rule set/table such that a packet can be matched again.</p><p>Furthermore, a rule can set registers associated with a packet which that
can be matched in subsequent rules. In that way, a packet can be tagged
such that it will only match specific rules downstream that match the
tag.</p><p>Xapi configures 3 rule sets:</p><h2 id=table-0---entry-rules>Table 0 - Entry Rules</h2><p>Rules match UDP traffic between VM/PVS, Proxy/VM, and PVS/VM where the
PVS server is identified by its IP and all other components by their MAC
address. All packets are tagged with the direction they are going and
re-submitted into Table 101 which handles ports.</p><h2 id=table-101---port-rules>Table 101 - Port Rules</h2><p>Rules match UDP traffic going to a specific port of the PVS server and
re-submit it into Table 102.</p><h2 id=table-102---exit-rules>Table 102 - Exit Rules</h2><p>These rules implement the redirection:</p><ul><li>Rules matching packets coming from VM to PVS are directed to the Proxy.</li><li>Rules matching packets coming from PVS to VM are directed to the Proxy.</li><li>Rules matching packets coming from the Proxy are already addressed
properly (to the VM) are handled normally.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=requirements-for-suspend-image-framing>Requirements for suspend image framing</h1><p>We are currently (Dec 2013) undergoing a transition from the &lsquo;classic&rsquo; xenopsd
backend (built upon calls to libxc) to the &lsquo;xenlight&rsquo; backend built on top of
the officially supported libxl API.</p><p>During this work, we have come across an incompatibility between the suspend
images created using the &lsquo;classic&rsquo; backend and those created using the new
libxl-based backend. This needed to be fixed to enable RPU to any new version
of XenServer.</p><h2 id=historic-classic-stack>Historic &lsquo;classic&rsquo; stack</h2><p>Prior to this work, xenopsd was involved in the construction of the suspend
image and we ended up with an image with the following format:</p><pre><code>+-----------------------------+
| &quot;XenSavedDomain\n&quot;          |  &lt;-- added by xenopsd-classic
|-----------------------------|
|  Memory image dump          |  &lt;-- libxc
|-----------------------------|
| &quot;QemuDeviceModelRecord\n&quot;   |
|  &lt;size of following record&gt; |  &lt;-- added by xenopsd-classic
|  (a 32-bit big-endian int)  |
|-----------------------------|
| &quot;QEVM&quot;                      |  &lt;-- libxc/qemu
|  Qemu device record         |
+-----------------------------+
</code></pre><p>We have also been carrying a patch in the Xen patchqueue against
xc_domain_restore. This patch (revert_qemu_tail.patch) stopped
xc_domain_restore from attempting to read past the memory image dump. At which
point xenopsd-classic would just take over and restore what it had put there.</p><h2 id=requirements-for-new-stack>Requirements for new stack</h2><p>For xenopsd-xenlight to work, we need to operate without the
revert_qemu_tail.patch since libxl assumes it is operating on top of an
upstream libxc.</p><p>We need the following relationship between suspend images created on one
backend being able to be restored on another backend. Where the backends are
old-classic (OC), new-classic (NC) and xenlight (XL). Obviously all suspend
images created on any backend must be able to be restored on the same backend:</p><pre><code>                OC _______ NC _______ XL
                 \  &gt;&gt;&gt;&gt;&gt;      &gt;&gt;&gt;&gt;&gt;  /
                  \__________________/
                    &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</code></pre><p>It turns out this was not so simple. After removing the patch against
xc_domain_restore and allowing libxc to restore the hvm_buffer_tail, we found
that supsend images created with OC (detailed in the previous section) are not
of a valid format for two reasons:</p><pre><code>i. The &quot;XenSavedDomain\n&quot; was extraneous;
</code></pre><p>ii. The Qemu signature section (prior to the record) is not of valid form.</p><p>It turns out that the section with the Qemu signature can be one of the
following:</p><pre><code>a. &quot;QemuDeviceModelRecord&quot; (NB. no newline) followed by the record to EOF;
b. &quot;DeviceModelRecord0002&quot; then a uint32_t length followed by record;
c. &quot;RemusDeviceModelState&quot; then a uint32_t length followed by record;
</code></pre><p>The old-classic (OC) backend not only uses an invalid signature (since it
contains a trailing newline) but it also includes a length, <em>and</em> the length is
in big-endian when the uint32_t is seen to be little-endian.</p><p>We considered creating a proxy for the fd in the incompatible cases but since
this would need to be a 22-lookahead byte-by-byte proxy this was deemed
impracticle. Instead we have made patched libxc with a much simpler patch to
understand this legacy format.</p><p>Because peek-ahead is not possible on pipes, the patch for (ii) needed to be
applied at a point where the hvm tail had been read completely. We piggy-backed
on the point after (a) had been detected. At this point the remainder of the fd
is buffered (only around 7k) and the magic &ldquo;QEVM&rdquo; is expected at the head of
this buffer. So we simply added a patch to check if there was a pesky newline
and the buffer[5:8] was &ldquo;QEVM&rdquo; and if it was we could discard the first
5 bytes:</p><pre><code>                              0    1    2    3    4    5   6   7   8
Legacy format from OC:  [...| \n | \x | \x | \x | \x | Q | E | V | M |...]

Required at this point: [...|  Q |  E |  V |  M |...]
</code></pre><h2 id=changes-made>Changes made</h2><p>To make the above use-cases work, we have made the following changes:</p><pre><code>1. Make new-classic (NC) not restore Qemu tail (let libxc do it)
    xenopsd.git:ef3bf4b

2. Make new-classic use valid signature (b) for future restore images
    xenopsd.git:9ccef3e

3. Make xc_domain_restore in libxc understand legacy xenopsd (OC) format
    xen-4.3.pq.hg:libxc-restore-legacy-image.patch

4. Remove revert-qemu-tail.patch from Xen patchqueue
    xen-4.3.pq.hg:3f0e16f2141e

5. Make xenlight (XL) use &quot;XenSavedDomain\n&quot; start-of-image signature
    xenopsd.git:dcda545
</code></pre><p>This has made the required use-cases work as follows:</p><pre><code>                OC __134__ NC __245__ XL
                 \  &gt;&gt;&gt;&gt;&gt;      &gt;&gt;&gt;&gt;&gt;  /
                  \_______345________/
                    &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</code></pre><p>And the suspend-resume on same backends work by virtue of:</p><pre><code>OC --&gt; OC : Just works
NC --&gt; NC : By 1,2,4
XL --&gt; XL : By 4 (5 is used but not required)
</code></pre><h2 id=new-components>New components</h2><p>The output of the changes above are:</p><ul><li>A new xenops-xc binary for NC</li><li>A new xenops-xl binary for XL</li><li>A new libxenguest.4.3 for both of NC and XL</li></ul><h2 id=future-considerations>Future considerations</h2><p>This should serve as a useful reference when considering making changes to the
suspend image in any way.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=suspend-image-framing-format>Suspend image framing format</h1><p>Example suspend image layout:</p><pre><code>+----------------------------+
| 1. Suspend image signature |
+============================+
| 2.0 Xenops header          |
| 2.1 Xenops record          |
+============================+
| 3.0 Libxc header           |
| 3.1 Libxc record           |
+============================+
| 4.0 Qemu header            |
| 4.1 Qemu save record       |
+============================+
| 5.0 End_of_image footer    |
+----------------------------+
</code></pre><p>A suspend image is now constructed as a series of header-record pairs. The
initial signature (1.) is used to determine whether we are dealing with the
unstructured, &ldquo;legacy&rdquo; suspend image or the new, structured format.</p><p>Each header is two 64-bit integers: the first identifies the header type and
the second is the length of the record that follows in bytes. The following
types have been defined (the ones marked with a (*) have yet to be
implemented):</p><pre><code>* Xenops       : Metadata for the suspend image
* Libxc        : The result of a xc_domain_save
* Libxl*       : Not implemented
* Libxc_legacy : Marked as a libxc record saved using pre-Xen-4.5
* Qemu_trad    : The qemu save file for the Qemu used in XenServer
* Qemu_xen*    : Not implemented
* Demu*        : Not implemented
* End_of_image : A footer marker to denote the end of the suspend image
</code></pre><p>Some of the above types do not have the notion of a length since they cannot be
known upfront before saving and also are delegated to other layers of the stack
on restoring. Specifically these are the memory image sections, libxc and
libxl.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=tasks>Tasks</h1><p>Some operations performed by Xenopsd are blocking, for example:</p><ul><li>suspend/resume/migration</li><li>attaching disks (where the SMAPI VDI.attach/activate calls can perform network
I/O)</li></ul><p>We want to be able to</p><ul><li>present the user with an idea of progress (perhaps via a &ldquo;progress bar&rdquo;)</li><li>allow the user to cancel a blocked operation that is taking too long</li><li>associate logging with the user/client-initiated actions that spawned them</li></ul><h2 id=principles>Principles</h2><ul><li>all operations which may block (the vast majority) should be written in an
asynchronous style i.e. the operations should immediately return a Task id</li><li>all operations should guarantee to respond to a cancellation request in a
bounded amount of time (30s)</li><li>when cancelled, the system should always be left in a valid state</li><li>clients are responsible for destroying Tasks when they are finished with the
results</li></ul><h2 id=types>Types</h2><p>A task has a state, which may be Pending, Completed or failed:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>type</span> async_result <span style=color:#f92672>=</span> <span style=color:#66d9ef>unit</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>type</span> completion_t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>		duration <span style=color:#f92672>:</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		result <span style=color:#f92672>:</span> async_result option
</span></span><span style=display:flex><span>	<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>type</span> state <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>Pending</span> <span style=color:#66d9ef>of</span> <span style=color:#66d9ef>float</span>
</span></span><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>Completed</span> <span style=color:#66d9ef>of</span> completion_t
</span></span><span style=display:flex><span>		<span style=color:#f92672>|</span> <span style=color:#a6e22e>Failed</span> <span style=color:#66d9ef>of</span> Rpc.t</span></span></code></pre></div><p>When a task is Failed, we assocate it with a marshalled exception (a value of type
Rpc.t). This exception must be one from the set defined in the
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L46 rel=external target=_blank>Xenops_interface</a>.
To see how they are marshalled, see
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L564 rel=external target=_blank>Xenops_server</a>.</p><p>From the point of view of a client, a Task has the immutable type (which can be
queried with a <code>Task.stat</code>):</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>type</span> t <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>		id<span style=color:#f92672>:</span> id<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		dbg<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		ctime<span style=color:#f92672>:</span> <span style=color:#66d9ef>float</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		state<span style=color:#f92672>:</span> state<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		subtasks<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>string</span> <span style=color:#f92672>*</span> state<span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		debug_info<span style=color:#f92672>:</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>string</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>	<span style=color:#f92672>}</span></span></span></code></pre></div><p>where</p><ul><li>id is a unique (integer) id generated by Xenopsd. This is how a Task is
represented to clients</li><li>dbg is a client-provided debug key which will be used in log lines, allowing
lines from the same Task to be associated together</li><li>ctime is the creation time</li><li>state is the current state (Pending/Completed/Failed)</li><li>subtasks lists logical internal sub-operations for debugging</li><li>debug_info includes miscellaneous key/value pairs used for debugging</li></ul><p>Internally, Xenopsd uses a
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L73 rel=external target=_blank>mutable record type</a>
to track Task state. This is broadly similar to the interface type except</p><ul><li>the state is mutable: this allows Tasks to complete</li><li>the task contains a &ldquo;do this now&rdquo; thunk</li><li>there is a &ldquo;cancelling&rdquo; boolean which is toggled to request a cancellation.</li><li>there is a list of cancel callbacks</li><li>there are some fields related to &ldquo;cancel points&rdquo;</li></ul><h2 id=persistence>Persistence</h2><p>The Tasks are intended to represent activities associated with in-memory queues
and threads. Therefore the active Tasks are kept in memory in a map, and will
be lost over a process restart. This is desirable since we will also lose the
queued items and the threads, so there is no need to resync on start.</p><p>Note that every operation must ensure that the state of the system is recoverable
on restart by not leaving it in an invalid state. It is not necessary to either
guarantee to complete or roll-back a Task. Tasks are not expected to be
transactional.</p><h2 id=lifecycle-of-a-task>Lifecycle of a Task</h2><p>All Tasks returned by API functions are created as part of the enqueue functions:
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1451 rel=external target=_blank>queue_operation_*</a>.
Even operations which are performed internally are normally wrapped in Tasks by
the function
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1451 rel=external target=_blank>immediate_operation</a>.</p><p>A queued operation will be processed by one of the
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L554 rel=external target=_blank>queue worker threads</a>.
It will</p><ul><li>set the thread-local debug key to the Task.dbg</li><li>call <code>task.Xenops_task.run</code>, taking care to catch exceptions and update
the <code>task.Xenops_task.state</code></li><li>unset the thread-local debug key</li><li>generate an event on the Task to provoke clients to query the current state.</li></ul><p>Task implementations must update their progress as they work. For the common
case of a compound operation like <code>VM_start</code> which is decomposed into
multiple &ldquo;micro-ops&rdquo; (e.g. <code>VM_create</code> <code>VM_build</code>) there is a useful
helper function
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1092 rel=external target=_blank>perform_atomics</a>
which divides the progress &lsquo;bar&rsquo; into sections, where each &ldquo;micro-op&rdquo; can have
a different size (<code>weight</code>). A progress callback function is passed into
each Xenopsd backend function so it can be updated with fine granularity. For
example note the arguments to
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1092 rel=external target=_blank>B.VM.save</a></p><p>Clients are expected to destroy Tasks they are responsible for creating. Xenopsd
cannot do this on their behalf because it does not know if they have successfully
queried the Task status/result.</p><p>When Xenopsd is a client of itself, it will take care to destroy the Task
properly, for example see
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L1451 rel=external target=_blank>immediate_operation</a>.</p><h2 id=cancellation>Cancellation</h2><p>The goal of cancellation is to unstick a blocked operation and to return the
system to <em>some</em> valid state, not any valid state in particular.
Xenopsd does not treat operations as transactions;
when an operation is cancelled it may</p><ul><li>fully complete (e.g. if it was about to do this anyway)</li><li>fully abort (e.g. if it had made no progress)</li><li>enter some other valid state (e.g. if it had gotten half way through)</li></ul><p>Xenopsd will never leave the system in an invalid state after cancellation.</p><p>Every Xenopsd operation should unblock and return the system to a valid state within
a reasonable amount of time after a cancel request. This should be as quick as possible
but up to 30s may be acceptable.
Bear in mind that a human is probably impatiently watching a UI say &ldquo;please wait&rdquo;
and which doesn&rsquo;t have any notion of progress itself. Keep it quick!</p><p>Cancellation is triggered by TASK.cancel which calls
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L194 rel=external target=_blank>cancel</a>.
This</p><ul><li>sets the cancelling boolean</li><li>calls all registered cancel callbacks</li></ul><p>Implementations respond to cancellation by</p><ul><li>if running: periodically call <a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L213 rel=external target=_blank>check_cancelling</a></li><li>if about to block: register a suitable cancel callback safely with <a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L224 rel=external target=_blank>with_cancel</a>.</li></ul><p>Xenopsd&rsquo;s libxc backend can block in 2 different ways, and therefore has 2 different
types of cancel callback:</p><ol><li>cancellable Xenstore watches</li><li>cancellable subprocesses</li></ol><p>Xenstore watches are used for device hotplug and unplug. Xenopsd has to wait for
the backend or for a udev script to do something. If that blocks, we need
a way to cancel the watch. The easiest way to cancel a watch is to watch an
additional path (a &ldquo;cancel path&rdquo;) and delete it, see
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/xc/cancel_utils.ml#L117 rel=external target=_blank>cancellable_watch</a>.
The &ldquo;cancel paths&rdquo; are placed within the VM&rsquo;s Xenstore directory to ensure that
cleanup code which does <code>xenstore-rm</code> will automatically &ldquo;cancel&rdquo; all outstanding
watches. Note that we trigger a cancel by deleting rather than creating, to avoid
racing with delete and creating orphaned Xenstore entries.</p><p>Subprocesses are used for suspend/resume/migrate. Xenopsd hands file descriptors
to libxenguest by running a subprocess and passing the fds to it. Xenopsd therefore
gets the process id and can send it a signal to cancel it. See
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/xc/cancel_utils.ml#L117 rel=external target=_blank>Cancellable_subprocess.run</a>.</p><h2 id=testing-with-cancel-points>Testing with cancel points</h2><p>Cancellation is difficult to test, as it is completely asynchronous. Therefore
Xenopsd has some built-in cancellation testing infrastructure known as &ldquo;cancel points&rdquo;.
A &ldquo;cancel point&rdquo; is a point in the code where a <code>Cancelled</code> exception could
be thrown, either by checking the cancelling boolean or as a side-effect of
a cancel callback. The
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L216 rel=external target=_blank>check_cancelling</a>
function increments a counter every time it passes one of these points, and
this value is returned to clients in the
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/xenops_server.ml#L135 rel=external target=_blank>Task.debug_info</a>.</p><p>A <a href=https://github.com/xapi-project/xen-api/blob/a365545c3b113fcd4bedecbc9146d4b6e3efbb04/ocaml/xapi/cancel_tests.ml rel=external target=_blank>test harness</a>
runs a series of operations. Each operation is first run all the way through to
completion to discover the total number of cancel points. The operation is then
re-run with a
<a href=https://github.com/xapi-project/xenopsd/blob/f876f9029cf53f14a52bf42a4a3a03265e048926/lib/task_server.ml#L84 rel=external target=_blank>request to cancel at a particular point</a>.
The test then waits for the system to stabilise and verifies that it appears to be
in a valid state.</p><h2 id=preventing-tasks-leaking>Preventing Tasks leaking</h2><p>The client who creates a Task must destroy it when the Task is finished, and
they have processed the result. What if a client like xapi is restarted while
a Task is running?</p><p>We assume that, if xapi is talking to a xenopsd, then xapi completely owns it.
Therefore xapi should destroy any completed tasks that it doesn&rsquo;t recognise.</p><p>If a user wishes to manage VMs with xenopsd in parallel with xapi, the user
should run a separate xenopsd.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=features>Features</h1><h2 id=general>General</h2><ul><li>Pluggable backends including<ul><li>xc: drives Xen via libxc and xenguest</li><li>simulator: simulates operations for component-testing</li></ul></li><li>Supports running multiple instances and backends on the same host, looking
after different sets of VMs</li><li>Extensive configuration via command-line (see manpage) and config
file</li><li>Command-line tool for easy VM administration and troubleshooting</li><li>User-settable degree of concurrency to get VMs started quickly</li></ul><h2 id=vms>VMs</h2><ul><li>VM start/shutdown/reboot</li><li>VM suspend/resume/checkpoint/migrate</li><li>VM pause/unpause</li><li>VM s3suspend/s3resume</li><li>customisable SMBIOS tables for OEM-locked VMs</li><li>hooks for 3rd party extensions:<ul><li>pre-start</li><li>pre-destroy</li><li>post-destroy</li><li>pre-reboot</li></ul></li><li>per-VM xenguest replacement</li><li>suppression of VM reboot loops</li><li>live vCPU hotplug and unplug</li><li>vCPU to pCPU affinity setting</li><li>vCPU QoS settings (weight and cap for the Xen credit2 scheduler)</li><li>DMC memory-ballooning support</li><li>support for storage driver domains</li><li>live update of VM shadow memory</li><li>guest-initiated disk/nic hotunplug</li><li>guest-initiated disk eject</li><li>force disk/nic unplug</li><li>support for &lsquo;surprise-removable&rsquo; devices</li><li>disk QoS configuration</li><li>nic QoS configuration</li><li>persistent RTC</li><li>two-way guest agent communication for monitoring and control</li><li>network carrier configuration</li><li>port-locking for nics</li><li>text and VNC consoles over TCP and Unix domain sockets</li><li>PV kernel and ramdisk whitelisting</li><li>configurable VM videoram</li><li>programmable action-after-crash behaviour including: shutting down
the VM, taking a crash dump or leaving the domain paused for inspection</li><li>ability to move nics between bridges/switches</li><li>advertises the VM memory footprints</li><li>PCI passthrough</li><li>support for discrete emulators (e.g. &lsquo;demu&rsquo;)</li><li>PV keyboard and mouse</li><li>qemu stub domains</li><li>cirrus and stdvga graphics cards</li><li>HVM serial console (useful for debugging)</li><li>support for vGPU</li><li>workaround for &lsquo;spurious page faults&rsquo; kernel bug</li><li>workaround for &lsquo;machine address size&rsquo; kernel bug</li></ul><h2 id=hosts>Hosts</h2><ul><li>CPUid masking for heterogenous pools: reports true features and current
features</li><li>Host console reading</li><li>Hypervisor version and capabilities reporting</li><li>Host CPU querying</li></ul><h2 id=apis>APIs</h2><ul><li>versioned JSON-RPC API with feature advertisements</li><li>clients can disconnect, reconnect and easily resync with the latest
VM state without losing updates</li><li>all operations have task control including<ul><li>asynchronous cancellation: for both subprocesses and xenstore watches</li><li>progress updates</li><li>subtasks</li><li>per-task debug logs</li></ul></li><li>asynchronous event watching API</li><li>advertises VM metrics<ul><li>memory usage</li><li>balloon driver co-operativeness</li><li>shadow memory usage</li><li>domain ids</li></ul></li><li>channel passing (via sendmsg(2)) for efficient memory image copying</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=operation-walk-throughs>Operation Walk-Throughs</h1><p>Let&rsquo;s trace through interesting operations to see how the whole system
works.</p><ul class="children children-li children-sort-"><li><a href=/new-docs/xenopsd/walkthroughs/VM.start/index.html>Starting a VM</a><p>Complete walkthrough of starting a VM, from receiving the request to unpause.</p><ul></ul></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.build/index.html>Building a VM</a><p>After VM_create, VM_build builds the core of the domain (vCPUs, memory)</p><ul><li><a href=/new-docs/xenopsd/walkthroughs/VM.build/VM_build/index.html>VM_build μ-op</a><p>Overview of the VM_build μ-op (runs after the VM_create μ-op created the domain).</p></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html>Domain.build</a><p>Prepare the build of a VM: Wait for scrubbing, do NUMA placement, run xenguest.</p></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.build/xenguest/index.html>xenguest</a><p>Perform building VMs: Allocate and populate the domain's system memory.</p></li></ul></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html>Migrating a VM</a><p>Walkthrough of migrating a VM from one host to another.</p><ul></ul></li><li><a href=/new-docs/xenopsd/walkthroughs/live-migration/index.html>Live Migration</a><p>Sequence diagram of the process of Live Migration.</p><ul></ul></li></ul><p>Inspiration for other walk-throughs:</p><ul><li>Shutting down a VM and waiting for it to happen</li><li>A VM wants to reboot itself</li><li>A disk is hotplugged</li><li>A disk refuses to hotunplug</li><li>A VM is suspended</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Walk-throughs</h1><article class=default><header class=headline></header><h1 id=walkthrough-starting-a-vm>Walkthrough: Starting a VM</h1><p>A Xenopsd client wishes to start a VM. They must first tell Xenopsd the VM
configuration to use. A VM configuration is broken down into objects:</p><ul><li>VM: A device-less Virtual Machine</li><li>VBD: A virtual block device for a VM</li><li>VIF: A virtual network interface for a VM</li><li>PCI: A virtual PCI device for a VM</li></ul><p>Treating devices as first-class objects is convenient because we wish to expose
operations on the devices such as hotplug, unplug, eject (for removable media),
carrier manipulation (for network interfaces) etc.</p><p>The &ldquo;add&rdquo; functions in the Xenopsd interface cause Xenopsd to create the
objects:</p><ul><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L420 rel=external target=_blank>VM.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L464 rel=external target=_blank>VBD.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L475 rel=external target=_blank>VIF.add</a></li><li><a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L457 rel=external target=_blank>PCI.add</a></li></ul><p>In the case of <a href=https://github.com/xapi-project/xen-api rel=external target=_blank>xapi</a>, there are a set
of functions which
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L380 rel=external target=_blank>convert between the XenAPI objects and the Xenopsd objects</a>.
The two interfaces are slightly different because they have different expected
users:</p><ul><li>the XenAPI has many clients which are updated on long release cycles. The
main property needed is backwards compatibility, so that new release of xapi
remain compatible with these older clients. Quite often, we will choose to
&ldquo;grandfather in&rdquo; some poorly designed interface simply because we wish to
avoid imposing churn on 3rd parties.</li><li>the Xenopsd API clients are all open-source and are part of the xapi-project.
These clients can be updated as the API is changed. The main property needed
is to keep the interface clean, so that it properly hides the complexity
of dealing with Xen from other components.</li></ul><p>The Xenopsd &ldquo;VM.add&rdquo; function has code like this:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>	<span style=color:#66d9ef>let</span> add&#39; x <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>		debug <span style=color:#e6db74>&#34;VM.add %s&#34;</span> <span style=color:#f92672>(</span>Jsonrpc.to_string <span style=color:#f92672>(</span>rpc_of_t x<span style=color:#f92672>));</span>
</span></span><span style=display:flex><span>		DB.write x<span style=color:#f92672>.</span>id x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> <span style=color:#66d9ef>module</span> <span style=color:#a6e22e>B</span> <span style=color:#f92672>=</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>val</span> get_backend () <span style=color:#f92672>:</span> <span style=color:#a6e22e>S</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>		B.VM.add x<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		x<span style=color:#f92672>.</span>id</span></span></code></pre></div><p>This function does 2 things:</p><ul><li>it stores the VM configuration in the &ldquo;database&rdquo;</li><li>it tells the &ldquo;backend&rdquo; that the VM exists</li></ul><p>The Xenopsd database is really a set of config files in the filesystem. All
objects belonging to a VM (recall we only have VMs, VBDs, VIFs, PCIs and not
stand-alone entities like disks) and are placed into a subdirectory named after
the VM e.g.:</p><div class="highlight wrap-code"><pre tabindex=0><code># ls /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701
config	vbd.xvda  vbd.xvdb
# cat /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701/config
{&#34;id&#34;: &#34;7b719ce6-0b17-9733-e8ee-dbc1e6e7b701&#34;, &#34;name&#34;: &#34;fedora&#34;,
 ...
}</code></pre></div><p>Xenopsd doesn&rsquo;t have as persistent a notion of a VM as xapi, it is expected that
all objects are deleted when the host is rebooted. However the objects should
be persisted over a simple Xenopsd restart, which is why the objects are stored
in the filesystem.</p><p>Aside: it would probably be more appropriate to store the metadata in Xenstore
since this has the exact object lifetime we need. This will require a more
performant Xenstore to realise.</p><p>Every running Xenopsd process is linked with a single backend. Currently backends
exist for:</p><ul><li>Xen via libxc, libxenguest and xenstore</li><li>Xen via libxl, libxc and xenstore</li><li>Xen via libvirt</li><li>KVM by direct invocation of qemu</li><li>Simulation for testing</li></ul><p>From here we shall assume the use of the &ldquo;Xen via libxc, libxenguest and xenstore&rdquo; (a.k.a.
&ldquo;Xenopsd classic&rdquo;) backend.</p><p>The backend <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L1603-L1659 rel=external target=_blank>VM.add</a>
function checks whether the VM we have to manage already exists &ndash; and if it does
then it ensures the Xenstore configuration is intact. This Xenstore configuration
is important because at any time a client can query the state of a VM with
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L438 rel=external target=_blank>VM.stat</a>
and this relies on certain Xenstore keys being present.</p><p>Once the VM metadata has been registered with Xenopsd, the client can call
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L443 rel=external target=_blank>VM.start</a>.
Like all potentially-blocking Xenopsd APIs, this function returns a Task id.
Please refer to the <a href=/new-docs/xenopsd/design/Tasks/index.html>Task handling design</a> for a general
overview of how tasks are handled.</p><p>Clients can poll the state of a task by calling <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L404 rel=external target=_blank>TASK.stat</a>
but most clients will prefer to use the event system instead.
Please refer to the <a href=/new-docs/xenopsd/design/Events/index.html>Event handling design</a> for a general
overview of how events are handled.</p><p>The event model is similar to the XenAPI: clients call a blocking
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L487 rel=external target=_blank>UPDATES.get</a>
passing in a token which represents the point in time when the last UPDATES.get
returned. The call blocks until some objects have changed state, and these object
ids are returned (NB in the XenAPI the current object states are returned)
The client must then call the relevant &ldquo;stat&rdquo; function, in this
case <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L404 rel=external target=_blank>TASK.stat</a></p><p>The client will be able to see the task make progress and use this to &ndash; for example &ndash;
populate a progress bar in a UI. If the client needs to cancel the task then it
can call the <a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L405 rel=external target=_blank>TASK.cancel</a>;
again see the <a href=/new-docs/xenopsd/design/Tasks/index.html>Task handling design</a> to understand how this is
implemented.</p><p>When the Task has completed successfully, then calls to *.stat will show:</p><ul><li>the power state is Paused</li><li>exactly one valid Xen domain id</li><li>all VBDs have active = plugged = true</li><li>all VIFs have active = plugged = true</li><li>all PCI devices have plugged = true</li><li>at least one active console</li><li>a valid start time</li><li>valid &ldquo;targets&rdquo; for memory and vCPU</li></ul><p>Note: before a Task completes, calls to *.stat will show partial updates. E.g.
the power state may be paused, but no disk may have been plugged.
UI clients must choose whether they are happy displaying this in-between state
or whether they wish to hide it and pretend the whole operation has happened
transactionally. If a particular, when a client wishes to perform side-effects in
response to <code>xenopsd</code> state changes (for example, to clean up an external resource
when a VIF becomes unplugged), it must be very careful to avoid responding
to these in-between states. Generally, it is safest to passively report these
values without driving things directly from them.</p><p>Note: the Xenopsd implementation guarantees that, if it is restarted at any point
during the start operation, on restart the VM state shall be &ldquo;fixed&rdquo; by either
(i) shutting down the VM; or (ii) ensuring the VM is intact and running.</p><p>In the case of <a href=https://github.com/xapi-project/xen-api rel=external target=_blank>xapi</a> every Xenopsd
Task id bound one-to-one with a XenAPI task by the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1831 rel=external target=_blank>sync_with_task</a>.
The function <a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1450 rel=external target=_blank>update_task</a>
is called when xapi receives a notification that a Xenopsd Task has changed state,
and updates the corresponding XenAPI task.
Xapi launches exactly one thread per Xenopsd instance (&ldquo;queue&rdquo;) to monitor for
background events via the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467 rel=external target=_blank>events_watch</a>
while each thread performing a XenAPI call waits for its specific Task to complete
via the function
<a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L30 rel=external target=_blank>event_wait</a>.</p><p>It is the responsibility of the client to call
<a href=https://github.com/xapi-project/xcp-idl/blob/2e5c3dd79c63e3711227892271a6bece98eb0fa1/xen/xenops_interface.ml#L406 rel=external target=_blank>TASK.destroy</a>
when the Task is no longer needed. Xenopsd won&rsquo;t destroy the task because it contains
the success/failure result of the operation which is needed by the client.</p><p>What happens when a Xenopsd receives a VM.start request?</p><p>When Xenopsd receives the request it adds it to the appropriate per-VM queue
via the function
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1744 rel=external target=_blank>queue_operation</a>.
To understand this and other internal details of Xenopsd, consult the
<a href=/new-docs/xenopsd/architecture/index.html>architecture description</a>.
The <a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L1457 rel=external target=_blank>queue_operation_int</a>
function looks like this:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> queue_operation_int dbg id op <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>let</span> task <span style=color:#f92672>=</span> Xenops_task.add tasks dbg <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> t <span style=color:#f92672>-&gt;</span> perform op t<span style=color:#f92672>;</span> <span style=color:#a6e22e>None</span><span style=color:#f92672>)</span> <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>	Redirector.push id <span style=color:#f92672>(</span>op<span style=color:#f92672>,</span> task<span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>	task</span></span></code></pre></div><p>The &ldquo;task&rdquo; is a record containing Task metadata plus a &ldquo;do it now&rdquo; function
which will be executed by a thread from the thread pool. The
<a href=https://github.com/xapi-project/xenopsd/blob/524d57b3c70/lib/xenops_server.ml#L396 rel=external target=_blank>module Redirector</a>
takes care of:</p><ul><li>pushing operations to the right queue</li><li>ensuring at most one worker thread is working on a VM&rsquo;s operations</li><li>reducing the queue size by coalescing items together</li><li>providing a diagnostics interface</li></ul><p>Once a thread from the worker pool becomes free, it will execute the &ldquo;do it now&rdquo;
function. In the example above this is <code>perform op t</code> where <code>op</code> is
<code>VM_start vm</code> and <code>t</code> is the Task. The function
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/lib/xenops_server.ml#L2533 rel=external target=_blank>perform_exn</a>
has fragments like this:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#f92672>|</span> <span style=color:#a6e22e>VM_start</span> <span style=color:#f92672>(</span>id<span style=color:#f92672>,</span> force<span style=color:#f92672>)</span> <span style=color:#f92672>-&gt;</span> <span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>      debug <span style=color:#e6db74>&#34;VM.start %s (force=%b)&#34;</span> id force <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>let</span> power <span style=color:#f92672>=</span> <span style=color:#f92672>(</span>B.VM.get_state <span style=color:#f92672>(</span>VM_DB.read_exn id<span style=color:#f92672>)).</span>Vm.power_state <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>match</span> power <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> <span style=color:#a6e22e>Running</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          info <span style=color:#e6db74>&#34;VM %s is already running&#34;</span> id
</span></span><span style=display:flex><span>      <span style=color:#f92672>|</span> <span style=color:#f92672>_</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>          perform_atomics <span style=color:#f92672>(</span>atomics_of_operation op<span style=color:#f92672>)</span> t <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>          VM_DB.signal id <span style=color:#e6db74>&#34;^^^^^^^^^^^^^^^^^^^^--------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    )</span></span></span></code></pre></div><p>Each &ldquo;operation&rdquo; (e.g. <code>VM_start vm</code>) is decomposed into &ldquo;micro-ops&rdquo; by the
function
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/lib/xenops_server.ml#L1583 rel=external target=_blank>atomics_of_operation</a>
where the micro-ops are small building-block actions common to the higher-level
operations. Each operation corresponds to a list of &ldquo;micro-ops&rdquo;, where there is
no if/then/else. Some of the &ldquo;micro-ops&rdquo; may be a no-op depending on the VM
configuration (for example a PV domain may not need a qemu). In the case of
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/lib/xenops_server.ml#L1584 rel=external target=_blank><code>VM_start vm</code></a>
the <code>Xenopsd</code> server starts by calling the <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/lib/xenops_server.ml#L1612-L1714 rel=external target=_blank>functions that
decompose</a>
the <code>VM_hook_script</code>, <code>VM_create</code> and <code>VM_build</code> micro-ops:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ml data-lang=ml><span style=display:flex><span>        dequarantine_ops vgpus
</span></span><span style=display:flex><span>      <span style=color:#f92672>;</span> <span style=color:#f92672>[</span>
</span></span><span style=display:flex><span>          <span style=color:#a6e22e>VM_hook_script</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>(</span>id<span style=color:#f92672>,</span> Xenops_hooks.<span style=color:#a6e22e>VM_pre_start</span><span style=color:#f92672>,</span> Xenops_hooks.reason__none<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>;</span> <span style=color:#a6e22e>VM_create</span> <span style=color:#f92672>(</span>id<span style=color:#f92672>,</span> <span style=color:#a6e22e>None</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>None</span><span style=color:#f92672>,</span> no_sharept<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>;</span> <span style=color:#a6e22e>VM_build</span> <span style=color:#f92672>(</span>id<span style=color:#f92672>,</span> force<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>]</span></span></span></code></pre></div><p>This is the complete sequence of micro-ops:</p><h2 id=1-run-the-vm_pre_start-scripts>1. run the &ldquo;VM_pre_start&rdquo; scripts</h2><p>The <code>VM_hook_script</code> micro-op runs the corresponding &ldquo;hook&rdquo; scripts. The
code is all in the
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/lib/xenops_hooks.ml rel=external target=_blank>Xenops_hooks</a>
module and looks for scripts in the hardcoded path <code>/etc/xapi.d</code>.</p><h2 id=2-create-a-xen-domain>2. create a Xen domain</h2><p>The <code>VM_create</code> micro-op calls the <code>VM.create</code> function in the backend.
In the classic Xenopsd backend, the
<a href=https://github.com/xapi-project/xen-api/blob/bae7526faeb2a02a2fe5b71410083983f4695963/ocaml/xenopsd/xc/xenops_server_xen.ml#L1421-L1586 rel=external target=_blank>VM.create_exn</a>
function must</p><ol><li>check if we&rsquo;re creating a domain for a fresh VM or resuming an existing one:
if it&rsquo;s a resume then the domain configuration stored in the VmExtra database
table must be used</li><li>ask <em>squeezed</em> to create a memory &ldquo;reservation&rdquo; big enough to hold the VM
memory. Unfortunately the domain cannot be created until the memory is free
because domain create often fails in low-memory conditions. This means the
&ldquo;reservation&rdquo; is associated with our &ldquo;session&rdquo; with squeezed; if Xenopsd
crashes and restarts the reservation will be freed automatically.</li><li>create the Domain via the libxc hypercall <code>Xenctrl.domain_create</code></li><li><a href=https://github.com/xapi-project/xen-api/blob/bae7526faeb2a02a2fe5b71410083983f4695963/ocaml/xenopsd/xc/xenops_server_xen.ml#L1547 rel=external target=_blank>call</a>
<a href=https://github.com/xapi-project/xen-api/blob/bae7526faeb2a02a2fe5b71410083983f4695963/ocaml/xenopsd/xc/xenops_server_xen.ml#L1302-L1419 rel=external target=_blank>generate_create_info()</a>
for storing the platform data (vCPUs, etc) the domain&rsquo;s Xenstore tree.
<code>xenguest</code> then uses this in the <code>build</code> phase (see below) to build the domain.</li><li>&ldquo;transfer&rdquo; the squeezed reservation to the domain such that squeezed will
free the memory if the domain is destroyed later</li><li>compute and set an initial balloon target depending on the amount of memory
reserved (recall we ask for a range between <em>dynamic_min</em> and <em>dynamic_max</em>)</li><li>apply the &ldquo;suppress spurious page faults&rdquo; workaround if requested</li><li>set the &ldquo;machine address size&rdquo;</li><li>&ldquo;hotplug&rdquo; the vCPUs. This operates a lot like memory ballooning &ndash; Xen creates
lots of vCPUs and then the guest is asked to only use some of them. Every VM
therefore starts with the &ldquo;VCPUs_max&rdquo; setting and co-operative hotplug is
used to reduce the number. Note there is no enforcement mechanism: a VM which
cheats and uses too many vCPUs would have to be caught by looking at the
performance statistics.</li></ol><h2 id=3-build-the-domain>3. build the domain</h2><p>The <code>build</code> phase waits, if necessary, for the Xen memory scrubber to catch
up reclaiming memory, runs NUMA placement, sets vCPU affinity and invokes
the <code>xenguest</code> to build the system memory layout of the domain.
See the <a href=/new-docs/xenopsd/walkthroughs/VM.build/index.html>walk-through of the VM_build μ-op</a> for details.</p><h2 id=4-mark-each-vbd-as-active>4. mark each VBD as &ldquo;active&rdquo;</h2><p>VBDs and VIFs are said to be &ldquo;active&rdquo; when they are intended to be used by a
particular VM, even if the backend/frontend connection hasn&rsquo;t been established,
or has been closed. If someone calls <code>VBD.stat</code> or <code>VIF.stat</code> then
the result includes both &ldquo;active&rdquo; and &ldquo;plugged&rdquo;, where &ldquo;plugged&rdquo; is true if
the frontend/backend connection is established.
For example xapi will
set <a href=https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1300 rel=external target=_blank>VBD.currently_attached</a>
to &ldquo;active || plugged&rdquo;. The &ldquo;active&rdquo; flag is conceptually very similar to the
traditional &ldquo;online&rdquo; flag (which is not documented in the upstream Xen tree
as of Oct/2014 but really should be) except that on unplug, one would set
the &ldquo;online&rdquo; key to &ldquo;0&rdquo; (false) <em>first</em> before initiating the hotunplug. By
contrast the &ldquo;active&rdquo; flag is set to false <em>after</em> the unplug i.e. &ldquo;set_active&rdquo;
calls bracket plug/unplug. If the &ldquo;active&rdquo; flag was set before the unplug
attempt then as soon as the frontend/backend connection is removed clients
would see the VBD as completely dissociated from the VM &ndash; this would be misleading
because Xenopsd will not have had time to use the storage API to release locks
on the disks. By cleaning up before setting &ldquo;active&rdquo; to false, clients
can be assured that the disks are now free to be reassigned.</p><h2 id=5-handle-non-persistent-disks>5. handle non-persistent disks</h2><p>A non-persistent disk is one which is reset to a known-good state on every
VM start. The <code>VBD_epoch_begin</code> is the signal to perform any necessary reset.</p><h2 id=6-plug-vbds>6. plug VBDs</h2><p>The <code>VBD_plug</code> micro-op will plug the VBD into the VM. Every VBD is plugged
in a carefully-chosen order.
Generally, plug order is important for all types of devices. For VBDs, we must
work around the deficiency in the storage interface where a VDI, once attached
read/only, cannot be attached read/write. Since it is legal to attach the same
VDI with multiple VBDs, we must plug them in such that the read/write VBDs
come first. From the guest&rsquo;s point of view the order we plug them doesn&rsquo;t
matter because they are indexed by the Xenstore device id (e.g. 51712 = xvda).</p><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1631 rel=external target=_blank>VBD.plug</a>
will</p><ul><li>call <code>VDI.attach</code> and <code>VDI.activate</code> in the storage API to make the
devices ready (start the tapdisk processes etc)</li><li>add the Xenstore frontend/backend directories containing the block device
info</li><li>add the extra xenstore keys returned by the <code>VDI.attach</code> call that are
needed for SCSIid passthrough which is needed to support VSS</li><li>write the VBD information to the Xenopsd database so that future calls to
<em>VBD.stat</em> can be told about the associated disk (this is needed so clients
like xapi can cope with CD insert/eject etc)</li><li>if the qemu is going to be in a different domain to the storage, a frontend
device in the qemu domain is created.</li></ul><p>The Xenstore keys are written by the functions
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L486 rel=external target=_blank>Device.Vbd.add_async</a>
and
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L550 rel=external target=_blank>Device.Vbd.add_wait</a>.
In a Linux domain (such as dom0) when the backend directory is created, the kernel
creates a &ldquo;backend device&rdquo;. Creating any device will cause a kernel UEVENT to fire
which is picked up by udev. The udev rules run a script whose only job is to
stat(2) the device (from the &ldquo;params&rdquo; key in the backend) and write the major
and minor number to Xenstore for blkback to pick up. (Aside: FreeBSD doesn&rsquo;t do
any of this, instead the FreeBSD kernel module simply opens the device in the
&ldquo;params&rdquo; key). The script also writes the backend key &ldquo;hotplug-status=connected&rdquo;.
We currently wait for this key to be written so that later calls to <em>VBD.stat</em>
will return with &ldquo;plugged=true&rdquo;. If the call returns before this key is written
then sometimes we receive an event, call <em>VBD.stat</em> and conclude erroneously
that a spontaneous VBD unplug occurred.</p><h2 id=7-mark-each-vif-as-active>7. mark each VIF as &ldquo;active&rdquo;</h2><p>This is for the same reason as VBDs are marked &ldquo;active&rdquo;.</p><h2 id=8-plug-vifs>8. plug VIFs</h2><p>Again, the order matters. Unlike VBDs,
there is no read/write read/only constraint and the devices
have unique indices (0, 1, 2, &mldr;) <em>but</em> Linux kernels have often (always?)
ignored the actual index and instead relied on the order of results from the
<code>xenstore-ls</code> listing. The order that xenstored returns the items happens
to be the order the nodes were created so this means that (i) xenstored must
continue to store directories as ordered lists rather than maps (which would
be more efficient); and (ii) Xenopsd must make sure to plug the vifs in
the same order. Note that relying on ethX device numbering has always been a
bad idea but is still common. I bet if you change this, many tests will
suddenly start to fail!</p><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1945 rel=external target=_blank>VIF.plug_exn</a>
will</p><ul><li>compute the port locking configuration required and write this to a well-known
location in the filesystem where it can be read from the udev scripts. This
really should be written to Xenstore instead, since this scheme doesn&rsquo;t work
with driver domains.</li><li>add the Xenstore frontend/backend directories containing the network device
info</li><li>write the VIF information to the Xenopsd database so that future calls to
<em>VIF.stat</em> can be told about the associated network</li><li>if the qemu is going to be in a different domain to the storage, a frontend
device in the qemu domain is created.</li></ul><p>Similarly to the VBD case, the function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/device.ml#L642 rel=external target=_blank>Device.Vif.add</a>
will write the Xenstore keys and wait for the &ldquo;hotplug-status=connected&rdquo; key.
We do this because we cannot apply the port locking rules until the backend
device has been created, and we cannot know the rules have been applied
until after the udev script has written the key. If we didn&rsquo;t wait for it then
the VM might execute without all the port locking properly configured.</p><h2 id=9-create-the-device-model>9. create the device model</h2><p>The <code>VM_create_device_model</code> micro-op will create a qemu device model if</p><ul><li>the VM is HVM; or</li><li>the VM uses a PV keyboard or mouse (since only qemu currently has backend
support for these devices).</li></ul><p>The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1090 rel=external target=_blank>VM.create_device_model_exn</a>
will</p><ul><li>(if using a qemu stubdom) it will create and build the qemu domain</li><li>compute the necessary qemu arguments and launch it.</li></ul><p>Note that qemu (aka the &ldquo;device model&rdquo;) is created after the VIFs and VBDs have
been plugged but before the PCI devices have been plugged. Unfortunately qemu
traditional infers the needed emulated hardware by inspecting the Xenstore
VBD and VIF configuration and assuming that we want one emulated device per
PV device, up to the natural limits of the emulated buses (i.e. there can be
at most 4 IDE devices: {primary,secondary}{master,slave}). Not only does this
create an ordering dependency that needn&rsquo;t exist &ndash; and which impacts migration
downtime &ndash; but it also completely ignores the plain fact that, on a Xen system,
qemu can be in a different domain than the backend disk and network devices.
This hack only works because we currently run everything in the same domain.
There is an option (off by default) to list the emulated devices explicitly
on the qemu command-line. If we switch to this by default then we ought to be
able to start up qemu early, as soon as the domain has been created (qemu will
need to know the domain id so it can map the I/O request ring).</p><h2 id=10-plug-pci-devices>10. plug PCI devices</h2><p>PCI devices are treated differently to VBDs and VIFs.
If we are attaching the device to an
HVM guest then instead of relying on the traditional Xenstore frontend/backend
state machine we instead send RPCs to qemu requesting they be hotplugged. Note
the domain is paused at this point, but qemu still supports PCI hotplug/unplug.
The reasons why this doesn&rsquo;t follow the standard Xenstore model are known only
to the people who contributed this support to qemu.
Again the order matters because it determines the position of the virtual device
in the VM.</p><p>Note that Xenopsd doesn&rsquo;t know anything about the PCI devices; concepts such
as &ldquo;GPU groups&rdquo; belong to higher layers, such as xapi.</p><h2 id=11-mark-the-domain-as-alive>11. mark the domain as alive</h2><p>A design principle of Xenopsd is that it should tolerate failures such as being
suddenly restarted. It guarantees to always leave the system in a valid state,
in particular there should never be any &ldquo;half-created VMs&rdquo;. We achieve this for
VM start by exploiting the mechanism which is necessary for reboot. When a VM
wishes to reboot it causes the domain to exit (via SCHEDOP_shutdown) with a
&ldquo;reason code&rdquo; of &ldquo;reboot&rdquo;. When Xenopsd sees this event <code>VM_check_state</code>
operation is queued. This operation calls
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L1443 rel=external target=_blank>VM.get_domain_action_request</a>
to ask the question, &ldquo;what needs to be done to make this VM happy now?&rdquo;. The
implementation checks the domain state for shutdown codes and also checks a
special Xenopsd Xenstore key. When Xenopsd creates a Xen domain it sets this
key to &ldquo;reboot&rdquo; (meaning &ldquo;please reboot me if you see me&rdquo;) and when Xenopsd
finishes starting the VM it clears this key. This means that if Xenopsd crashes
while starting a VM, the new Xenopsd will conclude that the VM needs to be rebooted
and will clean up the current domain and create a fresh one.</p><h2 id=12-unpause-the-domain>12. unpause the domain</h2><p>A Xenopsd VM.start will always leave the domain paused, so strictly speaking
this is a separate &ldquo;operation&rdquo; queued by the client (such as xapi) after the
VM.start has completed. The function
<a href=https://github.com/xapi-project/xenopsd/blob/b33bab13080cea91e2fd59d5088622cd68152339/xc/xenops_server_xen.ml#L808 rel=external target=_blank>VM.unpause</a>
is reassuringly simple:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>		<span style=color:#66d9ef>if</span> di<span style=color:#f92672>.</span>Xenctrl.total_memory_pages <span style=color:#f92672>=</span> 0n <span style=color:#66d9ef>then</span> <span style=color:#66d9ef>raise</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>Domain_not_built</span><span style=color:#f92672>);</span>
</span></span><span style=display:flex><span>		Domain.unpause <span style=color:#f92672>~</span>xc di<span style=color:#f92672>.</span>Xenctrl.domid<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>		Opt.iter
</span></span><span style=display:flex><span>			<span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> stubdom_domid <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>				Domain.unpause <span style=color:#f92672>~</span>xc stubdom_domid
</span></span><span style=display:flex><span>			<span style=color:#f92672>)</span> <span style=color:#f92672>(</span>get_stubdom <span style=color:#f92672>~</span>xs di<span style=color:#f92672>.</span>Xenctrl.domid<span style=color:#f92672>)</span></span></span></code></pre></div><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=building-a-vm>Building a VM</h1><pre class="mermaid align-center">flowchart
subgraph xenopsd VM_build[xenopsd:&amp;nbsp;VM_build&amp;nbsp;micro#8209;op]
direction LR
VM_build --&gt; VM.build
VM.build --&gt; VM.build_domain
VM.build_domain --&gt; VM.build_domain_exn
VM.build_domain_exn --&gt; Domain.build
click VM_build &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/lib/xenops_server.ml#L2255-L2271&#34; _blank
click VM.build &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/xenops_server_xen.ml#L2290-L2291&#34; _blank
click VM.build_domain &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/xenops_server_xen.ml#L2250-L2288&#34; _blank
click VM.build_domain_exn &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/xenops_server_xen.ml#L2024-L2248&#34; _blank
click Domain.build &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/domain.ml#L1111-L1210&#34; _blank
end</pre><p>Walk-through documents for the <code>VM_build</code> phase:</p><ul class="children children-li children-sort-"><li><a href=/new-docs/xenopsd/walkthroughs/VM.build/VM_build/index.html>VM_build μ-op</a><p>Overview of the VM_build μ-op (runs after the VM_create μ-op created the domain).</p></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html>Domain.build</a><p>Prepare the build of a VM: Wait for scrubbing, do NUMA placement, run xenguest.</p></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.build/xenguest/index.html>xenguest</a><p>Perform building VMs: Allocate and populate the domain's system memory.</p></li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Building a VM</h1><article class=default><header class=headline></header><h1 id=vm_build-micro-op>VM_build micro-op</h1><h2 id=overview>Overview</h2><p>On Xen, <code>Xenctrl.domain_create</code> creates an empty domain and
returns the domain ID (<code>domid</code>) of the new domain to <code>xenopsd</code>.</p><p>In the <code>build</code> phase, the <code>xenguest</code> program is called to create
the system memory layout of the domain, set vCPU affinity and a
lot more.</p><p>The <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/lib/xenops_server.ml#L2255-L2271 rel=external target=_blank>VM_build</a>
micro-op collects the VM build parameters and calls
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2290-L2291 rel=external target=_blank>VM.build</a>,
which calls
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2250-L2288 rel=external target=_blank>VM.build_domain</a>,
which calls
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2024-L2248 rel=external target=_blank>VM.build_domain_exn</a>
which calls <a href=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html>Domain.build</a>:</p><pre class="mermaid align-center">flowchart
subgraph xenopsd VM_build[xenopsd:&amp;nbsp;VM_build&amp;nbsp;micro#8209;op]
direction LR
VM_build --&gt; VM.build
VM.build --&gt; VM.build_domain
VM.build_domain --&gt; VM.build_domain_exn
VM.build_domain_exn --&gt; Domain.build
click VM_build &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/lib/xenops_server.ml#L2255-L2271&#34; _blank
click VM.build &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/xenops_server_xen.ml#L2290-L2291&#34; _blank
click VM.build_domain &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/xenops_server_xen.ml#L2250-L2288&#34; _blank
click VM.build_domain_exn &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/xenops_server_xen.ml#L2024-L2248&#34; _blank
click Domain.build &#34;
https://github.com/xapi-project/xen-api/blob/83555067/ocaml/xenopsd/xc/domain.ml#L1111-L1210&#34; _blank
end</pre><p>The function
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2024 rel=external target=_blank>VM.build_domain_exn</a>
must:</p><ol><li><p>Run pygrub (or eliloader) to extract the kernel and initrd, if necessary</p></li><li><p><a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2222-L2225 rel=external target=_blank>Call</a>
<a href=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html>Domain.build</a> to</p><ul><li>optionally run NUMA placement and</li><li>invoke <a href=VM.build/xenguest>xenguest</a> to set up the domain memory.</li></ul><p>See the walk-through of the <a href=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html>Domain.build</a> function
for more details on this phase.</p></li><li><p>Apply the <code>cpuid</code> configuration</p></li><li><p>Store the current domain configuration on disk &ndash; it&rsquo;s important to know
the difference between the configuration you started with and the configuration
you would use after a reboot because some properties (such as maximum memory
and vCPUs) as fixed on create.</p></li></ol><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=domainbuild>Domain.build</h1><h2 id=overview>Overview</h2><pre class="mermaid align-center">flowchart LR
subgraph xenopsd VM_build[
  xenopsd&amp;nbsp;thread&amp;nbsp;pool&amp;nbsp;with&amp;nbsp;two&amp;nbsp;VM_build&amp;nbsp;micro#8209;ops:
  During&amp;nbsp;parallel&amp;nbsp;VM_start,&amp;nbsp;Many&amp;nbsp;threads&amp;nbsp;run&amp;nbsp;this&amp;nbsp;in&amp;nbsp;parallel!
]
direction LR
build_domain_exn[
  VM.build_domain_exn
  from thread pool Thread #1
]  --&gt; Domain.build
Domain.build --&gt; build_pre
build_pre --&gt; wait_xen_free_mem
build_pre --&gt;|if NUMA/Best_effort| numa_placement
Domain.build --&gt; xenguest[Invoke xenguest]
click Domain.build &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1111-L1210&#34; _blank
click build_domain_exn &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2222-L2225&#34; _blank
click wait_xen_free_mem &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L236-L272&#34; _blank
click numa_placement &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L862-L897&#34; _blank
click build_pre &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L899-L964&#34; _blank
click xenguest &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1139-L1146&#34; _blank

build_domain_exn2[
  VM.build_domain_exn
  from thread pool Thread #2]  --&gt; Domain.build2[Domain.build]
Domain.build2 --&gt; build_pre2[build_pre]
build_pre2 --&gt; wait_xen_free_mem2[wait_xen_free_mem]
build_pre2 --&gt;|if NUMA/Best_effort| numa_placement2[numa_placement]
Domain.build2 --&gt; xenguest2[Invoke xenguest]
click Domain.build2 &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1111-L1210&#34; _blank
click build_domain_exn2 &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2222-L2225&#34; _blank
click wait_xen_free_mem2 &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L236-L272&#34; _blank
click numa_placement2 &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L862-L897&#34; _blank
click build_pre2 &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L899-L964&#34; _blank
click xenguest2 &#34;https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1139-L1146&#34; _blank
end</pre><p><a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2024-L2248 rel=external target=_blank><code>VM.build_domain_exn</code></a>
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/xenops_server_xen.ml#L2222-L2225 rel=external target=_blank>calls</a>
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1111-L1210 rel=external target=_blank><code>Domain.build</code></a>
to call:</p><ul><li><code>build_pre</code> to prepare the build of a VM:<ul><li>If the <code>xe</code> config <code>numa_placement</code> is set to <code>Best_effort</code>, invoke the NUMA placement algorithm.</li><li>Run <code>xenguest</code></li></ul></li><li><code>xenguest</code> to invoke the <a href=/new-docs/xenopsd/walkthroughs/VM.build/xenguest/index.html>xenguest</a> program to setup the domain&rsquo;s system memory.</li></ul><h2 id=build_pre-prepare-building-the-vm>build_pre: Prepare building the VM</h2><p><a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1111-L1210 rel=external target=_blank>Domain.build</a>
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1137 rel=external target=_blank>calls</a>
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L899-L964 rel=external target=_blank>build_pre</a>
(which is also used for VM restore) to:</p><ol><li><p><a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L902-L911 rel=external target=_blank>Call</a>
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L236-L272 rel=external target=_blank>wait_xen_free_mem</a>
to wait (if necessary), for the Xen memory scrubber to catch up reclaiming memory.
It</p><ol><li>calls <code>Xenctrl.physinfo</code> which returns:<ul><li><code>hostinfo.free_pages</code> - the free and already scrubbed pages (available)</li><li><code>host.scrub_pages</code> - the not yet scrubbed pages (not yet available)</li></ul></li><li>repeats this until a timeout as long as <code>free_pages</code> is <em>lower</em>
than the <em>required</em> pages<ul><li>unless if <code>scrub_pages</code> is 0 (no scrubbing left to do)</li></ul></li></ol><p>Note: <code>free_pages</code> is system-wide memory, not memory specific to a NUMA node.
Because this is not NUMA-aware, in case of temporary node-specific memory shortage,
this check is not sufficient to prevent the VM from being spread over all NUMA nodes.
It is planned to resolve this issue by claiming NUMA node memory during NUMA placement.</p></li><li><p>Call the hypercall to set the timer mode</p></li><li><p>Call the hypercall to set the number of vCPUs</p></li><li><p>Call the <code>numa_placement</code> function
as described in the <a href=/new-docs/toolstack/features/NUMA/index.html>NUMA feature description</a>
when the <code>xe</code> configuration option <code>numa_placement</code> is set to <code>Best_effort</code>
(except when the VM has a hard CPU affinity).</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ml data-lang=ml><span style=display:flex><span><span style=color:#66d9ef>match</span> <span style=color:#f92672>!</span>Xenops_server.numa_placement <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span> <span style=color:#a6e22e>Any</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    ()
</span></span><span style=display:flex><span><span style=color:#f92672>|</span> <span style=color:#a6e22e>Best_effort</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    log_reraise <span style=color:#f92672>(</span>Printf.sprintf <span style=color:#e6db74>&#34;NUMA placement&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>fun</span> () <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> has_hard_affinity <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>          D.debug <span style=color:#e6db74>&#34;VM has hard affinity set, skipping NUMA optimization&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>          numa_placement domid <span style=color:#f92672>~</span>vcpus
</span></span><span style=display:flex><span>            <span style=color:#f92672>~</span>memory<span style=color:#f92672>:(</span>Int64.mul memory<span style=color:#f92672>.</span>xen_max_mib 1048576L<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>)</span></span></span></code></pre></div></li></ol><h2 id=numa-placement>NUMA placement</h2><p><code>build_pre</code> passes the <code>domid</code>, the number of <code>vCPUs</code> and <code>xen_max_mib</code> to the
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L862-L897 rel=external target=_blank>numa_placement</a>
function to run the algorithm to find the best NUMA placement.</p><p>When it returns a NUMA node to use, it calls the Xen hypercalls
to set the vCPU affinity to this NUMA node:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ml data-lang=ml><span style=display:flex><span>  <span style=color:#66d9ef>let</span> vm <span style=color:#f92672>=</span> NUMARequest.make <span style=color:#f92672>~</span>memory <span style=color:#f92672>~</span>vcpus <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> nodea <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>match</span> <span style=color:#f92672>!</span>numa_resources <span style=color:#66d9ef>with</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> <span style=color:#a6e22e>None</span> <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        Array.of_list nodes
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span> <span style=color:#a6e22e>Some</span> a <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>        Array.map2 NUMAResource.min_memory <span style=color:#f92672>(</span>Array.of_list nodes<span style=color:#f92672>)</span> a
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>  numa_resources <span style=color:#f92672>:=</span> <span style=color:#a6e22e>Some</span> nodea <span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  Softaffinity.plan <span style=color:#f92672>~</span>vm host nodea</span></span></code></pre></div><p>By using the default <code>auto_node_affinity</code> feature of Xen,
setting the vCPU affinity causes the Xen hypervisor to activate
NUMA node affinity for memory allocations to be aligned with
the vCPU affinity of the domain.</p><p>Summary: This passes the information to the hypervisor that memory
allocation for this domain should preferably be done from this NUMA node.</p><h2 id=invoke-the-xenguest-program>Invoke the xenguest program</h2><p>With the preparation in <code>build_pre</code> completed, <code>Domain.build</code>
<a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/domain.ml#L1127-L1155 rel=external target=_blank>calls</a>
the <code>xenguest</code> function to invoke the <a href=/new-docs/xenopsd/walkthroughs/VM.build/xenguest/index.html>xenguest</a> program to build the domain.</p><h2 id=notes-on-future-design-improvements>Notes on future design improvements</h2><p>The Xen domain feature flag
<a href=https://wiki.xenproject.org/wiki/NUMA_node_affinity_in_the_Xen_hypervisor rel=external target=_blank>domain->auto_node_affinity</a>
can be disabled by calling
<a href=../../references/xc_domain_node_setaffinity.md>xc_domain_node_setaffinity()</a>
to set a specific NUMA node affinity in special cases:</p><p>This can be used, for example, when there might not be enough memory on the preferred
NUMA node, and there are other NUMA nodes (in the same CPU package) to use
(<a href=/new-docs/lib/xenctrl/xc_domain_node_setaffinity/index.html>reference</a>).</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=xenguest>xenguest</h1><p>As part of starting a new domain in VM_build, <code>xenopsd</code> calls <code>xenguest</code>.
When multiple domain build threads run in parallel,
also multiple instances of <code>xenguest</code> also run in parallel:</p><pre class="mermaid align-center">flowchart
subgraph xenopsd VM_build[xenopsd&amp;nbsp;VM_build&amp;nbsp;micro#8209;ops]
direction LR
xenopsd1[Domain.build - Thread #1] --&gt; xenguest1[xenguest #1]
xenopsd2[Domain.build - Thread #2] --&gt; xenguest2[xenguest #2]
xenguest1 --&gt; libxenguest
xenguest2 --&gt; libxenguest2[libxenguest]
click xenopsd1 &#34;../Domain.build/index.html&#34;
click xenopsd2 &#34;../Domain.build/index.html&#34;
click xenguest1 &#34;https://github.com/xenserver/xen.pg/blob/XS-8/patches/xenguest.patch&#34; _blank
click xenguest2 &#34;https://github.com/xenserver/xen.pg/blob/XS-8/patches/xenguest.patch&#34; _blank
click libxenguest &#34;https://github.com/xen-project/xen/tree/master/tools/libs/guest&#34; _blank
click libxenguest2 &#34;https://github.com/xen-project/xen/tree/master/tools/libs/guest&#34; _blank
libxenguest --&gt; Xen[Xen&lt;br&gt;Hypervisor]
libxenguest2 --&gt; Xen
end</pre><h2 id=about-xenguest>About xenguest</h2><p><code>xenguest</code> is called by the xenopsd <a href=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html>Domain.build</a> function
to perform the build phase for new VMs, which is part of the <code>xenopsd</code>
<a href=VM.start>VM.start operation</a>.</p><p><a href=https://github.com/xenserver/xen.pg/blob/XS-8/patches/xenguest.patch rel=external target=_blank>xenguest</a>
was created as a separate program due to issues with
<code>libxenguest</code>:</p><ul><li>It wasn&rsquo;t threadsafe: fixed, but it still uses a per-call global struct</li><li>It had an incompatible licence, but now licensed under the LGPL.</li></ul><p>Those were fixed, but we still shell out to <code>xenguest</code>, which is currently
carried in the patch queue for the Xen hypervisor packages, but could become
an individual package once planned changes to the Xen hypercalls are stabilised.</p><p>Over time, <code>xenguest</code> has evolved to build more of the initial domain state.</p><h2 id=interface-to-xenguest>Interface to xenguest</h2><pre class="mermaid align-center">flowchart
subgraph xenopsd VM_build[xenopsd&amp;nbsp;VM_build&amp;nbsp;micro#8209;op]
direction TB
mode
domid
memmax
Xenstore
end
mode[--mode build_hvm] --&gt; xenguest
domid --&gt; xenguest
memmax --&gt; xenguest
Xenstore[Xenstore platform data] --&gt; xenguest</pre><p><code>xenopsd</code> must pass this information to <code>xenguest</code> to build a VM:</p><ul><li>The domain type to build for (HVM, PHV or PV).<ul><li>It is passed using the command line option <code>--mode hvm_build</code>.</li></ul></li><li>The <code>domid</code> of the created empty domain,</li><li>The amount of system memory of the domain,</li><li>A number of other parameters that are domain-specific.</li></ul><p><code>xenopsd</code> uses the Xenstore to provide platform data:</p><ul><li>the vCPU affinity</li><li>the vCPU credit2 weight/cap parameters</li><li>whether the NX bit is exposed</li><li>whether the viridian CPUID leaf is exposed</li><li>whether the system has PAE or not</li><li>whether the system has ACPI or not</li><li>whether the system has nested HVM or not</li><li>whether the system has an HPET or not</li></ul><p>When called to build a domain, <code>xenguest</code> reads those and builds the VM accordingly.</p><h2 id=walkthrough-of-the-xenguest-build-mode>Walkthrough of the xenguest build mode</h2><pre class="mermaid align-center">flowchart
subgraph xenguest[xenguest&amp;nbsp;#8209;#8209;mode&amp;nbsp;hvm_build&amp;nbsp;domid]
direction LR
stub_xc_hvm_build[stub_xc_hvm_build#40;#41;] --&gt; get_flags[
    get_flags#40;#41;&amp;nbsp;&lt;#8209;&amp;nbsp;Xenstore&amp;nbsp;platform&amp;nbsp;data
]
stub_xc_hvm_build --&gt; configure_vcpus[
    configure_vcpus#40;#41;&amp;nbsp;#8209;&gt;&amp;nbsp;Xen&amp;nbsp;hypercall
]
stub_xc_hvm_build --&gt; setup_mem[
    setup_mem#40;#41;&amp;nbsp;#8209;&gt;&amp;nbsp;Xen&amp;nbsp;hypercalls&amp;nbsp;to&amp;nbsp;setup&amp;nbsp;domain&amp;nbsp;memory
]
end</pre><p>Based on the given domain type, the <code>xenguest</code> program calls dedicated
functions for the build process of the given domain type.</p><p>These are:</p><ul><li><code>stub_xc_hvm_build()</code> for HVM,</li><li><code>stub_xc_pvh_build()</code> for PVH, and</li><li><code>stub_xc_pv_build()</code> for PV domains.</li></ul><p>These domain build functions call these functions:</p><ol><li><code>get_flags()</code> to get the platform data from the Xenstore</li><li><code>configure_vcpus()</code> which uses the platform data from the Xenstore to configure vCPU affinity and the credit scheduler parameters vCPU weight and vCPU cap (max % pCPU time for throttling)</li><li>The <code>setup_mem</code> function for the given VM type.</li></ol><h2 id=the-function-hvm_build_setup_mem>The function hvm_build_setup_mem()</h2><p>For HVM domains, <code>hvm_build_setup_mem()</code> is responsible for deriving the memory
layout of the new domain, allocating the required memory and populating for the
new domain. It must:</p><ol><li>Derive the <code>e820</code> memory layout of the system memory of the domain
including memory holes depending on PCI passthrough and vGPU flags.</li><li>Load the BIOS/UEFI firmware images</li><li>Store the final MMIO hole parameters in the Xenstore</li><li>Call the <code>libxenguest</code> function <code>xc_dom_boot_mem_init()</code> (see below)</li><li>Call <code>construct_cpuid_policy()</code> to apply the CPUID <code>featureset</code> policy</li></ol><h2 id=the-function-xc_dom_boot_mem_init>The function xc_dom_boot_mem_init()</h2><pre class="mermaid align-center">flowchart LR
subgraph xenguest
hvm_build_setup_mem[hvm_build_setup_mem#40;#41;]
end
subgraph libxenguest
hvm_build_setup_mem --&gt; xc_dom_boot_mem_init[xc_dom_boot_mem_init#40;#41;]
xc_dom_boot_mem_init --&gt;|vmemranges| meminit_hvm[meninit_hvm#40;#41;]
click xc_dom_boot_mem_init &#34;https://github.com/xen-project/xen/blob/39c45c/tools/libs/guest/xg_dom_boot.c#L110-L126&#34; _blank
click meminit_hvm &#34;https://github.com/xen-project/xen/blob/39c45c/tools/libs/guest/xg_dom_x86.c#L1348-L1648&#34; _blank
end</pre><p><code>hvm_build_setup_mem()</code> calls
<a href=https://github.com/xen-project/xen/blob/39c45c/tools/libs/guest/xg_dom_boot.c#L110-L126 rel=external target=_blank>xc_dom_boot_mem_init()</a>
to allocate and populate the domain&rsquo;s system memory.</p><p>It calls
<a href=https://github.com/xen-project/xen/blob/39c45c/tools/libs/guest/xg_dom_x86.c#L1348-L1648 rel=external target=_blank>meminit_hvm()</a>
to loop over the <code>vmemranges</code> of the domain for mapping the system RAM
of the guest from the Xen hypervisor heap. Its goals are:</p><ul><li>Attempt to allocate 1GB superpages when possible</li><li>Fall back to 2MB pages when 1GB allocation failed</li><li>Fall back to 4k pages when both failed</li></ul><p>It uses the hypercall
<a href=https://github.com/xen-project/xen/blob/39c45c/xen/common/memory.c#L1408-L1477 rel=external target=_blank>XENMEM_populate_physmap</a>
to perform memory allocation and to map the allocated memory
to the system RAM ranges of the domain.</p><p><a href=https://github.com/xen-project/xen/blob/39c45c/xen/common/memory.c#L1022-L1071 rel=external target=_blank>https://github.com/xen-project/xen/blob/39c45c/xen/common/memory.c#L1022-L1071</a></p><p><code>XENMEM_populate_physmap</code>:</p><ol><li>Uses
<a href=https://github.com/xen-project/xen/blob/39c45c/xen/common/memory.c#L1022-L1071 rel=external target=_blank>construct_memop_from_reservation</a>
to convert the arguments for allocating a page from
<a href=https://github.com/xen-project/xen/blob/master/xen/include/public/memory.h#L46-L80 rel=external target=_blank>struct xen_memory_reservation</a>
to <code>struct memop_args</code>.</li><li>Sets flags and calls functions according to the arguments</li><li>Allocates the requested page at the most suitable place<ul><li>depending on passed flags, allocate on a specific NUMA node</li><li>else, if the domain has node affinity, on the affine nodes</li><li>also in the most suitable memory zone within the NUMA node</li></ul></li><li>Falls back to less desirable places if this fails<ul><li>or fail for &ldquo;exact&rdquo; allocation requests</li></ul></li><li>When no pages of the requested size are free,
it splits larger superpages into pages of the requested size.</li></ol><p>For more details on the VM build step involving <code>xenguest</code> and Xen side see:
<a href=https://wiki.xenproject.org/wiki/Walkthrough:_VM_build_using_xenguest rel=external target=_blank>https://wiki.xenproject.org/wiki/Walkthrough:_VM_build_using_xenguest</a></p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=walkthrough-migrating-a-vm>Walkthrough: Migrating a VM</h1><p>At the end of this walkthrough, a sequence diagram of the overall process is included.</p><h2 id=invocation>Invocation</h2><p>The command to migrate the VM is dispatched
by the autogenerated <code>dispatch_call</code> function from <strong>xapi/server.ml</strong>. For
more information about the generated functions you can have a look to
<a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/idl/ocaml_backend rel=external target=_blank>XAPI IDL model</a>.</p><p>The command triggers the operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/lib/xenops_server.ml#L2572 rel=external target=_blank>VM_migrate</a>
that uses many low level atomics operations. These are:</p><ul><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#VM-restore>VM.restore</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#VM-rename>VM.rename</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#restoring-devices>VBD.set_active</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#restoring-devices>VBD.plug</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#restoring-devices>VIF.set_active</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#restoring-devices>VGPU.set_active</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#creating-the-device-model>VM.create_device_model</a></li><li><a href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html#pci-plug>PCI.plug</a></li></ul><p>The migrate command has several parameters such as:</p><ul><li>Should it be started asynchronously,</li><li>Should it be forwarded to another host,</li><li>How arguments should be marshalled, and so on.</li></ul><p>A new thread is created by <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xapi/server_helpers.ml#L55 rel=external target=_blank>xapi/server_helpers.ml</a>
to handle the command asynchronously. The helper thread checks if
the command should be passed to the <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xapi/message_forwarding.ml rel=external target=_blank>message forwarding</a>
layer in order to be executed on another host (the destination) or locally (if
it is already at the destination host).</p><p>It will finally reach <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xapi/api_server.ml#L242 rel=external target=_blank>xapi/api_server.ml</a> that will take the action
of posted a command to the message broker <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/message-switch rel=external target=_blank>message switch</a>.
It is a JSON-RPC HTTP request sends on a Unix socket to communicate between some
XAPI daemons. In the case of the migration this message sends by <strong>XAPI</strong> will be
consumed by the <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd rel=external target=_blank>xenopsd</a>
daemon that will do the job of migrating the VM.</p><h2 id=overview>Overview</h2><p>The migration is an asynchronous task and a thread is created to handle this task.
The task reference is returned to the client, which can then check
its status until completion.</p><p>As shown in the introduction, <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd rel=external target=_blank>xenopsd</a>
fetches the
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/lib/xenops_server.ml#L2572 rel=external target=_blank>VM_migrate</a>
operation from the message broker.</p><p>All tasks specific to <a href=/new-docs/lib/xenctrl/index.html>libxenctrl</a>,
<a href=/new-docs/xenopsd/walkthroughs/VM.build/xenguest/index.html>xenguest</a> and <a href=https://wiki.xenproject.org/wiki/XenStore rel=external target=_blank>Xenstore</a>
are handled by the xenopsd
<a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/xenopsd/xc rel=external target=_blank>xc backend</a>.</p><p>The entities that need to be migrated are: <em>VDI</em>, <em>VIF</em>, <em>VGPU</em> and <em>PCI</em> components.</p><p>During the migration process, the destination domain will be built with the same
UUID as the original VM, except that the last part of the UUID will be
<code>XXXXXXXX-XXXX-XXXX-XXXX-000000000001</code>. The original domain will be removed using
<code>XXXXXXXX-XXXX-XXXX-XXXX-000000000000</code>.</p><h2 id=preparing-vm-migration>Preparing VM migration</h2><p>At specific places, <code>xenopsd</code> can execute <em>hooks</em> to run scripts.
In case a pre-migrate script is in place, a command to run this script
is sent to the original domain.</p><p>Likewise, a command is sent to Qemu using the Qemu Machine Protocol (QMP)
to check that the domain can be suspended (see <a href=https://github.com/xapi-project/xen-api/blob/master/ocaml/xenopsd/xc/device_common.ml rel=external target=_blank>xenopsd/xc/device_common.ml</a>).
After checking with Qemu that the VM is can be suspended, the migration can begin.</p><h2 id=importing-metadata>Importing metadata</h2><p>As for <em>hooks</em>, commands to source domain are sent using <a href=https://github.com/xapi-project/xen-api/tree/master/ocaml/libs/stunnel rel=external target=_blank>stunnel</a> a daemon which
is used as a wrapper to manage SSL encryption communication between two hosts on the same
pool. To import the metadata, an XML RPC command is sent to the original domain.</p><p>Once imported, it will give us a reference id and will allow building the new domain
on the destination using the temporary VM uuid <code>XXXXXXXX-XXXX-XXXX-XXXX-000000000001</code>
where <code>XXX...</code> is the reference id of the original VM.</p><h2 id=memory-setup>Memory setup</h2><p>One of the first steps the setup of the VM&rsquo;s memory: The backend checks that there
is no ballooning operation in progress. If so, the migration could fail.</p><p>Once memory has been checked, the daemon will get the state of the VM (running, halted, &mldr;) and
The backend retrieves the domain&rsquo;s platform data (memory, vCPUs setc) from the Xenstore.</p><p>Once this is complete, we can restore VIF and create the domain.</p><p>The synchronisation of the memory is the first point of synchronisation and everything
is ready for VM migration.</p><h2 id=destination-vm-setup>Destination VM setup</h2><p>After receiving memory we can set up the destination domain. If we have a vGPU we need to kick
off its migration process. We will need to wait for the acknowledgement that the
GPU entry has been successfully initialized before starting the main VM migration.</p><p>The receiver informs the sender using a handshake protocol
that everything is set up and ready for save/restore.</p><h2 id=destination-vm-restore>Destination VM restore</h2><p>VM restore is a low level atomic operation <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L2684 rel=external target=_blank>VM.restore</a>.
This operation is represented by a function call to <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/domain.ml#L1540 rel=external target=_blank>backend</a>.
It uses <strong>Xenguest</strong>, a low-level utility from XAPI toolstack, to interact with the Xen hypervisor
and <code>libxc</code> for sending a migration request to the <strong>emu-manager</strong>.</p><p>After sending the request results coming from <strong>emu-manager</strong> are collected
by the main thread. It blocks until results are received.</p><p>During the live migration, <strong>emu-manager</strong> helps in ensuring the correct state
transitions for the devices and handling the message passing for the VM as
it&rsquo;s moved between hosts. This includes making sure that the state of the
VM&rsquo;s virtual devices, like disks or network interfaces, is correctly moved over.</p><h2 id=destination-vm-rename>Destination VM rename</h2><p>Once all operations are done, <code>xenopsd</code> renames the target VM from its temporary
name to its real UUID. This operation is a low-level atomic
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L1667 rel=external target=_blank>VM.rename</a>
which takes care of updating the Xenstore on the destination host.</p><h2 id=restoring-devices>Restoring devices</h2><p>Restoring devices starts by activating VBD using the low level atomic operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3674 rel=external target=_blank>VBD.set_active</a>. It is an update of Xenstore. VBDs that are read-write must
be plugged before read-only ones. Once activated the low level atomic operation
<a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3721 rel=external target=_blank>VBD.plug</a>
is called. VDI are attached and activate.</p><p>Next devices are VIFs that are set as active <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L4296 rel=external target=_blank>VIF.set_active</a> and plug <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L4394 rel=external target=_blank>VIF.plug</a>.
If there are VGPUs we will set them as active now using the atomic <a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3490 rel=external target=_blank>VGPU.set_active</a>.</p><h3 id=creating-the-device-model>Creating the device model</h3><p><a href=https://github.com/xapi-project/xen-api/blob/ec3b62ee/ocaml/xenopsd/xc/xenops_server_xen.ml#L2293-L2349 rel=external target=_blank>create_device_model</a>
configures <strong>qemu-dm</strong> and starts it. This allows to manage PCI devices.</p><h3 id=pci-plug>PCI plug</h3><p><a href=https://github.com/xapi-project/xen-api/blob/7ac88b90e762065c5ebb94a8ea61c61bdbf62c5c/ocaml/xenopsd/xc/xenops_server_xen.ml#L3399 rel=external target=_blank>PCI.plug</a>
is executed by the backend. It plugs a PCI device and advertises it to QEMU if this option is set. It is
the case for NVIDIA SR-IOV vGPUs.</p><h2 id=unpause>Unpause</h2><p>The libxenctrl call
<a href=https://github.com/xen-project/xen/blob/414dde3/tools/libs/ctrl/xc_domain.c#L76 rel=external target=_blank>xc_domain_unpause()</a>
unpauses the domain, and it starts running.</p><h2 id=cleanup>Cleanup</h2><ol><li><p><a href=https://github.com/xapi-project/xen-api/blob/ec3b62ee/ocaml/xenopsd/lib/xenops_server.ml#L3004 rel=external target=_blank>VM_set_domain_action_request</a>
marks the domain as alive: In case <code>xenopsd</code> restarts, it no longer reboots the VM.
See the chapter on <a href=/new-docs/xenopsd/walkthroughs/VM.start/index.html#11-mark-the-domain-as-alive>marking domains as alive</a>
for more information.</p></li><li><p>If a post-migrate script is in place, it is executed by the
<a href=https://github.com/xapi-project/xen-api/blob/ec3b62ee/ocaml/xenopsd/lib/xenops_server.ml#L3005-L3009 rel=external target=_blank>Xenops_hooks.VM_post_migrate</a>
hook.</p></li><li><p>The final step is a handshake to seal the success of the migration
and the old VM can now be cleaned up.</p></li></ol><p><a href=https://github.com/xapi-project/xen-api/blob/ec3b62ee/ocaml/xenopsd/lib/xenops_server.ml#L3014 rel=external target=_blank>Syncronisation point 4</a>
has been reached, the migration is complete.</p><h2 id=live-migration-flowchart>Live migration flowchart</h2><p>This flowchart gives a visual representation of the VM migration workflow:</p><pre class="mermaid align-center">sequenceDiagram
autonumber
participant tx as sender
participant rx0 as receiver thread 0
participant rx1 as receiver thread 1
participant rx2 as receiver thread 2

activate tx
tx-&gt;&gt;rx0: VM.import_metadata
tx-&gt;&gt;tx: Squash memory to dynamic-min

tx-&gt;&gt;rx1: HTTP /migrate/vm
activate rx1
rx1-&gt;&gt;rx1: VM_receive_memory&lt;br/&gt;VM_create (00000001)&lt;br/&gt;VM_restore_vifs
rx1-&gt;&gt;tx: handshake (control channel)&lt;br/&gt;Synchronisation point 1

tx-&gt;&gt;rx2: HTTP /migrate/mem
activate rx2
rx2-&gt;&gt;tx: handshake (memory channel)&lt;br/&gt;Synchronisation point 1-mem

tx-&gt;&gt;rx1: handshake (control channel)&lt;br/&gt;Synchronisation point 1-mem ACK

rx2-&gt;&gt;rx1: memory fd

tx-&gt;&gt;rx1: VM_save/VM_restore&lt;br/&gt;Synchronisation point 2
tx-&gt;&gt;tx: VM_rename
rx1-&gt;&gt;rx2: exit
deactivate rx2

tx-&gt;&gt;rx1: handshake (control channel)&lt;br/&gt;Synchronisation point 3

rx1-&gt;&gt;rx1: VM_rename&lt;br/&gt;VM_restore_devices&lt;br/&gt;VM_unpause&lt;br/&gt;VM_set_domain_action_request

rx1-&gt;&gt;tx: handshake (control channel)&lt;br/&gt;Synchronisation point 4

deactivate rx1

tx-&gt;&gt;tx: VM_shutdown&lt;br/&gt;VM_remove
deactivate tx</pre><h2 id=references>References</h2><p>These pages might help for a better understanding of the XAPI toolstack:</p><ul><li>See the <a href=/new-docs/xapi/index.html>XAPI architecture</a> for the overall architecture of Xapi</li><li>See the <a href=https://wiki.xenproject.org/wiki/XAPI_Dispatch rel=external target=_blank>XAPI dispatcher</a> for service dispatch and message forwarding</li><li>See the <a href=/new-docs/xenopsd/architecture/index.html>Xenopsd architecture</a> for the overall architecture of Xenopsd</li><li>See the <a href=https://mirage.io/docs/xen-suspend rel=external target=_blank>How Xen suspend and resume works</a> for very similar operations in more detail.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=live-migration-sequence-diagram>Live Migration Sequence Diagram</h1><pre class="mermaid align-center">sequenceDiagram
autonumber
participant tx as sender
participant rx0 as receiver thread 0
participant rx1 as receiver thread 1
participant rx2 as receiver thread 2

activate tx
tx-&gt;&gt;rx0: VM.import_metadata
tx-&gt;&gt;tx: Squash memory to dynamic-min

tx-&gt;&gt;rx1: HTTP /migrate/vm
activate rx1
rx1-&gt;&gt;rx1: VM_receive_memory&lt;br/&gt;VM_create (00000001)&lt;br/&gt;VM_restore_vifs
rx1-&gt;&gt;tx: handshake (control channel)&lt;br/&gt;Synchronisation point 1

tx-&gt;&gt;rx2: HTTP /migrate/mem
activate rx2
rx2-&gt;&gt;tx: handshake (memory channel)&lt;br/&gt;Synchronisation point 1-mem

tx-&gt;&gt;rx1: handshake (control channel)&lt;br/&gt;Synchronisation point 1-mem ACK

rx2-&gt;&gt;rx1: memory fd

tx-&gt;&gt;rx1: VM_save/VM_restore&lt;br/&gt;Synchronisation point 2
tx-&gt;&gt;tx: VM_rename
rx1-&gt;&gt;rx2: exit
deactivate rx2

tx-&gt;&gt;rx1: handshake (control channel)&lt;br/&gt;Synchronisation point 3

rx1-&gt;&gt;rx1: VM_rename&lt;br/&gt;VM_restore_devices&lt;br/&gt;VM_unpause&lt;br/&gt;VM_set_domain_action_request

rx1-&gt;&gt;tx: handshake (control channel)&lt;br/&gt;Synchronisation point 4

deactivate rx1

tx-&gt;&gt;tx: VM_shutdown&lt;br/&gt;VM_remove
deactivate tx</pre><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article></section></section></div></main></div><script src=/new-docs/js/clipboard.min.js?1741781726 defer></script><script src=/new-docs/js/perfect-scrollbar.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-color.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-dispatch.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-drag.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-ease.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-interpolate.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-selection.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-timer.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-transition.min.js?1741781726 defer></script><script src=/new-docs/js/d3/d3-zoom.min.js?1741781726 defer></script><script src=/new-docs/js/js-yaml.min.js?1741781726 defer></script><script src=/new-docs/js/mermaid.min.js?1741781726 defer></script><script>window.themeUseMermaid=JSON.parse('{ "fontFamily": "Roboto Flex", "securityLevel": "loose" }')</script><script src=/new-docs/js/theme.js?1741781726 defer></script><script>function apply_image_invert_filter(e){document.querySelectorAll("img").forEach(function(t){if(t.classList.contains("no-invert"))return;t.style="filter: invert("+e+");"})}function darkThemeUsed(){const t=window.getComputedStyle(document.querySelector("body")),n=t.getPropertyValue("background-color");var e=n.match(/\d+/g).map(function(e){return parseInt(e,10)});return e.length===3&&.2126*e[0]+.7152*e[1]+.0722*e[2]<165}const invertToDarkGray=.85;darkThemeUsed()&&apply_image_invert_filter(invertToDarkGray),document.addEventListener("themeVariantLoaded",function(e){apply_image_invert_filter(e.detail.variant.endsWith("dark")?invertToDarkGray:0)})</script></body></html>