<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Design Documents :: XAPI Toolstack Developer Documentation</title><link>https://xapi-project.github.io/new-docs/design/index.html</link><description>Key: Revision Proposed Confirmed Released (vA.B) Unrecognised status Improving snapshot revert behaviour v1 confirmed Multiple Cluster Managers v2 confirmed SR-Level RRDs v11 confirmed Backtrace support v1 confirmed Aggregated Local Storage and Host Reboots v3 proposed Code Coverage Profiling v2 proposed Distributed database v1 proposed FCoE capable NICs v3 proposed Local database v1 proposed Management Interface on VLAN v3 proposed Multiple device emulators v1 proposed OCFS2 storage v1 proposed patches in VDIs v1 proposed PCI passthrough support v1 proposed Pool-wide SSH v1 proposed Process events from xenopsd in a timely manner v1 proposed RRDD plugin protocol v3 v1 proposed Schedule Snapshot Design v2 proposed Specifying Emulated PCI Devices v1 proposed thin LVHD storage v3 proposed XenPrep v2 proposed TLS vertification for intra-pool communications v2 released (22.</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://xapi-project.github.io/new-docs/design/index.xml" rel="self" type="application/rss+xml"/><item><title>Aggregated Local Storage and Host Reboots</title><link>https://xapi-project.github.io/new-docs/design/aggr-storage-reboots/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/aggr-storage-reboots/index.html</guid><description>Introduction When hosts use an aggregated local storage SR, then disks are going to be mirrored to several different hosts in the pool (RAID). This ensures that if a host goes down (e.g. due to a reboot after installing a hotfix or upgrade, or when “fenced” by the HA feature), all disk contents in the SR are still accessible. This also means that if all disks are mirrored to just two hosts (worst-case scenario), just one host may be down at any point in time to keep the SR fully available.</description></item><item><title>Backtrace support</title><link>https://xapi-project.github.io/new-docs/design/backtraces/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/backtraces/index.html</guid><description>We want to make debugging easier by recording exception backtraces which are
reliable cross-process (e.g. xapi to xenopsd) cross-language cross-host (e.g. master to slave) We therefore need
to ensure that backtraces are captured in our OCaml and python code a marshalling format for backtraces conventions for storing and retrieving backtraces Backtraces in OCaml OCaml has fast exceptions which can be used for both
control flow i.e. fast jumps from inner scopes to outer scopes reporting errors to users (e.</description></item><item><title>Bonding Improvements design</title><link>https://xapi-project.github.io/new-docs/design/bonding-improvements/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/bonding-improvements/index.html</guid><description>This document describes design details for the PR-1006 requirements.
XAPI and XenAPI Creating a Bond Current Behaviour on Bond creation Steps for a user to create a bond:
Shutdown all VMs with VIFs using the interfaces that will be bonded, in order to unplug those VIFs. Create a Network to be used by the bond: Network.create Call Bond.create with a ref to this Network, a list of refs of slave PIFs, and a MAC address to use.</description></item><item><title>Code Coverage Profiling</title><link>https://xapi-project.github.io/new-docs/design/coverage/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/coverage/index.html</guid><description>We would like to add optional coverage profiling to existing OCaml projects in the context of XenServer and XenAPI. This article presents how we do it.
Binaries instrumented for coverage profiling in the XenServer project need to run in an environment where several services act together as they provide operating-system-level services. This makes it a little harder than profiling code that can be profiled and executed in isolation.
TL;DR To build binaries with coverage profiling, do:</description></item><item><title>CPU feature levelling 2.0</title><link>https://xapi-project.github.io/new-docs/design/cpu-levelling-v2/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/cpu-levelling-v2/index.html</guid><description>Executive Summary The old XS 5.6-style Heterogeneous Pool feature that is based around hardware-level CPUID masking will be replaced by a safer and more flexible software-based levelling mechanism.
History Original XS 5.6 design: heterogeneous-pools Changes made in XS 5.6 FP1 for the DR feature (added CPUID checks upon migration) XS 6.1: migration checks extended for cross-pool scenario High-level Interfaces and Behaviour A VM can only be migrated safely from one host to another if both hosts offer the set of CPU features which the VM expects.</description></item><item><title>Distributed database</title><link>https://xapi-project.github.io/new-docs/design/distributed-database/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/distributed-database/index.html</guid><description>All hosts in a pool use the shared database by sending queries to the pool master. This creates
a performance bottleneck as the pool size increases a reliability problem when the master fails. The reliability problem can be ameliorated by running with HA enabled, but this is not always possible.
Both problems can be addressed by observing that the database objects correspond to distinct physical objects where eventual consistency is perfectly ok.</description></item><item><title>Emergency Network Reset Design</title><link>https://xapi-project.github.io/new-docs/design/emergency-network-reset/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/emergency-network-reset/index.html</guid><description>This document describes design details for the PR-1032 requirements.
The design consists of four parts:
A new XenAPI call Host.reset_networking, which removes all the PIFs, Bonds, VLANs and tunnels associated with the given host, and a call PIF.scan_bios to bring back the PIFs with device names as defined in the BIOS. A xe-reset-networking script that can be executed on a XenServer host, which prepares the reset and causes the host to reboot.</description></item><item><title>FCoE capable NICs</title><link>https://xapi-project.github.io/new-docs/design/fcoe-nics/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/fcoe-nics/index.html</guid><description>It has been possible to identify the NICs of a Host which can support FCoE. This property can be listed in PIF object under capabilities field.
Introduction FCoE supported on a NIC is a hardware property. With the help of dcbtool, we can identify which NIC support FCoE. The new field capabilities will be Set(String) in PIF object. For FCoE capable NIC will have string “fcoe” in PIF capabilities field. capabilities field will be ReadOnly, This field cannot be modified by user.</description></item><item><title>GPU pass-through support</title><link>https://xapi-project.github.io/new-docs/design/gpu-passthrough/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/gpu-passthrough/index.html</guid><description>This document contains the software design for GPU pass-through. This code was originally included in the version of Xapi used in XenServer 6.0.
Overview Rather than modelling GPU pass-through from a PCI perspective, and having the user manipulate PCI devices directly, we are taking a higher-level view by introducing a dedicated graphics model. The graphics model is similar to the networking and storage model, in which virtual and physical devices are linked through an intermediate abstraction layer (e.</description></item><item><title>GPU support evolution</title><link>https://xapi-project.github.io/new-docs/design/gpu-support-evolution/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/gpu-support-evolution/index.html</guid><description>Introduction As of XenServer 6.5, VMs can be provisioned with access to graphics processors (either emulated or passed through) in four different ways. Virtualisation of Intel graphics processors will exist as a fifth kind of graphics processing available to VMs. These five situations all require the VM’s device model to be created in subtly different ways:
Pure software emulation
qemu is launched either with no special parameter, if the basic Cirrus graphics processor is required, otherwise qemu is launched with the -std-vga flag.</description></item><item><title>GRO and other properties of PIFs</title><link>https://xapi-project.github.io/new-docs/design/pif-properties/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/pif-properties/index.html</guid><description>It has been possible to enable and disable GRO and other “ethtool” features on PIFs for a long time, but there was never an official API for it. Now there is.
Introduction The former way to enable GRO via the CLI is as follows:
xe pif-param-set uuid=&lt;pif-uuid> other-config:ethtool-gro=on xe pif-plug uuid=&lt;pif-uuid> The other-config field is a grab-bag of options that are not clearly defined. The options exposed through other-config are mostly experimental features, and the interface is not considered stable.</description></item><item><title>Heterogeneous pools</title><link>https://xapi-project.github.io/new-docs/design/heterogeneous-pools/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/heterogeneous-pools/index.html</guid><description>Notes The cpuid instruction is used to obtain a CPU’s manufacturer, family, model, stepping and features information. The feature bitvector is 128 bits wide: 2 times 32 bits of base features plus 2 times 32 bits of extended features, which are referred to as base_ecx, base_edx, ext_ecx and ext_edx (after the registers used by cpuid to store the results). The feature bits can be masked by Intel FlexMigration and AMD Extended Migration.</description></item><item><title>Improving snapshot revert behaviour</title><link>https://xapi-project.github.io/new-docs/design/snapshot-revert/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/snapshot-revert/index.html</guid><description>Currently there is a XenAPI VM.revert which reverts a “VM” to the state it was in when a VM-level snapshot was taken. There is no VDI.revert so VM.revert uses VDI.clone to change the state of the disks.
The use of VDI.clone has the side-effect of changing VDI refs and uuids. This causes the following problems:
It is difficult for clients such as Apache CloudStack to keep track of the disks it is actively managing VDI snapshot metadata (VDI.</description></item><item><title>Integrated GPU passthrough support</title><link>https://xapi-project.github.io/new-docs/design/integrated-gpu-passthrough/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/integrated-gpu-passthrough/index.html</guid><description>Introduction Passthrough of discrete GPUs has been available since XenServer 6.0. With some extensions, we will also be able to support passthrough of integrated GPUs.
Whether an integrated GPU will be accessible to dom0 or available to passthrough to guests must be configurable via XenAPI. Passthrough of an integrated GPU requires an extra flag to be sent to qemu. Host Configuration New fields will be added (both read-only):
PGPU.dom0_access enum(enabled|disable_on_reboot|disabled|enable_on_reboot) host.</description></item><item><title>Local database</title><link>https://xapi-project.github.io/new-docs/design/local-database/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/local-database/index.html</guid><description>All hosts in a pool use the shared database by sending queries to the pool master. This creates a performance bottleneck as the pool size increases. All hosts in a pool receive a database backup from the master periodically, every couple of hours. This creates a reliability problem as updates may be lost if the master fails during the window before the backup.
The reliability problem can be avoided by running with HA or the redo log enabled, but this is not always possible.</description></item><item><title>Management Interface on VLAN</title><link>https://xapi-project.github.io/new-docs/design/management-interface-on-vlan/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/management-interface-on-vlan/index.html</guid><description>This document describes design details for the REQ-42: Support Use of VLAN on XAPI Management Interface.
XAPI and XCP-Networkd Creating a VLAN Creating a VLAN is already there, Lisiting the steps to create a VLAN which is used later in the document. Steps:
Check the PIFs created on a Host for physical devices eth0, eth1. xe pif-list params=uuid physical=true host-uuid=UUID this will list pif-UUID Create a new network for the VLAN interface.</description></item><item><title>Multiple Cluster Managers</title><link>https://xapi-project.github.io/new-docs/design/multiple-cluster-managers/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/multiple-cluster-managers/index.html</guid><description>Introduction Xapi currently uses a cluster manager called xhad. Sometimes other software comes with its own built-in way of managing clusters, which would clash with xhad (example: xhad could choose to fence node ‘a’ while the other system could fence node ‘b’ resulting in a total failure). To integrate xapi with this other software we have 2 choices:
modify the other software to take membership information from xapi; or modify xapi to take membership information from this other software.</description></item><item><title>Multiple device emulators</title><link>https://xapi-project.github.io/new-docs/design/multiple-device-emulators/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/multiple-device-emulators/index.html</guid><description>Xen’s ioreq-server feature allows for several device emulator processes to be attached to the same domain, each emulating different sets of virtual hardware. This makes it possible, for example, to emulate network devices in a separate process for improved security and isolation, or to provide special purpose emulators for particular virtual hardware devices.
ioreq-server is currently used in XenServer to support vGPU, where it is configured via the legacy toolstack interface.</description></item><item><title>OCFS2 storage</title><link>https://xapi-project.github.io/new-docs/design/ocfs2/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/ocfs2/index.html</guid><description>OCFS2 is a (host-)clustered filesystem which runs on top of a shared raw block device. Hosts using OCFS2 form a cluster using a combination of network and storage heartbeats and host fencing to avoid split-brain.
The following diagram shows the proposed architecture with xapi:
Please note the following:
OCFS2 is configured to use global heartbeats rather than per-mount heartbeats because we quite often have many SRs and therefore many mountpoints The OCFS2 global heartbeat should be collocated on the same SR as the XenServer HA SR so that we depend on fewer SRs (the storage is a single point of failure for OCFS2) The OCFS2 global heartbeat should itself be a raw VDI within an LVHDSR.</description></item><item><title>patches in VDIs</title><link>https://xapi-project.github.io/new-docs/design/patches-in-vdis/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/patches-in-vdis/index.html</guid><description>“Patches” are signed binary blobs which can be queried and applied. They are stored in the dom0 filesystem under /var/patch. Unfortunately the patches can be quite large – imagine a repo full of RPMs – and the dom0 filesystem is usually quite small, so it can be difficult to upload and apply some patches.
Instead of writing patches to the dom0 filesystem, we shall write them to disk images (VDIs) instead.</description></item><item><title>PCI passthrough support</title><link>https://xapi-project.github.io/new-docs/design/pci-passthrough/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/pci-passthrough/index.html</guid><description>Introduction GPU passthrough is already available in XAPI, this document proposes to also offer passthrough for all PCI devices through XAPI.
Design proposal New methods for PCI object:
PCI.enable_dom0_access
PCI.disable_dom0_access
PCI.get_dom0_access_status: compares the outputs of /opt/xensource/libexec/xen-cmdline and /proc/cmdline to produce one of the four values that can be currently contained in the PGPU.dom0_access field:
disabled disabled_on_reboot enabled enabled_on_reboot How do determine the expected dom0 access state: If the device id is present in both pciback.</description></item><item><title>Pool-wide SSH</title><link>https://xapi-project.github.io/new-docs/design/pool-wide-ssh/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/pool-wide-ssh/index.html</guid><description>Background The SMAPIv3 plugin architecture requires that storage plugins are able to work in the absence of xapi. Amongst other benefits, this allows them to be tested in isolation, are able to be shared more widely than just within the XenServer community and will cause less load on xapi’s database.
However, many of the currently existing SMAPIv1 backends require inter-host operations to be performed. This is achieved via the use of the Xen-API call ‘host.</description></item><item><title>Process events from xenopsd in a timely manner</title><link>https://xapi-project.github.io/new-docs/design/xenopsd_events/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/xenopsd_events/index.html</guid><description>Background There is a significant delay between the VM being unpaused and XAPI reporting it as started during a bootstorm. It can happen that the VM is able to send UDP packets already, but XAPI still reports it as not started for minutes.
XAPI currently processes all events from xenopsd in a single thread, the unpause events get queued up behind a lot of other events generated by the already running VMs.</description></item><item><title>RDP control</title><link>https://xapi-project.github.io/new-docs/design/RDP/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/RDP/index.html</guid><description>Purpose To administer guest VMs it can be useful to connect to them over Remote Desktop Protocol (RDP). XenCenter supports this; it has an integrated RDP client.
First it is necessary to turn on the RDP service in the guest.
This can be controlled from XenCenter. Several layers are involved. This description starts in the guest and works up the stack to XenCenter.
This feature was completed in the first quarter of 2015, and released in Service Pack 1 for XenServer 6.</description></item><item><title>RRDD archival redesign</title><link>https://xapi-project.github.io/new-docs/design/archival-redesign/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/archival-redesign/index.html</guid><description>Introduction Current problems with rrdd:
rrdd stores knowledge about whether it is running on a master or a slave This determines the host to which rrdd will archive a VM’s rrd when the VM’s domain disappears - rrdd will always try to archive to the master. However, when a host joins a pool as a slave rrdd is not restarted so this knowledge is out of date. When a VM shuts down on the slave rrdd will archive the rrd locally.</description></item><item><title>RRDD plugin protocol v2</title><link>https://xapi-project.github.io/new-docs/design/plugin-protocol-v2/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/plugin-protocol-v2/index.html</guid><description>Motivation rrdd plugins currently report datasources via a shared-memory file, using the following format:
DATASOURCES 000001e4 dba4bf7a84b6d11d565d19ef91f7906e { "timestamp": 1339685573.245, "data_sources": { "cpu-temp-cpu0": { "description": "Temperature of CPU 0", "type": "absolute", "units": "degC", "value": "64.33" "value_type": "float", }, "cpu-temp-cpu1": { "description": "Temperature of CPU 1", "type": "absolute", "units": "degC", "value": "62.14" "value_type": "float", } } } This format contains four main components:
A constant header string DATASOURCES
This should always be present.</description></item><item><title>RRDD plugin protocol v3</title><link>https://xapi-project.github.io/new-docs/design/plugin-protocol-v3/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/plugin-protocol-v3/index.html</guid><description>Motivation rrdd plugins protocol v2 report datasources via shared-memory file, however it has various limitations :
metrics are unique by their names, thus it is not possible cannot have several metrics that shares a same name (e.g vCPU usage per vm) only number metrics are supported, for example we can’t expose string metrics (e.g CPU Model) Therefore, it implies various limitations on plugins and limits OpenMetrics support for the metrics daemon.</description></item><item><title>Schedule Snapshot Design</title><link>https://xapi-project.github.io/new-docs/design/schedule-snapshot/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/schedule-snapshot/index.html</guid><description>The scheduled snapshot feature will utilize the existing architecture of VMPR. In terms of functionality, scheduled snapshot is basically VMPR without its archiving capability.
Introduction Schedule snapshot will be a new object in xapi as VMSS. A pool can have multiple VMSS. Multiple VMs can be a part of VMSS but a VM cannot be a part of multiple VMSS. A VMSS takes VMs snapshot with type [snapshot, checkpoint, snapshot_with_quiesce]. VMSS takes snapshot of VMs on configured intervals: hourly -> On everyday, Each hour, Mins [0;15;30;45] daily -> On everyday, Hour [0 to 23], Mins [0;15;30;45] weekly -> Days [Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday], Hour[0 to 23], Mins [0;15;30;45] VMSS will have a limit on retaining number of VM snapshots in range [1 to 10].</description></item><item><title>SMAPIv3</title><link>https://xapi-project.github.io/new-docs/design/smapiv3/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/smapiv3/index.html</guid><description>Xapi accesses storage through “plugins” which currently use a protocol called “SMAPIv1”. This protocol has a number of problems:
the protocol has many missing features, and this leads to people using the XenAPI from within a plugin, which is racy, difficult to get right, unscalable and makes component testing impossible.
the protocol expects plugin authors to have a deep knowledge of the Xen storage datapath (tapdisk, blkback etc) and the storage.</description></item><item><title>Specifying Emulated PCI Devices</title><link>https://xapi-project.github.io/new-docs/design/emulated-pci-spec/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/emulated-pci-spec/index.html</guid><description>Background and goals At present (early March 2015) the datamodel defines a VM as having a “platform” string-string map, in which two keys are interpreted as specifying a PCI device which should be emulated for the VM. Those keys are “device_id” and “revision” (with int values represented as decimal strings).
Limitations:
Hardcoded defaults are used for the vendor ID and all other parameters except device_id and revision. Only one emulated PCI device can be specified.</description></item><item><title>SR-Level RRDs</title><link>https://xapi-project.github.io/new-docs/design/sr-level-rrds/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/sr-level-rrds/index.html</guid><description>Introduction Xapi has RRDs to track VM- and host-level metrics. There is a desire to have SR-level RRDs as a new category, because SR stats are not specific to a certain VM or host. Examples are size and free space on the SR. While recording SR metrics is relatively straightforward within the current RRD system, the main question is where to archive them, which is what this design aims to address.</description></item><item><title>thin LVHD storage</title><link>https://xapi-project.github.io/new-docs/design/thin-lvhd/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/thin-lvhd/index.html</guid><description>LVHD is a block-based storage system built on top of Xapi and LVM. LVHD disks are represented as LVM LVs with vhd-format data inside. When a disk is snapshotted, the LVM LV is “deflated” to the minimum-possible size, just big enough to store the current vhd data. All other disks are stored “inflated” i.e. consuming the maximum amount of storage space. This proposal describes how we could add dynamic thin-provisioning to LVHD such that</description></item><item><title>TLS vertification for intra-pool communications</title><link>https://xapi-project.github.io/new-docs/design/pool-certificates/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/pool-certificates/index.html</guid><description>Overview Xenserver has used TLS-encrypted communications between xapi daemons in a pool since its first release. However it does not use TLS certificates to authenticate the servers it connects to. This allows possible attackers opportunities to impersonate servers when the pools’ management network is compromised.
In order to enable certificate verification, certificate exchange as well as proper set up to trust them must be provided by xapi. This is currently done by allowing users to generate, sign and install the certificates themselves; and then enable the Common Criteria mode.</description></item><item><title>Tunnelling API design</title><link>https://xapi-project.github.io/new-docs/design/tunnelling/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/tunnelling/index.html</guid><description>To isolate network traffic between VMs (e.g. for security reasons) one can use VLANs. The number of possible VLANs on a network, however, is limited, and setting up a VLAN requires configuring the physical switches in the network. GRE tunnels provide a similar, though more flexible solution. This document proposes a design that integrates the use of tunnelling in the XenAPI. The design relies on the recent introduction of the Open vSwitch, and requires an Open vSwitch (OpenFlow) controller (further referred to as the controller) to set up and maintain the actual GRE tunnels.</description></item><item><title>User-installable host certificates</title><link>https://xapi-project.github.io/new-docs/design/user-certificates/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/user-certificates/index.html</guid><description>Introduction It is often necessary to replace the TLS certificate used to secure communications to Xenservers hosts, for example to allow a XenAPI user such as Citrix Virtual Apps and Desktops (CVAD) to validate that the host is genuine and not impersonating the actual host.
Historically there has not been a supported mechanism to do this, and as a result users have had to rely on guides written by third parties that show how to manually replace the xapi-ssl.</description></item><item><title>VGPU type identifiers</title><link>https://xapi-project.github.io/new-docs/design/vgpu-type-identifiers/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/vgpu-type-identifiers/index.html</guid><description>Introduction When xapi starts, it may create a number of VGPU_type objects. These act as VGPU presets, and exactly which VGPU_type objects are created depends on the installed hardware and in certain cases the presence of certain files in dom0.
When deciding which VGPU_type objects need to be created, xapi needs to determine whether a suitable VGPU_type object already exists, as there should never be duplicates. At the moment the combination of vendor name and model name is used as a primary key, but this is not ideal as these values are subject to change.</description></item><item><title>Virtual Hardware Platform Version</title><link>https://xapi-project.github.io/new-docs/design/virt-hw-platform-vn/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/virt-hw-platform-vn/index.html</guid><description>Background and goal Some VMs can only be run on hosts of sufficiently recent versions.
We want a clean way to ensure that xapi only tries to run a guest VM on a host that supports the “virtual hardware platform” required by the VM.
Suggested design In the datamodel, VM has a new integer field “hardware_platform_version” which defaults to zero. In the datamodel, Host has a corresponding new integer-list field “virtual_hardware_platform_versions” which defaults to list containing a single zero element (i.</description></item><item><title>XenPrep</title><link>https://xapi-project.github.io/new-docs/design/xenprep/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xapi-project.github.io/new-docs/design/xenprep/index.html</guid><description>Background Windows guests should have XenServer-specific drivers installed. As of mid-2015 these have been always been installed and upgraded by an essentially manual process involving an ISO carrying the drivers. We have a plan to enable automation through the standard Windows Update mechanism. This will involve a new additional virtual PCI device being provided to the VM, to trigger Windows Update to fetch drivers for the device.
There are many existing Windows guests that have drivers installed already.</description></item></channel></rss>