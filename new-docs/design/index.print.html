<!doctype html><html lang=en-us dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.127.0"><meta name=generator content="Relearn 7.3.2"><meta name=description content="Key: Revision Proposed Confirmed Released (vA.B) Unrecognised status Improving snapshot revert behaviour v1 confirmed Multiple Cluster Managers v2 confirmed SR-Level RRDs v11 confirmed Backtrace support v1 confirmed Add qcow tool to allow VDI import/export v1 proposed Aggregated Local Storage and Host Reboots v3 proposed Code Coverage Profiling v2 proposed Distributed database v1 proposed FCoE capable NICs v3 proposed Local database v1 proposed Management Interface on VLAN v3 proposed Multiple device emulators v1 proposed OCFS2 storage v1 proposed patches in VDIs v1 proposed PCI passthrough support v1 proposed Pool-wide SSH v1 proposed Process events from xenopsd in a timely manner v1 proposed RRDD plugin protocol v3 v1 proposed Schedule Snapshot Design v2 proposed Specifying Emulated PCI Devices v1 proposed thin LVHD storage v3 proposed XenPrep v2 proposed TLS vertification for intra-pool communications v2 released (22."><meta name=author content><meta name=twitter:card content="summary"><meta name=twitter:title content="Design Documents :: XAPI Toolstack Developer Documentation"><meta name=twitter:description content="Key: Revision Proposed Confirmed Released (vA.B) Unrecognised status Improving snapshot revert behaviour v1 confirmed Multiple Cluster Managers v2 confirmed SR-Level RRDs v11 confirmed Backtrace support v1 confirmed Add qcow tool to allow VDI import/export v1 proposed Aggregated Local Storage and Host Reboots v3 proposed Code Coverage Profiling v2 proposed Distributed database v1 proposed FCoE capable NICs v3 proposed Local database v1 proposed Management Interface on VLAN v3 proposed Multiple device emulators v1 proposed OCFS2 storage v1 proposed patches in VDIs v1 proposed PCI passthrough support v1 proposed Pool-wide SSH v1 proposed Process events from xenopsd in a timely manner v1 proposed RRDD plugin protocol v3 v1 proposed Schedule Snapshot Design v2 proposed Specifying Emulated PCI Devices v1 proposed thin LVHD storage v3 proposed XenPrep v2 proposed TLS vertification for intra-pool communications v2 released (22."><meta property="og:url" content="https://xapi-project.github.io/new-docs/design/index.html"><meta property="og:site_name" content="XAPI Toolstack Developer Documentation"><meta property="og:title" content="Design Documents :: XAPI Toolstack Developer Documentation"><meta property="og:description" content="Key: Revision Proposed Confirmed Released (vA.B) Unrecognised status Improving snapshot revert behaviour v1 confirmed Multiple Cluster Managers v2 confirmed SR-Level RRDs v11 confirmed Backtrace support v1 confirmed Add qcow tool to allow VDI import/export v1 proposed Aggregated Local Storage and Host Reboots v3 proposed Code Coverage Profiling v2 proposed Distributed database v1 proposed FCoE capable NICs v3 proposed Local database v1 proposed Management Interface on VLAN v3 proposed Multiple device emulators v1 proposed OCFS2 storage v1 proposed patches in VDIs v1 proposed PCI passthrough support v1 proposed Pool-wide SSH v1 proposed Process events from xenopsd in a timely manner v1 proposed RRDD plugin protocol v3 v1 proposed Schedule Snapshot Design v2 proposed Specifying Emulated PCI Devices v1 proposed thin LVHD storage v3 proposed XenPrep v2 proposed TLS vertification for intra-pool communications v2 released (22."><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta itemprop=name content="Design Documents :: XAPI Toolstack Developer Documentation"><meta itemprop=description content="Key: Revision Proposed Confirmed Released (vA.B) Unrecognised status Improving snapshot revert behaviour v1 confirmed Multiple Cluster Managers v2 confirmed SR-Level RRDs v11 confirmed Backtrace support v1 confirmed Add qcow tool to allow VDI import/export v1 proposed Aggregated Local Storage and Host Reboots v3 proposed Code Coverage Profiling v2 proposed Distributed database v1 proposed FCoE capable NICs v3 proposed Local database v1 proposed Management Interface on VLAN v3 proposed Multiple device emulators v1 proposed OCFS2 storage v1 proposed patches in VDIs v1 proposed PCI passthrough support v1 proposed Pool-wide SSH v1 proposed Process events from xenopsd in a timely manner v1 proposed RRDD plugin protocol v3 v1 proposed Schedule Snapshot Design v2 proposed Specifying Emulated PCI Devices v1 proposed thin LVHD storage v3 proposed XenPrep v2 proposed TLS vertification for intra-pool communications v2 released (22."><meta itemprop=wordCount content="238"><title>Design Documents :: XAPI Toolstack Developer Documentation</title>
<link href=https://xapi-project.github.io/new-docs/design/index.html rel=canonical type=text/html title="Design Documents :: XAPI Toolstack Developer Documentation"><link href=/new-docs/design/index.xml rel=alternate type=application/rss+xml title="Design Documents :: XAPI Toolstack Developer Documentation"><link href=/new-docs/images/favicon.png?1741171267 rel=icon type=image/png><link href=/new-docs/css/fontawesome-all.min.css?1741171267 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/fontawesome-all.min.css?1741171267 rel=stylesheet></noscript><link href=/new-docs/css/auto-complete.css?1741171267 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/auto-complete.css?1741171267 rel=stylesheet></noscript><link href=/new-docs/css/perfect-scrollbar.min.css?1741171267 rel=stylesheet><link href=/new-docs/css/theme.min.css?1741171267 rel=stylesheet><link href=/new-docs/css/format-print.min.css?1741171267 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://xapi-project.github.io/new-docs",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["auto","zen-light","zen-dark","red","blue","green","learn","neon","relearn-light","relearn-bright","relearn-dark"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script><link rel=stylesheet href=https://xapi-project.github.io/new-docs/css/misc.css></head><body class="mobile-support print" data-url=/new-docs/design/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button><div class=topbar-content><div class=topbar-content-wrapper></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/new-docs/index.html><span itemprop=name>XAPI Toolstack Developer Guide</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Designs</span><meta itemprop=position content="2"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-edit" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=https://github.com/xapi-project/xen-api/edit/master/doc/content/design/_index.md target=_blank title="Edit (CTRL+ALT+w)"><i class="fa-fw fas fa-pen"></i></a></div><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/design/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></div><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/xapi-guard/index.html title="Xapi-guard (ðŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/design/add-qcow-tool-for-vdi-import-export/index.html title="Add qcow tool to allow VDI import/export (ðŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable design" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=design-documents>Design Documents</h1><div><strong>Key: </strong><span class="label label-default">Revision</span>
<span class="label label-danger">Proposed</span>
<span class="label label-warning">Confirmed</span>
<span class="label label-success">Released (vA.B)</span>
<span class="label label-info">Unrecognised status</span></div><table class="table table-striped table-condensed"><tbody><tr><td><a href=/new-docs/design/snapshot-revert/ class="btn btn-link">Improving snapshot revert behaviour</a>
<span class="label label-default">v1</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/multiple-cluster-managers/ class="btn btn-link">Multiple Cluster Managers</a>
<span class="label label-default">v2</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/sr-level-rrds/ class="btn btn-link">SR-Level RRDs</a>
<span class="label label-default">v11</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/backtraces/ class="btn btn-link">Backtrace support</a>
<span class="label label-default">v1</span>
<span class="label
label-warning">confirmed</span></td></tr><tr><td><a href=/new-docs/design/add-qcow-tool-for-vdi-import-export/ class="btn btn-link">Add qcow tool to allow VDI import/export</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/aggr-storage-reboots/ class="btn btn-link">Aggregated Local Storage and Host Reboots</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/coverage/ class="btn btn-link">Code Coverage Profiling</a>
<span class="label label-default">v2</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/distributed-database/ class="btn btn-link">Distributed database</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/fcoe-nics/ class="btn btn-link">FCoE capable NICs</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/local-database/ class="btn btn-link">Local database</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/management-interface-on-vlan/ class="btn btn-link">Management Interface on VLAN</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/multiple-device-emulators/ class="btn btn-link">Multiple device emulators</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/ocfs2/ class="btn btn-link">OCFS2 storage</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/patches-in-vdis/ class="btn btn-link">patches in VDIs</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/pci-passthrough/ class="btn btn-link">PCI passthrough support</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/pool-wide-ssh/ class="btn btn-link">Pool-wide SSH</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/xenopsd_events/ class="btn btn-link">Process events from xenopsd in a timely manner</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/plugin-protocol-v3/ class="btn btn-link">RRDD plugin protocol v3</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/schedule-snapshot/ class="btn btn-link">Schedule Snapshot Design</a>
<span class="label label-default">v2</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/emulated-pci-spec/ class="btn btn-link">Specifying Emulated PCI Devices</a>
<span class="label label-default">v1</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/thin-lvhd/ class="btn btn-link">thin LVHD storage</a>
<span class="label label-default">v3</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/xenprep/ class="btn btn-link">XenPrep</a>
<span class="label label-default">v2</span>
<span class="label
label-danger">proposed</span></td></tr><tr><td><a href=/new-docs/design/pool-certificates/ class="btn btn-link">TLS vertification for intra-pool communications</a>
<span class="label label-default">v2</span>
<span class="label
label-success">released (22.6.0)</span></td></tr><tr><td><a href=/new-docs/design/tunnelling/ class="btn btn-link">Tunnelling API design</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (5.6 fp1)</span></td></tr><tr><td><a href=/new-docs/design/heterogeneous-pools/ class="btn btn-link">Heterogeneous pools</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (5.6)</span></td></tr><tr><td><a href=/new-docs/design/emergency-network-reset/ class="btn btn-link">Emergency Network Reset Design</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.0.2)</span></td></tr><tr><td><a href=/new-docs/design/bonding-improvements/ class="btn btn-link">Bonding Improvements design</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.0)</span></td></tr><tr><td><a href=/new-docs/design/gpu-passthrough/ class="btn btn-link">GPU pass-through support</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.0)</span></td></tr><tr><td><a href=/new-docs/design/integrated-gpu-passthrough/ class="btn btn-link">Integrated GPU passthrough support</a>
<span class="label label-default">v3</span>
<span class="label
label-success">released (6.5 sp1)</span></td></tr><tr><td><a href=/new-docs/design/pif-properties/ class="btn btn-link">GRO and other properties of PIFs</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (6.5)</span></td></tr><tr><td><a href=/new-docs/design/archival-redesign/ class="btn btn-link">RRDD archival redesign</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7,0)</span></td></tr><tr><td><a href=/new-docs/design/cpu-levelling-v2/ class="btn btn-link">CPU feature levelling 2.0</a>
<span class="label label-default">v7</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/gpu-support-evolution/ class="btn btn-link">GPU support evolution</a>
<span class="label label-default">v3</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/plugin-protocol-v2/ class="btn btn-link">RRDD plugin protocol v2</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/vgpu-type-identifiers/ class="btn btn-link">VGPU type identifiers</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/virt-hw-platform-vn/ class="btn btn-link">Virtual Hardware Platform Version</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.0)</span></td></tr><tr><td><a href=/new-docs/design/smapiv3/ class="btn btn-link">SMAPIv3</a>
<span class="label label-default">v1</span>
<span class="label
label-success">released (7.6)</span></td></tr><tr><td><a href=/new-docs/design/user-certificates/ class="btn btn-link">User-installable host certificates</a>
<span class="label label-default">v2</span>
<span class="label
label-success">released (8.2)</span></td></tr><tr><td><a href=/new-docs/design/RDP/ class="btn btn-link">RDP control</a>
<span class="label label-default">v2</span>
<span class="label
label-success">released (xenserver 6.5 sp1)</span></td></tr></tbody></table><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Designs</h1><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=add-qcow-tool-to-allow-vdi-importexport>Add qcow tool to allow VDI import/export</h1><h1 id=introduction>Introduction</h1><p>At XCP-ng, we are working on overcoming the 2TiB limitation for VM disks while
preserving essential features such as snapshots, copy-on-write capabilities, and
live migration.</p><p>To achieve this, we are introducing Qcow2 support in SMAPI and the blktap driver.
With the alpha release, we can:
- Create a VDI
- Snapshot it
- Export and import it to/from XVA
- Perform full backups</p><p>However, we currently cannot export a VDI to a Qcow2 file, nor import one.</p><p>The purpose of this design proposal is to outline a solution for implementing VDI
import/export in Qcow2 format.</p><h1 id=design-proposal>Design Proposal</h1><p>The import and export of VHD-based VDIs currently rely on <em>vhd-tool</em>, which is
responsible for streaming data between a VDI and a file. It supports both Raw and
VHD formats, but not Qcow2.</p><p>There is an existing tool called <a href=https://opam.ocaml.org/packages/qcow-tool/ rel=external target=_blank>qcow-tool</a>
originally packaged by MirageOS. It is no longer actively maintained, but it can
produce Qcow files readable by QEMU.</p><p>Currently, <em>qcow-tool</em> does not support streaming, but we propose to add this
capability. This means replicating the approach used in <em>vhd-tool</em>, where data is
pushed to a socket.</p><p>We have contacted the original developer, David Scott, and there are no objections
to us maintaining the tool if needed.</p><p>Therefore, the most appropriate way to enable Qcow2 import/export in XAPI is to
add streaming support to <code>qcow-tool</code>.</p><h1 id=xenapi-changes>XenAPI changes</h1><h2 id=the-workflow>The workflow</h2><ul><li>The export and import of VDIs are handled by the XAPI HTTP server:<ul><li><code>GET /export_raw_vdi</code></li><li><code>PUT /import_raw_vdi</code></li></ul></li><li>The corresponding handlers are <code>Export_raw_vdi.handler</code> and
<code>Import_raw_vdi.handler</code>.</li><li>Since the format is checked in the handler, we need to add support for <code>Qcow2</code>,
as currently only <code>Raw</code>, <code>Tar</code>, and <code>Vhd</code> are supported.</li><li>This requires adding a new type in the <code>Importexport.Format</code> module and a new
content type: <code>application/x-qemu-disk</code>.
See <a href=https://www.digipres.org/formats/mime-types/#application/x-qemu-disk rel=external target=_blank>mime-types format</a>.</li><li>This allows the format to be properly decoded. Currently, all formats use a
wrapper called <code>Vhd_tool_wrapper</code>, which sets up parameters for <code>vhd-tool</code>.
We need to add a new wrapper for the Qcow2 format, which will instead use
<code>qcow-tool</code>, a tool that we will package (see the section below).</li><li>The new wrapper will be responsible for setting up parameters (source,
destination, etc.). Since it only manages Qcow2 files, we donâ€™t need to pass
additional format information.</li><li>The format (<code>qcow2</code>) will be specified in the URI. For example:<ul><li><code>/import_raw_vdi?session_id=&lt;OpaqueRef>&amp;task_id=&lt;OpaqueRef>&amp;vdi=&lt;OpaqueRef>&amp;format=qcow2</code></li></ul></li></ul><h2 id=adding-and-modifying-qcow-tool>Adding and modifying qcow-tool</h2><ul><li><p>We need to package <a href=https://opam.ocaml.org/packages/qcow-tool rel=external target=_blank>qcow-tool</a>.</p></li><li><p>This new tool will be called from <code>ocaml/xapi/qcow_tool_wrapper.ml</code>, as
described in the previous section.</p></li><li><p>To export a VDI to a Qcow2 file, we need to add functionality similar to
<code>Vhd_tool_wrapper.send</code>, which calls <code>vhd-tool stream</code>.</p><ul><li>It writes data from the source to a destination. Unlike <code>vhd-tool</code>, which
supports multiple destinations, we will only support Qcow2 files.</li><li>Here is a typicall call to <code>vhd-tool stream</code></li></ul></li></ul><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>/bin/vhd-tool stream <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --source-protocol none <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --source-format hybrid <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --source /dev/sm/backend/ff1b27b1-3c35-972e-76ec-a56fe9f25e36/87711319-2b05-41a3-8ee0-3b63a2fc7035:/dev/VG_XenStorage-ff1b27b1-3c35-972e-76ec-a56fe9f25e36/VHD-87711319-2b05-41a3-8ee0-3b63a2fc7035 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --destination-protocol none <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --destination-format vhd <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --destination-fd 2585f988-7374-8131-5b66-77bbc239cbb2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --tar-filename-prefix  <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --progress <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --machine <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --direct <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --path /dev/mapper:.</span></span></code></pre></div><ul><li>To import a VDI from a Qcow2 file, we need to implement functionality similar
to <code>Vhd_tool_wrapper.receive</code>, which calls <code>vhd-tool serve</code>.<ul><li>This is the reverse of the export process. As with export, we will only
support a single type of import: from a Qcow2 file.</li><li>Here is a typical call to <code>vhd-tool serve</code></li></ul></li></ul><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>/bin/vhd-tool serve <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --source-format raw <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --source-protocol none <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --source-fd 3451d7ed-9078-8b01-95bf-293d3bc53e7a <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --tar-filename-prefix  <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --destination file:///dev/sm/backend/f939be89-5b9f-c7c7-e1e8-30c419ee5de6/4868ac1d-8321-4826-b058-952d37a29b82 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --destination-format raw <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --progress <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --machine <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --direct <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --destination-size <span style=color:#ae81ff>180405760</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --prezeroed</span></span></code></pre></div><ul><li>We don&rsquo;t need to propose different protocol and different format. As we will
not support different formats we just to handle data copy from socket into file
and from file to socket. Sockets and files will be managed into the
<code>qcow_tool_wrapper</code>. The <code>forkhelpers.ml</code> manages the list of file descriptors
and we will mimic what the vhd tool wrapper does to link a UUID to socket.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/144>#144</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Included some open questions under Xapi point 2</td></tr><tr><td><span class="label label-default">v3</span></td><td>Added new error, task, and assumptions</td></tr></table></header><h1 id=aggregated-local-storage-and-host-reboots>Aggregated Local Storage and Host Reboots</h1><h2 id=introduction>Introduction</h2><p>When hosts use an aggregated local storage SR, then disks are going to be mirrored to several different hosts in the pool (RAID). This ensures that if a host goes down (e.g. due to a reboot after installing a hotfix or upgrade, or when &ldquo;fenced&rdquo; by the HA feature), all disk contents in the SR are still accessible. This also means that if all disks are mirrored to just two hosts (worst-case scenario), just one host may be down at any point in time to keep the SR fully available.</p><p>When a node comes back up after a reboot, it will resynchronise all its disks with the related mirrors on the other hosts in the pool. This syncing takes some time, and only after this is done, we may consider the host &ldquo;up&rdquo; again, and allow another host to be shut down.</p><p>Therefore, when installing a hotfix to a pool that uses aggregated local storage, or doing a rolling pool upgrade, we need to make sure that we do hosts one-by-one, and we wait for the storage syncing to finish before doing the next.</p><p>This design aims to provide guidance and protection around this by blocking hosts to be shut down or rebooted from the XenAPI except when safe, and setting the <code>host.allowed_operations</code> field accordingly.</p><h2 id=xenapi>XenAPI</h2><p>If an aggregated local storage SR is in use, and one of the hosts is rebooting or down (for whatever reason), or resynchronising its storage, the operations <code>reboot</code> and <code>shutdown</code> will be removed from the <code>host.allowed_operations</code> field of <em>all</em> hosts in the pool that have a PBD for the SR.</p><p>This is a conservative approach in that assumes that this kind of SR tolerates only one node &ldquo;failure&rdquo;, and assumes no knowledge about how the SR distributes its mirrors. We may refine this in future, in order to allow some hosts to be down simultaneously.</p><p>The presence of the <code>reboot</code> operation in <code>host.allowed_operations</code> indicates whether the <code>host.reboot</code> XenAPI call is allowed or not (similarly for <code>shutdown</code> and <code>host.shutdown</code>). It will not, of course, prevent anyone from rebooting a host from the dom0 console or power switch.</p><p>Clients, such as XenCenter, can use <code>host.allowed_operations</code>, when applying an update to a pool, to guide them when it is safe to update and reboot the next host in the sequence.</p><p>In case <code>host.reboot</code> or <code>host.shutdown</code> is called while the storage is busy resyncing mirrors, the call will fail with a new error <code>MIRROR_REBUILD_IN_PROGRESS</code>.</p><h2 id=xapi>Xapi</h2><p>Xapi needs to be able to:</p><ol><li>Determine whether aggregated local storage is in use; this just means that a PBD for such an SR present.<ul><li>TBD: To avoid SR-specific code in xapi, the storage backend should tell us whether it is an aggregated local storage SR.</li></ul></li><li>Determine whether the storage system is resynchronising its mirrors; it will need to be able to query the storage backend for this kind of information.<ul><li>Xapi will poll for this and will reflect that a resync is happening by creating a <code>Task</code> for it (in the DB). This task can be used to track progress, if available.</li><li>The exact way to get the syncing information from the storage backend is SR specific. The check may be implemented in a separate script or binary that xapi calls from the polling thread. Ideally this would be integrated with the storage backend.</li></ul></li><li>Update <code>host.allowed_operations</code> for all hosts in the pool according to the rules described above. This comes down to updating the function <code>valid_operations</code> in <code>xapi_host_helpers.ml</code>, and will need to use a combination of the functionality from the two points above, plus and indication of host liveness from <code>host_metrics.live</code>.</li><li>Trigger an update of the allowed operations when a host shuts down or reboots (due to a XenAPI call or otherwise), and when it has finished resynchronising when back up. Triggers must be in the following places (some may already be present, but are listed for completeness, and to confirm this):<ul><li>Wherever <code>host_metrics.live</code> is updated to detect pool slaves going up and down (probably at least in <code>Db_gc.check_host_liveness</code> and <code>Xapi_ha</code>).</li><li>Immediately when a <code>host.reboot</code> or <code>host.shutdown</code> call is executed: <code>Message_forwarding.Host.{reboot,shutdown,with_host_operation}</code>.</li><li>When a storage resync is starting or finishing.</li></ul></li></ol><p>All of the above runs on the pool master (= SR master) only.</p><h2 id=assumptions>Assumptions</h2><p>The above will be safe if the storage cluster is equal to the XenServer pool. In general, however, it may be desirable to have a storage cluster that is larger than the pool, have multiple XS pools on a single cluster, or even share the cluster with other kinds of nodes.</p><p>To ensure that the storage is &ldquo;safe&rdquo; in these scenarios, xapi needs to be able to ask the storage backend:</p><ol><li>if a mirror is being rebuilt &ldquo;somewhere&rdquo; in the cluster, AND</li><li>if &ldquo;some node&rdquo; in the cluster is offline (even if the node is not in the XS pool).</li></ol><p>If the cluster is equal to the pool, then xapi can do point 2 without asking the storage backend, which will simplify things. For the moment, we assume that the storage cluster is equal to the XS pool, to avoid making things too complicated (while still need to keep in mind that we may change this in future).</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr></table></header><h1 id=backtrace-support>Backtrace support</h1><p>We want to make debugging easier by recording exception backtraces which are</p><ul><li>reliable</li><li>cross-process (e.g. xapi to xenopsd)</li><li>cross-language</li><li>cross-host (e.g. master to slave)</li></ul><p>We therefore need</p><ul><li>to ensure that backtraces are captured in our OCaml and python code</li><li>a marshalling format for backtraces</li><li>conventions for storing and retrieving backtraces</li></ul><h1 id=backtraces-in-ocaml>Backtraces in OCaml</h1><p>OCaml has fast exceptions which can be used for both</p><ul><li>control flow i.e. fast jumps from inner scopes to outer scopes</li><li>reporting errors to users (e.g. the toplevel or an API user)</li></ul><p>To keep the exceptions fast, exceptions and backtraces are decoupled:
there is a single active backtrace per-thread at any one time. If you
have caught an exception and then throw another exception, the backtrace
buffer will be reinitialised, destroying your previous records. For example
consider a &lsquo;finally&rsquo; function:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> finally f cleanup <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> f () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    result
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e <span style=color:#75715e>(* &lt;-- backtrace starts here now *)</span></span></span></code></pre></div><p>This function performs some action (i.e. <code>f ()</code>) and guarantees to
perform some cleanup action (<code>cleanup ()</code>) whether or not an exception
is thrown. This is a common pattern to ensure resources are freed (e.g.
closing a socket or file descriptor). Unfortunately the <code>raise e</code> in
the exception handler loses the backtrace context: when the exception
gets to the toplevel, <code>Printexc.get_backtrace ()</code> will point at the
<code>finally</code> rather than the real cause of the error.</p><p>We will use a variant of the solution proposed by
<a href=http://gallium.inria.fr/blog/a-library-to-record-ocaml-backtraces/ rel=external target=_blank>Jacques-Henri Jourdan</a>
where we will record backtraces when we catch exceptions, before the
buffer is reinitialised. Our <code>finally</code> function will now look like this:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> finally f cleanup <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> f () <span style=color:#66d9ef>in</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    result
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>with</span> e <span style=color:#f92672>-&gt;</span>
</span></span><span style=display:flex><span>    Backtrace.is_important e<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    cleanup ()<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> e</span></span></code></pre></div><p>The function <code>Backtrace.is_important e</code> associates the exception <code>e</code>
with the current backtrace before it gets deleted.</p><p>Xapi always has high-level exception handlers or other wrappers around all the
threads it spawns. In particular Xapi tries really hard to associate threads
with active tasks, so it can prefix all log lines with a task id. This helps
admins see the related log lines even when there is lots of concurrent activity.
Xapi also tries very hard to label other threads with names for the same reason
(e.g. <code>db_gc</code>). Every thread should end up being wrapped in <code>with_thread_named</code>
which allows us to catch exceptions and log stacktraces from <code>Backtrace.get</code>
on the way out.</p><h2 id=ocaml-design-guidelines>OCaml design guidelines</h2><p>Making nice backtraces requires us to think when we write our exception raising
and handling code. In particular:</p><ul><li>If a function handles an exception and re-raise it, you must call
<code>Backtrace.is_important e</code> with the exception to capture the backtrace first.</li><li>If a function raises a different exception (e.g. <code>Not_found</code> becoming a XenAPI
<code>INTERNAL_ERROR</code>) then you must use <code>Backtrace.reraise &lt;old> &lt;new></code> to
ensure the backtrace is preserved.</li><li>All exceptions should be printable &ndash; if the generic printer doesn&rsquo;t do a good
enough job then register a custom printer.</li><li>If you are the last person who will see an exception (because you aren&rsquo;t going
to rethrow it) then you <em>may</em> log the backtrace via <code>Debug.log_backtrace e</code>
<em>if and only if</em> you reasonably expect the resulting backtrace to be helpful
and not spammy.</li><li>If you aren&rsquo;t the last person who will see an exception (because you are going
to rethrow it or another exception), then <em>do not</em> log the backtrace; the
next handler will do that.</li><li>All threads should have a final exception handler at the outermost level
for example <code>Debug.with_thread_named</code> will do this for you.</li></ul><h1 id=backtraces-in-python>Backtraces in python</h1><p>Python exceptions behave similarly to the OCaml ones: if you raise a new
exception while handling an exception, the backtrace buffer is overwritten.
Therefore the same considerations apply.</p><h2 id=python-design-guidelines>Python design guidelines</h2><p>The function <a href=https://docs.python.org/2/library/sys.html#sys.exc_info rel=external target=_blank>sys.exc_info()</a>
can be used to capture the traceback associated with the last exception.
We must guarantee to call this before constructing another exception. In
particular, this does not work:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>  <span style=color:#66d9ef>raise</span> MyException(sys<span style=color:#f92672>.</span>exc_info())</span></span></code></pre></div><p>Instead you must capture the traceback first:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>  exc_info <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>exc_info()
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>raise</span> MyException(exc_info)</span></span></code></pre></div><h1 id=marshalling-backtraces>Marshalling backtraces</h1><p>We need to be able to take an exception thrown from python code, gather
the backtrace, transmit it to an OCaml program (e.g. xenopsd) and glue
it onto the end of the OCaml backtrace. We will use a simple json marshalling
format for the raw backtrace data consisting of</p><ul><li>a string summary of the error (e.g. an exception name)</li><li>a list of filenames</li><li>a corresponding list of lines</li></ul><p>(Note we don&rsquo;t use the more natural list of pairs as this confuses the
&ldquo;rpclib&rdquo; code generating library)</p><p>In python:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    results <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;error&#34;</span>: str(s[<span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;files&#34;</span>: files,
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;lines&#34;</span>: lines,
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    print json<span style=color:#f92672>.</span>dumps(results)</span></span></code></pre></div><p>In OCaml:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span>  <span style=color:#66d9ef>type</span> error <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    error<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    files<span style=color:#f92672>:</span> <span style=color:#66d9ef>string</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>    lines<span style=color:#f92672>:</span> <span style=color:#66d9ef>int</span> <span style=color:#66d9ef>list</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>}</span> <span style=color:#66d9ef>with</span> rpc
</span></span><span style=display:flex><span>  print_string <span style=color:#f92672>(</span>Jsonrpc.to_string <span style=color:#f92672>(</span>rpc_of_error <span style=color:#f92672>...))</span></span></span></code></pre></div><h1 id=retrieving-backtraces>Retrieving backtraces</h1><p>Backtraces will be written to syslog as usual. However it will also be
possible to retrieve the information via the CLI to allow diagnostic
tools to be written more easily.</p><h2 id=the-cli>The CLI</h2><p>We add a global CLI argument &ldquo;&ndash;trace&rdquo; which requests the backtrace be
printed, if one is available:</p><div class="highlight wrap-code"><pre tabindex=0><code># xe vm-start vm=hvm --trace
Error code: SR_BACKEND_FAILURE_202
Error parameters: , General backend error [opterr=exceptions must be old-style classes or derived from BaseException, not str],
Raised Server_error(SR_BACKEND_FAILURE_202, [ ; General backend error [opterr=exceptions must be old-style classes or derived from BaseException, not str];  ])
Backtrace:
0/50 EXT @ st30 Raised at file /opt/xensource/sm/SRCommand.py, line 110
1/50 EXT @ st30 Called from file /opt/xensource/sm/SRCommand.py, line 159
2/50 EXT @ st30 Called from file /opt/xensource/sm/SRCommand.py, line 263
3/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1486
4/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 83
5/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1519
6/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1567
7/50 EXT @ st30 Called from file /opt/xensource/sm/blktap2.py, line 1065
8/50 EXT @ st30 Called from file /opt/xensource/sm/EXTSR.py, line 221
9/50 xenopsd-xc @ st30 Raised by primitive operation at file &#34;lib/storage.ml&#34;, line 32, characters 3-26
10/50 xenopsd-xc @ st30 Called from file &#34;lib/task_server.ml&#34;, line 176, characters 15-19
11/50 xenopsd-xc @ st30 Raised at file &#34;lib/task_server.ml&#34;, line 184, characters 8-9
12/50 xenopsd-xc @ st30 Called from file &#34;lib/storage.ml&#34;, line 57, characters 1-156
13/50 xenopsd-xc @ st30 Called from file &#34;xc/xenops_server_xen.ml&#34;, line 254, characters 15-63
14/50 xenopsd-xc @ st30 Called from file &#34;xc/xenops_server_xen.ml&#34;, line 1643, characters 15-76
15/50 xenopsd-xc @ st30 Called from file &#34;lib/xenctrl.ml&#34;, line 127, characters 13-17
16/50 xenopsd-xc @ st30 Re-raised at file &#34;lib/xenctrl.ml&#34;, line 127, characters 56-59
17/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 937, characters 3-54
18/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1103, characters 4-71
19/50 xenopsd-xc @ st30 Called from file &#34;list.ml&#34;, line 84, characters 24-34
20/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1098, characters 2-367
21/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1203, characters 3-46
22/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1441, characters 3-9
23/50 xenopsd-xc @ st30 Raised at file &#34;lib/xenops_server.ml&#34;, line 1452, characters 9-10
24/50 xenopsd-xc @ st30 Called from file &#34;lib/xenops_server.ml&#34;, line 1458, characters 48-60
25/50 xenopsd-xc @ st30 Called from file &#34;lib/task_server.ml&#34;, line 151, characters 15-26
26/50 xapi @ st30 Raised at file &#34;xapi_xenops.ml&#34;, line 1719, characters 11-14
27/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
28/50 xapi @ st30 Raised at file &#34;xapi_xenops.ml&#34;, line 2005, characters 13-14
29/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
30/50 xapi @ st30 Raised at file &#34;xapi_xenops.ml&#34;, line 1785, characters 15-16
31/50 xapi @ st30 Called from file &#34;message_forwarding.ml&#34;, line 233, characters 25-44
32/50 xapi @ st30 Called from file &#34;message_forwarding.ml&#34;, line 915, characters 15-67
33/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
34/50 xapi @ st30 Raised at file &#34;lib/pervasiveext.ml&#34;, line 26, characters 9-12
35/50 xapi @ st30 Called from file &#34;message_forwarding.ml&#34;, line 1205, characters 21-199
36/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
37/50 xapi @ st30 Raised at file &#34;lib/pervasiveext.ml&#34;, line 26, characters 9-12
38/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
9/50 xapi @ st30 Raised at file &#34;rbac.ml&#34;, line 236, characters 10-15
40/50 xapi @ st30 Called from file &#34;server_helpers.ml&#34;, line 75, characters 11-41
41/50 xapi @ st30 Raised at file &#34;cli_util.ml&#34;, line 78, characters 9-12
42/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9
43/50 xapi @ st30 Raised at file &#34;lib/pervasiveext.ml&#34;, line 26, characters 9-12
44/50 xapi @ st30 Called from file &#34;cli_operations.ml&#34;, line 1889, characters 2-6
45/50 xapi @ st30 Re-raised at file &#34;cli_operations.ml&#34;, line 1898, characters 10-11
46/50 xapi @ st30 Called from file &#34;cli_operations.ml&#34;, line 1821, characters 14-18
47/50 xapi @ st30 Called from file &#34;cli_operations.ml&#34;, line 2109, characters 7-526
48/50 xapi @ st30 Called from file &#34;xapi_cli.ml&#34;, line 113, characters 18-56
49/50 xapi @ st30 Called from file &#34;lib/pervasiveext.ml&#34;, line 22, characters 3-9</code></pre></div><p>One can automatically set &ldquo;&ndash;trace&rdquo; for a whole shell session as follows:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export XE_EXTRA_ARGS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;--trace&#34;</span></span></span></code></pre></div><h2 id=the-xenapi>The XenAPI</h2><p>We already store error information in the XenAPI &ldquo;Task&rdquo; object and so we
can store backtraces at the same time. We shall add a field &ldquo;backtrace&rdquo;
which will have type &ldquo;string&rdquo; but which will contain s-expression encoded
backtrace data. Clients should not attempt to parse this string: its
contents may change in future. The reason it is different from the json
mentioned before is that it also contains host and process information
supplied by Xapi, and may be extended in future to contain other diagnostic
information.</p><h2 id=the-xenopsd-api>The Xenopsd API</h2><p>We already store error information in the xenopsd API &ldquo;Task&rdquo; objects,
we can extend these to store the backtrace in an additional field (&ldquo;backtrace&rdquo;).
This field will have type &ldquo;string&rdquo; but will contain s-expression encoded
backtrace data.</p><h2 id=the-smapiv1-api>The SMAPIv1 API</h2><p>Errors in SMAPIv1 are returned as XMLRPC &ldquo;Faults&rdquo; containing a code and
a status line. Xapi transforms these into XenAPI exceptions usually of the
form <code>SR_BACKEND_FAILURE_&lt;code></code>. We can extend the SM backends to use the
XenAPI exception type directly: i.e. to marshal exceptions as dictionaries:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>  results <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Status&#34;</span>: <span style=color:#e6db74>&#34;Failure&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;ErrorDescription&#34;</span>: [ code, param1, <span style=color:#f92672>...</span>, paramN ]
</span></span><span style=display:flex><span>  }</span></span></code></pre></div><p>We can then define a new backtrace-carrying error:</p><ul><li>code = <code>SR_BACKEND_FAILURE_WITH_BACKTRACE</code></li><li>param1 = json-encoded backtrace</li><li>param2 = code</li><li>param3 = reason</li></ul><p>which is internally transformed into <code>SR_BACKEND_FAILURE_&lt;code></code> and
the backtrace is appended to the current Task backtrace. From the client&rsquo;s
point of view the final exception should look the same, but Xapi will have
a chance to see and log the whole backtrace.</p><p>As a side-effect, it is possible for SM plugins to throw XenAPI errors directly,
without interpretation by Xapi.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.0)</span></td></tr></table></header><h1 id=bonding-improvements-design>Bonding Improvements design</h1><p>This document describes design details for the
PR-1006 requirements.</p><h1 id=xapi-and-xenapi>XAPI and XenAPI</h1><h2 id=creating-a-bond>Creating a Bond</h2><h3 id=current-behaviour-on-bond-creation>Current Behaviour on Bond creation</h3><p>Steps for a user to create a bond:</p><ol><li>Shutdown all VMs with VIFs using the interfaces that will be bonded,
in order to unplug those VIFs.</li><li>Create a Network to be used by the bond: <code>Network.create</code></li><li>Call <code>Bond.create</code> with a ref to this Network, a list of refs of
slave PIFs, and a MAC address to use.</li><li>Call <code>PIF.reconfigure_ip</code> to configure the bond master.</li><li>Call <code>Host.management_reconfigure</code> if one of the slaves is the
management interface. This command will call <code>interface-reconfigure</code>
to bring up the master and bring down the slave PIFs, thereby
activating the bond. Otherwise, call <code>PIF.plug</code> to activate the
bond.</li></ol><p><code>Bond.create</code> XenAPI call:</p><ol><li><p>Remove duplicates in the list of slaves.</p></li><li><p>Validate the following:</p><ul><li>Slaves must not be in a bond already.</li><li>Slaves must not be VLAN masters.</li><li>Slaves must be on the same host.</li><li>Network does not already have a PIF on the same host as the
slaves.</li><li>The given MAC is valid.</li></ul></li><li><p>Create the master PIF object.</p><ul><li>The device name of this PIF is <code>bond</code><em>x</em>, with <em>x</em> the smallest
unused non-negative integer.</li><li>The MAC of the first-named slave is used if no MAC was
specified.</li></ul></li><li><p>Create the Bond object, specifying a reference to the master. The
value of the <code>PIF.master_of</code> field on the master is dynamically
computed on request.</p></li><li><p>Set the <code>PIF.bond_slave_of</code> fields of the slaves. The value of the
<code>Bond.slaves</code> field is dynamically computed on request.</p></li></ol><h3 id=new-behaviour-on-bond-creation>New Behaviour on Bond creation</h3><p>Steps for a user to create a bond:</p><ol><li>Create a Network to be used by the bond: <code>Network.create</code></li><li>Call <code>Bond.create</code> with a ref to this Network, a list of refs of
slave PIFs, and a MAC address to use.<br>The new bond will automatically be plugged if one of the slaves was
plugged.</li></ol><p>In the following, for a host <em>h</em>, a <em>VIF-to-move</em> is a VIF associated
with a VM that is either</p><ul><li>running, suspended or paused on <em>h</em>, OR</li><li>halted, and <em>h</em> is the only host that the VM can be started on.</li></ul><p>The <code>Bond.create</code> XenAPI call is updated to do the following:</p><ol><li><p>Remove duplicates in the list of slaves.</p></li><li><p>Validate the following, and raise an exception if any of these check
fails:</p><ul><li>Slaves must not be in a bond already.</li><li>Slaves must not be VLAN masters.</li><li>Slaves must not be Tunnel access PIFs.</li><li>Slaves must be on the same host.</li><li>Network does not already have a PIF on the same host as the
slaves.</li><li>The given MAC is valid.</li></ul></li><li><p>Try unplugging all currently attached VIFs of the set of VIFs that
need to be moved. Roll back and raise an exception of one of the
VIFs cannot be unplugged (e.g. due to the absence of PV drivers in
the VM).</p></li><li><p>Determine the <em>primary slave</em>: the management PIF (if among the
slaves), or the first slave with IP configuration.</p></li><li><p>Create the master PIF object.</p><ul><li>The device name of this PIF is <code>bond</code><em>x</em>, with <em>x</em> the smallest
unused non-negative integer.</li><li>The MAC of the primary slave is used if no MAC was specified.</li><li>Include the IP configuration of the primary slave.</li><li>If any of the slaves has <code>PIF.disallow_unplug = true</code>, this will
be copied to the master.</li></ul></li><li><p>Create the Bond object, specifying a reference to the master. The
value of the <code>PIF.master_of</code> field on the master is dynamically
computed on request. Also a reference to the primary slave is
written to <code>Bond.primary_slave</code> on the new Bond object.</p></li><li><p>Set the <code>PIF.bond_slave_of</code> fields of the slaves. The value of the
<code>Bond.slaves</code> field is dynamically computed on request.</p></li><li><p>Move VLANs, plus the VIFs-to-move on them, to the master.</p><ul><li>If all VLANs on the slaves have different tags, all VLANs will
be moved to the bond master, while the same Network is used. The
network effectively moves up to the bond and therefore no VIFs
need to be moved.</li><li>If multiple VLANs on different slaves have the same tag, they
necessarily have different Networks as well. Only one VLAN with
this tag is created on the bond master. All VIFs-to-move on the
remaining VLAN networks are moved to the Network that was moved
up.</li></ul></li><li><p>Move Tunnels to the master. The tunnel Networks move up with the
tunnels. As tunnel keys are different for all tunnel networks, there
are no complications as in the VLAN case.</p></li><li><p>Move VIFs-to-move on the slaves to the master.</p></li><li><p>If one of the slaves is the current management interface, move
management to the master; the master will automatically be plugged.
If none of the slaves is the management interface, plug the master
if any of the slaves was plugged. In both cases, the slaves will
automatically be unplugged.</p></li><li><p>On all slaves, reset the IP configuration and set <code>disallow_unplug</code>
to false.</p></li></ol><p><em>Note: &ldquo;moving&rdquo; a VIF, VLAN or tunnel means &ldquo;re-creating somewhere else,
and destroying the old one&rdquo;.</em></p><h2 id=destroying-a-bond>Destroying a Bond</h2><h3 id=current-behaviour-on-bond-destruction>Current Behaviour on Bond destruction</h3><p>Steps for a user to destroy a bond:</p><ol><li>If the management interface is on the bond, move it to another PIF
using <code>PIF.reconfigure_ip</code> and <code>Host.management_reconfigure</code>.
Otherwise, no <code>PIF.unplug</code> needs to be called on the bond master, as
<code>Bond.destroy</code> does this automatically.</li><li>Call <code>Bond.destroy</code> with a ref to the Bond object.</li><li>If desired, bring up the former slave PIFs by calls to <code>PIF.plug</code>
(this is does not happen automatically).</li></ol><p><code>Bond.destroy</code> XenAPI call:</p><ol><li><p>Validate the following constraints:</p><ul><li>No VLANs are attached to the bond master.</li><li>The bond master is not the management PIF.</li></ul></li><li><p>Bring down the master PIF and clean up the underlying network
devices.</p></li><li><p>Remove the Bond and master PIF objects.</p></li></ol><h3 id=new-behaviour-on-bond-destruction>New Behaviour on Bond destruction</h3><p>Steps for a user to destroy a bond:</p><ol><li>Call <code>Bond.destroy</code> with a ref to the Bond object.</li><li>If desired, move VIFs/VLANs/tunnels/management from (former) primary
slave to other PIFs.</li></ol><p><code>Bond.destroy</code> XenAPI call is updated to do the following:</p><ol><li>Try unplugging all currently attached VIFs of the set of VIFs that
need to be moved. Roll back and raise an exception of one of the
VIFs cannot be unplugged (e.g. due to the absence of PV drivers in
the VM).</li><li>Copy the IP configuration of the master to the primary slave.</li><li>Move VLANs, with their Networks, to the primary slave.</li><li>Move Tunnels, with their Networks, to the primary slave.</li><li>Move VIFs-to-move on the master to the primary slave.</li><li>If the master is the current management interface, move management
to the primary slave. The primary slave will automatically be
plugged.</li><li>If the master was plugged, plug the primary slave. This will
automatically clean up the underlying devices of the bond.</li><li>If the master has <code>PIF.disallow_unplug = true</code>, this will be copied
to the primary slave.</li><li>Remove the Bond and master PIF objects.</li></ol><h2 id=using-bond-slaves>Using Bond Slaves</h2><h3 id=current-behaviour-for-bond-slaves>Current Behaviour for Bond Slaves</h3><ul><li>It possible to plug any existing PIF, even bond slaves. Any other
PIFs that cannot be attached at the same time as the PIF that is
being plugged, are automatically unplugged.</li><li>Similarly, it is possible to make a bond slave the management
interface. Any other PIFs that cannot be attached at the same time
as the PIF that is being plugged, are automatically unplugged.</li><li>It is possible to have a VIF on a Network associated with a bond
slave. When the VIF&rsquo;s VM is started, or the VIF is hot-plugged, the
PIF is relies on is automatically plugged, and any other PIFs that
cannot be attached at the same time as this PIF are automatically
unplugged.</li><li>It is possible to have a VLAN on a bond slave, though the bond
(master) and the VLAN may not be simultaneously attached. This is
not currently enforced (which may be considered a bug).</li></ul><h3 id=new-behaviour-for-bond-slaves>New behaviour for Bond Slaves</h3><ul><li>It is no longer possible to plug a bond slave. The exception
CANNOT_PLUG_BOND_SLAVE is raised when trying to do so.</li><li>It is no longer possible to make a bond slave the management
interface. The exception CANNOT_PLUG_BOND_SLAVE is raised when
trying to do so.</li><li>It is still possible to have a VIF on the Network of a bond slave.
However, it is not possible to start such a VIF&rsquo;s VM on a host, if
this would need a bond slave to be plugged. Trying this will result
in a CANNOT_PLUG_BOND_SLAVE exception. Likewise, it is not
possible to hot-plug such a VIF.</li><li>It is no longer possible to place a VLAN on a bond slave. The
exception CANNOT_ADD_VLAN_TO_BOND_SLAVE is raised when trying
to do so.</li><li>It is no longer possible to place a tunnel on a bond slave. The
exception CANNOT_ADD_TUNNEL_TO_BOND_SLAVE is raised when trying
to do so.</li></ul><h2 id=actions-on-start-up>Actions on Start-up</h2><h3 id=current-behaviour-on-start-up>Current Behaviour on Start-up</h3><p>When a pool slave starts up, bonds and VLANs on the pool master are
replicated on the slave:</p><ul><li>Create all VLANs that the master has, but the slave has not. VLANs
are identified by their tag, the device name of the slave PIF, and
the Networks of the master and slave PIFs.</li><li>Create all bonds that the master has, but the slave has not. If the
interfaces needed for the bond are not all available on the slave, a
partial bond is created. If some of these interface are already
bonded on the slave, this bond is destroyed first.</li></ul><h3 id=new-behaviour-on-start-up>New Behaviour on Start-up</h3><ul><li>The current VLAN/tunnel/bond recreation code is retained, as it uses
the new Bond.create and Bond.destroy functions, and therefore does
what it needs to do.</li><li>Before VLAN/tunnel/bond recreation, any violations of the rules
defined in R2 are rectified, by moving VIFs, VLANs, tunnels or
management up to bonds.</li></ul><h1 id=cli>CLI</h1><p>The behaviour of the <code>xe</code> CLI commands <code>bond-create</code>, <code>bond-destroy</code>,
<code>pif-plug</code>, and <code>host-management-reconfigure</code> is changed to match their
associated XenAPI calls.</p><h1 id=xencenter>XenCenter</h1><p>XenCenter already automatically moves the management interface when a
bond is created or destroyed. This is no longer necessary, as the
<code>Bond.create/destroy</code> calls already do this. XenCenter only needs to
copy any <code>PIF.other_config</code> keys that is needs between primary slave and
bond master.</p><h1 id=manual-tests>Manual Tests</h1><ul><li>Create a bond of two interfaces&mldr;<ul><li>without VIFs/VLANs/management on them;</li><li>with management on one of them;</li><li>with a VLAN on one of them;</li><li>with two VLANs on two different interfaces, having the same VLAN
tag;</li><li>with a VIF associated with a halted VM on one of them;</li><li>with a VIF associated with a running VM (with and without PV
drivers) on one of them.</li></ul></li><li>Destroy a bond of two interfaces&mldr;<ul><li>without VIFs/VLANs/management on it;</li><li>with management on it;</li><li>with a VLAN on it;</li><li>with a VIF associated with a halted VM on it;</li><li>with a VIF associated with a running VM (with and without PV
drivers) on it.</li></ul></li><li>In a pool of two hosts, having VIFs/VLANs/management on the
interfaces of the pool slave, create a bond on the pool master, and
restart XAPI on the slave.</li><li>Restart XAPI on a host with a networking configuration that has
become illegal due to these requirements.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=code-coverage-profiling>Code Coverage Profiling</h1><p>We would like to add optional coverage profiling to existing <a href=http://ocaml.org rel=external target=_blank>OCaml</a>
projects in the context of <a href=https://github.com/xenserver rel=external target=_blank>XenServer</a> and <a href=https://github.com/xapi-project rel=external target=_blank>XenAPI</a>. This article
presents how we do it.</p><p>Binaries instrumented for coverage profiling in the XenServer project
need to run in an environment where several services act together as
they provide operating-system-level services. This makes it a little
harder than profiling code that can be profiled and executed in
isolation.</p><h2 id=tldr>TL;DR</h2><p>To build binaries with coverage profiling, do:</p><pre><code>./configure --enable-coverage
make
</code></pre><p>Binaries will log coverage data to <code>/tmp/bisect*.out</code> from which a
coverage report can be generated in <code>coverage/</code>:</p><pre><code>bisect-ppx-report -I _build -html coverage /tmp/bisect*.out
</code></pre><h2 id=profiling-framework-bisect-ppx>Profiling Framework Bisect-PPX</h2><p>The open-source <a href=https://github.com/aantron/bisect_ppx rel=external target=_blank>BisectPPX</a> instrumentation framework uses extension
points (PPX) in the <a href=http://ocaml.org rel=external target=_blank>OCaml</a> compiler to instrument code during
compilation. Instrumented code for a binary is then compiled as usual
and logs during execution data to in-memory data structures. Before an
instrumented binary terminates, it writes the logged data to a file.
This data can then be analysed with the <code>bisect-ppx-report</code> tool, to
produce a summary of annotated code that highlights what part of a
codebase was executed.</p><p><a href=https://github.com/aantron/bisect_ppx rel=external target=_blank>BisectPPX</a> has several desirable properties:</p><ul><li>a robust code base that is well tested</li><li>it is easy to integrate into the compilation pipeline (see below)</li><li>is specific to the <a href=http://ocaml.org rel=external target=_blank>OCaml</a> language; an expression-oriented language
like OCaml doesn&rsquo;t fit the traditional statement coverage well</li><li>it is actively maintained</li><li>is generates useful reports for interactive and non-interactive use
that help to improve code coverage</li></ul><p><img alt="Coverage Analysis" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/coverage/coverage-screenshot.png style=height:auto;width:auto></p><p>Red parts indicate code that wasn&rsquo;t executed whereas green parts were.
Hovering over a dark green spot reveals how often that point was
executed.</p><p>The individual steps of instrumenting code with <a href=https://github.com/aantron/bisect_ppx rel=external target=_blank>BisectPPX</a> are greatly
abstracted by OCamlfind (OCaml&rsquo;s library manager) and OCamlbuild
(OCaml&rsquo;s compilation manager):</p><pre><code># write code
vim example.ml

# build it with instrumentation from bisect_ppx
ocamlbuild -use-ocamlfind -pkg bisect_ppx -pkg unix example.native

# execute it - generates files ./bisect*.out
./example.native

# generate report
bisect-ppx-report -I _build -html coverage bisect000*

# view coverage/index.html

Summary:
 - 'binding' points: 2/2 (100.00%)
 - 'sequence' points: 10/10 (100.00%)
 - 'match/function' points: 5/8 (62.50%)
 - total: 17/20 (85.00%)
</code></pre><p>The fourth step generates a HTML report in <code>coverage/</code>. All it takes is
to declare to <a href=https://github.com/ocaml/ocamlbuild/blob/master/manual/manual.adoc rel=external target=_blank>OCamlbuild</a> that a module depends on <code>bisect_ppx</code> and it
will be instrumented during compilation. Behind the scenes <code>ocamlfind</code>
makes sure that the compiler uses a preprocessing step that instruments
the code.</p><h2 id=signal-handling>Signal Handling</h2><p>During execution the code instrumentation leads to the collection of
data. This code registers a function with <code>at_exit</code> that writes the data
to <code>bisect*.out</code> when <code>exit</code> is called. A binary can terminate without
calling <code>exit</code> and in that case the file would not be written. It is
therefore important to make sure that <code>exit</code> is called. If this does not
happen naturally, for example in the context of a daemon that is
terminated by receiving the <code>TERM</code> signal, a signal handler must be
installed:</p><pre><code>let stop signal =
  printf &quot;caught signal %a\n&quot; Debug.Pp.signal signal;
  exit 0

Sys.set_signal Sys.sigterm (Sys.Signal_handle stop)
</code></pre><h2 id=dumping-coverage-information-at-runtime>Dumping coverage information at runtime</h2><p>By default coverage data can only be dumped at exit, which is inconvenient if you have a test-suite
that needs to reuse a long running daemon, and starting/stopping it each time is not feasible.</p><p>In such cases we need an API to dump coverage at runtime, which <em>is</em> provided by <code>bisect_ppx >= 1.3.0</code>.
However each daemon will need to set up a way to listen to an event that triggers this coverage dump,
furthermore it is desirable to make runtime coverage dumping compiled in conditionally to be absolutely sure
that production builds do <em>not</em> use coverage preprocessed code.</p><p>Hence instead of duplicating all this build logic in each daemon (<code>xapi</code>, <code>xenopsd</code>, etc.) provide this
functionality in a common library <code>xapi-idl</code> that:</p><ul><li>logs a message on startup so we know it is active</li><li>sets BISECT_FILE environment variable to dump coverage in the appropriate place</li><li>listens on <code>org.xen.xapi.coverage.&lt;name></code> message queue for runtime coverage dump commands:<ul><li>sending <code>dump &lt;Number></code> will cause runtime coverage to be dumped to a file
named <code>bisect-&lt;name>-&lt;random>.&lt;Number>.out</code></li><li>sending <code>reset</code> will cause the runtime coverage counters to be reset</li></ul></li></ul><p>Daemons that use <code>Xcp_service.configure2</code> (e.g. <code>xenopsd</code>) will benefit from this runtime trigger automatically,
provided they are themselves preprocessed with <code>bisect_ppx</code>.</p><p>Since we are interested in collecting coverage data for system-wide test-suite runs we need a way to trigger
dumping of coverage data centrally, and a good candidate for that is <code>xapi</code> as the top-level daemon.</p><p>It will call <code>Xcp_coverage.dispatcher_init ()</code>, which listens on <code>org.xen.xapi.coverage.dispatch</code> and
dispatches the coverage dump command to all message queues under <code>org.xen.xapi.coverage.*</code> except itself.</p><p>On production, and regular builds all of this is a no-op, ensured by using separate <code>lib/coverage/disabled.ml</code> and <code>lib/coverage/enabled.ml</code>
files which implement the same interface, and choosing which one to use at build time.</p><h2 id=where-data-is-written>Where Data is Written</h2><p>By default, <a href=https://github.com/aantron/bisect_ppx rel=external target=_blank>BisectPPX</a> writes data in a binary&rsquo;s current working
directory as <code>bisectXXXX.out</code>. It doesn&rsquo;t overwrite existing files and
files from several runs can be combined during analysis. However, this
name and the location can be inconvenient when multiple programs share a
directory.</p><p><a href=https://github.com/aantron/bisect_ppx rel=external target=_blank>BisectPPX</a>&rsquo;s default can be overridden with the <code>BISECT_FILE</code>
environment variable. This can happen on the command line:</p><pre><code>BISECT_FILE=/tmp/example ./example.native
</code></pre><p>In the context of XenServer we could do this in startup scripts.
However, we added a bit of code</p><pre><code>val Coverage.init: string -&gt; unit
</code></pre><p>that sets the environment variable from inside the program. The files
are written to a temporary directory (respecting <code>$TMP</code> or using <code>/tmp</code>)
and uses the <code>string</code>-typed argument to include it in the name. To be
effective, this function must be called before the programs exits. For
clarity it is called at the begin of program execution.</p><h2 id=instrumenting-an-oasis-project>Instrumenting an Oasis Project</h2><p>While instrumentation is easy on the level of a small file or project it
is challenging in a bigger project. We decided to focus on projects that
are build with the <a href=http://oasis.forge.ocamlcore.org rel=external target=_blank>Oasis</a> build and packaging manager. These have a
well-defined structure and compilation process that is controlled by a
central <code>_oasis</code> file. This file describes for each library and binary
its dependencies at a package level. From this, <a href=http://oasis.forge.ocamlcore.org rel=external target=_blank>Oasis</a> generates a
<code>configure</code> script and compilation rules for the <a href=https://github.com/ocaml/ocamlbuild/blob/master/manual/manual.adoc rel=external target=_blank>OCamlbuild</a> system.
<a href=http://oasis.forge.ocamlcore.org rel=external target=_blank>Oasis</a> is designed that the generated files can be shipped without
requiring <a href=http://oasis.forge.ocamlcore.org rel=external target=_blank>Oasis</a> itself being available.</p><p>Goals for instrumentation are:</p><ul><li>what files are instrumented should be obvious and easy to manage</li><li>instrumentation must be optional, yet easy to activate</li><li>avoid methods that require to keep several files in sync like multiple
<code>_oasis</code> files</li><li>avoid separate Git branches for instrumented and non-instrumented
code</li></ul><p>In the ideal case, we could introduce a configuration switch
<code>./configure --enable-coverage</code> that would prepare compilation for
coverage instrumentation. While <a href=http://oasis.forge.ocamlcore.org rel=external target=_blank>Oasis</a> supports the creation of such
switches, they cannot be used to control build dependencies like
compiling a file with or without package <code>bisec_ppx</code>. We have chosen a
different method:</p><p>A <code>Makefile</code> target <code>coverage</code> augments the <code>_tags</code> file to include the
rules in file <code>_tags.coverage</code> that cause files to be instrumented:</p><pre><code>make coverage # prepare
make          # build
</code></pre><p>leads to the execution of this code during preparation:</p><pre><code>coverage: _tags _tags.coverage
  test ! -f _tags.orig &amp;&amp; mv _tags _tags.orig || true
  cat _tags.coverage _tags.orig &gt; _tags
</code></pre><p>The file <code>_tags.coverage</code> contains two simple <a href=https://github.com/ocaml/ocamlbuild/blob/master/manual/manual.adoc rel=external target=_blank>OCamlbuild</a> rules that
could be tweaked to instrument only some files:</p><pre><code>&lt;**/*.ml{,i,y}&gt;: pkg_bisect_ppx
&lt;**/*.native&gt;:   pkg_bisect_ppx
</code></pre><p>When <code>make coverage</code> is not called, these rules are not active and
hence, code is not instrumented for coverage. We believe that this
solution to control instrumentation meets the goals from above. In
particular, what files are instrumented and when is controlled by very
few lines of declarative code that lives in the main repository of a
project.</p><h2 id=project-layout>Project Layout</h2><p>The crucial files in an <a href=http://oasis.forge.ocamlcore.org rel=external target=_blank>Oasis</a>-controlled project that is set up for
coverage analysis are:</p><pre><code>./_oasis                      - make &quot;profiling&quot; a build depdency
./_tags.coverage              - what files get instrumented
./profiling/coverage.ml       - support file, sets env var
./Makefile                    - target 'coverage'
</code></pre><p>The <code>_oasis</code> file bundles the files under <code>profiling/</code> into an internal
library which executables then depend on:</p><pre><code>	# Support files for profiling
	Library profiling
		CompiledObject:     best
		Path:               profiling
		Install:            false
		Findlibname:        profiling
		Modules:            Coverage
		BuildDepends:

	Executable set_domain_uuid
		CompiledObject:     best
		Path:               tools
		ByteOpt:            -warn-error +a-3
		NativeOpt:          -warn-error +a-3
		MainIs:             set_domain_uuid.ml
		Install:            false
		BuildDepends:
			xenctrl,
			uuidm,
			cmdliner,
			profiling			# &lt;-- here
</code></pre><p>The <code>Makefile</code> target <code>coverage</code> primes the project for a profiling build:</p><pre><code>	# make coverage - prepares for building with coverage analysis

	coverage: _tags _tags.coverage
		test ! -f _tags.orig &amp;&amp; mv _tags _tags.orig || true
		cat _tags.coverage _tags.orig &gt; _tags
</code></pre><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v7</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Add details about VM migration and import</td></tr><tr><td><span class="label label-default">v3</span></td><td>Included and excluded use cases</td></tr><tr><td><span class="label label-default">v4</span></td><td>Rolling Pool Upgrade use cases</td></tr><tr><td><span class="label label-default">v5</span></td><td>Lots of changes to simplify the design</td></tr><tr><td><span class="label label-default">v6</span></td><td>Use case refresh based on simplified design</td></tr><tr><td><span class="label label-default">v7</span></td><td>RPU refresh based on simplified design</td></tr></table></header><h1 id=cpu-feature-levelling-20>CPU feature levelling 2.0</h1><h1 id=executive-summary>Executive Summary</h1><p>The old XS 5.6-style Heterogeneous Pool feature that is based around hardware-level CPUID masking will be replaced by a safer and more flexible software-based levelling mechanism.</p><h1 id=history>History</h1><ul><li>Original XS 5.6 design: <a href=/new-docs/design/heterogeneous-pools/index.html>heterogeneous-pools</a></li><li>Changes made in XS 5.6 FP1 for the DR feature (added CPUID checks upon migration)</li><li>XS 6.1: migration checks extended for cross-pool scenario</li></ul><h1 id=high-level-interfaces-and-behaviour>High-level Interfaces and Behaviour</h1><p>A VM can only be migrated safely from one host to another if both hosts offer the set of CPU features which the VM expects. If this is not the case, CPU features may appear or disappear as the VM is migrated, causing it to crash. The purpose of feature levelling is to hide features which the hosts do not have in common from the VM, so that it does not see any change in CPU capabilities when it is migrated.</p><p>Most pools start off with homogenous hardware, but over time it may become impossible to source new hosts with the same specifications as the ones already in the pool. The main use of feature levelling is to allow such newer, more capable hosts to be added to an existing pool while preserving the ability to migrate existing VMs to any host in the pool.</p><h2 id=principles-for-migration>Principles for Migration</h2><p>The CPU levelling feature aims to both:</p><ol><li>Make VM migrations <em>safe</em> by ensuring that a VM will see the same CPU features before and after a migration.</li><li>Make VMs as <em>mobile</em> as possible, so that it can be freely migrated around in a XenServer pool.</li></ol><p>To make migrations safe:</p><ul><li>A migration request will be blocked if the destination host does not offer the some of the CPU features that the VM currently sees.</li><li>Any additional CPU features that the destination host is able to offer will be hidden from the VM.</li></ul><p><em>Note:</em> Due to the limitations of the old Heterogeneous Pools feature, we are not able to guarantee the safety of VMs that are migrated to a Levelling-v2 host from an older host, during a rolling pool upgrade. This is because such VMs may be using CPU features that were not captured in the old feature sets, of which we are therefore unaware. However, migrations between the same two hosts, but before the upgrade, may have already been unsafe. The promise is that we will not make migrations <em>more</em> unsafe during a rolling pool upgrade.</p><p>To make VMs mobile:</p><ul><li>A VM that is started in a XenServer pool will be able to see only CPU features that are common to all hosts in the pool. The set of common CPU features is referred to in this document as the <em>pool CPU feature level</em>, or simply the <em>pool level</em>.</li></ul><h2 id=use-cases-for-pools>Use Cases for Pools</h2><ol><li><p>A user wants to add a new host to an existing XenServer pool. The new host has all the features of the existing hosts, plus extra features which the existing hosts do not. The new host will be allowed to join the pool, but its extra features will be hidden from VMs that are started on the host or migrated to it. The join does not require any host reboots.</p></li><li><p>A user wants to add a new host to an existing XenServer pool. The new host does not have all the features of the existing ones. XenCenter warns the user that adding the host to the pool is possible, but it would lower the pool&rsquo;s CPU feature level. The user accepts this and continues the join. The join does not require any host reboots. VMs that are started anywhere on the pool, from now on, will only see the features of the new host (the lowest common denominator), such that they are migratable to any host in the pool, including the new one. VMs that were running before the pool join will not be migratable to the new host, because these VMs may be using features that the new host does not have. However, after a reboot, such VMs will be fully mobile.</p></li><li><p>A user wants to add a new host to an existing XenServer pool. The new host does not have all the features of the existing ones, and at the same time, it has certain features that the pool does not have (the feature sets overlap). This is essentially a combination of the two use cases above, where the pool&rsquo;s CPU feature level will be downgraded to the intersection of the feature sets of the pool and the new host. The join does not require any host reboots.</p></li><li><p>A user wants to upgrade or repair the hardware of a host in an existing XenServer pool. After upgrade the host has all the features it used to have, plus extra features which other hosts in the pool do not have. The extra features are masked out and the host resumes its place in the pool when it is booted up again.</p></li><li><p>A user wants to upgrade or repair the hardware of a host in an existing XenServer pool. After upgrade the host has fewer features than it used to have. When the host is booted up again, the pool CPU&rsquo;s feature level will be automatically lowered, and the user will be alerted of this fact (through the usual alerting mechanism).</p></li><li><p>A user wants to remove a host from an existing XenServer pool. The host will be removed as normal after any VMs on it have been migrated away. The feature set offered by the pool will be automatically re-levelled upwards in case the host which was removed was the least capable in the pool, and additional features common to the remaining hosts will be unmasked.</p></li></ol><h2 id=rolling-pool-upgrade>Rolling Pool Upgrade</h2><ul><li><p>A VM which was running on the pool before the upgrade is expected to continue to run afterwards. However, when the VM is migrated to an upgraded host, some of the CPU features it had been using might disappear, either because they are not offered by the host or because the new feature-levelling mechanism hides them. To have the best chance for such a VM to successfully migrate (see the note under &ldquo;Principles for Migration&rdquo;), it will be given a temporary VM-level feature set providing all of the destination&rsquo;s CPU features that were unknown to XenServer before the upgrade. When the VM is rebooted it will inherit the pool-level feature set.</p></li><li><p>A VM which is started during the upgrade will be given the current pool-level feature set. The pool-level feature set may drop after the VM is started, as more hosts are upgraded and re-join the pool, however the VM is guaranteed to be able to migrate to any host which has already been upgraded. If the VM is started on the master, there is a risk that it may only be able to run on that host.</p></li><li><p>To allow the VMs with grandfathered-in flags to be migrated around in the pool, the intra pool VM migration pre-checks will compare the VM&rsquo;s feature flags to the target host&rsquo;s flags, not the pool flags. This will maximise the chance that a VM can be migrated somewhere in a heterogeneous pool, particularly in the case where only a few hosts in the pool do not have features which the VMs require.</p></li><li><p>To allow cross-pool migration, including to pool of a higher XenServer version, we will still check the VM&rsquo;s requirements against the <em>pool-level</em> features of the target pool. This is to avoid the possibility that we migrate a VM to an &lsquo;island&rsquo; in the other pool, from which it cannot be migrated any further.</p></li></ul><h2 id=xenapi-changes>XenAPI Changes</h2><h3 id=fields>Fields</h3><ul><li><code>host.cpu_info</code> is a field of type <code>(string -> string) map</code> that contains information about the CPUs in a host. It contains the following keys: <code>cpu_count</code>, <code>socket_count</code>, <code>vendor</code>, <code>speed</code>, <code>modelname</code>, <code>family</code>, <code>model</code>, <code>stepping</code>, <code>flags</code>, <code>features</code>, <code>features_after_reboot</code>, <code>physical_features</code> and <code>maskable</code>.<ul><li>The following keys are specific to hardware-based CPU masking and will be removed: <code>features_after_reboot</code>, <code>physical_features</code> and <code>maskable</code>.</li><li>The <code>features</code> key will continue to hold the current CPU features that the host is able to use. In practise, these features will be available to Xen itself and dom0; guests may only see a subset. The current format is a string of four 32-bit words represented as four groups of 8 hexadecimal digits, separated by dashes. This will change to an arbitrary number of 32-bit words. Each bit at a particular position (starting from the left) still refers to a distinct CPU feature (<code>1</code>: feature is present; <code>0</code>: feature is absent), and feature strings may be compared between hosts. The old format simply becomes a special (4 word) case of the new format, and bits in the same position may be compared between old and new feature strings.</li><li>The new key <code>features_pv</code> will be added, representing the subset of <code>features</code> that the host is able to offer to a PV guest.</li><li>The new key <code>features_hvm</code> will be added, representing the subset of <code>features</code> that the host is able to offer to an HVM guest.</li></ul></li><li>A new field <code>pool.cpu_info</code> of type <code>(string -> string) map</code> (read only) will be added. It will contain:<ul><li><code>vendor</code>: The common CPU vendor across all hosts in the pool.</li><li><code>features_pv</code>: The intersection of <code>features_pv</code> across all hosts in the pool, representing the feature set that a PV guest will see when started on the pool.</li><li><code>features_hvm</code>: The intersection of <code>features_hvm</code> across all hosts in the pool, representing the feature set that an HVM guest will see when started on the pool.</li><li><code>cpu_count</code>: the total number of CPU cores in the pool.</li><li><code>socket_count</code>: the total number of CPU sockets in the pool.</li></ul></li><li>The <code>pool.other_config:cpuid_feature_mask</code> override key will no longer have any effect on pool join or VM migration.</li><li>The field <code>VM.last_boot_CPU_flags</code> will be updated to the new format (see <code>host.cpu_info:features</code>). It will still contain the feature set that the VM was started with as well as the vendor (under the <code>features</code> and <code>vendor</code> keys respectively).</li></ul><h3 id=messages>Messages</h3><ul><li><code>pool.join</code> currently requires that the CPU vendor and feature set (according to <code>host.cpu_info:vendor</code> and <code>host.cpu_info:features</code>) of the joining host are equal to those of the pool master. This requirement will be loosened to mandate only equality in CPU vendor:<ul><li>The join will be allowed if <code>host.cpu_info:vendor</code> equals <code>pool.cpu_info:vendor</code>.</li><li>This means that xapi will additionally allow hosts that have a <em>more</em> extensive feature set than the pool (as long as the CPU vendor is common). Such hosts are transparently down-levelled to the pool level (without needing reboots).</li><li>This further means that xapi will additionally allow hosts that have a <em>less</em> extensive feature set than the pool (as long as the CPU vendor is common). In this case, the pool is transparently down-levelled to the new host&rsquo;s level (without needing reboots). Note that this does not affect any running VMs in any way; the mobility of running VMs will not be restricted, which can still migrate to any host they could migrate to before. It does mean that those running VMs will not be migratable to the new host.</li><li>The current error raised in case of a CPU mismatch is <code>POOL_HOSTS_NOT_HOMOGENEOUS</code> with <code>reason</code> argument <code>"CPUs differ"</code>. This will remain the error that is raised if the pool join fails due to incompatible CPU vendors.</li><li>The <code>pool.other_config:cpuid_feature_mask</code> override key will no longer have any effect.</li></ul></li><li><code>host.set_cpu_features</code> and <code>host.reset_cpu_features</code> will be removed: it is no longer to use the old method of CPU feature masking (CPU feature sets are controlled automatically by xapi). Calls will fail with <code>MESSAGE_REMOVED</code>.</li><li>VM lifecycle operations will be updated internally to use the new feature fields, to ensure that:<ul><li>Newly started VMs will be given CPU features according to the pool level for maximal mobility.</li><li>For safety, running VMs will maintain their feature set across migrations and suspend/resume cycles. CPU features will transparently be hidden from VMs.</li><li>Furthermore, migrate and resume will only be allowed in case the target host&rsquo;s CPUs are capable enough, i.e. <code>host.cpu_info:vendor</code> = <code>VM.last_boot_CPU_flags:vendor</code> and <code>host.cpu_info:features_{pv,hvm}</code> âŠ‡ <code>VM.last_boot_CPU_flags:features</code>. A <code>VM_INCOMPATIBLE_WITH_THIS_HOST</code> error will be returned otherwise (as happens today).</li><li>For cross pool migrations, to ensure maximal mobility in the target pool, a stricter condition will apply: the VM must satisfy the pool CPU level rather than just the target host&rsquo;s level: <code>pool.cpu_info:vendor</code> = <code>VM.last_boot_CPU_flags:vendor</code> and <code>pool.cpu_info:features_{pv,hvm}</code> âŠ‡ <code>VM.last_boot_CPU_flags:features</code></li></ul></li></ul><h2 id=cli-changes>CLI Changes</h2><p>The following changes to the <code>xe</code> CLI will be made:</p><ul><li><code>xe host-cpu-info</code> (as well as <code>xe host-param-list</code> and friends) will return the fields of <code>host.cpu_info</code> as described above.</li><li><code>xe host-set-cpu-features</code> and <code>xe host-reset-cpu-features</code> will be removed.</li><li><code>xe host-get-cpu-features</code> will still return the value of <code>host.cpu_info:features</code> for a given host.</li></ul><h1 id=low-level-implementation>Low-level implementation</h1><h2 id=xenctrl>Xenctrl</h2><p>The old <code>xc_get_boot_cpufeatures</code> hypercall will be removed, and replaced by two new functions, which are available to xenopsd through the Xenctrl module:</p><pre><code>external get_levelling_caps : handle -&gt; int64 = &quot;stub_xc_get_levelling_caps&quot;

type featureset_index = Featureset_host | Featureset_pv | Featureset_hvm
external get_featureset : handle -&gt; featureset_index -&gt; int64 array = &quot;stub_xc_get_featureset&quot;
</code></pre><p>In particular, the <code>get_featureset</code> function will be used by xapi/xenopsd to ask Xen which are the widest sets of CPU features that it can offer to a VM (PV or HVM). I don&rsquo;t think there is a use for <code>get_levelling_caps</code> yet.</p><h2 id=xenopsd>Xenopsd</h2><ul><li>Update the type <code>Host.cpu_info</code>, which contains all the fields that need to go into the <code>host.cpu_info</code> field in the xapi DB. The type already exists but is unused. Add the function <code>HOST.get_cpu_info</code> to obtain an instance of the type. Some code from xapi and the cpuid.ml from xen-api-libs can be reused.</li><li>Add a platform key <code>featureset</code> (<code>Vm.t.platformdata</code>), which xenopsd will write to xenstore along with the other platform keys (no code change needed in xenopsd). Xenguest will pick this up when a domain is created, and will apply the CPUID policy to the domain. This has the effect of masking out features that the host may have, but which have a <code>0</code> in the feature set bitmap.</li><li>Review current cpuid-related functions in <code>xc/domain.ml</code>.</li></ul><h2 id=xapi>Xapi</h2><h3 id=xapi-startup>Xapi startup</h3><ul><li>Update <code>Create_misc.create_host_cpu</code> function to use the new xenopsd call.</li><li>If the host features fall below pool level, e.g. due to a change in hardware: down-level the pool by updating <code>pool.cpu_info.features_{pv,hvm}</code>. Newly started VMs will inherit the new level; already running VMs will not be affected, but will not be able to migrate to this host.</li><li>To notify the admin of this event, an API alert (message) will be set: <code>pool_cpu_features_downgraded</code>.</li></ul><h3 id=vm-start>VM start</h3><ul><li>Inherit feature set from pool (<code>pool.cpu_info.features_{pv,hvm}</code>) and set <code>VM.last_boot_CPU_flags</code> (<code>cpuid_helpers.ml</code>).</li><li>The domain will be started with this CPU feature set enabled, by writing the feature set string to <code>platformdata</code> (see above).</li></ul><h3 id=vm-migrate-and-resume>VM migrate and resume</h3><ul><li>There are already CPU compatiblity checks on migration, both in-pool and cross-pool, as well as resume. Xapi compares <code>VM.last_boot_CPU_flags</code> of the VM to-migrate with <code>host.cpu_info</code> of the receiving host. Migration is only allowed if the CPU vendors and the same, and <code>host.cpu_info:features</code> âŠ‡ <code>VM.last_boot_CPU_flags:features</code>. The check can be overridden by setting the <code>force</code> argument to <code>true</code>.</li><li>For in-pool migrations, these checks will be updated to use the appropriate <code>features_pv</code> or <code>features_hvm</code> field.</li><li>For cross-pool migrations. These checks will be updated to use <code>pool.cpu_info</code> (<code>features_pv</code> or <code>features_hvm</code> depending on how the VM was booted) rather than <code>host.cpu_info</code>.</li><li>If the above checks pass, then the <code>VM.last_boot_CPU_flags</code> will be maintained, and the new domain will be started with the same CPU feature set enabled, by writing the feature set string to <code>platformdata</code> (see above).</li><li>In case the VM is migrated to a host with a higher xapi software version (e.g. a migration from a host that does not have CPU levelling v2), the feature string may be longer. This may happen during a rolling pool upgrade or a cross-pool migration, or when a suspended VM is resume after an upgrade. In this case, the following safety rules apply:<ul><li>Only the existing (shorter) feature string will be used to determine whether the migration will be allowed. This is the best we can do, because we are unaware of the state of the extended feature set on the older host.</li><li>The existing feature set in <code>VM.last_boot_CPU_flags</code> will be extended with the extra bits in <code>host.cpu_info:features_{pv,hvm}</code>, i.e. the widest feature set that can possibly be granted to the VM (just in case the VM was using any of these features before the migration).</li><li>Strictly speaking, a migration of a VM from host A to B that was allowed before B was upgraded, may no longer be allowed after the upgrade, due to stricter feature sets in the new implementation (from the <code>xc_get_featureset</code> hypercall). However, the CPU features that are switched off by the new implementation are features that a VM would not have been able to actually use. We therefore need a don&rsquo;t-care feature set (similar to the old <code>pool.other_config:cpuid_feature_mask</code> key) with bits that we may ignore in migration checks, and switch off after the migration. This will be a xapi config file option.</li><li>XXX: Can we actually block a cross-pool migration at the receiver end??</li></ul></li></ul><h3 id=vm-import>VM import</h3><p>The <code>VM.last_boot_CPU_flags</code> field must be upgraded to the new format (only really needed for VMs that were suspended while exported; <code>preserve_power_state=true</code>), as described above.</p><h3 id=pool-join>Pool join</h3><p>Update pool join checks according to the rules above (see <code>pool.join</code>), i.e. remove the CPU features constraints.</p><h3 id=upgrade>Upgrade</h3><ul><li>The pool level (<code>pool.cpu_info</code>) will be initialised when the pool master upgrades, and automatically adjusted if needed (downwards) when slaves are upgraded, by each upgraded host&rsquo;s started sequence (as above under &ldquo;Xapi startup&rdquo;).</li><li>The <code>VM.last_boot_CPU_flags</code> fields of running and suspended VMs will be &ldquo;upgraded&rdquo; to the new format on demand, when a VM is migrated to or resume on an upgraded host, as described above.</li></ul><h2 id=xencenter-integration>XenCenter integration</h2><ul><li>Don&rsquo;t explicitly down-level upon join anymore</li><li>Become aware of new pool join rule</li><li>Update Rolling Pool Upgrade</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=distributed-database>Distributed database</h1><p>All hosts in a pool use the shared database by sending queries to
the pool master. This creates</p><ul><li>a performance bottleneck as the pool size increases</li><li>a reliability problem when the master fails.</li></ul><p>The reliability problem can be ameliorated by running with HA enabled,
but this is not always possible.</p><p>Both problems can be addressed by observing that the database objects
correspond to distinct physical objects where eventual consistency is
perfectly ok. For example if host &lsquo;A&rsquo; is running a VM and changes the
VM&rsquo;s name, it doesn&rsquo;t matter if it takes a while before the change shows
up on host &lsquo;B&rsquo;. If host &lsquo;B&rsquo; changes its network configuration then it
doesn&rsquo;t matter how long it takes host &lsquo;A&rsquo; to notice. We would still like
the metadata to be replicated to cope with failure, but we can allow
changes to be committed locally and synchronised later.</p><p>Note the one exception to this pattern: the current SM plugins use database
fields to implement locks. This should be shifted to a special-purpose
lock acquire/release API.</p><h2 id=using-git-via-irmin>Using git via Irmin</h2><p>A git repository is a database of key=value pairs with branching history.
If we placed our host and VM metadata in git then we could <code>commit</code>
changes and <code>pull</code> and <code>push</code> them between replicas. The
<a href=https://github.com/mirage/irmin rel=external target=_blank>Irmin</a> library provides an easy programming
interface on top of git which we could link with the Xapi database layer.</p><h2 id=proposed-new-architecture>Proposed new architecture</h2><p><img alt="Pools of one" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/distributed-database/architecture.png style=height:auto;width:auto></p><p>The diagram above shows two hosts: one a master and the other a regular host.
The XenAPI client has sent a request to the wrong host; normally this would
result in a <code>HOST_IS_SLAVE</code> error being sent to the client. In the new
world, the host is able to process the request, only contacting the master
if it is necessary to acquire a lock. Starting a VM would require a lock; but
rebooting or migrating an existing VM would not. Assuming the lock can
be acquired, then the operation is executed locally with all state updates
being made to a git topic branch.</p><p><img alt="Topic branches" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/distributed-database/topic.png style=height:auto;width:auto></p><p>Roughly we would have 1 topic branch per
pending XenAPI Task. Once the Task completes successfully, the topic branch
(containing the new VM state) is merged back into master.
Separately each
host will pull and push updates between each other for replication.</p><p>We would avoid merge conflicts by construction; either</p><ul><li>a host&rsquo;s configuration will always be &ldquo;owned&rdquo; by the host and it will be
an error for anyone else to merge updates to it</li><li>the master&rsquo;s locking will guarantee that a VM is running on at most one
host at a time. It will be an error for anyone else to merge updates to it.</li></ul><h2 id=what-we-gain>What we gain</h2><p>We will gain the following</p><ul><li>the master will only be a bottleneck when the number of VM locks gets
really large;</li><li>you will be able to connect XenCenter to hosts without a master and manage
them. Today such hosts are unmanageable.</li><li>the database will have a history and you&rsquo;ll be able to &ldquo;go back in time&rdquo;
either for debugging or to recover from mistakes</li><li>bugs caused by concurrent threads (in separate Tasks) confusing each other
will be vanquished. A typical failure mode is: one active thread destroys
an object; a passive thread sees the object and then tries to read it
and gets a database failure instead. Since every thread is operating a
separate Task they will all have their own branch and will be isolated from
each other.</li></ul><h2 id=what-we-lose>What we lose</h2><p>We will lose the following</p><ul><li>the ability to use the Xapi database as a &ldquo;lock&rdquo;</li><li>coherence between hosts: there will be no guarantee that an effect seen
by host &lsquo;A&rsquo; will be seen immediately by host &lsquo;B&rsquo;. In particular this means
that clients should send all their commands and <code>event.from</code> calls to
the same host (although any host will do)</li></ul><h2 id=stuff-we-need-to-build>Stuff we need to build</h2><ul><li><p>A <code>pull</code>/<code>push</code> replicator: this would have to monitor the list
of hosts in the pool and distribute updates to them in some vaguely
efficient manner. Ideally we would avoid hassling the pool master and
use some more efficient topology: perhaps a tree?</p></li><li><p>A <code>git diff</code> to XenAPI event converter: whenever a host <code>pull</code>s
updates from another it needs to convert the diff into a set of touched
objects for any <code>event.from</code> to read. We could send the changeset hash
as the <code>event.from</code> token.</p></li><li><p>Irmin nested views: since Tasks can be nested (and git branches can be
nested) we need to make sure that Irmin views can be nested.</p></li><li><p>We need to go through the xapi code and convert all mixtures of database
access and XenAPI updates into pure database calls. With the previous system
it was better to use a XenAPI to remote large chunks of database effects to
the master than to perform them locally. It will now be better to run them
all locally and merge them at the end. Additionally since a Task will have
a local branch, it won&rsquo;t be possible to see the state on a remote host
without triggering an early merge (which would harm efficiency)</p></li><li><p>We need to create a first-class locking API to use instead of the
<code>VDI.sm_config</code> locks.</p></li></ul><h2 id=prototype>Prototype</h2><p>A basic prototype has been created:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ opam pin xen-api-client git://github.com/djs55/xen-api-client#improvements
</span></span><span style=display:flex><span>$ opam pin add xapi-database git://github.com/djs55/xapi-database
</span></span><span style=display:flex><span>$ opam pin add xapi git://github.com/djs55/xen-api#schema-sexp</span></span></code></pre></div><p>The <code>xapi-database</code> is clone of the existing Xapi database code
configured to run as a separate process. There is
<a href=https://github.com/djs55/xapi-database/blob/master/core/db_git.ml#L55 rel=external target=_blank>code to convert from XML to git</a>
and
<a href=https://github.com/djs55/xapi-database/blob/master/core/db_git.ml#L186 rel=external target=_blank>an implementation of the Xapi remote database API</a>
which uses the following layout:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ git clone /xapi.db db
</span></span><span style=display:flex><span>Cloning into <span style=color:#e6db74>&#39;db&#39;</span>...
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ cd db; ls
</span></span><span style=display:flex><span>xapi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi
</span></span><span style=display:flex><span>console   host_metrics  PCI          pool     SR      user  VM
</span></span><span style=display:flex><span>host      network       PIF          session  tables  VBD   VM_metrics
</span></span><span style=display:flex><span>host_cpu  PBD           PIF_metrics  SM       task    VDI
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi/pool
</span></span><span style=display:flex><span>OpaqueRef:39adc911-0c32-9e13-91a8-43a25939110b
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi/pool/OpaqueRef<span style=color:#ae81ff>\:</span>39adc911-0c32-9e13-91a8-43a25939110b/
</span></span><span style=display:flex><span>crash_dump_SR                 __mtime           suspend_image_SR
</span></span><span style=display:flex><span>__ctime                       name_description  uuid
</span></span><span style=display:flex><span>default_SR                    name_label        vswitch_controller
</span></span><span style=display:flex><span>ha_allow_overcommit           other_config      wlb_enabled
</span></span><span style=display:flex><span>ha_enabled                    redo_log_enabled  wlb_password
</span></span><span style=display:flex><span>ha_host_failures_to_tolerate  redo_log_vdi      wlb_url
</span></span><span style=display:flex><span>ha_overcommitted              ref               wlb_username
</span></span><span style=display:flex><span>ha_plan_exists_for            _ref              wlb_verify_cert
</span></span><span style=display:flex><span>master                        restrictions
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ ls xapi/pool/OpaqueRef<span style=color:#ae81ff>\:</span>39adc911-0c32-9e13-91a8-43a25939110b/other_config/
</span></span><span style=display:flex><span>cpuid_feature_mask  memory-ratio-hvm  memory-ratio-pv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ cat xapi/pool/OpaqueRef<span style=color:#ae81ff>\:</span>39adc911-0c32-9e13-91a8-43a25939110b/other_config/cpuid_feature_mask
</span></span><span style=display:flex><span>ffffff7f-ffffffff-ffffffff-ffffffff</span></span></code></pre></div><p>Notice how:</p><ul><li>every object is a directory</li><li>every key/value pair is represented as a file</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.0.2)</span></td></tr></table></header><h1 id=emergency-network-reset-design>Emergency Network Reset Design</h1><p>This document describes design details for the PR-1032 requirements.</p><p>The design consists of four parts:</p><ol><li>A new XenAPI call <code>Host.reset_networking</code>, which removes all the
PIFs, Bonds, VLANs and tunnels associated with the given host, and a
call <code>PIF.scan_bios</code> to bring back the PIFs with device names as
defined in the BIOS.</li><li>A <code>xe-reset-networking</code> script that can be executed on a XenServer
host, which prepares the reset and causes the host to reboot.</li><li>An xsconsole page that essentially does the same as
<code>xe-reset-networking</code>.</li><li>A new item in the XAPI start-up sequence, which when triggered by
<code>xe-reset-networking</code>, calls <code>Host.reset_networking</code> and re-creates
the PIFs.</li></ol><h2 id=command-line-utility>Command-Line Utility</h2><p>The <code>xe-reset-networking</code> script takes the following parameters:</p><table><tr><th>Parameter</th><th>Description</th></tr><tr><td><code>-m</code>, <code>--master</code></td><td>The IP address of the master. Optional if the host is pool slave, ignored otherwise.</td></tr><tr><td><code>--device</code></td><td>Device name of management interface. Optional. If not specified, it is taken from the firstboot data.</td></tr><tr><td><code>--mode</code></td><td>IP configuration mode for management interface. Optional. Either <code>dhcp</code> or <code>static</code> (default is <code>dhcp</code>).</td></tr><tr><td><code>--ip</code></td><td>IP address for management interface. Required if <code>--mode=static</code>, ignored otherwise.</td></tr><tr><td><code>--netmask</code></td><td>Netmask for management interface. Required if <code>--mode=static</code>, ignored otherwise.</td></tr><tr><td><code>--gateway</code></td><td>Gateway for management interface. Optional; ignored if <code>--mode=dhcp</code>.</td></tr><tr><td><code>--dns</code></td><td>DNS server for management interface. Optional; ignored if <code>--mode=dhcp</code>.</td></tr></table><p>DNS server for management interface. Optional; ignored if <code>--mode=dhcp</code>.</p><p>The script takes the following steps after processing the given
parameters:</p><ol><li>Inform the user that the host will be restarted, and that any
running VMs should be shut down. Make the user confirm that they
really want to reset the networking by typing &lsquo;yes&rsquo;.</li><li>Read <code>/etc/xensource/pool.conf</code> to determine whether the host is a
pool master or pool slave.</li><li>If a pool slave, update the IP address in the <code>pool.conf</code> file to
the one given in the <code>-m</code> parameter, if present.</li><li>Shut down networking subsystem (<code>service network stop</code>).</li><li>If no management device is specified, take it from
/etc/firstboot.d/data/management.conf.</li><li>If XAPI is running, stop it.</li><li>Reconfigure the management interface and associated bridge by
<code>interface-reconfigure --force</code>.</li><li>Update <code>MANAGEMENT_INTERFACE</code> and clear <code>CURRENT_INTERFACES</code> in
<code>/etc/xensource-inventory</code>.</li><li>Create the file <code>/tmp/network-reset</code> to trigger XAPI to complete the
network reset after the reboot. This file should contain the full
configuration details of the management interface as key/value pairs
(format: <code>&lt;key>=&lt;value>\n</code>), and looks similar to the firstboot data
files. The file contains at least the keys <code>DEVICE</code> and <code>MODE</code>, and
<code>IP</code>, <code>NETMASK</code>, <code>GATEWAY</code>, or <code>DNS</code> when appropriate.</li><li>Reboot</li></ol><h2 id=xapi>XAPI</h2><h3 id=xenapi>XenAPI</h3><p>A new <em>hidden</em> API call:</p><ul><li><code>Host.reset_networking</code><ul><li>Parameter: host reference <code>host</code></li><li>Calling this function removes all the PIF, Bond, VLAN and tunnel
objects associated with the given host from the master database.
All Network and VIF objects are maintained, as these do not
necessarily belong to a single host.</li></ul></li></ul><h3 id=start-up-sequence>Start-up Sequence</h3><p>After reboot, in the XAPI start-up sequence trigged by the presence of
<code>/tmp/network-reset</code>:</p><ol><li>Read the desired management configuration from <code>/tmp/network-reset</code>.</li><li>Call <code>Host.reset_networking</code> with a ref to the localhost.</li><li>Call <code>PIF.scan</code> with a ref to the localhost to recreate the
(physical) PIFs.</li><li>Call <code>PIF.reconfigure_ip</code> to configure the management interface.</li><li>Call <code>Host.management_reconfigure</code>.</li><li>Delete <code>/tmp/network-reset</code>.</li></ol><h2 id=xsconsole>xsconsole</h2><p>Add an &ldquo;Emergency Network Reset&rdquo; option under the &ldquo;Network and
Management Interface&rdquo; menu. Selecting this option will show some
explanation in the pane on the right-hand side. Pressing &lt;Enter> will
bring up a dialogue to select the interfaces to use as management
interface after the reset. After choosing a device, the dialogue
continues with configuration options like in the &ldquo;Configure Management
Interface&rdquo; dialogue. After completing the dialogue, the same steps as
listed for <code>xe-reset-networking</code> are executed.</p><h2 id=notes>Notes</h2><ul><li>On a pool slave, the management interface should be the same as on
the master (the same device name, e.g. eth0).</li><li>Resetting the networking configuration on the master should be
ideally be followed by resets of the pool slaves as well, in order
to synchronise their configuration (especially bonds/VLANs/tunnels).
Furthermore, in case the IP address of the master has changed, as a
result of a network reset or <code>Host.management_reconfigure</code>, pool
slaves may also use the network reset functionality to reconnect to
the master on its new IP.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/120>#120</a></td></tr></table></header><h1 id=fcoe-capable-nics>FCoE capable NICs</h1><p>It has been possible to identify the NICs of a Host which can support FCoE.
This property can be listed in PIF object under capabilities field.</p><h2 id=introduction>Introduction</h2><ul><li>FCoE supported on a NIC is a hardware property. With the help of dcbtool, we can identify which NIC support FCoE.</li><li>The new field capabilities will be <code>Set(String)</code> in PIF object. For FCoE capable NIC will have string &ldquo;fcoe&rdquo; in PIF capabilities field.</li><li><code>capabilities</code> field will be ReadOnly, This field cannot be modified by user.</li></ul><h2 id=pif-object>PIF Object</h2><p>New field:</p><ul><li>Field <code>PIF.capabilities</code> will be type <code>Set(string)</code>.</li><li>Default value in PIF capabilities will have an empty set.</li></ul><h2 id=xapi-changes>Xapi Changes</h2><ul><li>Set the field capabilities &ldquo;fcoe&rdquo; depending on output of xcp-networkd call <code>get_capabilities</code>.</li><li>Field capabilities &ldquo;fcoe&rdquo; can be set during <code>introduce_internal</code> on when creating a PIF.</li><li>Field capabilities &ldquo;fcoe&rdquo; can be updated during <code>refresh_all</code> on xapi startup.</li><li>The above field will be set everytime when xapi-restart.</li></ul><h2 id=xcp-networkd-changes>XCP-Networkd Changes</h2><p>New function:</p><ul><li>String list <code>string list get_capabilties (string)</code></li><li>Argument: device_name for the PIF.</li><li>This function calls method <code>capable</code> exposed by <code>fcoe_driver.py</code> as part of dom0.</li><li>It returns string list [&ldquo;fcoe&rdquo;] or [] depending on <code>capable</code> method output.</li></ul><h2 id=defaults-installation-and-upgrade>Defaults, Installation and Upgrade</h2><ul><li>Any newly introduced PIF will have its capabilities field as empty set until <code>fcoe_driver</code> method <code>capable</code> states FCoE is supported on the NIC.</li><li>It includes PIFs obtained after a fresh install of Xenserver, as well as PIFs created using <code>PIF.introduce</code> then <code>PIF.scan</code>.</li><li>During an upgrade Xapi Restart will call <code>refresh_all</code> which then populate the capabilities field as empty set.</li></ul><h2 id=command-line-interface>Command Line Interface</h2><ul><li>The <code>PIF.capabilities</code> field is exposed through <code>xe pif-list</code> and <code>xe pif-param-list</code> as usual.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.0)</span></td></tr></table></header><h1 id=gpu-pass-through-support>GPU pass-through support</h1><p>This document contains the software design for GPU pass-through. This
code was originally included in the version of Xapi used in XenServer 6.0.</p><h2 id=overview>Overview</h2><p>Rather than modelling GPU pass-through from a PCI perspective, and
having the user manipulate PCI devices directly, we are taking a
higher-level view by introducing a dedicated graphics model. The
graphics model is similar to the networking and storage model, in which
virtual and physical devices are linked through an intermediate
abstraction layer (e.g. the &ldquo;Network&rdquo; class in the networking model).</p><p>The basic graphics model is as follows:</p><ul><li>A host owns a number of physical GPU devices (<em>pGPUs</em>), each of
which is available for passing through to a VM.</li><li>A VM may have a virtual GPU device (<em>vGPU</em>), which means it expects
to have access to a GPU when it is running.</li><li>Identical pGPUs are grouped across a resource pool in <em>GPU groups</em>.
GPU groups are automatically created and maintained by XS.</li><li>A GPU group connects vGPUs to pGPUs in the same way as VIFs are
connected to PIFs by Network objects: for a VM <em>v</em> having a vGPU on
GPU group <em>p</em> to run on host <em>h</em>, host <em>h</em> must have a pGPU in GPU
group <em>p</em> and pass it through to VM <em>v</em>.</li><li>VM start and non-live migration rules are analogous to the network
API and follow the above rules.</li><li>In case a VM that has a vGPU is started, while no pGPU available, an
exception will occur and the VM won&rsquo;t start. As a result, in order
to guarantee that a VM always has access to a pGPU, the number of
vGPUs should not exceed the number of pGPUs in a GPU group.</li></ul><p>Currently, the following restrictions apply:</p><ul><li>Hotplug is not supported.</li><li>Suspend/resume and checkpointing (memory snapshots) are not
supported.</li><li>Live migration (XenMotion) is not supported.</li><li>No more than one GPU per VM will be supported.</li><li>Only Windows guests will be supported.</li></ul><h2 id=xenapi-changes>XenAPI Changes</h2><p>The design introduces a new generic class called <em>PCI</em> to capture state
and information about relevant PCI devices in a host. By default, xapi
would not create PCI objects for all PCI devices, but only for the ones
that are managed and configured by xapi; currently only GPU devices.</p><p>The PCI class has no fields specific to the type of the PCI device (e.g.
a graphics card or NIC). Instead, device specific objects will contain a
link to their underlying PCI device&rsquo;s object.</p><p>The new XenAPI classes and changes to existing classes are detailed
below.</p><h3 id=pci-class>PCI class</h3><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>class_id</td><td>string</td><td>PCI class ID (hidden field)</td></tr><tr><td>class_name</td><td>string</td><td>PCI class name (GPU, NIC, &mldr;)</td></tr><tr><td>vendor_id</td><td>string</td><td>Vendor ID (hidden field).</td></tr><tr><td>vendor_name</td><td>string</td><td>Vendor name.</td></tr><tr><td>device_id</td><td>string</td><td>Device ID (hidden field).</td></tr><tr><td>device_name</td><td>string</td><td>Device name.</td></tr><tr><td>host</td><td>host ref</td><td>The host that owns the PCI device.</td></tr><tr><td>pci_id</td><td>string</td><td>BDF (domain/Bus/Device/Function identifier) of the (physical) PCI function, e.g. &ldquo;0000:00:1a.1&rdquo;. The format is hhhh:hh:hh.h, where h is a hexadecimal digit.</td></tr><tr><td>functions</td><td>int</td><td>Number of (physical + virtual) functions; currently fixed at 1 (hidden field).</td></tr><tr><td>attached_VMs</td><td>VM ref set</td><td>List of VMs that have this PCI device &ldquo;currently attached&rdquo;, i.e. plugged, i.e. passed-through to (hidden field).</td></tr><tr><td>dependencies</td><td>PCI ref set</td><td>List of dependent PCI devices: all of these need to be passed-thru to the same VM (co-location).</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr></tbody></table><p><em>Hidden fields</em> are only for use by xapi internally, and not visible to
XenAPI users.</p><p>Messages: none.</p><h3 id=pgpu-class>PGPU class</h3><p>A physical GPU device (pGPU).</p><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>PCI</td><td>PCI ref</td><td>Link to the underlying PCI device.</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr><tr><td>host</td><td>host ref</td><td>The host that owns the GPU.</td></tr><tr><td>GPU_group</td><td>GPU_group ref</td><td>GPU group the pGPU is contained in. Can be Null.</td></tr></tbody></table><p>Messages: none.</p><h3 id=gpu_group-class>GPU_group class</h3><p>A group of identical GPUs across hosts. A VM that is associated with a
GPU group can use any of the GPUs in the group. A VM does not need to
install new GPU drivers if moving from one GPU to another one in the
same GPU group.</p><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>VGPUs</td><td>VGPU ref set</td><td>List of vGPUs in the group.</td></tr><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>PGPUs</td><td>PGPU ref set</td><td>List of pGPUs in the group.</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr><tr><td>name_label</td><td>string</td><td>A human-readable name.</td></tr><tr><td>name_description</td><td>string</td><td>A notes field containing human-readable description.</td></tr><tr><td>GPU_types</td><td>string set</td><td>List of GPU types (vendor+device ID) that can be in this group (hidden field).</td></tr></tbody></table><p>Messages: none.</p><h3 id=vgpu-class>VGPU class</h3><p>A virtual GPU device (vGPU).</p><p>Fields:</p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>uuid</td><td>string</td><td>Unique identifier/object reference.</td></tr><tr><td>VM</td><td>VM ref</td><td>VM that owns the vGPU.</td></tr><tr><td>GPU_group</td><td>GPU_group ref</td><td>GPU group the vGPU is contained in.</td></tr><tr><td>currently_attached</td><td>bool</td><td>Reflects whether the virtual device is currently &ldquo;connected&rdquo; to a physical device.</td></tr><tr><td>device</td><td>string</td><td>Order in which the devices are plugged into the VM. Restricted to &ldquo;0&rdquo; for now.</td></tr><tr><td>other_config</td><td>(string -> string) map</td><td>Additional optional configuration (as usual).</td></tr></tbody></table><p>Messages:</p><table><thead><tr><th>Prototype</th><th>Description</th><th></th></tr></thead><tbody><tr><td>VGPU ref create (GPU_group ref, string, VM ref)</td><td>Manually assign the vGPU device to the VM given a device number, and link it to the given GPU group.</td><td></td></tr><tr><td>void destroy (VGPU ref)</td><td>Remove the association between the GPU group and the VM.</td><td></td></tr></tbody></table><p>It is possible to assign more vGPUs to a group than number number of
pGPUs in the group. When a VM is started, a pGPU must be available; if
not, the VM will not start. Therefore, to guarantee that a VM has access
to a pGPU at any time, one must manually enforce that the number of
vGPUs in a GPU group does not exceed the number of pGPUs. XenCenter
might display a warning, or simply refuse to assign a vGPU, if this
constraint is violated. This is analogous to the handling of memory
availability in a pool: a VM may not be able to start if there is no
host having enough free memory.</p><h3 id=vm-class>VM class</h3><p>Fields:</p><ul><li>Deprecate (unused) <code>PCI_bus</code> field</li><li>Add field <code>VGPU ref set VGPUs</code>: List of vGPUs.</li><li>Add field <code>PCI ref set attached_PCIs</code>: List of PCI devices that are
&ldquo;currently attached&rdquo; (plugged, passed-through) (<em>hidden field</em>).</li></ul><h3 id=host-class>host class</h3><p>Fields:</p><ul><li>Add field <code>PCI ref set PCIs</code>: List of PCI devices.</li><li>Add field <code>PGPU ref set PGPUs</code>: List of physical GPU devices.</li><li>Add field <code>(string -> string) map chipset_info</code>, which contains at
least the key <code>iommu</code>. If <code>"true"</code>, this key indicates whether the
host has IOMMU/VT-d support build in, <strong>and</strong> this functionality is
enabled by Xen; the value will be <code>"false"</code> otherwise.</li></ul><h2 id=initialisation-and-operations>Initialisation and Operations</h2><h3 id=enabling-iommuvt-d>Enabling IOMMU/VT-d</h3><p>(This may not be needed in Xen 4.1. Confirm with Simon.)</p><p>Provide a command that does this:</p><ul><li><code>/opt/xensource/libexec/xen-cmdline --set-xen iommu=1</code></li><li>reboot</li></ul><h3 id=xapi-startup>Xapi startup</h3><p>Definitions:</p><ul><li>PCI devices are matched on the combination of their <code>pci_id</code>,
<code>vendor_id</code>, and <code>device_id</code>.</li></ul><p>First boot and any subsequent xapi start:</p><ol><li><p>Find out from dmesg whether IOMMU support is present and enabled in
Xen, and set <code>host.chipset_info:iommu</code> accordingly.</p></li><li><p>Detect GPU devices currently present in the host. For each:</p><ol><li>If there is no matching PGPU object yet, create a PGPU object,
and add it to a GPU group containing identical PGPUs, or a new
group.</li><li>If there is no matching PCI object yet, create one, and also
create or update the PCI objects for dependent devices.</li></ol></li><li><p>Destroy all existing PCI objects of devices that are not currently
present in the host (i.e. objects for devices that have been
replaced or removed).</p></li><li><p>Destroy all existing PGPU objects of GPUs that are not currently
present in the host. Send a XenAPI alert to notify the user of this
fact.</p></li><li><p>Update the list of <code>dependencies</code> on all PCI objects.</p></li><li><p>Sync <code>VGPU.currently_attached</code> on all <code>VGPU</code> objects.</p></li></ol><h3 id=upgrade>Upgrade</h3><p>For any VMs that have <code>VM.other_config:pci</code> set to use a GPU, create an
appropriate vGPU, and remove the <code>other_config</code> option.</p><h3 id=generic-pci-interface>Generic PCI Interface</h3><p>A generic PCI interface exposed to higher-level code, such as the
networking and GPU management modules within Xapi. This functionality
relies on Xenops.</p><p>The PCI module exposes the following functions:</p><ul><li>Check whether a PCI device has free (unassigned) functions. This is
the case if the number of assignments in <code>PCI.attached_VMs</code> is
smaller than <code>PCI.functions</code>.</li><li>Plug a PCI function into a running VM.<ol><li>Raise exception if there are no free functions.</li><li>Plug PCI device, as well as dependent PCI devices. The PCI
module must also tell device-specific modules to update the
<code>currently_attached</code> field on dependent <code>VGPU</code> objects etc.</li><li>Update <code>PCI.attached_VMs</code>.</li></ol></li><li>Unplug a PCI function from a running VM.<ol><li>Raise exception if the PCI function is not owned by (passed
through to) the VM.</li><li>Unplug PCI device, as well as dependent PCI devices. The PCI
module must also tell device-specific modules to update the
<code>currently_attached</code> field on dependent <code>VGPU</code> objects etc.</li><li>Update <code>PCI.attached_VMs</code>.</li></ol></li></ul><h3 id=construction-and-destruction>Construction and Destruction</h3><p>VGPU.create:</p><ol><li>Check license. Raise FEATURE_RESTRICTED if the GPU feature has not
been enabled.</li><li>Raise INVALID_DEVICE if the given device number is not &ldquo;0&rdquo;, or
DEVICE_ALREADY_EXISTS if (indeed) the device already exists. This
is a convenient way of enforcing that only one vGPU per VM is
supported, for now.</li><li>Create <code>VGPU</code> object in the DB.</li><li>Initialise <code>VGPU.currently_attached = false</code>.</li><li>Return a ref to the new object.</li></ol><p>VGPU.destroy:</p><ol><li>Raise OPERATION_NOT_ALLOWED if <code>VGPU.currently_attached = true</code>
and the VM is running.</li><li>Destroy <code>VGPU</code> object.</li></ol><h3 id=vm-operations>VM Operations</h3><p>VM.start(_on):</p><ol><li>If <code>host.chipset_info:iommu = "false"</code>, raise VM_REQUIRES_IOMMU.</li><li>Raise FEATURE_REQUIRES_HVM (carrying the string &ldquo;GPU passthrough
needs HVM&rdquo;) if the VM is PV rather than HVM.</li><li>For each of the VM&rsquo;s vGPUs:<ol><li>Confirm that the given host has a pGPU in its associated GPU
group. If not, raise VM_REQUIRES_GPU.</li><li>Consult the generic PCI module for all pGPUs in the group to
find out whether a suitable PCI function is available. If a
physical device is not available, raise VM_REQUIRES_GPU.</li><li>Ask PCI module to plug an available pGPU into the VM&rsquo;s domain
and set <code>VGPU.currently_attached</code> to <code>true</code>. As a side-effect,
any dependent PCI devices would be plugged.</li></ol></li></ol><p>VM.shutdown:</p><ol><li>Ask PCI module to unplug all GPU devices.</li><li>Set <code>VGPU.currently_attached</code> to <code>false</code> for all the VM&rsquo;s VGPUs.</li></ol><p>VM.suspend, VM.resume(_on):</p><ul><li>Raise VM_HAS_PCI_ATTACHED if the VM has any plugged <code>VGPU</code>
objects, as suspend/resume for VMs with GPUs is currently not
supported.</li></ul><p>VM.pool_migrate:</p><ul><li>Raise VM_HAS_PCI_ATTACHED if the VM has any plugged <code>VGPU</code>
objects, as live migration for VMs with GPUs is currently not
supported.</li></ul><p>VM.clone, VM.copy, VM.snapshot:</p><ul><li>Copy <code>VGPU</code> objects along with the VM.</li></ul><p>VM.import, VM.export:</p><ul><li>Include <code>VGPU</code> and <code>GPU_group</code> objects in the VM export format.</li></ul><p>VM.checkpoint</p><ul><li>Raise VM_HAS_PCI_ATTACHED if the VM has any plugged <code>VGPU</code>
objects, as checkpointing for VMs with GPUs is currently not
supported.</li></ul><h3 id=pool-join-and-eject>Pool Join and Eject</h3><p>Pool join:</p><ol><li><p>For each <code>PGPU</code>:</p><ol><li>Copy it to the pool.</li><li>Add it to a <code>GPU_group</code> of identical PGPUs, or a new one.</li></ol></li><li><p>Copy each <code>VGPU</code> to the pool together with the VM that owns it, and
add it to the GPU group containing the same <code>PGPU</code> as before the
join.</p></li></ol><p>Step 1 is done automatically by the xapi startup code, and step 2 is
handled by the VM export/import code. Hence, no work needed.</p><p>Pool eject:</p><ol><li><code>VGPU</code> objects will be automatically GC&rsquo;ed when the VMs are removed.</li><li>Xapi&rsquo;s startup code recreates the <code>PGPU</code> and <code>GPU_group</code> objects.</li></ol><p>Hence, no work needed.</p><h2 id=required-low-level-interface>Required Low-level Interface</h2><p>Xapi needs a way to obtain a list of all PCI devices present on a host.
For each device, xapi needs to know:</p><ul><li>The PCI ID (BDF).</li><li>The type of device (NIC, GPU, &mldr;) according to a well-defined and
stable list of device types (as in <code>/usr/share/hwdata/pci.ids</code>).</li><li>The device and vendor ID+name (currently, for PIFs, xapi looks up
the name in <code>/usr/share/hwdata/pci.ids</code>).</li><li>Which other devices/functions are required to be passed through to
the same VM (co-located), e.g. other functions of a compound PCI
device.</li></ul><h2 id=command-line-interface-xe>Command-Line Interface (xe)</h2><ul><li>xe pgpu-list</li><li>xe pgpu-param-list/get/set/add/remove/clear</li><li>xe gpu-group-list</li><li>xe gpu-group-param-list/get/set/add/remove/clear</li><li>xe vgpu-list</li><li>xe vgpu-create</li><li>xe vgpu-destroy</li><li>xe vgpu-param-list/get/set/add/remove/clear</li><li>xe host-param-get param-name=chipset-info param-key=iommu</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Documented interface changes between xapi and xenopsd for vGPU</td></tr><tr><td><span class="label label-default">v2</span></td><td>Added design for storing vGPU-to-pGPU allocation in xapi database</td></tr><tr><td><span class="label label-default">v3</span></td><td>Marked new xapi DB fields as internal-only</td></tr></table></header><h1 id=gpu-support-evolution>GPU support evolution</h1><h2 id=introduction>Introduction</h2><p>As of XenServer 6.5, VMs can be provisioned with access to graphics processors
(either emulated or passed through) in four different ways. Virtualisation of
Intel graphics processors will exist as a fifth kind of graphics processing
available to VMs. These five situations all require the VM&rsquo;s device model to be
created in subtly different ways:</p><p><strong>Pure software emulation</strong></p><ul><li>qemu is launched either with no special parameter, if the basic Cirrus
graphics processor is required, otherwise qemu is launched with the
<code>-std-vga</code> flag.</li></ul><p><strong>Generic GPU passthrough</strong></p><ul><li>qemu is launched with the <code>-priv</code> flag to turn on privilege separation</li><li>qemu can additionally be passed the <code>-std-vga</code> flag to choose the
corresponding emulated graphics card.</li></ul><p><strong>Intel integrated GPU passthrough (GVT-d)</strong></p><ul><li>As well as the <code>-priv</code> flag, qemu must be launched with the <code>-std-vga</code> and
<code>-gfx_passthru</code> flags. The actual PCI passthrough is handled separately
via xen.</li></ul><p><strong>NVIDIA vGPU</strong></p><ul><li>qemu is launched with the <code>-vgpu</code> flag</li><li>a secondary display emulator, demu, is launched with the following parameters:<ul><li><code>--domain</code> - the VM&rsquo;s domain ID</li><li><code>--vcpus</code> - the number of vcpus available to the VM</li><li><code>--gpu</code> - the PCI address of the physical GPU on which the emulated GPU will
run</li><li><code>--config</code> - the path to the config file which contains detail of the GPU to
emulate</li></ul></li></ul><p><strong>Intel vGPU (GVT-g)</strong></p><ul><li>here demu is not used, but instead qemu is launched with five parameters:<ul><li><code>-xengt</code></li><li><code>-vgt_low_gm_sz</code> - the low GM size in MiB</li><li><code>-vgt_high_gm_sz</code> - the high GM size in MiB</li><li><code>-vgt_fence_sz</code> - the number of fence registers</li><li><code>-priv</code></li></ul></li></ul><h2 id=xenopsd>xenopsd</h2><p>To handle all these possibilities, we will add some new types to xenopsd&rsquo;s
interface:</p><div class="highlight wrap-code"><pre tabindex=0><code>module Pci = struct
  type address = {
    domain: int;
    bus: int;
    device: int;
    fn: int;
  }

  ...
end

module Vgpu = struct
  type gvt_g = {
    physical_pci_address: Pci.address;
    low_gm_sz: int64;
    high_gm_sz: int64;
    fence_sz: int;
  }

  type nvidia = {
    physical_pci_address: Pci.address;
    config_file: string
  }

  type implementation =
    | GVT_g of gvt_g
    | Nvidia of nvidia

  type id = string * string

  type t = {
    id: id;
    position: int;
    implementation: implementation;
  }

  type state = {
    plugged: bool;
    emulator_pid: int option;
  }
end

module Vm = struct
  type igd_passthrough of
    | GVT_d

  type video_card =
    | Cirrus
    | Standard_VGA
    | Vgpu
    | Igd_passthrough of igd_passthrough

  ...
end

module Metadata = struct
  type t = {
    vm: Vm.t;
    vbds: Vbd.t list;
    vifs: Vif.t list;
    pcis: Pci.t list;
    vgpus: Vgpu.t list;
    domains: string option;
  }
end</code></pre></div><p>The <code>video_card</code> type is used to indicate to the function
<code>Xenops_server_xen.VM.create_device_model_config</code> how the VM&rsquo;s emulated graphics
card will be implemented. A value of <code>Vgpu</code> indicates that the VM needs to be
started with one or more virtualised GPUs - the function will need to look at
the list of GPUs associated with the VM to work out exactly what parameters to
send to qemu.</p><p>If <code>Vgpu.state.emulator_pid</code> of a plugged vGPU is <code>None</code>, this indicates that
the emulation of the vGPU is being done by qemu rather than by a separate
emulator.</p><p>n.b. adding the <code>vgpus</code> field to <code>Metadata.t</code> will break backwards compatibility
with old versions of xenopsd, so some upgrade logic will be required.</p><p>This interface will allow us to support multiple vGPUs per VM in future if
necessary, although this may also require reworking the interface between
xenopsd, qemu and demu. For now, xenopsd will throw an exception if it is asked
to start a VM with more than one vGPU.</p><h2 id=xapi>xapi</h2><p>To support the above interface, xapi will convert all of a VM&rsquo;s non-passthrough
GPUs into <code>Vgpu.t</code> objects when sending VM metadata to xenopsd.</p><p>In contrast to GVT-d, which can only be run on an Intel GPU which has been
has been hidden from dom0, GVT-g will only be allowed to run on a GPU which has
<em>not</em> been hidden from dom0.</p><p>If a GVT-g-capable GPU is detected, and it is not hidden from dom0, xapi will
create a set of VGPU_type objects to represent the vGPU presets which can run on
the physical GPU. Exactly how these presets are defined is TBD, but a likely
solution is via a set of config files as with NVIDIA vGPU.</p><p><strong>Allocation of vGPUs to physical GPUs</strong></p><p>For NVIDIA vGPU, when starting a VM, each vGPU attached to the VM is assigned
to a physical GPU as a result of capacity planning at the pool level. The
resulting configuration is stored in the VM.platform dictionary, under
specific keys:</p><ul><li><code>vgpu_pci_id</code> - the address of the physical GPU on which the vGPU will run</li><li><code>vgpu_config</code> - the path to the vGPU config file which the emulator will use</li></ul><p>Instead of storing the assignment in these fields, we will add a new
internal-only database field:</p><ul><li><code>VGPU.scheduled_to_be_resident_on (API.ref_PGPU)</code></li></ul><p>This will be set to the ref of the physical GPU on which the vGPU will run. From
here, xapi can easily obtain the GPU&rsquo;s PCI address. Capacity planning will also
take into account which vGPUs are scheduled to be resident on a physical GPU,
which will avoid races resulting from many vGPU-enabled VMs being started at
once.</p><p>The path to the config file is already stored in the <code>VGPU_type.internal_config</code>
dictionary, under the key <code>vgpu_config</code>. xapi will use this value directly
rather than copying it to VM.platform.</p><p>To support other vGPU implementations, we will add another internal-only
database field:</p><ul><li><code>VGPU_type.implementation enum(Passthrough|Nvidia|GVT_g)</code></li></ul><p>For the <code>GVT_g</code> implementation, no config file is needed. Instead,
<code>VGPU_type.internal_config</code> will contain three key-value pairs, with the keys</p><ul><li><code>vgt_low_gm_sz</code></li><li><code>vgt_high_gm_sz</code></li><li><code>vgt_fence_sz</code></li></ul><p>The values of these pairs will be used to construct a value of type
<code>Xenops_interface.Vgpu.gvt_g</code>, which will be passed down to xenopsd.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.5)</span></td></tr></table></header><h1 id=gro-and-other-properties-of-pifs>GRO and other properties of PIFs</h1><p>It has been possible to enable and disable GRO and other &ldquo;ethtool&rdquo; features on
PIFs for a long time, but there was never an official API for it. Now there is.</p><h2 id=introduction>Introduction</h2><p>The former way to enable GRO via the CLI is as follows:</p><pre><code>xe pif-param-set uuid=&lt;pif-uuid&gt; other-config:ethtool-gro=on
xe pif-plug uuid=&lt;pif-uuid&gt;
</code></pre><p>The <code>other-config</code> field is a grab-bag of options that are not clearly defined.
The options exposed through <code>other-config</code> are mostly experimental features, and
the interface is not considered stable. Furthermore, the field is read/write
and does not have any input validation, and cannot not trigger any actions
immediately. The latter is why it is needed to call <code>pif-plug</code> after setting
the <code>ethtool-gro</code> key, in order to actually make things happen.</p><h2 id=new-api>New API</h2><p>New field:</p><ul><li>Field <code>PIF.properties</code> of type <code>(string -> string) map</code>.</li><li>Physical and bond PIFs have a <code>gro</code> key in their <code>properties</code>, with possible values <code>on</code> and <code>off</code>. There are currently no other properties defined.</li><li>VLAN and Tunnel PIFs do not have any properties. They implicitly inherit the properties from the PIF they are based upon (either a physical PIF or a bond).</li><li>For backwards compatibility, if there is a <code>other-config:ethtool-gro</code> key present on the PIF, it will be treated as an override of the <code>gro</code> key in <code>PIF.properties</code>.</li></ul><p>New function:</p><ul><li>Message <code>void PIF.set_property (PIF ref, string, string)</code>.</li><li>First argument: the reference of the PIF to act on.</li><li>Second argument: the key to change in the <code>properties</code> field.</li><li>Third argument: the value to write.</li><li>The function can only be used on physical PIFs that are not bonded, and on bond PIFs. Attempts to call the function on bond slaves, VLAN PIFs, or Tunnel PIFs, fail with <code>CANNOT_CHANGE_PIF_PROPERTIES</code>.</li><li>Calls with invalid keys or values fail with <code>INVALID_VALUE</code>.</li><li>When called on a bond PIF, the key in the <code>properties</code> of the associated bond slaves will also be set to same value.</li><li>The function automatically causes the settings to be applied to the network devices (no additional <code>plug</code> is needed). This includes any VLANs that are on top of the PIF to-be-changed, as well as any bond slaves.</li></ul><h2 id=defaults-installation-and-upgrade>Defaults, Installation and Upgrade</h2><ul><li>Any newly introduced PIF will have its <code>properties</code> field set to <code>"gro" -> "on"</code>. This includes PIFs obtained after a fresh installation of XenServer, as well as PIFs created using <code>PIF.introduce</code> or <code>PIF.scan</code>. In other words, GRO will be &ldquo;on&rdquo; by default.</li><li>An upgrade from a version of XenServer that does not have the <code>PIF.properties</code> field, will give every physical and bond PIF a <code>properties</code> field set to <code>"gro" -> "on"</code>. In other words, GRO will be &ldquo;on&rdquo; by default after an upgrade.</li></ul><h2 id=bonding>Bonding</h2><ul><li>When creating a bond, the bond-slaves-to-be must all have equal <code>PIF.properties</code>. If not, the <code>bond.create</code> call will fail with <code>INCOMPATIBLE_BOND_PROPERTIES</code>.</li><li>When a bond is created successfully, the <code>properties</code> of the bond PIF will be equal to the properties of the bond slaves.</li></ul><h2 id=command-line-interface>Command Line Interface</h2><ul><li>The <code>PIF.properties</code> field is exposed through <code>xe pif-list</code> and <code>xe pif-param-list</code> as usual.</li><li>The <code>PIF.set_property</code> call is exposed through <code>xe pif-param-set</code>. For example: <code>xe pif-param-set uuid=&lt;pif-uuid> properties:gro=off</code>.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (5.6)</span></td></tr></table></header><h1 id=heterogeneous-pools>Heterogeneous pools</h1><h1 id=notes>Notes</h1><ul><li>The <code>cpuid</code> instruction is used to obtain a CPU&rsquo;s manufacturer,
family, model, stepping and features information.</li><li>The feature bitvector is 128 bits wide: 2 times 32 bits of base
features plus 2 times 32 bits of extended features, which are
referred to as <code>base_ecx</code>, <code>base_edx</code>, <code>ext_ecx</code> and <code>ext_edx</code>
(after the registers used by <code>cpuid</code> to store the results).</li><li>The feature bits can be masked by Intel FlexMigration and AMD
Extended Migration. This means that features can be made to appear
as absent. Hence, a CPU can appear as a less-capable CPU.<ul><li>AMD Extended Migration is able to mask both base and extended
features.</li><li>Intel FlexMigration on Core 2 CPUs (Penryn) is able to mask
<strong>only the base features</strong> (<code>base_ecx</code> and <code>base_edx</code>). The
newer Nehalem and Westmere CPUs support extended-feature masking
as well.</li></ul></li><li>A process in dom0 (e.g. xapi) is able to call <code>cpuid</code> to obtain the
(possibly modified) CPU info, or can obtain this information from
Xen. Masking is done only by Xen at boot time, before any domains
are loaded.</li><li>To apply a feature mask, a dom0 process may specify the mask in the
Xen command line in the file <code>/boot/extlinux.conf</code>. After a reboot,
the mask will be enforced.</li><li>It is not possible to obtain the original features from a dom0
process, if the features have been masked. Before applying the first
mask, the process could remember/store the original feature vector,
or obtain the information from Xen.</li><li>All CPU cores on a host can be assumed to be identical. Masking will
be done simultaneously on all cores in a host.</li><li>Whether a CPU supports FlexMigration/Extended Migration can (only)
be derived from the family/model/stepping information.</li><li>XS5.5 has an exception for the EST feature in base_ecx. This flag
is ignored on pool join.</li></ul><h1 id=overview-of-xenapi-changes>Overview of XenAPI Changes</h1><h2 id=fields>Fields</h2><p>Currently, the datamodel has <code>Host_cpu</code> objects for each CPU core in a
host. As they are all identical, we are considering keeping just one CPU
record in the <code>Host</code> object itself, and deprecating the <code>Host_cpu</code>
class. For backwards compatibility, the <code>Host_cpu</code> objects will remain
as they are in MNR, but may be removed in subsequent releases.</p><p>Hence, there will be a new field called <code>Host.cpu_info</code>, a read-only
string-string map, containing the following fixed set of keys:</p><table><thead><tr class=header><th align=left>Key name</th><th align=left>Description</th></tr></thead><tbody><tr class=odd><td align=left><code>cpu_count</code></td><td align=left>The number of CPU cores in the host.</td></tr><tr class=even><td align=left><code>family</code></td><td align=left>The family (number) of the CPU.</td></tr><tr class=odd><td align=left><code>features</code></td><td align=left>The <em>current</em> (possibly masked) feature vector, as given by <code>cpuid</code>. Format: <code>"&lt;base_ecx>-&lt;base_edx>-&lt;ext_ecx>-&lt;ext_edx>"</code>, 4 groups of 8 hexadecimal digits, separated by dashes.</td></tr><tr class=even><td align=left><code>features_after_reboot</code></td><td align=left>The feature vector to be used after rebooting the host. This field can be modified by calling <code>Host.set_cpu_features</code>. Same format as <code>features</code>.</td></tr><tr class=odd><td align=left><code>flags</code></td><td align=left>The flags of the physical CPU (a decoded version of the features field).</td></tr><tr class=even><td align=left><code>maskable</code></td><td align=left>Indicating whether the CPU supports Intel FlexMigration or AMD Extended Migration. There are three possible values: <code>"no"</code> means that masking is not possible, <code>"base"</code> means that only base features can be masked, and <code>"full"</code> means that base as well as extended features can be masked.</td></tr><tr class=odd><td align=left><code>model</code></td><td align=left>The model number of the CPU.</td></tr><tr class=even><td align=left><code>modelname</code></td><td align=left>The model name of the CPU.</td></tr><tr class=odd><td align=left><code>physical_features</code></td><td align=left>The <em>original</em>, unmasked features. Same format as <code>features</code>.</td></tr><tr class=even><td align=left><code>speed</code></td><td align=left>The speed of the CPU.</td></tr><tr class=odd><td align=left><code>stepping</code></td><td align=left>The stepping of the CPU.</td></tr><tr class=even><td align=left><code>vendor</code></td><td align=left>The manufacturer of the CPU.</td></tr></tbody></table><p>Indicating whether the CPU supports Intel FlexMigration or AMD Extended
Migration. There are three possible values: <code>"no"</code> means that masking is
not possible, <code>"base"</code> means that only base features can be masked, and
<code>"full"</code> means that base as well as extended features can be masked.</p><p>Note: When the <code>features</code> and <code>features_after_reboot</code> are different,
XenCenter could display a warning saying that a reboot is needed to
enforce the feature masking.</p><p>The <code>Pool.other_config:cpuid_feature_mask</code> key is recognised. If this
key is present and if it contains a value in the same format as
<code>Host.cpu_info:features</code>, the value is used to mask the feature vectors
before comparisons during any pool join in the pool it is defined on.
This can be used to white-list certain feature flags, i.e. to ignore
them when adding a new host to a pool. The default it
<code>ffffff7f-ffffffff-ffffffff-ffffffff</code>, which white-lists the EST feature
for compatibility with XS 5.5 and earlier.</p><h2 id=messages>Messages</h2><p>New messages:</p><ul><li><code>Host.set_cpu_features</code><ul><li>Parameters: Host reference <code>host</code>, new CPU feature vector
<code>features</code>.</li><li>Roles: only Pool Operator and Pool Admin.</li><li>Sets the feature vector to be used after a reboot
(<code>Host.cpu_info:features_after_reboot</code>), if <code>features</code> is valid.</li></ul></li><li><code>Host.reset_cpu_features</code><ul><li>Parameter: Host reference <code>host</code>.</li><li>Roles: only Pool Operator and Pool Admin.</li><li>Removes the feature mask, such that after a reboot all features
of the CPU are enabled.</li></ul></li></ul><h1 id=xapi>XAPI</h1><h2 id=back-end>Back-end</h2><ul><li>Xen keeps the physical (unmasked) CPU features in memory when
starts, before applying any masks. Xen exposes the physical
features, as well as the current (possibly masked) features, to
dom0/xapi via the function <code>xc_get_boot_cpufeatures</code> in libxc.</li><li>A dom0 script <code>/etc/xensource/libexec/xen-cmdline</code>, which provides a
future-proof way of modifying the Xen command-line key/value pairs.
This script has the following options, where <code>mask</code> is one of
<code>cpuid_mask_ecx</code>, <code>cpuid_mask_edx</code>, <code>cpuid_mask_ext_ecx</code> or
<code>cpuid_mask_ext_edx</code>, and <code>value</code> is <code>0xhhhhhhhh</code> (<code>h</code> is represents
a hex digit).:<ul><li><code>--list-cpuid-masks</code></li><li><code>--set-cpuid-masks mask=value mask=value</code></li><li><code>--delete-cpuid-masks mask mask</code></li></ul></li><li>A <code>restrict_cpu_masking</code> key has been added to the host licensing
restrictions map. This will be <code>true</code> when the <code>Host.edition</code> is
<code>free</code>, and <code>false</code> if it is <code>enterprise</code> or <code>platinum</code>.</li></ul><h2 id=start-up>Start-up</h2><p>The <code>Host.cpu_info</code> field is refreshed:</p><ul><li>The values for the keys <code>cpu_count</code>, <code>vendor</code>, <code>speed</code>, <code>modelname</code>,
<code>flags</code>, <code>stepping</code>, <code>model</code>, and <code>family</code> are obtained from
<code>/etc/xensource/boot_time_cpus</code> (and ultimately from
<code>/proc/cpuinfo</code>).</li><li>The values of the <code>features</code> and <code>physical_features</code> are obtained
from Xen and the <code>features_after_reboot</code> key is made equal to the
<code>features</code> field.</li><li>The value of the <code>maskable</code> key is determined by the CPU details.<ul><li>for Intel Core2 (Penryn) CPUs:
<code>family = 6 and (model = 1dh or (model = 17h and stepping >= 4))</code>
(<code>maskable = "base"</code>)</li><li>for Intel Nehalem/Westmere CPUs:
<code>family = 6 and ((model = 1ah and stepping > 2) or model = 1eh or model = 25h or model = 2ch or model = 2eh or model = 2fh)</code>
(<code>maskable = "full"</code>)</li><li>for AMD CPUs: <code>family >= 10h</code> (<code>maskable = "full"</code>)</li></ul></li></ul><h2 id=setting-masking-and-resetting-the-cpu-features>Setting (Masking) and Resetting the CPU Features</h2><ul><li>The <code>Host.set_cpu_features</code> call:<ul><li>checks whether the license of the host is Enterprise or
Platinum; throws FEATURE_RESTRICTED if not.</li><li>expects a string of 32 hexadecimal digits, optionally containing
spaces; throws INVALID_FEATURE_STRING if malformed.</li><li>checks whether the given feature vector can be formed by masking
the physical feature vector; throws INVALID_FEATURE_STRING if
not. Note that on Intel Core 2 CPUs, it is only possible to the
mask the base features!</li><li>checks whether the CPU supports FlexMigration/Extended
Migration; throws CPU_FEATURE_MASKING_NOT_SUPPORTED if not.</li><li>sets the value of <code>features_after_reboot</code> to the given feature
vector.</li><li>adds the new feature mask to the Xen command-line via the
<code>xen-cmdline</code> script. The mask is represented by one or more of
the following key/value pairs (where <code>h</code> represents a hex
digit):<ul><li><code>cpuid_mask_ecx=0xhhhhhhhh</code></li><li><code>cpuid_mask_edx=0xhhhhhhhh</code></li><li><code>cpuid_mask_ext_ecx=0xhhhhhhhh</code></li><li><code>cpuid_mask_ext_edx=0xhhhhhhhh</code></li></ul></li></ul></li><li>The <code>Host.reset_cpu_features</code> call:<ul><li>copies <code>physical_features</code> to <code>features_after_reboot</code>.</li><li>removes the feature mask from the Xen command-line via the
<code>xen-cmdline</code> script (if any).</li></ul></li></ul><h2 id=pool-join-and-eject>Pool Join and Eject</h2><ul><li><code>Pool.join</code> fails when the <code>vendor</code> and <code>feature</code> keys do not match,
and disregards any other key in <code>Host.cpu_info</code>.<ul><li>However, as XS5.5 disregards the EST flag, there is a new way to
disregard/ignore feature flags on pool join, by setting a mask
in <code>Pool.other_config:cpuid_feature_mask</code>. The value of this
field should have the same format as <code>Host.cpu_info:features</code>.
When comparing the CPUID features of the pool and the joining
host for equality, this mask is applied before the comparison.
The default is <code>ffffff7f-ffffffff-ffffffff-ffffffff</code>, which
defines the EST feature, bit 7 of the base ecx flags, as &ldquo;don&rsquo;t
care&rdquo;.</li></ul></li><li><code>Pool.eject</code> clears the database (as usual), and additionally
removes the feature mask from <code>/boot/extlinux.conf</code> (if any).</li></ul><h1 id=cli>CLI</h1><p>New commands:</p><ul><li><code>host-cpu-info</code><ul><li>Parameters: <code>uuid</code> (optional, uses localhost if absent).</li><li>Lists <code>Host.cpu_info</code> associated with the host.</li></ul></li><li><code>host-get-cpu-features</code><ul><li>Parameters: <code>uuid</code> (optional, uses localhost if absent).</li><li>Returns the value of <code>Host.cpu_info:features]</code> associated with
the host.</li></ul></li><li><code>host-set-cpu-features</code><ul><li>Parameters: <code>features</code> (string of 32 hexadecimal digits,
optionally containing spaces or dashes), <code>uuid</code> (optional, uses
localhost if absent).</li><li>Calls <code>Host.set_cpu_features</code>.</li></ul></li><li><code>host-reset-cpu-features</code><ul><li>Parameters: <code>uuid</code> (optional, uses localhost if absent).</li><li>Calls <code>Host.reset_cpu_features</code>.</li></ul></li></ul><p>The following commands will be deprecated: <code>host-cpu-list</code>,
<code>host-cpu-param-get</code>, <code>host-cpu-param-list</code>.</p><p>WARNING:</p><p>If the user is able to set any mask they like, they may end up disabling
CPU features that are required by dom0 (and probably other guest OSes),
resulting in a kernel panic when the machine restarts. Hence, using the
set function is potentially dangerous.</p><p>It is apparently not easy to find out exactly which flags are safe to
mask and which aren&rsquo;t, so we cannot prevent an API/CLI user from making
mistakes in this way. However, using XenCenter would always be safe, as
XC always copies features masks from real hosts.</p><p>If a machine ends up in such a bad state, there is a way to get out of
it. At the boot prompt (before Xen starts), you can type &ldquo;menu.c32&rdquo;,
select a boot option and alter the Xen command-line to remove the
feature masks, after which the machine will again boot normally (note:
in our set-up, there is first a PXE boot prompt; the second prompt is
the one we mean here).</p><p>The API/CLI documentation should stress the potential danger of using
this functionality, and explain how to get out of trouble again.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr></table></header><h1 id=improving-snapshot-revert-behaviour>Improving snapshot revert behaviour</h1><p>Currently there is a XenAPI <code>VM.revert</code> which reverts a &ldquo;VM&rdquo; to the state it
was in when a VM-level snapshot was taken. There is no <code>VDI.revert</code> so
<code>VM.revert</code> uses <code>VDI.clone</code> to change the state of the disks.</p><p>The use of <code>VDI.clone</code> has the side-effect of changing VDI refs and uuids.
This causes the following problems:</p><ul><li>It is difficult for clients
such as <a href=http://cloudstack.apache.org rel=external target=_blank>Apache CloudStack</a> to keep track
of the disks it is actively managing</li><li>VDI snapshot metadata (<code>VDI.snapshot_of</code> et al) has to be carefully
fixed up since all the old refs are now dangling</li></ul><p>We will fix these problems by:</p><ol><li>adding a <code>VDI.revert</code> to the SMAPIv2 and calling this from <code>VM.revert</code></li><li>defining a new SMAPIv1 operation <code>vdi_revert</code> and a corresponding capability
<code>VDI_REVERT</code></li><li>the Xapi implementation of <code>VDI.revert</code> will first try the <code>vdi_revert</code>,
and fall back to <code>VDI.clone</code> if that fails</li><li>implement <code>vdi_revert</code> for common storage types, including File and LVM-based
SRs.</li></ol><h2 id=xenapi-changes>XenAPI changes</h2><p>We will add the function <code>VDI.revert</code> with arguments:</p><ul><li>in: <code>snapshot: Ref(VDI)</code>: the snapshot to which we want to revert</li><li>in: <code>driver_params: Map(String,String)</code>: optional extra parameters</li><li>out: <code>Ref(VDI)</code> the new VDI</li></ul><p>The function will look up the VDI which this is a <code>snapshot_of</code>, and change
the VDI to have the same contents as the snapshot. The snapshot will not be
modified. If the implementation is able to revert in-place, then the reference
returned will be the VDI this is a <code>snapshot_of</code>; otherwise it is a reference
to a fresh VDI (created by the <code>VDI.clone</code> fallback path)</p><p>References:</p><ul><li>@johnelse&rsquo;s <a href=https://github.com/xapi-project/xen-api/pull/1963 rel=external target=_blank>pull request</a>
which implements this</li></ul><h2 id=smapiv1-changes>SMAPIv1 changes</h2><p>We will define the function <code>vdi_revert</code> with arguments:</p><ul><li>in: <code>sr_uuid</code>: the UUID of the SR containing both the VDI and the snapshot</li><li>in: <code>vdi_uuid</code>: the UUID of the snapshot whose contents should be duplicated</li><li>in: <code>target_uuid</code>: the UUID of the target whose contents should be replaced</li></ul><p>The function will replace the contents of the <code>target_uuid</code> VDI with the
contents of the <code>vdi_uuid</code> VDI without changing the identify of the target
(i.e. name-label, uuid and location are guaranteed to remain the same).
The <code>vdi_uuid</code> is preserved by this operation. The operation is obvoiusly
idempotent.</p><h2 id=xapi-changes>Xapi changes</h2><p>Xapi will</p><ul><li>use <code>VDI.revert</code> in the <code>VM.revert</code> code-path</li><li>expose a new <code>xe vdi-revert</code> CLI command</li><li>implement the <code>VDI.revert</code> by calling the SMAPIv1 function and falling back
to <code>VDI.clone</code> if a <code>Not_implemented</code> exception is thrown</li></ul><p>References:</p><ul><li>@johnelse&rsquo;s <a href=https://github.com/xapi-project/xen-api/pull/1963 rel=external target=_blank>pull request</a></li></ul><h2 id=sm-changes>SM changes</h2><p>We will modify</p><ul><li>SRCommand.py and VDI.py to add a new <code>vdi_revert</code> function which throws
a &rsquo;not implemented&rsquo; exception</li><li>FileSR.py to implement <code>VDI.revert</code> using a variant of the existing
snapshot/clone machinery</li><li>EXTSR.py and NFSSR.py to advertise the <code>VDI_REVERT</code> capability</li><li>LVHDSR.py to implement <code>VDI.revert</code> using a variant of the existing
snapshot/clone machinery</li><li>LVHDoISCSISR.py and LVHDoHBASR.py to advertise the <code>VDI_REVERT</code> capability</li></ul><h1 id=prototype-code>Prototype code</h1><p>Prototype code exists here:</p><ul><li><a href=https://github.com/xapi-project/xcp-idl/pull/37 rel=external target=_blank>xapi-project/xcp-idl#37</a> by @johnelse</li><li><a href=https://github.com/xapi-project/xen-api/pull/2058 rel=external target=_blank>xapi-project/xen-api#2058</a> mainly by @johnelse but with 2 extra patches from me</li><li><a href=https://github.com/djs55/sm/commit/cbc28755c9c4300ed067abc089081f58f821f504 rel=external target=_blank>Definition of SMAPIv1 vdi_revert</a></li><li><a href=https://github.com/djs55/sm/commit/eb31d6205ccd707152a5b59c9a733fd48db5316b rel=external target=_blank>Hacky implementation for EXT/NFS</a></li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (6.5 sp1)</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/33>#33</a></td></tr></table></header><h1 id=integrated-gpu-passthrough-support>Integrated GPU passthrough support</h1><h2 id=introduction>Introduction</h2><p>Passthrough of discrete GPUs has been
<a href=/new-docs/design/gpu-passthrough/index.html>available since XenServer 6.0</a>.
With some extensions, we will also be able to support passthrough of integrated
GPUs.</p><ul><li>Whether an integrated GPU will be accessible to dom0 or available to
passthrough to guests must be configurable via XenAPI.</li><li>Passthrough of an integrated GPU requires an extra flag to be sent to qemu.</li></ul><h2 id=host-configuration>Host Configuration</h2><p>New fields will be added (both read-only):</p><ul><li><code>PGPU.dom0_access enum(enabled|disable_on_reboot|disabled|enable_on_reboot)</code></li><li><code>host.display enum(enabled|disable_on_reboot|disabled|enable_on_reboot)</code></li></ul><p>as well as new API calls used to modify the state of these fields:</p><ul><li><code>PGPU.enable_dom0_access</code></li><li><code>PGPU.disable_dom0_access</code></li><li><code>host.enable_display</code></li><li><code>host.disable_display</code></li></ul><p>Each of these API calls will return the new state of the field e.g. calling
<code>host.disable_display</code> on a host with <code>display = enabled</code> will return
<code>disable_on_reboot</code>.</p><p>Disabling dom0 access will modify the xen commandline (using the xen-cmdline
tool) such that dom0 will not be able to access the GPU on next boot.</p><p>Calling host.disable_display will modify the xen and dom0 commandlines such
that neither will attempt to send console output to the system display device.</p><p>A state diagram for the fields <code>PGPU.dom0_access</code> and <code>host.display</code> is shown
below:</p><p><img alt="host.integrated_GPU_passthrough flow diagram" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/integrated-gpu-passthrough/integrated-gpu-passthrough.png style=height:auto;width:auto></p><p>While it is possible for these two fields to be modified independently, a
client must disable both the host display and dom0 access to the system display
device before that device can be passed through to a guest.</p><p>Note that when a client enables or disables either of these fields, the change
can be cancelled until the host is rebooted.</p><h2 id=handling-vga_arbiter>Handling vga_arbiter</h2><p>Currently, xapi will not create a PGPU object for the PCI device with address
reported by <code>/dev/vga_arbiter</code>. This is to prevent a GPU in use by dom0 from
from being passed through to a guest. This behaviour will be changed - instead
of not creating a PGPU object at all, xapi will create a PGPU, but its
supported_VGPU_types field will be empty.</p><p>However, the PGPU&rsquo;s supported_VGPU_types will be populated as normal if:</p><ol><li>dom0 access to the GPU is disabled.</li><li>The host&rsquo;s display is disabled.</li><li>The vendor ID of the device is contained in a whitelist provided by xapi&rsquo;s
config file.</li></ol><p>A read-only field will be added:</p><ul><li><code>PGPU.is_system_display_device bool</code></li></ul><p>This will be true for a PGPU iff <code>/dev/vga_arbiter</code> reports the PGPU as the
system display device for the host on which the PGPU is installed.</p><h2 id=interfacing-with-xenopsd>Interfacing with xenopsd</h2><p>When starting a VM attached to an integrated GPU, the VM config sent to xenopsd
will contain a video_card of type IGD_passthrough. This will override the type
determined from VM.platform:vga. xapi will consider a GPU to be integrated if
both:</p><ol><li>It resides on bus 0.</li><li>The vendor ID of the device is contained in a whitelist provided by xapi&rsquo;s
config file.</li></ol><p>When xenopsd starts qemu for a VM with a video_card of type IGD_passthrough,
it will pass the flags &ldquo;-std-vga&rdquo; AND &ldquo;-gfx_passthru&rdquo;.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=local-database>Local database</h1><p>All hosts in a pool use the shared database by sending queries to
the pool master. This creates a performance bottleneck as the pool
size increases. All hosts in a pool receive a database backup from
the master periodically, every couple of hours. This creates a
reliability problem as updates may be lost if the master fails during
the window before the backup.</p><p>The reliability problem can be avoided by running with HA or the redo
log enabled, but this is not always possible.</p><p>We propose to:</p><ul><li>adapt the existing event machinery to allow every host to maintain
an up-to-date database replica;</li><li>actively cache the database locally on each host and satisfy read
operations from the cache. Most database operations are reads so
this should reduce the number of RPCs across the network.</li></ul><p>In a later phase we can move to a completely
<a href=/new-docs/design/distributed-database/index.html>distributed database</a>.</p><h2 id=replicating-the-database>Replicating the database</h2><p>We will create a database-level variant of the existing XenAPI <code>event.from</code>
API. The new RPC will block until a database event is generated, and then
the events will be returned using the existing &ldquo;redo-log&rdquo; event types. We
will add a few second delay into the RPC to batch the updates.</p><p>We will replace the pool database download logic with an <code>event.from</code>-like
loop which fetches all the events from the master&rsquo;s database and applies
them to the local copy. The first call will naturally return the full database
contents.</p><p>We will turn on the existing &ldquo;in memory db cache&rdquo; mechanism on all hosts,
not just the master. This will be where the database updates will go.</p><p>The result should be that every host will have a <code>/var/xapi/state.db</code> file,
with writes going to the master first and then filtering down to all slaves.</p><h2 id=using-the-replica-as-a-cache>Using the replica as a cache</h2><p>We will re-use the <a href=/new-docs/toolstack/features/DR/index.html>Disaster Recovery</a> multiple
database mechanism to allow slaves to access their local database. We will
change the defalult database &ldquo;context&rdquo; to snapshot the local database,
perform reads locally and write-through to the master.</p><p>We will add an HTTP header to all forwarded XenAPI calls from the master which
will include the current database generation count. When a forwarded XenAPI
operation is received, the slave will deliberately wait until the local cache
is at least as new as this, so that we always use fresh metadata for XenAPI
calls (e.g. the VM.start uses the absolute latest VM memory size).</p><p>We will document the new database coherence policy, i.e. that writes on a host
will not immediately be seen by reads on another host. We believe that this
is only a problem when we are using the database for locking and are attempting
to hand over a lock to another host. We are already using XenAPI calls forwarded
to the master for some of this, but may need to do a bit more of this; in
particular the storage backends may need some updating.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Addition of `networkd_db` update for Upgrade</td></tr><tr><td><span class="label label-default">v3</span></td><td>More info on `networkd_db` and API Errors</td></tr></table></header><h1 id=management-interface-on-vlan>Management Interface on VLAN</h1><p>This document describes design details for the
REQ-42: Support Use of VLAN on XAPI Management Interface.</p><h1 id=xapi-and-xcp-networkd>XAPI and XCP-Networkd</h1><h3 id=creating-a-vlan>Creating a VLAN</h3><p>Creating a VLAN is already there, Lisiting the steps to create a VLAN which is used later in the document.
Steps:</p><ol><li>Check the PIFs created on a Host for physical devices <code>eth0</code>, <code>eth1</code>.
<code>xe pif-list params=uuid physical=true host-uuid=UUID</code> this will list <code>pif-UUID</code></li><li>Create a new network for the VLAN interface.
<code>xe network-create name-label=VLAN1</code>
It returns a new <code>network-UUID</code></li><li>Create a VLAN PIF.
<code>xe vlan-create pif-uuid=pif-UUID network-uuid=network-UUID vlan=VLAN-ID</code>
It returns a new VLAN PIF <code>new-pif-UUID</code></li><li>Plug the VLAN PIF.
<code>xe pif-plug uuid=new-pif-UUID</code></li><li>Configure IP on the VLAN PIF.
<code>xe pif-reconfigure-ip uuid=new-pif-UUID mode= IP= netmask= gateway= DNS= </code>This will configure IP on the PIF, here <code>mode</code> is must and other parametrs are needed on selecting mode=static</li></ol><p>Similarly, creating a vlan pif can be achieved by corresponding XenAPI calls.</p><h2 id=recognise-vlan-config-from-managementconf>Recognise VLAN config from management.conf</h2><p>For a newly installed host, If host installer was asked to put the management interface on given VLAN.
We will expect a new entry <code>VLAN=ID</code> under <code>/etc/firstboot.d/data/management.conf</code>.</p><p>Listing current contents of management.conf which will be used later in the document.
<code>LABEL</code>=<code>eth0</code> -> Represents Pyhsical device on which Management Interface must reside.
<code>MODE</code>=<code>dhcp</code>||<code>static</code> -> Represents IP configuration mode for the Management Interface. There can be other parameters like IP, NETMASK, GATEWAY and DNS when we have <code>static</code> mode.
<code>VLAN</code>=<code>ID</code> -> New entry for specifying VLAN TAG going to be configured on device <code>LABEL</code>.
Management interface going to be configured on this VLAN ID with specified mode.</p><h3 id=firstboot-script-need-to-recognise-vlan-config>Firstboot script need to recognise VLAN config</h3><p>Firstboot script <code>/etc/firstboot.d/30-prepare-networking</code> need to be updated for configuring
management interface to be on provided VLAN ID.</p><p>Steps to be followed:</p><ol><li><code>PIF.scan</code> performed in the script must have created the PIFs for the underlying pyhsical devices.</li><li>Get the PIF UUID for physical device <code>LABEL</code>.</li><li>Repeat the steps mentioned in <code>Creating a VLAN</code>, i.e. network-create, vlan-create and pif-plug. Now we have a new PIF for the VLAN.</li><li>Perform <code>pif-reconfigure-ip</code> for the new VLAN PIF.</li><li>Perform <code>host-management-reconfigure</code> using new VLAN PIF.</li></ol><h3 id=xcp-networkd-need-to-recognise-vlan-config-during-startup>XCP-Networkd need to recognise VLAN config during startup</h3><p>XCP-Networkd during first boot and boot after pool eject gets the initial network setup from the <code>management.conf</code> and <code>xensource-inventory</code> file to update the network.db for management interface info.
XCP-Networkd must honour the new VLAN config.</p><p>Steps to be followed:</p><ol><li>During startup <code>read_config</code> step tries to read the <code>/var/lib/xcp/networkd.db</code> file which is not yet created just after host installation.</li><li>Since <code>networkd.db</code> read throws <code>Read_Error</code>, it tries to read <code>network.dbcache</code> which is also not available hence it goes to read <code>read_management_conf</code> file.</li><li>There can be two possible MODE <code>static</code> or <code>dhcp</code> taken from management.conf.</li><li><code>bridge_name</code> is taken as <code>MANAGEMENT_INTERFACE</code> from xensource-inventory, further <code>bridge_config</code> and <code>interface_config</code> are build based on MODE.</li><li>Call <code>Bridge.make_config()</code> and <code>Interface.make_config()</code> are performed with respective <code>bridge_config</code> and <code>interface_config</code>.</li></ol><h2 id=updating-networkd_db-program>Updating networkd_db program</h2><p><code>networkd_db</code> provides the management interface info to the host installer during upgrade.
It reads <code>/var/lib/xcp/networkd.db</code> file to output the Management Interface information. Here we need to update the networkd_db to output the VLAN information when vlan bridge is a input.</p><p>Steps to be followed:</p><ol><li>Currently VLAN interface IP information is provided correctly on passing VLAN bridge as input.
<code>networkd_db -iface xapi0</code> this will list <code>mode</code> as dhcp or static, if mode=static then it will provide <code>ipaddr</code> and <code>netmask</code> too.</li><li>We need to udpate this program to provide VLAN ID and parent bridge info on passing VLAN bridge as input.
<code>networkd_db -bridge xapi0</code> It should output the VLAN info like:
<code>interfaces=</code>
<code>vlan=vlanID</code>
<code>parent=xenbr0</code> using the parent bridge user can identify the physical interfaces.
Here we will extract VLAN and parent bridge from <code>bridge_config</code> under <code>networkd.db</code>.</li></ol><h2 id=additional-vlan-parameter-for-emergency-network-reset>Additional VLAN parameter for Emergency Network Reset</h2><p>Detail design is mentioned on <a href=http://xapi-project.github.io/xapi/design/emergency-network-reset.html rel=external target=_blank>http://xapi-project.github.io/xapi/design/emergency-network-reset.html</a>
For using <code>xe-reset-networking</code> utility to configure management interface on VLAN, We need to add one more parameter <code>--vlan=vlanID</code> to the utility.
There are certain parameters need to be passed to this utility: &ndash;master, &ndash;device, &ndash;mode, &ndash;ip, &ndash;netmask, &ndash;gateway, &ndash;dns and new one &ndash;vlan.</p><h3 id=vlan-parameter-addition-to-xe-reset-networking>VLAN parameter addition to xe-reset-networking</h3><p>Steps to be followed:</p><ol><li>Check if <code>VLANID</code> is passed then let bridge=<code>xapi0</code>.</li><li>Write the <code>bridge=xapi0</code> into xensource-inventory file, This should work as Xapi check avialable bridges while creating networks.</li><li>Write the <code>VLAN=vlanID</code> into <code>management.conf</code> and <code>/tmp/network-reset</code>.</li><li>Modify <code>check_network_reset</code> under xapi.ml to perform steps <code>Creating a VLAN</code> and perform <code>management_reconfigure</code> on vlan pif.
Step <code>Creating a VLAN</code> must have created the VLAN record in Xapi DB similar to firstboot script.</li><li>If no VLANID is specified then retain the current one, This utility must take the management interface info from <code>networkd_db</code> program and handle the VLAN config.</li></ol><h3 id=vlan-parameter-addition-to-xsconsole-emergency-network-reset>VLAN parameter addition to xsconsole Emergency Network Reset</h3><p>Under <code>Emergency Network Reset</code> option under the <code>Network and Management Interface</code> menu.
Selecting this option will show some explanation in the pane on the right-hand side.
Pressing <enter>will bring up a dialogue to select the interfaces to use as management interface after the reset.
After choosing a device, the dialogue continues with configuration options like in the <code>Configure Management Interface</code> dialogue.
There will be an additionall option for VLAN in the dialogue.
After completing the dialogue, the same steps as listed for xe-reset-networking are executed.</p><h2 id=updating-pool-joineject-operations>Updating Pool Join/Eject operations</h2><h3 id=pool-join-while-pool-having-management-interface-on-a-vlan>Pool Join while Pool having Management Interface on a VLAN</h3><p>Currently <code>pool-join</code> fails if VLANs are present on the host joining a pool.
We need to allow pool-join only if Pool and host joining a pool both has management interface on same VLAN.</p><p>Steps to be followed:</p><ol><li>Under <code>pre_join_checks</code> update function <code>assert_only_physical_pifs</code> to check Pool master management_interface is on same VLAN.</li><li>Call <code>Host.get_management_interface</code> on Pool master and get the vlanID, match it with <code>localhost</code> management_interface VLAN ID.
If it matches then allow pool-join.</li><li>In case if there are multiple VLANs on host joining a pool, fail the pool-join gracefully.</li><li>After the pool-join, Host xapi db will get sync from pool master xapi db, This will be fine to have management interface on VLAN.</li></ol><h3 id=pool-eject-while-host-ejected-having-management-interface-on-a-vlan>Pool Eject while host ejected having Management Interface on a VLAN</h3><p>Currently managament interface VLAN config on host is not been retained in <code>xensource-inventory</code> or <code>management.conf</code> file.
We need to retain the vlanID under config files.</p><p>Steps to be followed:</p><ol><li>Under call <code>Pool.eject</code> we need to update <code>write_first_boot_management_interface_configuration_file</code> function.</li><li>Check if management_interface is on VLAN then get the VLANID from the pif.</li><li>Update the VLANID into the <code>managament.conf</code> file and the <code>bridge</code> into <code>xensource-inventory</code> file.
In order to be retained by XCP-Networkd on startup after the host is ejected.</li></ol><h2 id=new-api-for-pool-management-reconfigure>New API for Pool Management Reconfigure</h2><p>Currently there is no Pool Level API to reconfigure management_interface for all of the Hosts in a Pool at once.
API <code>Pool.management_reconfigure</code> will be needed in order to reconfigure <code>manamegemnt_interface</code> on all hosts in a Pool to the same Network either VLAN or Physical.</p><h3 id=current-behaviour-to-change-the-management-interface-on-host>Current behaviour to change the Management Interface on Host</h3><p>Currently call <code>Host.management_reconfigure</code> with VLAN pif-uuid can change the management_interface to specified VLAN.
Listing the steps to understand the workflow of <code>management_interface</code> reconfigure. We will be using <code>Host.management_reconfigure</code> call inside the new API.</p><p>Steps performed during management_reconfigure:</p><ol><li><code>bring_pif_up</code> get called for the pif.</li><li><code>xensource-inventory</code> get updated with the latest info of interface.
3 <code>update-mh-info</code> updates the management_mac into xenstore.</li><li>Http server gets restarted, even though xapi listen on all IP addresses, This new interface as <code>_the_ management</code> interface is used by slaves to connect to pool master.</li><li><code>on_dom0_networking_change</code> refreshes console URIs for the new IP address.</li><li>Xapi db is updated with new management interface info.</li></ol><h3 id=management-reconfigure-on-pool-from-physical-network-to-vlan-network-or-from-vlan-network-to-other-vlan-network-or-from-vlan-network-to-physical-network>Management Reconfigure on Pool from Physical Network to VLAN Network or from VLAN Network to Other VLAN Network or from VLAN Network to Physical Network</h3><p>Listing steps to be performed manually on each Host or Pool as a prerequisite to use the New API.
We need to make sure that new network which is going to be a management interface has PIFs configured on each Host.
In case of pyhsical network we will assume pifs are configured on each host, In case of vlan network we need to create vlan pifs on each Host.
We would assume that VLAN is available on the switch/network.</p><p>Manual steps to be performed before calling new API:</p><ol><li>Create a vlan network on pool via <code>network.create</code>, In case of pyhsical NICs network must be present.</li><li>Create a vlan pif on each host via <code>VLAN.create</code> using above network ref, physical PIF ref and vlanID, Not needed in case of pyhsical network.
Or An Alternate call <code>pool.create_VLAN</code> providing <code>device</code> and above <code>network</code> will create vlan PIFs for all hosts in a pool.</li><li>Perform <code>PIF.reconfigure_ip</code> for each new Network PIF on each Host.</li></ol><p>If User wishes to change the management interface manually on each Host in a Pool, We should allow it, There will be a guideline for that:</p><p>User can individually change management interface on each host calling <code>Host.management_reconfigure</code> using pifs on physical devices or vlan pifs.
This must be perfomed on slaves first and lastly on Master, As changing management_interface on master will disconnect slaves from master then further calls <code>Host.management_reconfigure</code> cannot be performed till master recover slaves via call <code>pool.recover_slaves</code>.</p><h3 id=api-details>API Details</h3><ul><li><code>Pool.management_reconfigure</code><ul><li>Parameter: network reference <code>network</code>.</li><li>Calling this function configures <code>management_interface</code> on each host of a pool.</li><li>For the <code>network</code> provided it will check pifs are present on each Host,
In case of VLAN network it will check vlan pifs on provided network are present on each Host of Pool.</li><li>Check IP is configured on above pifs on each Host.</li><li>If PIFs are not present or IP is not configured on PIFs this call must fail gracefully, Asking user to configure them.</li><li>Call <code>Host.management_reconfigure</code> on each slave then lastly on master.</li><li>Call <code>pool.recover_slaves</code> on master inorder to recover slaves which might have lost the connection to master.</li></ul></li></ul><h3 id=api-errors>API errors</h3><p>Possible API errors that may be raised by <code>pool.management_reconfigure</code>:</p><ul><li><code>INTERFACE_HAS_NO_IP</code> : the specified PIF (<code>pif</code> parameter) has no IP configuration. The new API checks for all PIFs on the new Network has IP configured. There might be a case when user has forgotten to configure IP on PIF on one or many of the Hosts in a Pool.</li></ul><p>New API ERROR:</p><ul><li><code>REQUIRED_PIF_NOT_PRESENT</code> : the specified Network (<code>network</code> parameter) has no PIF present on the host in pool. There might be a case when user has forgotten to create vlan pif on one or many of the Hosts in a Pool.</li></ul><h2 id=cp-tickets>CP-Tickets</h2><ol><li>CP-14027</li><li>CP-14028</li><li>CP-14029</li><li>CP-14030</li><li>CP-14031</li><li>CP-14032</li><li>CP-14033</li></ol><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial revision</td></tr><tr><td><span class="label label-default">v2</span></td><td>Short-term simplications and scope reduction</td></tr></table></header><h1 id=multiple-cluster-managers>Multiple Cluster Managers</h1><h2 id=introduction>Introduction</h2><p>Xapi currently uses a cluster manager called <a href=/new-docs/toolstack/features/HA/index.html>xhad</a>. Sometimes other software comes with its own built-in way of managing clusters, which would clash with xhad (example: xhad could choose to fence node &lsquo;a&rsquo; while the other system could fence node &lsquo;b&rsquo; resulting in a total failure). To integrate xapi with this other software we have 2 choices:</p><ol><li>modify the other software to take membership information from xapi; or</li><li>modify xapi to take membership information from this other software.</li></ol><p>This document proposes a way to do the latter.</p><h2 id=xenapi-changes>XenAPI changes</h2><h3 id=new-field>New field</h3><p>We will add the following new field:</p><ul><li><code>pool.ha_cluster_stack</code> of type <code>string</code> (read-only)<ul><li>If HA is enabled, this field reflects which cluster stack is in use.</li><li>Set to <code>"xhad"</code> on upgrade, which implies that so far we have used XenServer&rsquo;s own cluster stack, called <code>xhad</code>.</li></ul></li></ul><h3 id=cluster-stack-choice>Cluster-stack choice</h3><p>We assume for now that a particular cluster manager will be mandated (only) by certain types of clustered storage, recognisable by SR type (e.g. OCFS2 or Melio). The SR backend will be able to inform xapi if the SR needs a particular cluster stack, and if so, what is the name of the stack.</p><p>When <code>pool.enable_ha</code> is called, xapi will determine which cluster stack to use based on the presence or absence of such SRs:</p><ul><li>If an SR that needs its own cluster stack is attached to the pool, then xapi will use that cluster stack.</li><li>If no SR that needs a particular cluster stack is attached to the pool, then xapi will use <code>xhad</code>.</li></ul><p>If multiple SRs that need a particular cluster stack exist, then the storage parts of xapi must ensure that no two such SRs are ever attached to a pool at the same time.</p><h3 id=new-errors>New errors</h3><p>We will add the following API error that may be raised by <code>pool.enable_ha</code>:</p><ul><li><code>INCOMPATIBLE_STATEFILE_SR</code>: the specified SRs (<code>heartbeat_srs</code> parameter) are not of the right type to hold the HA statefile for the <code>cluster_stack</code> that will be used. For example, there is a Melio SR attached to the pool, and therefore the required cluster stack is the Melio one, but the given heartbeat SR is not a Melio SR. The single parameter will be the name of the required SR type.</li></ul><p>The following new API error may be raised by <code>PBD.plug</code>:</p><ul><li><code>INCOMPATIBLE_CLUSTER_STACK_ACTIVE</code>: the operation cannot be performed because an incompatible cluster stack is active. The single parameter will be the name of the required cluster stack. This could happen (or example) if you tried to create an OCFS2 SR with XenServer HA already enabled.</li></ul><h3 id=future-extensions>Future extensions</h3><p>In future, we may add a parameter to explicitly choose the cluster stack:</p><ul><li>New parameter to <code>pool.enable_ha</code> called <code>cluster_stack</code> of type <code>string</code> which will have the default value of empty string (meaning: let the implementation choose).</li><li>With the additional parameter, <code>pool.enable_ha</code> may raise two new errors:<ul><li><code>UNKNOWN_CLUSTER_STACK</code>:
The operation cannot be performed because the requested cluster stack does not exist. The user should check the name was entered correctly and, failing that, check to see if the software is installed. The exception will have a single parameter: the name of the cluster stack which was not found.</li><li><code>CLUSTER_STACK_CONSTRAINT</code>: HA cannot be enabled with the provided cluster stack because some third-party software is already active which requires a different cluster stack setting. The two parameters are: a reference to an object (such as an SR) which has created the restriction, and the name of the cluster stack that this object requires.</li></ul></li></ul><h2 id=implementation>Implementation</h2><p>The <code>xapi.conf</code> file will have a new field: <code>cluster-stack-root</code> which will have the default value <code>/usr/libexec/xapi/cluster-stack</code>. The existing <code>xhad</code> scripts and tools will be moved to <code>/usr/libexec/xapi/cluster-stack/xhad/</code>. A hypothetical cluster stack called <code>foo</code> would be placed in <code>/usr/libexec/xapi/cluster-stack/foo/</code>.</p><p>In <code>Pool.enable_ha</code> with <code>cluster_stack="foo"</code> we will verify that the subdirectory <code>&lt;cluster-stack-root>/foo</code> exists. If it does not exist, then the call will fail with <code>UNKNOWN_CLUSTER_STACK</code>.</p><p>Alternative cluster stacks will need to conform to the exact same interface as <a href=/new-docs/toolstack/features/HA/index.html>xhad</a>.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=multiple-device-emulators>Multiple device emulators</h1><p>Xen&rsquo;s <code>ioreq-server</code> feature allows for several device emulator
processes to be attached to the same domain, each emulating different
sets of virtual hardware. This makes it possible, for example, to
emulate network devices in a separate process for improved security
and isolation, or to provide special purpose emulators for particular
virtual hardware devices.</p><p><code>ioreq-server</code> is currently used in XenServer to support vGPU, where it
is configured via the legacy toolstack interface. These changes will make
multiple emulators usable in open source Xen via the new libxl interface.</p><h2 id=libxl-changes>libxl changes</h2><ul><li><p>The singleton device_model_version, device_model_stubdomain and
device_model fields in the b_info structure will be replaced by a list of
(version, stubdomain, model, arguments) tuples, one for each emulator.</p></li><li><p>libxl_domain_create_new() will be changed to spawn a new device model
for each entry in the list.</p></li></ul><p>It may also be useful to spawn the device models separately and only
attach them during domain creation. This could be supported by
making each device_model entry a union of <code>pid | parameter_tuple</code>.
If such an entry specifies a parameter tuple, it is processed as above;
if it specifies a pid, libxl_domain_create_new(), the existing device
model with that pid is attached instead.</p><h2 id=qemu-changes>QEMU changes</h2><ul><li><p>Patches to make QEMU register with Xen as an ioreq-server have been
submitted upstream, but not yet applied.</p></li><li><p>QEMU&rsquo;s <code>--machine none</code> and <code>--nodefaults</code> options should make it
possible to create an empty machine and add just a host bus, PCI bus
and device. This has not yet been fully demonstrated, so QEMU changes
may be required.</p></li></ul><h2 id=xen-changes>Xen changes</h2><ul><li>Until now, <code>ioreq-server</code> has only been used to connect one extra
device model, in addition to the default one. Multiple emulators
should work, but there is a chance that bugs will be discovered.</li></ul><h2 id=interfacing-with-xenopsd>Interfacing with xenopsd</h2><p>This functionality will only be available through the experimental
Xenlight-based xenopsd.</p><ul><li>the <code>VM_build</code> clause in the <code>atomics_of_operation</code> function will be
changed to fill in the list of emulators to be created (or attached)
in the b_info struct</li></ul><h2 id=host-configuration>Host Configuration</h2><p>vGPU support is implemented mostly in xenopsd, so no Xapi changes are
required to support vGPU through the generic device model mechanism.
Changes would be required if we decided to expose the additional device
models through the API, but in the near future it is more likely that
any additional device models will be dealt with entirely by xenopsd.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=ocfs2-storage>OCFS2 storage</h1><p>OCFS2 is a (host-)clustered filesystem which runs on top of a shared raw block
device. Hosts using OCFS2 form a cluster using a combination of network and
storage heartbeats and host fencing to avoid split-brain.</p><p>The following diagram shows the proposed architecture with <code>xapi</code>:</p><p><img alt="Proposed architecture" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/ocfs2/ocfs2.png style=height:auto;width:auto></p><p>Please note the following:</p><ul><li>OCFS2 is configured to use global heartbeats rather than per-mount heartbeats
because we quite often have many SRs and therefore many mountpoints</li><li>The OCFS2 global heartbeat should be collocated on the same SR as the XenServer
HA SR so that we depend on fewer SRs (the storage is a single point of failure
for OCFS2)</li><li>The OCFS2 global heartbeat should itself be a raw VDI within an LVHDSR.</li><li>Every host can be in at-most-one OCFS2 cluster i.e. the host cluster membership
is a per-host thing rather than a per-SR thing. Therefore <code>xapi</code> will be
modified to configure the cluster and manage the cluster node numbers.</li><li>Every SR will be a filesystem mount, managed by a SM plugin called &ldquo;OCFS2&rdquo;.</li><li>Xapi HA uses the <code>xhad</code> process which runs in userspace but in the realtime
scheduling class so it has priority over all other userspace tasks. <code>xhad</code>
sends heartbeats via the <code>ha_statefile</code> VDI and via UDP, and uses the
Xen watchdog for host fencing.</li><li>OCFS2 HA uses the <code>o2cb</code> kernel driver which sends heartbeats via the
<code>o2cb_statefile</code> and via TCP, fencing the host by panicing domain 0.</li></ul><h1 id=managing-o2cb>Managing O2CB</h1><p>OCFS2 uses the O2CB &ldquo;cluster stack&rdquo; which is similar to our <code>xhad</code>. To configure
O2CB we need to</p><ul><li>assign each host an integer node number (from zero)</li><li>on pool/cluster join: update the configuration on every node to include the
new node. In OCFS2 this can be done online.</li><li>on pool/cluster leave/eject: update the configuration on every node to exclude
the old node. In OCFS2 this needs to be done offline.</li></ul><p>In the current Xapi toolstack there is a single global implicit cluster called a &ldquo;Pool&rdquo;
which is used for: resource locking; &ldquo;clustered&rdquo; storage repositories and fault handling (in HA). In the long term we will allow these types of clusters to be
managed separately or all together, depending on the sophistication of the
admin and the complexity of their environment. We will take a small step in that
direction by keeping the OCFS2 O2CB cluster management code at &ldquo;arms length&rdquo;
from the Xapi Pool.join code.</p><p>In
<a href=https://github.com/xapi-project/xcp-idl rel=external target=_blank>xcp-idl</a>
we will define a new API category called &ldquo;Cluster&rdquo; (in addition to the
categories for
<a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/xen/xenops_interface.ml#L102 rel=external target=_blank>Xen domains</a>
, <a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/memory/memory_interface.ml#L38 rel=external target=_blank>ballooning</a>
, <a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/rrd/rrd_interface.ml#L76 rel=external target=_blank>stats</a>
,
<a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/network/network_interface.ml#L106 rel=external target=_blank>networking</a>
and
<a href=https://github.com/xapi-project/xcp-idl/blob/37c676548a53b927ac411ab51f33892a7b891fda/storage/storage_interface.ml#L51 rel=external target=_blank>storage</a>
). These APIs will only be called by Xapi on localhost. In particular they will
not be called across-hosts and therefore do not have to be backward compatible.
These are &ldquo;cluster plugin APIs&rdquo;.</p><p>We will define the following APIs:</p><ul><li><code>Plugin:Membership.create</code>: add a host to a cluster. On exit the local host cluster software
will know about the new host but it may need to be restarted before the
change takes effect<ul><li>in:<code>hostname:string</code>: the hostname of the management domain</li><li>in:<code>uuid:string</code>: a UUID identifying the host</li><li>in:<code>id:int</code>: the lowest available unique integer identifying the host
where an integer will never be re-used unless it is guaranteed that
all nodes have forgotten any previous state associated with it</li><li>in:<code>address:string list</code>: a list of addresses through which the host
can be contacted</li><li>out: Task.id</li></ul></li><li><code>Plugin:Membership.destroy</code>: removes a named host from the cluster. On exit the local
host software will know about the change but it may need to be restarted
before it can take effect<ul><li>in:<code>uuid:string</code>: the UUID of the host to remove</li></ul></li><li><code>Plugin:Cluster.query</code>: queries the state of the cluster<ul><li>out:<code>maintenance_required:bool</code>: true if there is some outstanding configuration
change which cannot take effect until the cluster is restarted.</li><li>out:<code>hosts</code>: a list of all known hosts together with a state including:
whether they are known to be alive or dead; or whether they are currently
&ldquo;excluded&rdquo; because the cluster software needs to be restarted</li></ul></li><li><code>Plugin:Cluster.start</code>: turn on the cluster software and let the local host join</li><li><code>Plugin:Cluster.stop</code>: turn off the cluster software</li></ul><p>Xapi will be modified to:</p><ul><li>add table <code>Cluster</code> which will have columns<ul><li><code>name: string</code>: this is the name of the Cluster plugin (TODO: use same
terminology as SM?)</li><li><code>configuration: Map(String,String)</code>: this will contain any cluster-global
information, overrides for default values etc.</li><li><code>enabled: Bool</code>: this is true when the cluster &ldquo;should&rdquo; be running. It
may require maintenance to synchronise changes across the hosts.</li><li><code>maintenance_required: Bool</code>: this is true when the cluster needs to
be placed into maintenance mode to resync its configuration</li></ul></li><li>add method <code>XenAPI:Cluster.enable</code> which sets <code>enabled=true</code> and waits for all
hosts to report <code>Membership.enabled=true</code>.</li><li>add method <code>XenAPI:Cluster.disable</code> which sets <code>enabled=false</code> and waits for all
hosts to report <code>Membership.enabled=false</code>.</li><li>add table <code>Membership</code> which will have columns<ul><li><code>id: int</code>: automatically generated lowest available unique integer
starting from 0</li><li><code>cluster: Ref(Cluster)</code>: the type of cluster. This will never be NULL.</li><li><code>host: Ref(host)</code>: the host which is a member of the cluster. This may
be NULL.</li><li><code>left: Date</code>: if not 1/1/1970 this means the time at which the host
left the cluster.</li><li><code>maintenance_required: Bool</code>: this is true when the Host believes the
cluster needs to be placed into maintenance mode.</li></ul></li><li>add field <code>Host.memberships: Set(Ref(Membership))</code></li><li>extend enum <code>vdi_type</code> to include <code>o2cb_statefile</code> as well as <code>ha_statefile</code></li><li>add method <code>Pool.enable_o2cb</code> with arguments<ul><li>in: <code>heartbeat_sr: Ref(SR)</code>: the SR to use for global heartbeats</li><li>in: <code>configuration: Map(String,String)</code>: available for future configuration tweaks</li><li>Like <code>Pool.enable_ha</code> this will find or create the heartbeat VDI, create the
<code>Cluster</code> entry and the <code>Membership</code> entries. All <code>Memberships</code> will have
<code>maintenance_required=true</code> reflecting the fact that the desired cluster
state is out-of-sync with the actual cluster state.</li></ul></li><li>add method <code>XenAPI:Membership.enable</code><ul><li>in: <code>self:Host</code>: the host to modify</li><li>in: <code>cluster:Cluster</code>: the cluster.</li></ul></li><li>add method <code>XenAPI:Membership.disable</code><ul><li>in: <code>self:Host</code>: the host to modify</li><li>in: <code>cluster:Cluster</code>: the cluster name.</li></ul></li><li>add a cluster monitor thread which<ul><li>watches the <code>Host.memberships</code> field and calls <code>Plugin:Membership.create</code> and
<code>Plugin:Membership.destroy</code> to keep the local cluster software up-to-date
when any host in the pool changes its configuration</li><li>calls <code>Plugin:Cluster.query</code> after an <code>Plugin:Membership:create</code> or
<code>Plugin:Membership.destroy</code> to see whether the
SR needs maintenance</li><li>when all hosts have a last start time later than a <code>Membership</code>
record&rsquo;s <code>left</code> date, deletes the <code>Membership</code>.</li></ul></li><li>modify <code>XenAPI:Pool.join</code> to resync with the master&rsquo;s <code>Host.memberships</code> list.</li><li>modify <code>XenAPI:Pool.eject</code> to<ul><li>call <code>Membership.disable</code> in the cluster plugin to stop the <code>o2cb</code> service</li><li>call <code>Membership.destroy</code> in the cluster plugin to remove every other host
from the local configuration</li><li>remove the <code>Host</code> metadata from the pool</li><li>set <code>XenAPI:Membership.left</code> to <code>NOW()</code></li></ul></li><li>modify <code>XenAPI:Host.forget</code> to<ul><li>remove the <code>Host</code> metadata from the pool</li><li>set <code>XenAPI:Membership.left</code> to <code>NOW()</code></li><li>set <code>XenAPI:Cluster.maintenance_required</code> to true</li></ul></li></ul><p>A Cluster plugin called &ldquo;o2cb&rdquo; will be added which</p><ul><li>on <code>Plugin:Membership.destroy</code><ul><li>comment out the relevant node id in cluster.conf</li><li>set the &rsquo;needs a restart&rsquo; flag</li></ul></li><li>on <code>Plugin:Membership.create</code><ul><li>if the provided node id is too high: return an error. This means the
cluster needs to be rebooted to free node ids.</li><li>if the node id is not too high: rewrite the cluster.conf using
the &ldquo;online&rdquo; tool.</li></ul></li><li>on <code>Plugin:Cluster.start</code>: find the VDI with <code>type=o2cb_statefile</code>;
add this to the &ldquo;static-vdis&rdquo; list; <code>chkconfig</code> the service on. We
will use the global heartbeat mode of <code>o2cb</code>.</li><li>on <code>Plugin:Cluster.stop</code>: stop the service; <code>chkconfig</code> the service off;
remove the &ldquo;static-vdis&rdquo; entry; leave the VDI itself alone</li><li>keeps track of the current &rsquo;live&rsquo; cluster.conf which allows it to<ul><li>report the cluster service as &rsquo;needing a restart&rsquo; (which implies
we need maintenance mode)</li></ul></li></ul><p>Summary of differences between this and xHA:</p><ul><li>we allow for the possibility that hosts can join and leave, without
necessarily taking the whole cluster down. In the case of <code>o2cb</code> we
should be able to have <code>join</code> work live and only <code>eject</code> requires
maintenance mode</li><li>rather than write explicit RPCs to update cluster configuration state
we instead use an event watch and resync pattern, which is hopefully
more robust to network glitches while a reconfiguration is in progress.</li></ul><h1 id=managing-xhad>Managing xhad</h1><p>We need to ensure <code>o2cb</code> and <code>xhad</code> do not try to conflict by fencing
hosts at the same time. We shall:</p><ul><li><p>use the default <code>o2cb</code> timeouts (hosts fence if no I/O in 60s): this
needs to be short because disk I/O <em>on otherwise working hosts</em> can
be blocked while another host is failing/ has failed.</p></li><li><p>make the <code>xhad</code> host fence timeouts much longer: 300s. It&rsquo;s much more
important that this is reliable than fast. We will make this change
globally and not just when using OCFS2.</p></li></ul><p>In the <code>xhad</code> config we will cap the <code>HeartbeatInterval</code> and <code>StatefileInterval</code>
at 5s (the default otherwise would be 31s). This means that 60 heartbeat
messages have to be lost before <code>xhad</code> concludes that the host has failed.</p><h1 id=sm-plugin>SM plugin</h1><p>The SM plugin <code>OCFS2</code> will be a file-based plugin.</p><p>TODO: which file format by default?</p><p>The SM plugin will first check whether the <code>o2cb</code> cluster is active and fail
operations if it is not.</p><h1 id=io-paths>I/O paths</h1><p>When either HA or OCFS O2CB &ldquo;fences&rdquo; the host it will look to the admin like
a host crash and reboot. We need to (in priority order)</p><ol><li>help the admin <em>prevent</em> fences by monitoring their I/O paths
and fixing issues before they lead to trouble</li><li>when a fence/crash does happen, help the admin<ul><li>tell the difference between an I/O error (admin to fix) and a software
bug (which should be reported)</li><li>understand how to make their system more reliable</li></ul></li></ol><h2 id=monitoring-io-paths>Monitoring I/O paths</h2><p>If heartbeat I/O fails for more than 60s when running <code>o2cb</code> then the host will fence.
This can happen either</p><ul><li><p>for a good reason: for example the host software may have deadlocked or someone may
have pulled out a network cable.</p></li><li><p>for a bad reason: for example a network bond link failure may have been ignored
and then the second link failed; or the heartbeat thread may have been starved of
I/O bandwidth by other processes</p></li></ul><p>Since the consequences of fencing are severe &ndash; all VMs on the host crash simultaneously &ndash;
it is important to avoid the host fencing for bad reasons.</p><p>We should recommend that all users</p><ul><li>use network bonding for their network heartbeat</li><li>use multipath for their storage heartbeat</li></ul><p>Furthermore we need to <em>help</em> users monitor their I/O paths. It&rsquo;s no good if they use
a bonded network but fail to notice when one of the paths have failed.</p><p>The current XenServer HA implementation generates the following I/O-related alerts:</p><ul><li><code>HA_HEARTBEAT_APPROACHING_TIMEOUT</code> (priority 5 &ldquo;informational&rdquo;): when half the
network heartbeat timeout has been reached.</li><li><code>HA_STATEFILE_APPROACHING_TIMEOUT</code> (priority 5 &ldquo;informational&rdquo;): when half the
storage heartbeat timeout has been reached.</li><li><code>HA_NETWORK_BONDING_ERROR</code> (priority 3 &ldquo;service degraded&rdquo;): when one of the bond
links have failed.</li><li><code>HA_STATEFILE_LOST</code> (priority 2 &ldquo;service loss imminent&rdquo;): when the storage heartbeat
has completely failed and only the network heartbeat is left.</li><li>MULTIPATH_PERIODIC_ALERT (priority 3 &ldquo;service degrated&rdquo;): when one of the multipath
links have failed.</li></ul><p>Unfortunately alerts are triggered on &ldquo;edges&rdquo; i.e. when state changes, and not on &ldquo;levels&rdquo;
so it is difficult to see whether the link is currently broken.</p><p>We should define datasources suitable for use by xcp-rrdd to expose the current state
(and the history) of the I/O paths as follows:</p><ul><li><code>pif_&lt;name>_paths_failed</code>: the total number of paths which we know have failed.</li><li><code>pif_&lt;name>_paths_total</code>: the total number of paths which are configured.</li><li><code>sr_&lt;name>_paths_failed</code>: the total number of storage paths which we know have failed.</li><li><code>sr_&lt;name>_paths_total</code>: the total number of storage paths which are configured.</li></ul><p>The <code>pif</code> datasources should be generated by <code>xcp-networkd</code> which already has a
<a href=https://github.com/xapi-project/xcp-networkd/blob/bc0140feba19cf8dcced3bd66e54eeee112af819/networkd/network_monitor_thread.ml#L52 rel=external target=_blank>network bond monitoring thread</a>.
THe <code>sr</code> datasources should be generated by <code>xcp-rrdd</code> plugins since there is no
storage daemon to generate them.
We should create RRDs using the <code>MAX</code> consolidation function, otherwise information
about failures will be lost by averaging.</p><p>XenCenter (and any diagnostic tools) should warn when the system is at risk of fencing
in particular if any of the following are true:</p><ul><li><code>pif_&lt;name>_paths_failed</code> is non-zero</li><li><code>sr_&lt;name>_paths_failed</code> is non-zero</li><li><code>pif_&lt;name>_paths_total</code> is less than 2</li><li><code>sr_&lt;name>_paths_total</code> is less than 2</li></ul><p>XenCenter (and any diagnostic tools) should warn if any of the following <em>have been</em>
true over the past 7 days:</p><ul><li><code>pif_&lt;name>_paths_failed</code> is non-zero</li><li><code>sr_&lt;name>_paths_failed</code> is non-zero</li></ul><h2 id=heartbeat-qos>Heartbeat &ldquo;QoS&rdquo;</h2><p>The network and storage paths used by heartbeats <em>must</em> remain responsive otherwise
the host will fence (i.e. the host and all VMs will crash).</p><p>Outstanding issue: how slow can <code>multipathd</code> get? How does it scale with the number of
LUNs.</p><h1 id=post-crash-diagnostics>Post-crash diagnostics</h1><p>When a host crashes the effect on the user is severe: all the VMs will also
crash. In cases where the host crashed for a bad reason (such as a single failure
after a configuration error) we must help the user understand how they can
avoid the same situation happening again.</p><p>We must make sure the crash kernel runs reliably when <code>xhad</code> and <code>o2cb</code>
fence the host.</p><p>Xcp-rrdd will be modified to store RRDs in an <code>mmap(2)</code>d file sin the dom0
filesystem (rather than in-memory). Xcp-rrdd will call <code>msync(2)</code> every 5s
to ensure the historical records have hit the disk. We should use the same
on-disk format as RRDtool (or as close to it as makes sense) because it has
already been optimised to minimise the amount of I/O.</p><p>Xapi will be modified to run a crash-dump analyser program <code>xen-crash-analyse</code>.</p><p><code>xen-crash-analyse</code> will:</p><ul><li>parse the Xen and dom0 stacks and diagnose whether<ul><li>the dom0 kernel was panic&rsquo;ed by <code>o2cb</code></li><li>the Xen watchdog was fired by <code>xhad</code></li><li>anything else: this would indicate a bug that should be reported</li></ul></li><li>in cases where the system was fenced by <code>o2cb</code> or <code>xhad</code> then the analyser<ul><li>will read the archived RRDs and look for recent evidence of a path failure
or of a bad configuration (i.e. one where the total number of paths is 1)</li><li>will parse the <code>xhad.log</code> and look for evidence of heartbeats &ldquo;approaching
timeout&rdquo;</li></ul></li></ul><p>TODO: depending on what information we can determine from the analyser, we
will want to record some of it in the <code>Host_crash_dump</code> database table.</p><p>XenCenter will be modified to explain why the host crashed and explain what
the user should do to fix it, specifically:</p><ul><li>if the host crashed for no obvious reason then consider this a software
bug and recommend a bugtool/system-status-report is taken and uploaded somewhere</li><li>if the host crashed because of <code>o2cb</code> or <code>xhad</code> then either<ul><li>if there is evidence of path failures in the RRDs: recommend the user
increase the number of paths or investigate whether some of the equipment
(NICs or switches or HBAs or SANs) is unreliable</li><li>if there is evidence of insufficient paths: recommend the user add more
paths</li></ul></li></ul><h1 id=network-configuration>Network configuration</h1><p>The documentation should strongly recommend</p><ul><li>the management network is bonded</li><li>the management network is dedicated i.e. used only for management traffic
(including heartbeats)</li><li>the OCFS2 storage is multipathed</li></ul><p><code>xcp-networkd</code> will be modified to change the behaviour of the DHCP client.
Currently the <code>dhclient</code> will wait for a response and eventually background
itself. This is a big problem since DHCP can reset the hostname, and this can
break <code>o2cb</code>. Therefore we must insist that <code>PIF.reconfigure_ip</code> becomes
fully synchronous, supporting timeout and cancellation. Once the call returns
&ndash; whether through success or failure &ndash; there must not be anything in the
background which will change the system&rsquo;s hostname.</p><p>TODO: figure out whether we need to request &ldquo;maintenance mode&rdquo; for hostname
changes.</p><h1 id=maintenance-mode>Maintenance mode</h1><p>The purpose of &ldquo;maintenance mode&rdquo; is to take a host out of service and leave
it in a state where it&rsquo;s safe to fiddle with it without affecting services
in VMs.</p><p>XenCenter currently does the following:</p><ul><li><code>Host.disable</code>: prevents new VMs starting here</li><li>makes a list of all the VMs running on the host</li><li><code>Host.evacuate</code>: move the running VMs somewhere else</li></ul><p>The problems with maintenance mode are:</p><ul><li>it&rsquo;s not safe to fiddle with the host network configuration with storage
still attached. For NFS this risks deadlocking the SR. For OCFS2 this
risks fencing the host.</li><li>it&rsquo;s not safe to fiddle with the storage or network configuration if HA
is running because the host will be fenced. It&rsquo;s not safe to disable fencing
unless we guarantee to reboot the host on exit from maintenance mode.</li></ul><p>We should also</p><ul><li><code>PBD.unplug</code>: all storage. This allows the network to be safely reconfigured.
If the network is configured when NFS storage is plugged then the SR can
permanently deadlock; if the network is configured when OCFS2 storage is
plugged then the host can crash.</li></ul><p>TODO: should we add a <code>Host.prepare_for_maintenance</code> (better name TBD)
to take care of all this without XenCenter having to script it. This would also
help CLI and powershell users do the right thing.</p><p>TODO: should we insist that the host is rebooted to leave maintenance
mode? This would make maintenance mode more reliable and allow us to integrate
maintenance mode with xHA (where maintenance mode is a &ldquo;staged reboot&rdquo;)</p><p>TODO: should we leave all clusters as part of maintenance mode? We
probably need to do this to avoid fencing.</p><h1 id=walk-through-adding-ocfs2-storage>Walk-through: adding OCFS2 storage</h1><p>Assume you have an existing Pool of 2 hosts. First the client will set up
the O2CB cluster, choosing where to put the global heartbeat volume. The
client should check that the I/O paths have all been setup correctly with
bonding and multipath and prompt the user to fix any obvious problems.</p><p><img alt="The client enables O2CB and then creates an SR" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/ocfs2/o2cb-enable-external.svg style=height:auto;width:auto></p><p>Internally within <code>Pool.enable_o2cb</code> Xapi will set up the cluster metadata
on every host in the pool:</p><p><img alt="Xapi creates the cluster configuration and each host updates its metadata" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/ocfs2/o2cb-enable-internal1.svg style=height:auto;width:auto></p><p>At this point all hosts have in-sync <code>cluster.conf</code> files but all cluster
services are disabled. We also have <code>requires_mainenance=true</code> on all
<code>Membership</code> entries and the global <code>Cluster</code> has <code>enabled=false</code>.
The client will now try to enable the cluster with <code>Cluster.enable</code>:</p><p><img alt="Xapi enables the cluster software on all hosts" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/ocfs2/o2cb-enable-internal2.svg style=height:auto;width:auto></p><p>Now all hosts are in the cluster and the SR can be created using the standard
SM APIs.</p><h1 id=walk-through-remove-a-host>Walk-through: remove a host</h1><p>Assume you have an existing Pool of 2 hosts with <code>o2cb</code> clustering enabled
and at least one <code>ocfs2</code> filesystem mounted. If the host is online then
<code>XenAPI:Pool.eject</code> will:</p><p><img alt="Xapi ejects a host from the pool" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/ocfs2/pool-eject.svg style=height:auto;width:auto></p><p>Note that:</p><ul><li>All hosts will have modified their <code>o2cb</code> <code>cluster.conf</code> to comment out
the former host</li><li>The <code>Membership</code> table still remembers the node number of the ejected host&ndash;
this cannot be re-used until the SR is taken down for maintenance.</li><li>All hosts can see the difference between their current <code>cluster.conf</code>
and the one they would use if they restarted the cluster service, so all
hosts report that the cluster must be taken offline i.e. <code>requires_maintence=true</code>.</li></ul><h1 id=summary-of-the-impact-on-the-admin>Summary of the impact on the admin</h1><p>OCFS2 is fundamentally a different type of storage to all existing storage
types supported by xapi. OCFS2 relies upon O2CB, which provides
<a href=/new-docs/toolstack/features/HA/index.html>Host-level High Availability</a>. All HA implementations
(including O2CB and <code>xhad</code>) impose restrictions on the server admin to
prevent unnecessary host &ldquo;fencing&rdquo; (i.e. crashing). Once we have OCFS2 as
a feature, we will have to live with these restrictions which previously only
applied when HA was explicitly enabled. To reduce complexity we will not try
to enforce restrictions only when OCFS2 is being used or is likely to be used.</p><h2 id=impact-even-if-not-using-ocfs2>Impact even if not using OCFS2</h2><ul><li>&ldquo;Maintenance mode&rdquo; now includes detaching all storage.</li><li>Host network reconfiguration can only be done in maintenance mode</li><li>XenServer HA enable takes longer</li><li>XenServer HA failure detection takes longer</li><li>Network configuration with DHCP must be fully synchronous i.e. it wil block
until the DHCP server responds. On a timeout, the change will not be made.</li></ul><h2 id=impact-when-using-ocfs2>Impact when using OCFS2</h2><ul><li>Sometimes a host will not be able to join the pool without taking the
pool into maintenance mode</li><li>Every VM will have to be XSM&rsquo;ed (is that a verb?) to the new OCFS2 storage.
This means that VMs with more than 2 snapshots will have their snapshots
deleted; it means you need to provision another storage target, temporarily
doubling your storage needs; and it will take a long time.</li><li>There will now be 2 different reasons why a host has fenced which the
admin needs to understand.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=patches-in-vdis>patches in VDIs</h1><p>&ldquo;Patches&rdquo; are signed binary blobs which can be queried and applied.
They are stored in the dom0 filesystem under <code>/var/patch</code>. Unfortunately
the patches can be quite large &ndash; imagine a repo full of RPMs &ndash; and
the dom0 filesystem is usually quite small, so it can be difficult
to upload and apply some patches.</p><p>Instead of writing patches to the dom0 filesystem, we shall write them
to disk images (VDIs) instead. We can then take advantage of features like</p><ul><li>shared storage</li><li>cross-host <code>VDI.copy</code></li></ul><p>to manage the patches.</p><h1 id=xenapi-changes>XenAPI changes</h1><ol><li><p>Add a field <code>pool_patch.VDI</code> of type <code>Ref(VDI)</code>. When a new patch is
stored in a VDI, it will be referenced here. Older patches and cleaned
patches will have invalid references here.</p></li><li><p>The HTTP handler for uploading patches will choose an SR to stream the
patch into. It will prefer to use the <code>pool.default_SR</code> and fall back
to choosing an SR on the master whose driver supports the <code>VDI_CLONE</code>
capability: we want the ability to fast clone patches, one per host
concurrently installing them. A VDI will be created whose size is 4x
the apparent size of the patch, defaulting to 4GiB if we have no size
information (i.e. no <code>content-length</code> header)</p></li><li><p><code>pool_patch.clean_on_host</code> will be deprecated. It will still try to
clean a patch <em>from the local filesystem</em> but this is pointless for
the new VDI patch uploads.</p></li><li><p><code>pool_patch.clean</code> will be deprecated. It will still try to clean a patch
from the <em>local filesystem</em> of the master but this is pointless for the
new VDI patch uploads.</p></li><li><p><code>pool_patch.pool_clean</code> will be deprecated. It will destroy any associated
patch VDI. Users will be encouraged to call <code>VDI.destroy</code> instead.</p></li></ol><h1 id=changes-beneath-the-xenapi>Changes beneath the XenAPI</h1><ol><li><p><code>pool_patch</code> records will only be deleted if both the <code>filename</code> field
refers to a missing file on the master <em>and</em> the <code>VDI</code> field is a dangling
reference</p></li><li><p>Patches stored in VDIs will be stored within a filesystem, like we used
to do with suspend images. This is needed because (a) we want to execute
the patches and block devices cannot be executed; and (b) we can use
spare space in the VDI as temporary scratch space during the patch
application process. Within the VDI we will call patches <code>patch</code> rather
than using a complicated filename.</p></li><li><p>When a host wishes to apply a patch it will call <code>VDI.copy</code> to duplicate
the VDI to a locally-accessible SR, mount the filesystem and execute it.
If the patch is still in the master&rsquo;s dom0 filesystem then it will fall
back to the HTTP handler.</p></li></ol><h1 id=summary-of-the-impact-on-the-admin>Summary of the impact on the admin</h1><ul><li>There will no longer be a size limit on hotfixes imposed by the mechanism
itself.</li><li>There must be enough free space in an SR connected to the host to be able
to apply a patch on that host.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=pci-passthrough-support>PCI passthrough support</h1><h2 id=introduction>Introduction</h2><p>GPU passthrough is already available in XAPI, this document proposes to also
offer passthrough for all PCI devices through XAPI.</p><h2 id=design-proposal>Design proposal</h2><p>New methods for PCI object:</p><ul><li><p><code>PCI.enable_dom0_access</code></p></li><li><p><code>PCI.disable_dom0_access</code></p></li><li><p><code>PCI.get_dom0_access_status</code>: compares the outputs of <code>/opt/xensource/libexec/xen-cmdline</code>
and <code>/proc/cmdline</code> to produce one of the four values that can be currently contained
in the <code>PGPU.dom0_access</code> field:</p><ul><li>disabled</li><li>disabled_on_reboot</li><li>enabled</li><li>enabled_on_reboot</li></ul><p>How do determine the expected dom0 access state:
If the device id is present in both <code>pciback.hide</code> of <code>/proc/cmdline</code> and <code>xen-cmdline</code>: <code>enabled</code>
If the device id is present not in both <code>pciback.hide</code> of <code>/proc/cmdline</code> and <code>xen-cmdline</code>: <code>disabled</code>
If the device id is present in the <code>pciback.hide</code> of <code>/proc/cmdline</code> but not in the one of <code>xen-cmdline</code>: <code>disabled_on_reboot</code>
If the device id is not present in the <code>pciback.hide</code> of <code>/proc/cmdline</code> but is in the one of <code>xen-cmdline</code>: <code>enabled_on_reboot</code></p><p>A function rather than a field makes the data always accurate and even accounts for
changes made by users outside XAPI, directly through <code>/opt/xensource/libexec/xen-cmdline</code></p></li></ul><p>With these generic methods available, the following field and methods will be <em>deprecated</em>:</p><ul><li><code>PGPU.enable_dom0_access</code></li><li><code>PGPU.disable_dom0_access</code></li><li><code>PGPU.dom0_access</code> (DB field)</li></ul><p>They would still be usable and up to date with the same info as for the PCI methods.</p><h2 id=test-cases>Test cases</h2><ul><li><p>hide a PCI:</p><ul><li>call <code>PCI.disable_dom0_access</code> on an <code>enabled</code> PCI</li><li>check the PCI goes in state <code>disabled_on_reboot</code></li><li>reboot the host</li><li>check the PCI goes in state <code>disabled</code></li></ul></li><li><p>unhide a PCI:</p><ul><li>call <code>PCI.enable_dom0_access</code> on an <code>disabled</code> PCI</li><li>check the PCI goes in state <code>enabled_on_reboot</code></li><li>reboot the host</li><li>check the PCI goes in state <code>enabled</code></li></ul></li><li><p>get a PCI dom0 access state:</p><ul><li>on a <code>enabled</code> PCI, make sure the <code>get_dom0_access_status</code> returns <code>enabled</code></li><li>hide the PCI</li><li>make sure the <code>get_dom0_access_status</code> returns <code>disabled_on_reboot</code></li><li>reboot</li><li>make sure the <code>get_dom0_access_status</code> returns <code>disabled</code></li><li>unhide the PCI</li><li>make sure the <code>get_dom0_access_status</code> returns <code>enabled_on_reboot</code></li><li>reboot</li><li>make sure the <code>get_dom0_access_status</code> returns <code>enabled</code></li></ul></li><li><p>Check PCI/PGPU dom0 access coherence:</p><ul><li>hide a PCI belonging to a PGPU and make sure both states remains coherent at every step</li><li>unhide a PCI belonging to a PGPU and make sure both states remains coherent at every step</li><li>hide a PGPU and make sure its and its PCI&rsquo;s states remains coherent at every step</li><li>unhide a PGPU and make sure its and its PCI&rsquo;s states remains coherent at every step</li></ul></li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=pool-wide-ssh>Pool-wide SSH</h1><h2 id=background>Background</h2><p>The SMAPIv3 plugin architecture requires that storage plugins are able to work
in the absence of xapi. Amongst other benefits, this allows them to be tested
in isolation, are able to be shared more widely than just within the XenServer
community and will cause less load on xapi&rsquo;s database.</p><p>However, many of the currently existing SMAPIv1 backends require inter-host
operations to be performed. This is achieved via the use of the Xen-API call
&lsquo;host.call_plugin&rsquo;, which allows an API user to execute a pre-installed plugin
on any pool member. This is important for operations such as coalesce / snapshot
where the active data path for a VM somewhere in the pool needs to be refreshed
in order to complete the operation. In order to use this, the RPM in which the
SM backend lives is used to deliver a plugin script into /etc/xapi.d/plugins,
and this executes the required function when the API call is made.</p><p>In order to support these use-cases without xapi running, a new mechanism needs
to be provided to allow the execution of required functionality on remote hosts.
The canonical method for remotely executing scripts is ssh - the secure shell.
This design proposal is setting out how xapi might manage the public and
private keys to enable passwordless authentication of ssh sessions between all
hosts in a pool.</p><h2 id=modifications-to-the-host>Modifications to the host</h2><p>On firstboot (and after being ejected), the host should generate a
host key (already done I believe), and an authentication key for the
user (root/xapi?).</p><h2 id=modifications-to-xapi>Modifications to xapi</h2><p>Three new fields will be added to the host object:</p><ul><li><p><code>host.ssh_public_host_key : string</code>: This is the host key that identifies the host
during the initial ssh key exchange protocol. This should be added to the
&lsquo;known_hosts&rsquo; field of any other host wishing to ssh to this host.</p></li><li><p><code>host.ssh_public_authentication_key : string</code>: This field is the public
key used for authentication when sshing from the root account on that host -
host A. This can be added to host B&rsquo;s <code>authorized_keys</code> file in order to
allow passwordless logins from host A to host B.</p></li><li><p><code>host.ssh_ready : bool</code>: A boolean flag indicating that the configuration
files in use by the ssh server/client on the host are up to date.</p></li></ul><p>One new field will be added to the pool record:</p><ul><li><code>pool.revoked_authentication_keys : string list</code>: This field records all
authentication keys that have been used by hosts in the past. It is updated
when a host is ejected from the pool.</li></ul><h3 id=pool-join>Pool Join</h3><p>On pool join, the master creates the record for the new host and populates the
two public key fields with values supplied by the joining host. It then sets
the <code>ssh_ready</code> field on all other hosts to <code>false</code>.</p><p>On each host in the pool, a thread is watching for updates to the
<code>ssh_ready</code> value for the local host. When this is set to false, the host
then adds the keys from xapi&rsquo;s database to the appropriate places in the ssh
configuration files and restarts sshd. Once this is done, the host sets the
<code>ssh_ready</code> field to &rsquo;true&rsquo;</p><h3 id=pool-eject>Pool Eject</h3><p>On pool eject, the host&rsquo;s ssh_public_host_key is lost, but the authetication key is added to a list of revoked keys on the pool object. This allows all other hosts to remove the key from the authorized_keys list when they next sync, which in the usual case is immediately the database is modified due to the event watch thread. If the host is offline though, the authorized_keys file will be updated the next time the host comes online.</p><h2 id=questions>Questions</h2><ul><li>Do we want a new user? e.g. &lsquo;xapi&rsquo; - how would we then use this user to execute privileged things? setuid binaries?</li><li>Is keeping the revoked_keys list useful? If we &lsquo;control the world&rsquo; of the authorized_keys file, we could just remove anything that&rsquo;s currently in there that xapi doesn&rsquo;t know about</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=process-events-from-xenopsd-in-a-timely-manner>Process events from xenopsd in a timely manner</h1><h1 id=background>Background</h1><p>There is a significant delay between the VM being unpaused and XAPI reporting it
as started during a bootstorm.
It can happen that the VM is able to send UDP packets already, but XAPI still reports it as not started for minutes.</p><p>XAPI currently processes all events from xenopsd in a single thread, the unpause
events get queued up behind a lot of other events generated by the already
running VMs.</p><p>We need to ensure that unpause events from xenopsd get processed in a timely
manner, even if XAPI is busy processing other events.</p><h1 id=timely-processing-of-events>Timely processing of events</h1><p>If we process the events in a Round-Robin fashion then <code>unpause</code> events are reported in a timely fashion.
We need to ensure that events operating on the same VM are not processed in parallel.</p><p>Xenopsd already has code that does exactly this, the purpose of the <a href=https://github.com/xapi-project/xenopsd/pull/337 rel=external target=_blank>xapi-work-queues refactoring PR</a> is to
reuse this code in XAPI by creating a shared package between xenopsd and xapi: <code>xapi-work-queues</code>.</p><h1 id=xapi-work-queues>xapi-work-queues</h1><p>From the documentation of the new <a href=https://edwintorok.github.io/xapi-work-queues/Xapi_work_queues.html rel=external target=_blank>Worker Pool interface</a>:</p><p>A worker pool has a limited number of worker threads.
Each worker pops one tagged item from the queue in a round-robin fashion.
While the item is executed the tag temporarily doesn&rsquo;t participate in round-robin scheduling.
If during execution more items get queued with the same tag they get redirected to a private queue.
Once the item finishes execution the tag will participate in RR scheduling again.</p><p>This ensures that items with the same tag do not get executed in parallel,
and that a tag with a lot of items does not starve the execution of other tags.</p><p>The XAPI side of the changes will <a href="https://github.com/edwintorok/xen-api/commit/b367bf86d3af4f773db9bf5d1500a4dec0f99bfa?diff=unified#diff-344dc1d17c4663add7fe5500813feef2" rel=external target=_blank>look like this</a></p><p>Known limitations: The active per-VM events should be a small number, this is already ensured in the <code>push_with_coalesce</code> / <code>should_keep</code> code on the <a href=https://github.com/xapi-project/xenopsd/blob/master/lib/xenops_server.ml#L441 rel=external target=_blank>xenopsd side</a>. Events to XAPI from xenopsd should already arrive coalesced.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (xenserver 6.5 sp1)</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/12>#12</a></td></tr></table></header><h1 id=rdp-control>RDP control</h1><h3 id=purpose>Purpose</h3><p>To administer guest VMs it can be useful to connect to them over Remote Desktop Protocol (RDP). XenCenter supports this; it has an integrated RDP client.</p><p>First it is necessary to turn on the RDP service in the guest.</p><p>This can be controlled from XenCenter. Several layers are involved. This description starts in the guest and works up the stack to XenCenter.</p><p>This feature was completed in the first quarter of 2015, and released in Service Pack 1 for XenServer 6.5.</p><h3 id=the-guest-agent>The guest agent</h3><p>The XenServer guest agent installed in Windows VMs can turn the RDP service on and off, and can report whether it is running.</p><p>The guest agent is at <a href=https://github.com/xenserver/win-xenguestagent rel=external target=_blank>https://github.com/xenserver/win-xenguestagent</a></p><p>Interaction with the agent is done through some Xenstore keys:</p><p>The guest agent running in domain N writes two xenstore nodes when it starts up:</p><ul><li><code>/local/domain/N/control/feature-ts = 1</code></li><li><code>/local/domain/N/control/feature-ts2 = 1</code></li></ul><p>This indicates support for the rest of the functionality described below.</p><p>(The &ldquo;&mldr;ts2&rdquo; flag is new for this feature; older versions of the guest agent wrote the &ldquo;&mldr;ts&rdquo; flag and had support for only a subset of the functionality (no firewall modification), and had a bug in updating <code>.../data/ts</code>.)</p><p>To indicate whether RDP is running, the guest agent writes the string &ldquo;1&rdquo; (running) or &ldquo;0&rdquo; (disabled) to xenstore node</p><p><code>/local/domain/N/data/ts</code>.</p><p>It does this on start-up, and also in response to the deletion of that node.</p><p>The guest agent also watches xenstore node <code>/local/domain/N/control/ts</code> and it turns RDP on and off in response to &ldquo;1&rdquo; or &ldquo;0&rdquo; (respectively) being written to that node. The agent acknowledges the request by deleting the node, and afterwards it deletes <code>local/domain/N/data/ts</code>, thus triggering itself to update that node as described above.</p><p>When the guest agent turns the RDP service on/off, it also modifies the standard Windows firewall to allow/forbid incoming connections to the RDP port. This is the same as the firewall change that happens automatically when the RDP service is turned on/off through the standard Windows GUI.</p><h3 id=xapi-etc>XAPI etc.</h3><p>xenopsd sets up watches on xenstore nodes including the <code>control</code> tree and <code>data/ts</code>, and prompts xapi to react by updating the relevant VM guest metrics record, which is available through a XenAPI call.</p><p>XenAPI includes a new message (function call) which can be used to ask the guest agent to turn RDP on and off.</p><p>This is <code>VM.call_plugin</code> (analogous to <code>Host.call_plugin</code>) in the hope that it can be used for other purposes in the future, even though for now it does not really call a plugin.</p><p>To use it, supply <code>plugin="guest-agent-operation"</code> and either <code>fn="request_rdp_on"</code> or <code>fn="request_rdp_off"</code>.</p><p>See <a href=http://xapi-project.github.io/xen-api/classes/vm.html rel=external target=_blank>http://xapi-project.github.io/xen-api/classes/vm.html</a></p><p>The function strings are named with &ldquo;request&rdquo; (rather than, say, &ldquo;enable_rdp&rdquo; or &ldquo;turn_rdp_on&rdquo;) to make it clear that xapi only makes a request of the guest: when one of these calls returns successfully this means only that the appropriate string (1 or 0) was written to the <code>control/ts</code> node and it is up to the guest whether it responds.</p><h3 id=xencenter>XenCenter</h3><h4 id=behaviour-on-older-xenserver-versions-that-do-not-support-rdp-control>Behaviour on older XenServer versions that do not support RDP control</h4><p>Note that the current behaviour depends on some global options: &ldquo;Enable Remote Desktop console scanning&rdquo; and &ldquo;Automatically switch to the Remote Desktop console when it becomes available&rdquo;.</p><ol><li>When tools are not installed:<ul><li>As of XenCenter 6.5, the RDP button is absent.</li></ul></li><li>When tools are installed but RDP is not switched on in the guest:<ol><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is on:<ul><li>The RDP button is present but greyed out. (It seems to sometimes read &ldquo;Switch to Remote Desktop&rdquo; and sometimes read &ldquo;Looking for guest console&mldr;&rdquo;: I haven&rsquo;t yet worked out the difference).</li><li>We scan the RDP port to detect when RDP is turned on</li></ul></li><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is off:<ul><li>The RDP button is enabled and reads &ldquo;Switch to Remote Desktop&rdquo;</li></ul></li></ol></li><li>When tools are installed and RDP is switched on in the guest:<ol><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is on:<ul><li>The RDP button is enabled and reads &ldquo;Switch to Remote Desktop&rdquo;</li><li>If &ldquo;Automatically switch&rdquo; is on, we switch to RDP immediately we detect it</li></ul></li><li>If &ldquo;Enable Remote Desktop console scanning&rdquo; is off:<ul><li>As above, the RDP button is enabled and reads &ldquo;Switch to Remote Desktop&rdquo;</li></ul></li></ol></li></ol><h4 id=new-behaviour-on-xenserver-versions-that-support-rdp-control>New behaviour on XenServer versions that support RDP control</h4><ol><li>This new XenCenter behaviour is only for XenServer versions that support RDP control, with guests with the new guest agent: behaviour must be unchanged if the server or guest-agent is older.</li><li>There should be no change in the behaviour for Linux guests, either PV or HVM varieties: this must be tested.</li><li>We should never scan the RDP port; instead we should watch for a change in the relevant variable in guest_metrics.</li><li>The XenCenter option &ldquo;Enable Remote Desktop console scanning&rdquo; should change to read &ldquo;Enable Remote Desktop console scanning (XenServer 6.5 and earlier)&rdquo;</li><li>The XenCenter option &ldquo;Automatically switch to the Remote Desktop console when it becomes available&rdquo; should be enabled even when &ldquo;Enable Remote Desktop console scanning&rdquo; is off.</li><li>When tools are not installed:<ul><li>As above, the RDP button should be absent.</li></ul></li><li>When tools are installed but RDP is not switched on in the guest:<ul><li>The RDP button should be enabled and read &ldquo;Turn on Remote Desktop&rdquo;</li><li>If pressed, it should launch a dialog with the following wording: &ldquo;Would you like to turn on Remote Desktop in this VM, and then connect to it over Remote Desktop? [Yes] [No]&rdquo;</li><li>That button should turn on RDP, wait for RDP to become enabled, and switch to an RDP connection. It should do this even if &ldquo;Automatically switch&rdquo; is off.</li></ul></li><li>When tools are installed and RDP is switched on in the guest:<ul><li>The RDP button should be enabled and read &ldquo;Switch to Remote Desktop&rdquo;</li><li>If &ldquo;Automatically switch&rdquo; is on, we should switch to RDP immediately</li><li>There is no need for us to provide UI to switch RDP off again</li></ul></li><li>We should also test the case where RDP has been switched on in the guest before the tools are installed.</li></ol><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7,0)</span></td></tr></table></header><h1 id=rrdd-archival-redesign>RRDD archival redesign</h1><h2 id=introduction>Introduction</h2><p>Current problems with rrdd:</p><ul><li>rrdd stores knowledge about whether it is running on a master or a slave</li></ul><p>This determines the host to which rrdd will archive a VM&rsquo;s rrd when the VM&rsquo;s
domain disappears - rrdd will always try to archive to the master. However,
when a host joins a pool as a slave rrdd is not restarted so this knowledge is
out of date. When a VM shuts down on the slave rrdd will archive the rrd
locally. When starting this VM again the master xapi will attempt to push any
locally-existing rrd to the host on which the VM is being started, but since
no rrd archive exists on the master the slave rrdd will end up creating a new
rrd and the previous rrd will be lost.</p><ul><li>rrdd handles rebooting VMs unpredictably</li></ul><p>When rebooting a VM, there is a chance rrdd will attempt to update that VM&rsquo;s rrd
during the brief period when there is no domain for that VM. If this happens,
rrdd will archive the VM&rsquo;s rrd to the master, and then create a new rrd for the
VM when it sees the new domain. If rrdd doesn&rsquo;t attempt to update that VM&rsquo;s rrd
during this period, rrdd will continue to add data for the new domain to the old
rrd.</p><h2 id=proposal>Proposal</h2><p>To solve these problems, we will remove some of the intelligence from rrdd and
make it into more of a slave process of xapi. This will entail removing all
knowledge from rrdd of whether it is running on a master or a slave, and also
modifying rrdd to only start monitoring a VM when it is told to, and only
archiving an rrd (to a specified address) when it is told to. This matches the
way xenopsd only manages domains which it has been told to manage.</p><h2 id=design>Design</h2><p>For most VM lifecycle operations, xapi and rrdd processes (sometimes across more
than one host) cooperate to start or stop recording a VM&rsquo;s metrics and/or to
restore or backup the VM&rsquo;s archived metrics. Below we will describe, for each
relevant VM operation, how the VM&rsquo;s rrd is currently handled, and how we propose
it will be handled after the redesign.</p><h4 id=vmdestroy>VM.destroy</h4><p>The master xapi makes a remove_rrd call to the local rrdd, which causes rrdd to
to delete the VM&rsquo;s archived rrd from disk. This behaviour will remain unchanged.</p><h4 id=vmstart_on-and-vmresume_on>VM.start(_on) and VM.resume(_on)</h4><p>The master xapi makes a push_rrd call to the local rrdd, which causes rrdd to
send any locally-archived rrd for the VM in question to the rrdd of the host on
which the VM is starting. This behaviour will remain unchanged.</p><h4 id=vmshutdown-and-vmsuspend>VM.shutdown and VM.suspend</h4><p>Every update cycle rrdd compares its list of registered VMs to the list of
domains actually running on the host. Any registered VMs which do not have a
corresponding domain have their rrds archived to the rrdd running on the host
believed to be the master. We will change this behaviour by stopping rrdd from
doing the archiving itself; instead we will expose a new function in rrdd&rsquo;s
interface:</p><div class="highlight wrap-code"><pre tabindex=0><code>val archive_rrd : vm_uuid:string -&gt; remote_address:string -&gt; unit</code></pre></div><p>This will cause rrdd to remove the specified rrd from its table of registered
VMs, and archive the rrd to the specified host. When a VM has finished shutting
down or suspending, the xapi process on the host on which the VM was running
will call archive_rrd to ask the local rrdd to archive back to the master rrdd.</p><h4 id=vmreboot>VM.reboot</h4><p>Removing rrdd&rsquo;s ability to automatically archive the rrds for disappeared
domains will have the bonus effect of fixing how the rrds of rebooting VMs are
handled, as we don&rsquo;t want the rrds of rebooting VMs to be archived at all.</p><h4 id=vmcheckpoint>VM.checkpoint</h4><p>This will be handled automatically, as internally VM.checkpoint carries out a
VM.suspend followed by a VM.resume.</p><h4 id=vmpool_migrate-and-vmmigrate_send>VM.pool_migrate and VM.migrate_send</h4><p>The source host&rsquo;s xapi makes a migrate_rrd call to the local rrd, with a
destination address and an optional session ID. The session ID is only required
for cross-pool migration. The local rrdd sends the rrd for that VM to the
destination host&rsquo;s rrdd as an HTTP PUT. This behaviour will remain unchanged.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr></table></header><h1 id=rrdd-plugin-protocol-v2>RRDD plugin protocol v2</h1><h2 id=motivation>Motivation</h2><p>rrdd plugins currently report datasources via a shared-memory file, using the
following format:</p><div class="highlight wrap-code"><pre tabindex=0><code>DATASOURCES
000001e4
dba4bf7a84b6d11d565d19ef91f7906e
{
  &#34;timestamp&#34;: 1339685573.245,
  &#34;data_sources&#34;: {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;64.33&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;62.14&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    }
  }
}</code></pre></div><p>This format contains four main components:</p><ul><li>A constant header string</li></ul><p><code>DATASOURCES</code></p><p>This should always be present.</p><ul><li>The JSON data length, encoded as hexadecimal</li></ul><p><code>000001e4</code></p><ul><li>The md5sum of the JSON data</li></ul><p><code>dba4bf7a84b6d11d565d19ef91f7906e</code></p><ul><li>The JSON data itself, encoding the values and metadata associated with the
reported datasources.</li></ul><h3 id=example>Example</h3><div class="highlight wrap-code"><pre tabindex=0><code>{
  &#34;timestamp&#34;: 1339685573.245,
  &#34;data_sources&#34;: {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;64.33&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;value&#34;: &#34;62.14&#34;
      &#34;value_type&#34;: &#34;float&#34;,
    }
  }
}</code></pre></div><p>The disadvantage of this protocol is that rrdd has to parse the entire JSON
structure each tick, even though most of the time only the values will change.</p><p>For this reason a new protocol is proposed.</p><h2 id=protocol-v2>Protocol V2</h2><table><thead><tr><th>value</th><th>bits</th><th>format</th><th>notes</th></tr></thead><tbody><tr><td>header string</td><td>(string length)*8</td><td>string</td><td>&ldquo;DATASOURCES&rdquo; as in the V1 protocol</td></tr><tr><td>data checksum</td><td>32</td><td>int32</td><td>binary-encoded crc32 of the concatenation of the encoded timestamp and datasource values</td></tr><tr><td>metadata checksum</td><td>32</td><td>int32</td><td>binary-encoded crc32 of the metadata string (see below)</td></tr><tr><td>number of datasources</td><td>32</td><td>int32</td><td>only needed if the metadata has changed - otherwise RRDD can use a cached value</td></tr><tr><td>timestamp</td><td>64</td><td>double</td><td>Unix epoch</td></tr><tr><td>datasource values</td><td>n * 64</td><td>int64 | double</td><td>n is the number of datasources exported by the plugin, type dependent on the setting in the metadata for value_type [int64|float]</td></tr><tr><td>metadata length</td><td>32</td><td>int32</td><td></td></tr><tr><td>metadata</td><td>(string length)*8</td><td>string</td><td></td></tr></tbody></table><p>All integers/double are bigendian. The metadata will have the same JSON-based format as
in the V1 protocol, minus the timestamp and <code>value</code> key-value pair for each
datasource.</p><table><thead><tr><th>field</th><th>values</th><th>notes</th><th>required</th></tr></thead><tbody><tr><td>description</td><td>string</td><td>Description of the datasource</td><td>no</td></tr><tr><td>owner</td><td>host | vm | sr</td><td>The object to which the data relates</td><td>no, default host</td></tr><tr><td>value_type</td><td>int64 | float</td><td>The type of the datasource</td><td>yes</td></tr><tr><td>type</td><td>absolute | derive | gauge</td><td>The type of measurement being sent. Absolute for counters which are reset on reading, derive stores the derivative of the recorded values (useful for metrics which continually increase like amount of data written since start), gauge for things like temperature</td><td>no, default absolute</td></tr><tr><td>default</td><td>true | false</td><td>Whether the source is default enabled or not</td><td>no, default false</td></tr><tr><td>units</td><td><tbd></td><td>The units the data should be displayed in</td><td>no</td></tr><tr><td>min</td><td></td><td>The minimum value for the datasource</td><td>no, default -infinity</td></tr><tr><td>max</td><td></td><td>The maximum value for the datasource</td><td>no, default +infinity</td></tr></tbody></table><h3 id=example-1>Example</h3><div class="highlight wrap-code"><pre tabindex=0><code>{
  &#34;datasources&#34;: {
    &#34;memory_reclaimed&#34;: {
      &#34;description&#34;:&#34;Host memory reclaimed by squeezed&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;:&#34;int64&#34;,
      &#34;type&#34;:&#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;:&#34;B&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    },
    &#34;memory_reclaimed_max&#34;: {
      &#34;description&#34;:&#34;Host memory that could be reclaimed by squeezed&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;:&#34;int64&#34;,
      &#34;type&#34;:&#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;:&#34;B&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    },
    {
    &#34;cpu-temp-cpu0&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 0&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;: &#34;float&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    },
    &#34;cpu-temp-cpu1&#34;: {
      &#34;description&#34;: &#34;Temperature of CPU 1&#34;,
      &#34;owner&#34;:&#34;host&#34;,
      &#34;value_type&#34;: &#34;float&#34;,
      &#34;type&#34;: &#34;absolute&#34;,
      &#34;default&#34;:&#34;true&#34;,
      &#34;units&#34;: &#34;degC&#34;,
      &#34;min&#34;:&#34;-inf&#34;,
      &#34;max&#34;:&#34;inf&#34;
    }
  }
}</code></pre></div><p>The above formatting is not required, but added here for readability.</p><h2 id=reading-algorithm>Reading algorithm</h2><div class="highlight wrap-code"><pre tabindex=0><code>if header != expected_header:
    raise InvalidHeader()
if data_checksum == last_data_checksum:
    raise NoUpdate()
if data_checksum != crc32(encoded_timestamp_and_values):
    raise InvalidChecksum()
if metadata_checksum == last_metadata_checksum:
    for datasource, value in cached_datasources, values:
        update(datasource, value)
else:
    if metadata_checksum != crc32(metadata):
        raise InvalidChecksum()
    cached_datasources = create_datasources(metadata)
    for datasource, value in cached_datasources, values:
        update(datasource, value)</code></pre></div><p>This means that for a normal update, RRDD will only have to read the header plus
the first (16 + 16 + 4 + 8 + 8*n) bytes of data, where n is the number of
datasources exported by the plugin. If the metadata changes RRDD will have to
read all the data (and parse the metadata).</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr></table></header><h1 id=rrdd-plugin-protocol-v3>RRDD plugin protocol v3</h1><h2 id=motivation>Motivation</h2><p>rrdd plugins protocol v2 report datasources via shared-memory file, however it
has various limitations :</p><ul><li>metrics are unique by their names, thus it is not possible cannot have
several metrics that shares a same name (e.g vCPU usage per vm)</li><li>only number metrics are supported, for example we can&rsquo;t expose string
metrics (e.g CPU Model)</li></ul><p>Therefore, it implies various limitations on plugins and limits
<a href=https://openmetrics.io/ rel=external target=_blank>OpenMetrics</a> support for the metrics daemon.</p><p>Moreover, it may not be practical for plugin developpers and parser implementations :</p><ul><li>json implementations may not keep insersion order on maps, which can cause
issues to expose datasource values as it is sensitive to the order of the metadata map</li><li>header length is not constant and depends on datasource count, which complicates parsing</li><li>it still requires a quite advanced parser to convert between bytes and numbers according to metadata</li></ul><p>A simpler protocol is proposed, based on OpenMetrics binary format to ease plugin and parser implementations.</p><h2 id=protocol-v3>Protocol V3</h2><p>For this protocol, we still use a shared-memory file, but significantly change the structure of the file.</p><table><thead><tr><th>value</th><th>bits</th><th>format</th><th>notes</th></tr></thead><tbody><tr><td>header string</td><td>12*8=96</td><td>string</td><td>&ldquo;OPENMETRICS1&rdquo; which is one byte longer than &ldquo;DATASOURCES&rdquo;, intentionally made at 12 bytes for alignment purposes</td></tr><tr><td>data checksum</td><td>32</td><td>uint32</td><td>Checksum of the concatenation of the rest of the header (from timestamp) and the payload data</td></tr><tr><td>timestamp</td><td>64</td><td>uint64</td><td>Unix epoch</td></tr><tr><td>payload length</td><td>32</td><td>uint32</td><td>Payload length</td></tr><tr><td>payload data</td><td>8*(payload length)</td><td>binary</td><td>OpenMetrics encoded metrics data (protocol-buffers format)</td></tr></tbody></table><p>All values are big-endian.</p><p>The header size is constant (28 bytes) that implementation can rely on (read
the entire header in one go, simplify usage of memory mapping).</p><p>As opposed to protocol v2 but alike protocol v1, metadata is included along
metrics in OpenMetrics format.</p><p><code>owner</code> attribute for metric should be exposed using a OpenMetrics label instead (named <code>owner</code>).</p><p>Multiple metrics that shares the same name should be exposed under the same
Metric Family and be differenciated by labels (e.g <code>owner</code>).</p><h2 id=reading-algorithm>Reading algorithm</h2><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>if</span> header <span style=color:#f92672>!=</span> expected_header:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> InvalidHeader()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> data_checksum <span style=color:#f92672>==</span> last_data_checksum:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> NoUpdate()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> timestamp <span style=color:#f92672>==</span> last_timestamp:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> NoUpdate()
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> data_checksum <span style=color:#f92672>!=</span> crc32(concat_header_end_payload):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> InvalidChecksum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>metrics <span style=color:#f92672>=</span> parse_openmetrics(payload_data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> family <span style=color:#f92672>in</span> metrics:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> family_exists(family):
</span></span><span style=display:flex><span>        update_family(family)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>        create_family(family)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>track_removed_families(metrics)</span></span></code></pre></div><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/186>#186</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Renaming VMSS fields and APIs. API message_create superseeds vmss_create_alerts.</td></tr><tr><td><span class="label label-default">v3</span></td><td>Remove VMSS alarm_config details and use existing pool wide alarm config</td></tr><tr><td><span class="label label-default">v4</span></td><td>Renaming field from retention-value to retained-snapshots and schedule-snapshot to scheduled-snapshot</td></tr><tr><td><span class="label label-default">v5</span></td><td>Add new API task_set_status</td></tr></table></header><h1 id=schedule-snapshot-design>Schedule Snapshot Design</h1><p>The scheduled snapshot feature will utilize the existing architecture of VMPR. In terms of functionality, scheduled snapshot is basically VMPR without its archiving capability.</p><h2 id=introduction>Introduction</h2><ul><li>Schedule snapshot will be a new object in xapi as VMSS.</li><li>A pool can have multiple VMSS.</li><li>Multiple VMs can be a part of VMSS but a VM cannot be a part of multiple VMSS.</li><li>A VMSS takes VMs snapshot with type [<code>snapshot</code>, <code>checkpoint</code>, <code>snapshot_with_quiesce</code>].</li><li>VMSS takes snapshot of VMs on configured intervals:<ul><li><code>hourly</code> -> On every day, Each hour, Mins [0;15;30;45]</li><li><code>daily</code> -> On every day, Hour [0 to 23], Mins [0;15;30;45]</li><li><code>weekly</code> -> Days [<code>Monday</code>,<code>Tuesday</code>,<code>Wednesday</code>,<code>Thursday</code>,<code>Friday</code>,<code>Saturday</code>,<code>Sunday</code>], Hour[0 to 23], Mins [0;15;30;45]</li></ul></li><li>VMSS will have a limit on retaining number of VM snapshots in range [1 to 10].</li></ul><h2 id=datapath-design>Datapath Design</h2><ul><li>There will be a cron job for VMSS.</li><li>VMSS plugin will go through all the scheduled snapshot policies in the pool and check if any of them are due.</li><li>If a snapshot is due then : Go through all the VM objects in XAPI associated with this scheduled snapshot policy and create a new snapshot.</li><li>If the snapshot operation fails, create a notification alert for the event and move to the next VM.</li><li>Check if an older snapshot now needs to be deleted to comply with the retained snapshots defined in the scheduled policy.</li><li>If we need to delete any existing snapshots, delete the oldest snapshot created via scheduled policy.</li><li>Set the last-run timestamp in the scheduled policy.</li></ul><h2 id=xapi-changes>Xapi Changes</h2><p>There is a new record for VM Scheduled Snapshot with new fields.</p><p>New fields:</p><ul><li><code>name-label</code> type <code>String</code> : Name label for VMSS.</li><li><code>name-description</code> type <code>String</code> : Name description for VMSS.</li><li><code>enabled</code> type <code>Bool</code> : Enable/Disable VMSS to take snapshot.</li><li><code>type</code> type <code>Enum</code> [<code>snapshot</code>; <code>checkpoint</code>; <code>snapshot_with_quiesce</code>] : Type of snapshot VMSS takes.</li><li><code>retained-snapshots</code> type <code>Int64</code> : Number of snapshots limit for a VM, max limit is 10 and default is 7.</li><li><code>frequency</code> type <code>Enum</code> [<code>hourly</code>; <code>daily</code>; <code>weekly</code>] : Frequency of taking snapshot of VMs.</li><li><code>schedule</code> type <code>Map(String,String)</code> with (key, value) pair:<ul><li>hour : 0 to 23</li><li>min : [0;15;30;45]</li><li>days : [<code>Monday</code>,<code>Tuesday</code>,<code>Wednesday</code>,<code>Thursday</code>,<code>Friday</code>,<code>Saturday</code>,<code>Sunday</code>]</li></ul></li><li><code>last-run-time</code> type Date : DateTime of last execution of VMSS.</li><li><code>VMs</code> type VM refs : List of VMs part of VMSS.</li></ul><p>New fields to VM record:</p><ul><li><code>scheduled-snapshot</code> type VMSS ref : VM part of VMSS.</li><li><code>is-vmss-snapshot</code> type Bool : If snapshot created from VMSS.</li></ul><h2 id=new-apis>New APIs</h2><ul><li>vmss_snapshot_now (Ref vmss, Pool_Operater) -> String : This call executes the scheduled snapshot immediately.</li><li>vmss_set_retained_snapshots (Ref vmss, Int value, Pool_Operater) -> unit : Set the value of vmss retained snapshots, max is 10.</li><li>vmss_set_frequency (Ref vmss, String &ldquo;value&rdquo;, Pool_Operater) -> unit : Set the value of the vmss frequency field.</li><li>vmss_set_type (Ref vmss, String &ldquo;value&rdquo;, Pool_Operater) -> unit : Set the snapshot type of the vmss type field.</li><li>vmss_set_scheduled (Ref vmss, Map(String,String) &ldquo;value&rdquo;, Pool_Operater) -> unit : Set the vmss scheduled to take snapshot.</li><li>vmss_add_to_schedule (Ref vmss, String &ldquo;key&rdquo;, String &ldquo;value&rdquo;, Pool_Operater) -> unit : Add key value pair to VMSS schedule.</li><li>vmss_remove_from_schedule (Ref vmss, String &ldquo;key&rdquo;, Pool_Operater) -> unit : Remove key from VMSS schedule.</li><li>vmss_set_last_run_time (Ref vmss, DateTime &ldquo;value&rdquo;, Local_Root) -> unit : Set the last run time for VMSS.</li><li>task_set_status (Ref task, status_type &ldquo;value&rdquo;, READ_ONLY) -> unit : Set the status of task owned by same user, Pool_Operator can set status for any tasks.</li></ul><h2 id=new-clis>New CLIs</h2><ul><li>vmss-create (required : &ldquo;name-label&rdquo;;&ldquo;type&rdquo;;&ldquo;frequency&rdquo;, optional : &ldquo;name-description&rdquo;;&ldquo;enabled&rdquo;;&ldquo;schedule:&rdquo;;&ldquo;retained-snapshots&rdquo;) -> unit : Creates VM scheduled snapshot.</li><li>vmss-destroy (required : uuid) -> unit : Destroys a VM scheduled snapshot.</li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.6)</span></td></tr></table></header><h1 id=smapiv3>SMAPIv3</h1><p>Xapi accesses storage through &ldquo;plugins&rdquo; which currently use a protocol
called &ldquo;SMAPIv1&rdquo;. This protocol has a number of problems:</p><ol><li><p>the protocol has many missing features, and this leads to people
using the XenAPI from within a plugin, which is racy, difficult to
get right, unscalable and makes component testing impossible.</p></li><li><p>the protocol expects plugin authors to have a deep knowledge of the
Xen storage datapath (<code>tapdisk</code>, <code>blkback</code> etc) <em>and</em> the storage.</p></li><li><p>the protocol is undocumented.</p></li></ol><p>We shall create a new revision of the protocol (&ldquo;SMAPIv3&rdquo;) to address these
problems.</p><p>The following diagram shows the new control plane:</p><p><img alt="Storage control plane" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/smapiv3/smapiv3.png style=height:auto;width:auto></p><p>Requests from xapi are filtered through the existing <code>storage_access</code>
layer which is responsible for managing the mapping between VM VBDs and
VDIs.</p><p>Each plugin is represented by a named queue, with APIs for</p><ul><li>querying the state of each queue</li><li>explicitly cancelling or replying to messages</li></ul><p>Legacy SMAPIv1 plugins will be processed via the existing <code>storage_access.SMAPIv1</code>
module. Newer SMAPIv3 plugins will be handled by a new <code>xapi-storage-script</code>
service.</p><p>The SMAPIv3 APIs will be defined in an IDL format in a separate repo.</p><h1 id=xapi-storage-script>xapi-storage-script</h1><p>The <code>xapi-storage-script</code> will run as a service and will</p><ul><li>use <code>inotify</code> to monitor a well-known path in dom0</li><li>when a directory is created, check whether it contains storage plugins by
executing a <code>Plugin.query</code></li><li>assuming the directory contains plugins, it will register the queue name
and start listening for messages</li><li>when messages from <code>xapi</code> or the CLI are received, it will generate the SMAPIv3
.json message and fork the relevant script.</li></ul><h1 id=smapiv3-idl>SMAPIv3 IDL</h1><p>The IDL will support</p><ul><li>documentation for all functions, parameters and results<ul><li>this will be extended to be a XenAPI-style versioning scheme in future</li></ul></li><li>generating hyperlinked HTML documentation, published on github</li><li>generating libraries for python and OCaml<ul><li>the libraries will include marshalling, unmarshalling, type-checking
and command-line parsing and help generation</li></ul></li></ul><h1 id=diagnostic-tools>Diagnostic tools</h1><p>It will be possible to view the contents of the queue associated with any
plugin, and see whether</p><ul><li>the queue is being served or not (perhaps the <code>xapi-storage-script</code> has
crashed)</li><li>there are unanswered messages (perhaps one of the messages has caused
a deadlock in the implementation?)</li></ul><p>It will be possible to</p><ul><li>delete/clear queues/messages</li><li>download a message-sequence chart of the last N messages for inclusion in
bugtools.</li></ul><h1 id=anatomy-of-a-plugin>Anatomy of a plugin</h1><p>The following diagram shows what a plugin would look like:</p><p><img alt="Anatomy of a plugin" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/smapiv3/plugin.png style=height:auto;width:auto></p><h1 id=the-smapiv3>The SMAPIv3</h1><p>Please read <a href=https://xapi-project.github.io/xapi-storage rel=external target=_blank>the current SMAPIv3 documentation</a>.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=specifying-emulated-pci-devices>Specifying Emulated PCI Devices</h1><h3 id=background-and-goals>Background and goals</h3><p>At present (early March 2015) the datamodel defines a VM as having a &ldquo;platform&rdquo; string-string map, in which two keys are interpreted as specifying a PCI device which should be emulated for the VM. Those keys are &ldquo;device_id&rdquo; and &ldquo;revision&rdquo; (with int values represented as decimal strings).</p><p>Limitations:</p><ul><li>Hardcoded defaults are used for the vendor ID and all other parameters except device_id and revision.</li><li>Only one emulated PCI device can be specified.</li></ul><p>When instructing qemu to emulate PCI devices, qemu accepts twelve parameters for each device.</p><p>Future guest-agent features rely on additional emulated PCI devices. We cannot know in advance the full details of all the devices that will be needed, but we can predict some.</p><p>We need a way to configure VMs such that they will be given additional emulated PCI devices.</p><h3 id=design>Design</h3><p>In the datamodel, there will be a new type of object for emulated PCI devices.</p><p>Tentative name: &ldquo;emulated_pci_device&rdquo;</p><p>Fields to be passed through to qemu are the following, all static read-only, and all ints except devicename:</p><ul><li>devicename (string)</li><li>vendorid</li><li>deviceid</li><li>command</li><li>status</li><li>revision</li><li>classcode</li><li>headertype</li><li>subvendorid</li><li>subsystemid</li><li>interruptline</li><li>interruptpin</li></ul><p>We also need a &ldquo;built_in&rdquo; flag: see below.</p><p>Allow creation of these objects through the API (and CLI).</p><p>(It would be nice, but by no means essential, to be able to create one by specifying an existing one as a basis, along with one or more altered fields, e.g. &ldquo;Make a new one just like that existing one except with interruptpin=9.&rdquo;)</p><p>Create some of these devices to be defined as standard in XenServer, along the same lines as the VM templates. Those ones should have built_in=true.</p><p>Allow destruction of these objects through the API (and CLI), but not if they are in use or if they have built_in=true.</p><p>A VM will have a list of zero or more of these emulated-pci-device objects. (OPEN QUESTION: Should we forbid having more than one of a given device?)</p><p>Provide API (and CLI) commands to add and remove one of these devices from a VM (identifying the VM and device by uuid or other identifier such as name).</p><p>The CLI should allow performing this on multiple VMs in one go, based on a selector or filter for the VMs. We have this concept already in the CLI in commands such as vm-start.</p><p>In the function that adds an emulated PCI device to a VM, we must check if this is the first device to be added, and must refuse if the VM&rsquo;s Virtual Hardware Platform Version is too low. (Or should we just raise the version automatically if needed?)</p><p>When starting a VM, check its list of emulated pci devices and pass the details through to qemu (via xenopsd).</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v11</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-warning">confirmed</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/139>#139</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr><tr><td><span class="label label-default">v2</span></td><td>Added details about the VDI's binary format and size, and the SR capability name.</td></tr><tr><td><span class="label label-default">v3</span></td><td>Tar was not needed after all!</td></tr><tr><td><span class="label label-default">v4</span></td><td>Add details about discovering the VDI using a new vdi_type.</td></tr><tr><td><span class="label label-default">v5</span></td><td>Add details about the http handlers and interaction with xapi's database</td></tr><tr><td><span class="label label-default">v6</span></td><td>Add details about the framing of the data within the VDI</td></tr><tr><td><span class="label label-default">v7</span></td><td>Redesign semantics of the rrd_updates handler</td></tr><tr><td><span class="label label-default">v8</span></td><td>Redesign semantics of the rrd_updates handler (again)</td></tr><tr><td><span class="label label-default">v9</span></td><td>Magic number change in framing format of vdi</td></tr><tr><td><span class="label label-default">v10</span></td><td>Add details of new APIs added to xapi and xcp-rrdd</td></tr><tr><td><span class="label label-default">v11</span></td><td>Remove unneeded API calls</td></tr></table></header><h1 id=sr-level-rrds>SR-Level RRDs</h1><h2 id=introduction>Introduction</h2><p>Xapi has RRDs to track VM- and host-level metrics. There is a desire to have SR-level RRDs as a new category, because SR stats are not specific to a certain VM or host. Examples are size and free space on the SR. While recording SR metrics is relatively straightforward within the current RRD system, the main question is where to archive them, which is what this design aims to address.</p><h2 id=stats-collection>Stats Collection</h2><p>All SR types, including the existing ones, should be able to have RRDs defined for them. Some RRDs, such as a &ldquo;free space&rdquo; one, may make sense for multiple (if not all) SR types. However, the way to measure something like free space will be SR specific. Furthermore, it should be possible for each type of SR to have its own specialised RRDs.</p><p>It follows that each SR will need its own <code>xcp-rrdd</code> plugin, which runs on the SR master and defines and collects the stats. For the new thin-lvhd SR this could be <code>xenvmd</code> itself. The plugin registers itself with <code>xcp-rrdd</code>, so that the latter records the live stats from the plugin into RRDs.</p><h2 id=archiving>Archiving</h2><p>SR-level RRDs will be archived in the SR itself, in a VDI, rather than in the local filesystem of the SR master. This way, we don&rsquo;t need to worry about master failover.</p><p>The VDI will be 4MB in size. This is a little more space than we would need for the RRDs we have in mind at the moment, but will give us enough headroom for the foreseeable future. It will not have a filesystem on it for simplicity and performance. There will only be one RRD archive file for each SR (possibly containing data for multiple metrics), which is gzipped by <code>xcp-rrdd</code>, and can be copied onto the VDI.</p><p>There will be a simple framing format for the data on the VDI. This will be as follows:</p><table><thead><tr><th>Offset</th><th>Type</th><th>Name</th><th>Comment</th></tr></thead><tbody><tr><td>0</td><td>32 bit network-order int</td><td>magic</td><td>Magic number = 0x7ada7ada</td></tr><tr><td>4</td><td>32 bit network-order int</td><td>version</td><td>1</td></tr><tr><td>8</td><td>32 bit network-order int</td><td>length</td><td>length of payload</td></tr><tr><td>12</td><td>gzipped data</td><td>data</td><td></td></tr></tbody></table><p>Xapi will be in charge of the lifecycle of this VDI, not the plugin or <code>xcp-rrdd</code>, which will make it a little easier to manage them. Only xapi will attach/detach and read from/write to this VDI. We will keep <code>xcp-rrdd</code> as simple as possible, and have it archive to its standard path in the local file system. Xapi will then copy the RRDs in and out of the VDI.</p><p>A new value <code>"rrd"</code> in the <code>vdi_type</code> enum of the datamodel will be defined, and the <code>VDI.type</code> of the VDI will be set to that value. The storage backend will write the VDI type to the LVM metadata of the VDI, so that xapi can discover the VDI containing the SR-level RRDs when attaching an SR to a new pool. This means that SR-level RRDs are currently restricted to LVM SRs.</p><p>Because we will not write plugins for all SRs at once, and therefore do not need xapi to set up the VDI for all SRs, we will add an SR &ldquo;capability&rdquo; for the backends to be able to tell xapi whether it has the ability to record stats and will need storage for them. The capability name will be: <code>SR_STATS</code>.</p><h2 id=management-of-the-sr-stats-vdi>Management of the SR-stats VDI</h2><p>The SR-stats VDI will be attached/detached on <code>PBD.plug</code>/<code>unplug</code> on the SR master.</p><ul><li><p>On <code>PBD.plug</code> on the SR master, if the SR has the stats capability, xapi:</p><ul><li>Creates a stats VDI if not already there (search for an existing one based on the VDI type).</li><li>Attaches the stats VDI if it did already exist, and copies the RRDs to the local file system (standard location in the filesystem; asks <code>xcp-rrdd</code> where to put them).</li><li>Informs <code>xcp-rrdd</code> about the RRDs so that it will load the RRDs and add newly recorded data to them (needs a function like <code>push_rrd_local</code> for VM-level RRDs).</li><li>Detaches stats VDI.</li></ul></li><li><p>On <code>PBD.unplug</code> on the SR master, if the SR has the stats capability xapi:</p><ul><li>Tells <code>xcp-rrdd</code> to archive the RRDs for the SR, which it will do to the local filesystem.</li><li>Attaches the stats VDI, copies the RRDs into it, detaches VDI.</li></ul></li></ul><h2 id=periodic-archiving>Periodic Archiving</h2><p>Xapi&rsquo;s periodic scheduler regularly triggers <code>xcp-rrdd</code> to archive the host and VM RRDs. It will need to do this for the SR ones as well. Furthermore, xapi will need to attach the stats VDI and copy the RRD archives into it (as on <code>PBD.unplug</code>).</p><h2 id=exporting>Exporting</h2><p>There will be a new handler for downloading an SR RRD:</p><pre><code>http://&lt;server&gt;/sr_rrd?session_id=&lt;SESSION HANDLE&gt;&amp;uuid=&lt;SR UUID&gt;
</code></pre><p>RRD updates are handled via a single handler for the host, VM and SR UUIDs
RRD updates for the host, VMs and SRs are handled by a a single handler at
<code>/rrd_updates</code>. Exactly what is returned will be determined by the parameters
passed to this handler.</p><p>Whether the host RRD updates are returned is governed by the presence of
<code>host=true</code> in the parameters. <code>host=&lt;anything else></code> or the absence of the
<code>host</code> key will mean the host RRD is not returned.</p><p>Whether the VM RRD updates are returned is governed by the <code>vm_uuid</code> key in the
URL parameters. <code>vm_uuid=all</code> will return RRD updates for all VM RRDs.
<code>vm_uuid=xxx</code> will return the RRD updates for the VM with uuid <code>xxx</code> only.
If <code>vm_uuid</code> is <code>none</code> (or any other string which is not a valid VM UUID) then
the handler will return no VM RRD updates. If the <code>vm_uuid</code> key is absent, RRD
updates for all VMs will be returned.</p><p>Whether the SR RRD updates are returned is governed by the <code>sr_uuid</code> key in the
URL parameters. <code>sr_uuid=all</code> will return RRD updates for all SR RRDs.
<code>sr_uuid=xxx</code> will return the RRD updates for the SR with uuid <code>xxx</code> only.
If <code>sr_uuid</code> is <code>none</code> (or any other string which is not a valid SR UUID) then
the handler will return no SR RRD updates. If the <code>sr_uuid</code> key is absent, no
SR RRD updates will be returned.</p><p>It will be possible to mix and match these parameters; for example to return
RRD updates for the host and all VMs, the URL to use would be:</p><pre><code>http://&lt;server&gt;/rrd_updates?session_id=&lt;SESSION HANDLE&gt;&amp;start=10258122541&amp;host=true&amp;vm_uuid=all&amp;sr_uuid=none
</code></pre><p>Or, to return RRD updates for all SRs but nothing else, the URL to use would be:</p><pre><code>http://&lt;server&gt;/rrd_updates?session_id=&lt;SESSION HANDLE&gt;&amp;start=10258122541&amp;host=false&amp;vm_uuid=none&amp;sr_uuid=all
</code></pre><p>While behaviour is defined if any of the keys <code>host</code>, <code>vm_uuid</code> and <code>sr_uuid</code> is
missing, this is for backwards compatibility and it is recommended that clients
specify each parameter explicitly.</p><h2 id=database-updating>Database updating.</h2><p>If the SR is presenting a data source called &lsquo;physical_utilisation&rsquo;,
xapi will record this periodically in its database. In order to do
this, xapi will fork a thread that, every n minutes (2 suggested, but
open to suggestions here), will query the attached SRs, then query
RRDD for the latest data source for these, and update the database.</p><p>The utilisation of VDIs will <em>not</em> be updated in this way until
scalability worries for RRDs are addressed.</p><p>Xapi will cache whether it is SR master for every attached SR and only
attempt to update if it is the SR master.</p><h2 id=new-apis>New APIs.</h2><h4 id=xcp-rrdd>xcp-rrdd:</h4><ul><li><p>Get the filesystem location where sr rrds are archived: <code>val sr_rrds_path : uid:string -> string</code></p></li><li><p>Archive the sr rrds to the filesystem: <code>val archive_sr_rrd : sr_uuid:string -> unit</code></p></li><li><p>Load the sr rrds from the filesystem: <code>val push_sr_rrd : sr_uuid:string -> unit</code></p></li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=thin-lvhd-storage>thin LVHD storage</h1><p>LVHD is a block-based storage system built on top of Xapi and LVM. LVHD
disks are represented as LVM LVs with vhd-format data inside. When a
disk is snapshotted, the LVM LV is &ldquo;deflated&rdquo; to the minimum-possible
size, just big enough to store the current vhd data. All other disks are
stored &ldquo;inflated&rdquo; i.e. consuming the maximum amount of storage space.
This proposal describes how we could add dynamic thin-provisioning to
LVHD such that</p><ul><li>disks only consume the space they need (plus an adjustable small
overhead)</li><li>when a disk needs more space, the allocation can be done <em>locally</em>
in the common-case; in particular there is no network RPC needed</li><li>when the resource pool master host has failed, allocations can still
continue, up to some limit, allowing time for the master host to be
recovered; in particular there is no need for very low HA timeouts.</li><li>we can (in future) support in-kernel block allocation through the
device mapper dm-thin target.</li></ul><p>The following diagram shows the &ldquo;Allocation plane&rdquo;:</p><p><img alt="Allocation plane" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/thin-lvhd/allocation-plane.png style=height:auto;width:auto></p><p>All VM disk writes are channelled through <code>tapdisk</code> which keeps track
of the remaining reserved space within the device mapper device. When
the free space drops below a &ldquo;low-water mark&rdquo;, tapdisk sends a message
to a local per-SR daemon called <code>local-allocator</code> and requests more
space.</p><p>The <code>local-allocator</code> maintains a free pool of blocks available for
allocation locally (hence the name). It will pick some blocks and
transactionally send the update to the <code>xenvmd</code> process running
on the SRmaster via the shared ring (labelled <code>ToLVM queue</code> in the diagram)
and update the device mapper tables locally.</p><p>There is one <code>xenvmd</code> process per SR on the SRmaster. <code>xenvmd</code> receives
local allocations from all the host shared rings (labelled <code>ToLVM queue</code>
in the diagram) and combines them together, appending them to a redo-log
also on shared storage. When <code>xenvmd</code> notices that a host&rsquo;s free space
(represented in the metadata as another LV) is low it allocates new free blocks
and pushes these to the host via another shared ring (labelled <code>FromLVM queue</code>
in the diagram).</p><p>The <code>xenvmd</code> process maintains a cache of the current VG metadata for
fast query and update. All updates are appended to the redo-log to ensure
they operate in O(1) time. The redo log updates are periodically flushed
to the primary LVM metadata.</p><p>Since the operations are stored in the redo-log and will only be removed
after the real metadata has been written, the implication is that it is
possible for the operations to be performed more than once. This will
occur if the xenvmd process exits between flushing to the real metadata
and acknowledging the operations as completed. For this to work as expected,
every individual operation stored in the redo-log <em>must</em> be idempotent.</p><h2 id=note-on-running-out-of-blocks>Note on running out of blocks</h2><p>Note that, while the host has plenty of free blocks, local allocations should
be fast. If the master fails and the local free pool starts running out
and <code>tapdisk</code> asks for more blocks, then the local allocator won&rsquo;t be able
to provide them.
<code>tapdisk</code> should start to slow
I/O in order to provide the local allocator more time.
Eventually if <code>tapdisk</code> runs
out of space before the local allocator can satisfy the request then
guest I/O will block. Note Windows VMs will start to crash if guest
I/O blocks for more than 70s. Linux VMs, no matter PV or HVM, may suffer
from &ldquo;block for more than 120 seconds&rdquo; issue due to slow I/O. This
known issue is that, slow I/O during dirty pages writeback/flush may
cause memory starvation, then other userland process or kernel threads
would be blocked.</p><p>The following diagram shows the control-plane:</p><p><img alt="control plane" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/thin-lvhd/control-plane.png style=height:auto;width:auto></p><p>When thin-provisioning is enabled we will be modifying the LVM metadata at
an increased rate. We will cache the current metadata in the <code>xenvmd</code> process
and funnel all queries through it, rather than &ldquo;peeking&rdquo; at the metadata
on-disk. Note it will still be possible to peek at the on-disk metadata but it
will be out-of-date. Peeking can still be used to query the PV state of the volume
group.</p><p>The <code>xenvm</code> CLI uses a simple
RPC interface to query the <code>xenvmd</code> process, tunnelled through <code>xapi</code> over
the management network. The RPC interface can be used for</p><ul><li>activating volumes locally: <code>xenvm</code> will query the LV segments and program
device mapper</li><li>deactivating volumes locally</li><li>listing LVs, PVs etc</li></ul><p>Note that current LVHD requires the management network for these control-plane
functions.</p><p>When the SM backend wishes to query or update volume group metadata it should use the
<code>xenvm</code> CLI while thin-provisioning is enabled.</p><p>The <code>xenvmd</code> process shall use a redo-log to ensure that metadata updates are
persisted in constant time and flushed lazily to the regular metadata area.</p><p>Tunnelling through xapi will be done by POSTing to the localhost URI</p><pre><code>/services/xenvmd/&lt;SR uuid&gt;
</code></pre><p>Xapi will the either proxy the request transparently to the SRmaster, or issue an
http level redirect that the xenvm CLI would need to follow.</p><p>If the xenvmd process is not running on the host on which it should
be, xapi will start it.</p><h1 id=components-roles-and-responsibilities>Components: roles and responsibilities</h1><p><code>xenvmd</code>:</p><ul><li>one per plugged SRmaster PBD</li><li>owns the LVM metadata</li><li>provides a fast query/update API so we can (for example) create lots of LVs very fast</li><li>allocates free blocks to hosts when they are running low</li><li>receives block allocations from hosts and incorporates them in the LVM metadata</li><li>can safely flush all updates and downgrade to regular LVM</li></ul><p><code>xenvm</code>:</p><ul><li>a CLI which talks the <code>xenvmd</code> protocol to query / update LVs</li><li>can be run on any host, calls (except &ldquo;format&rdquo; and &ldquo;upgrade&rdquo;) are forwarded by <code>xapi</code></li><li>can &ldquo;format&rdquo; a LUN to prepare it for <code>xenvmd</code></li><li>can &ldquo;upgrade&rdquo; a LUN to prepare it for <code>xenvmd</code></li></ul><p><code>local_allocator</code>:</p><ul><li>one per plugged PBD</li><li>exposes a simple interface to <code>tapdisk</code> for requesting more space</li><li>receives free block allocations via a queue on the shared disk from <code>xenvmd</code></li><li>sends block allocations to <code>xenvmd</code> and updates the device mapper target locally</li></ul><p><code>tapdisk</code>:</p><ul><li>monitors the free space inside LVs and requests more space when running out</li><li>slows down I/O when nearly out of space</li></ul><p><code>xapi</code>:</p><ul><li>provides authenticated communication tunnels</li><li>ensures the xenvmd daemons are only running on the correct hosts.</li></ul><p><code>SM</code>:</p><ul><li>writes the configuration file for xenvmd (though doesn&rsquo;t start it)</li><li>has an on/off switch for thin-provisioning</li><li>can use either normal LVM or the <code>xenvm</code> CLI</li></ul><p><code>membership_monitor</code></p><ul><li>configures and manages the connections between <code>xenvmd</code> and the <code>local_allocator</code></li></ul><h1 id=queues-on-the-shared-disk>Queues on the shared disk</h1><p>The <code>local_allocator</code> communicates with <code>xenvmd</code> via a pair
of queues on the shared disk. Using the disk rather than the network means
that VMs will continue to run even if the management network is not working.
In particular</p><ul><li>if the (management) network fails, VMs continue to run on SAN storage</li><li>if a host changes IP address, nothing needs to be reconfigured</li><li>if xapi fails, VMs continue to run.</li></ul><h2 id=logical-messages-in-the-queues>Logical messages in the queues</h2><p>The <code>local_allocator</code> needs to tell the <code>xenvmd</code> which blocks have
been allocated to which guest LV. <code>xenvmd</code> needs to tell the
<code>local_allocator</code> which blocks have become free. Since we are based on
LVM, a &ldquo;block&rdquo; is an extent, and an &ldquo;allocation&rdquo; is a segment i.e. the
placing of a physical extent at a logical extent in the logical volume.</p><p>The <code>local_allocator</code> needs to send a message with logical contents:</p><ul><li><code>volume</code>: a human-readable name of the LV</li><li><code>segments</code>: a list of LVM segments which says
&ldquo;place physical extent x at logical extent y using a linear mapping&rdquo;.</li></ul><p>Note this message is idempotent.</p><p>The <code>xenvmd</code> needs to send a message with logical contents:</p><ul><li><code>extents</code>: a list of physical extents which are free for the host to use</li></ul><p>Although
for internal housekeeping <code>xenvmd</code> will want to assign these
physical extents to logical extents within the host&rsquo;s free LV, the
<code>local_allocator</code>
doesn&rsquo;t need to know the logical extents. It only needs to know
the set of blocks which it is free to allocate.</p><h2 id=starting-up-the-local_allocator>Starting up the local_allocator</h2><p>What happens when a <code>local_allocator</code> (re)starts, after a</p><ul><li>process crash, respawn</li><li>host crash, reboot?</li></ul><p>When the <code>local_allocator</code> starts up, there are 2 cases:</p><ol><li>the host has just rebooted, there are no attached disks and no running VMs</li><li>the process has just crashed, there are attached disks and running VMs</li></ol><p>Case 1 is uninteresting. In case 2 there may have been an allocation in
progress when the process crashed and this must be completed. Therefore
the operation is journalled in a local filesystem in a directory which
is deliberately deleted on host reboot (Case 1). The allocation operation
consists of:</p><ol><li><code>push</code>ing the allocation to <code>xenvmd</code> on the SRmaster</li><li>updating the device mapper</li></ol><p>Note that both parts of the allocation operation are idempotent and hence
the whole operation is idempotent. The journalling will guarantee it executes
at-least-once.</p><p>When the <code>local_allocator</code> starts up it needs to discover the list of
free blocks. Rather than have 2 code paths, it&rsquo;s best to treat everything
as if it is a cold start (i.e. no local caches already populated) and to
ask the master to resync the free block list. The resync is performed by
executing a &ldquo;suspend&rdquo; and &ldquo;resume&rdquo; of the free block queue, and requiring
the remote allocator to:</p><ul><li><code>pop</code> all block allocations and incorporate these updates</li><li>send the complete set of free blocks &ldquo;now&rdquo; (i.e. while the queue is
suspended) to the local allocator.</li></ul><h2 id=starting-xenvmd>Starting xenvmd</h2><p><code>xenvmd</code> needs to know</p><ul><li>the device containing the volume group</li><li>the hosts to &ldquo;connect&rdquo; to via the shared queues</li></ul><p>The device containing the volume group should be written to a config
file when the SR is plugged.</p><p><code>xenvmd</code> does not remember which hosts it is listening to across crashes,
restarts or master failovers. The <code>membership_monitor</code> will keep the
<code>xenvmd</code> list in sync with the <code>PBD.currently_attached</code> fields.</p><h2 id=shutting-down-the-local_allocator>Shutting down the local_allocator</h2><p>The <code>local_allocator</code> should be able to crash at any time and recover
afterwards. If the user requests a <code>PBD.unplug</code> we can perform a
clean shutdown by:</p><ul><li>signalling <code>xenvmd</code> to suspend the block allocation queue</li><li>arranging for the <code>local_allocator</code> to acknowledge the suspension and exit</li><li>when the <code>xenvmd</code> sees the acknowlegement, we know that the
<code>local_allocator</code> is offline and it doesn&rsquo;t need to poll the queue any more</li></ul><h2 id=downgrading-metadata>Downgrading metadata</h2><p><code>xenvmd</code> can be terminated at any time and restarted, since all compound
operations are journalled.</p><p>Downgrade is a special case of shutdown.
To downgrade, we need to stop all hosts allocating and ensure all updates
are flushed to the global LVM metadata. <code>xenvmd</code> can shutdown
by:</p><ul><li>shutting down all <code>local_allocator</code>s (see previous section)</li><li>flushing all outstanding block allocations to the LVM redo log</li><li>flushing the LVM redo log to the global LVM metadata</li></ul><h2 id=queues-as-rings>Queues as rings</h2><p>We can use a simple ring protocol to represent the queues on the disk.
Each queue will have a single consumer and single producer and reside within
a single logical volume.</p><p>To make diagnostics simpler, we can require the ring to only support <code>push</code>
and <code>pop</code> of <em>whole</em> messages i.e. there can be no partial reads or partial
writes. This means that the <code>producer</code> and <code>consumer</code> pointers will always
point to valid message boundaries.</p><p>One possible format used by the <a href=https://github.com/mirage/shared-block-ring/blob/master/lib/ring.ml rel=external target=_blank>prototype</a> is as follows:</p><ul><li>sector 0: a magic string</li><li>sector 1: producer state</li><li>sector 2: consumer state</li><li>sector 3&mldr;: data</li></ul><p>Within the producer state sector we can have:</p><ul><li>octets 0-7: producer offset: a little-endian 64-bit integer</li><li>octet 8: 1 means &ldquo;suspend acknowledged&rdquo;; 0 otherwise</li></ul><p>Within the consumer state sector we can have:</p><ul><li>octets 0-7: consumer offset: a little-endian 64-bit integer</li><li>octet 8: 1 means &ldquo;suspend requested&rdquo;; 0 otherwise</li></ul><p>The consumer and producer pointers point to message boundaries. Each
message is prefixed with a 4 byte length and padded to the next 4-byte
boundary.</p><p>To push a message onto the ring we need to</p><ul><li>check whether the message is too big to ever fit: this is a permanent
error</li><li>check whether the message is too big to fit given the current free
space: this is a transient error</li><li>write the message into the ring</li><li>advance the producer pointer</li></ul><p>To pop a message from the ring we need to</p><ul><li>check whether there is unconsumed space: if not this is a transient
error</li><li>read the message from the ring and process it</li><li>advance the consumer pointer</li></ul><h2 id=journals-as-queues>Journals as queues</h2><p>When we journal an operation we want to guarantee to execute it never
<em>or</em> at-least-once. We can re-use the queue implementation by <code>push</code>ing
a description of the work item to the queue and waiting for the
item to be <code>pop</code>ped, processed and finally consumed by advancing the
consumer pointer. The journal code needs to check for unconsumed data
during startup, and to process it before continuing.</p><h2 id=suspending-and-resuming-queues>Suspending and resuming queues</h2><p>During startup (resync the free blocks) and shutdown (flush the allocations)
we need to suspend and resume queues. The ring protocol can be extended
to allow the <em>consumer</em> to suspend the ring by:</p><ul><li>the consumer asserts the &ldquo;suspend requested&rdquo; bit</li><li>the producer <code>push</code> function checks the bit and writes &ldquo;suspend acknowledged&rdquo;</li><li>the producer also periodically polls the queue state and writes
&ldquo;suspend acknowledged&rdquo; (to catch the case where no items are to be pushed)</li><li>after the producer has acknowledged it will guarantee to <code>push</code> no more
items</li><li>when the consumer polls the producer&rsquo;s state and spots the &ldquo;suspend acknowledged&rdquo;,
it concludes that the queue is now suspended.</li></ul><p>The key detail is that the handshake on the ring causes the two sides
to synchronise and both agree that the ring is now suspended/ resumed.</p><h2 id=modelling-the-suspendresume-protocol>Modelling the suspend/resume protocol</h2><p>To check that the suspend/resume protocol works well enough to be used
to resynchronise the free blocks list on a slave, a simple
<a href=/new-docs/design/thin-lvhd/queue.pml>promela model</a> was created. We model the queue state as
2 boolean flags:</p><div class="highlight wrap-code"><pre tabindex=0><code>bool suspend /* suspend requested */
bool suspend_ack /* suspend acknowledged *./</code></pre></div><p>and an abstract representation of the data within the ring:</p><div class="highlight wrap-code"><pre tabindex=0><code>/* the queue may have no data (none); a delta or a full sync.
   the full sync is performed immediately on resume. */
mtype = { sync delta none }
mtype inflight_data = none</code></pre></div><p>There is a &ldquo;producer&rdquo; and a &ldquo;consumer&rdquo; process which run forever,
exchanging data and suspending and resuming whenever they want.
The special data item <code>sync</code> is only sent immediately after a resume
and we check that we never desynchronise with asserts:</p><div class="highlight wrap-code"><pre tabindex=0><code>  :: (inflight_data != none) -&gt;
    /* In steady state we receive deltas */
    assert (suspend_ack == false);
    assert (inflight_data == delta);
    inflight_data = none</code></pre></div><p>i.e. when we are receiving data normally (outside of the suspend/resume
code) we aren&rsquo;t suspended and we expect deltas, not full syncs.</p><p>The model-checker <a href=http://spinroot.com/spin/whatispin.html rel=external target=_blank>spin</a>
verifies this property holds.</p><h1 id=interaction-with-ha>Interaction with HA</h1><p>Consider what will happen if a host fails when HA is disabled:</p><ul><li>if the host is a slave: the VMs running on the host will crash but
no other host is affected.</li><li>if the host is a master: allocation requests from running VMs will
continue provided enough free blocks are cached on the hosts. If a
host eventually runs out of free blocks, then guest I/O will start to
block and VMs may eventually crash.</li></ul><p>Therefore we <em>recommend</em> that users enable HA and only disable it
for short periods of time. Note that, unlike other thin-provisioning
implementations, we will allow HA to be disabled.</p><h1 id=host-local-lvs>Host-local LVs</h1><p>When a host calls SMAPI <code>sr_attach</code>, it will use <code>xenvm</code> to tell <code>xenvmd</code> on the
SRmaster to connect to the <code>local_allocator</code> on the host. The <code>xenvmd</code>
daemon will create the volumes for queues and a volume to represent the
&ldquo;free blocks&rdquo; which a host is allowed to allocate.</p><h1 id=monitoring>Monitoring</h1><p>The <code>xenvmd</code> process should export RRD datasources over shared
memory named</p><ul><li><code>sr_&lt;SR uuid>_&lt;host uuid>_free</code>: the number of free blocks in
the local cache. It&rsquo;s useful to look at this and verify that it doesn&rsquo;t
usually hit zero, since that&rsquo;s when allocations will start to block.
For this reason we should use the <code>MIN</code> consolidation function.</li><li><code>sr_&lt;SR uuid>_&lt;host uuid>_requests</code>: a counter of the number
of satisfied allocation requests. If this number is too high then the quantum
of allocation should be increased. For this reason we should use the
<code>MAX</code> consolidation function.</li><li><code>sr_&lt;SR uuid>_&lt;host uuid>_allocations</code>: a counter of the number of
bytes being allocated. If the allocation rate is too high compared with
the number of free blocks divided by the HA timeout period then the
<code>SRmaster-allocator</code> should be reconfigured to supply more blocks with the host.</li></ul><h1 id=modifications-to-tapdisk>Modifications to tapdisk</h1><p>TODO: to be updated by Germano</p><p><code>tapdisk</code> will be modified to</p><ul><li>on open: discover the current maximum size of the file/LV (for a file
we assume there is no limit for now)</li><li>read a low-water mark value from a config file <code>/etc/tapdisk3.conf</code></li><li>read a very-low-water mark value from a config file <code>/etc/tapdisk3.conf</code></li><li>read a Unix domain socket path from a config file <code>/etc/tapdisk3.conf</code></li><li>when there is less free space available than the low-water mark: connect
to Unix domain socket and write an &ldquo;extend&rdquo; request</li><li>upon receiving the &ldquo;extend&rdquo; response, re-read the maximum size of the
file/LV</li><li>when there is less free space available than the very-low-water mark:
start to slow I/O responses and write a single &rsquo;error&rsquo; line to the log.</li></ul><h2 id=the-extend-request>The extend request</h2><p>TODO: to be updated by Germano</p><p>The request has the following format:</p><table><thead><tr><th>Octet offsets</th><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>0,1</td><td>tl</td><td>Total length (including this field) of message (in network byte order)</td></tr><tr><td>2</td><td>type</td><td>The value &lsquo;0&rsquo; indicating an extend request</td></tr><tr><td>3</td><td>nl</td><td>The length of the LV name in octets, including NULL terminator</td></tr><tr><td>4,..,4+nl-1</td><td>name</td><td>The LV name</td></tr><tr><td>4+nl,..,12+nl-1</td><td>vdi_size</td><td>The virtual size of the logical VDI (in network byte order)</td></tr><tr><td>12+nl,..,20+nl-1</td><td>lv_size</td><td>The current size of the LV (in network byte order)</td></tr><tr><td>20+nl,..,28+nl-1</td><td>cur_size</td><td>The current size of the vhd metadata (in network byte order)</td></tr></tbody></table><h2 id=the-extend-response>The extend response</h2><p>The response is a single byte value &ldquo;0&rdquo; which is a signal to re-examime
the LV size. The request will block indefinitely until it succeeds. The
request will block for a long time if</p><ul><li>the SR has genuinely run out of space. The admin should observe the
existing free space graphs/alerts and perform an SR resize.</li><li>the master has failed and HA is disabled. The admin should re-enable
HA or fix the problem manually.</li></ul><h1 id=the-local_allocator>The local_allocator</h1><p>There is one <code>local_allocator</code> process per plugged PBD.
The process will be
spawned by the SM <code>sr_attach</code> call, and shutdown from the <code>sr_detach</code> call.</p><p>The <code>local_allocator</code> accepts the following configuration (via a config file):</p><ul><li><code>socket</code>: path to a local Unix domain socket. This is where the <code>local_allocator</code>
listens for requests from <code>tapdisk</code></li><li><code>allocation_quantum</code>: number of megabytes to allocate to each tapdisk on request</li><li><code>local_journal</code>: path to a block device or file used for local journalling. This
should be deleted on reboot.</li><li><code>free_pool</code>: name of the LV used to store the host&rsquo;s free blocks</li><li><code>devices</code>: list of local block devices containing the PVs</li><li><code>to_LVM</code>: name of the LV containing the queue of block allocations sent to <code>xenvmd</code></li><li><code>from_LVM</code>: name of the LV containing the queue of messages sent from <code>xenvmd</code>.
There are two types of messages:<ol><li>Free blocks to put into the free pool</li><li>Cap requests to remove blocks from the free pool.</li></ol></li></ul><p>When the <code>local_allocator</code> process starts up it will read the host local
journal and</p><ul><li>re-execute any pending allocation requests from tapdisk</li><li>suspend and resume the <code>from_LVM</code> queue to trigger a full retransmit
of free blocks from <code>xenvmd</code></li></ul><p>The procedure for handling an allocation request from tapdisk is:</p><ol><li>if there aren&rsquo;t enough free blocks in the free pool, wait polling the
<code>from_LVM</code> queue</li><li>choose a range of blocks to assign to the tapdisk LV from the free LV</li><li>write the operation (i.e. exactly what we are about to do) to the journal.
This ensures that it will be repeated if the allocator crashes and restarts.
Note that, since the operation may be repeated multiple times, it must be
idempotent.</li><li>push the block assignment to the <code>toLVM</code> queue</li><li>suspend the device mapper device</li><li>add/modify the device mapper target</li><li>resume the device mapper device</li><li>remove the operation from the local journal (i.e. there&rsquo;s no need to repeat
it now)</li><li>reply to tapdisk</li></ol><h2 id=shutting-down-the-local-allocator>Shutting down the local-allocator</h2><p>The SM <code>sr_detach</code> called from <code>PBD.unplug</code> will use the <code>xenvm</code> CLI to request
that <code>xenvmd</code> disconnects from a host. The procedure is:</p><ol><li>SM calls <code>xenvm disconnect host</code></li><li><code>xenvm</code> sends an RPC to <code>xenvmd</code> tunnelled through <code>xapi</code></li><li><code>xenvmd</code> suspends the <code>to_LVM</code> queue</li><li>the <code>local_allocator</code> acknowledges the suspend and exits</li><li><code>xenvmd</code> flushes all updates from the <code>to_LVM</code> queue and stops listening</li></ol><h1 id=xenvmd>xenvmd</h1><p><code>xenvmd</code> is a daemon running per SRmaster PBD, started in <code>sr_attach</code> and
terminated in <code>sr_detach</code>. <code>xenvmd</code> has a config file containing:</p><ul><li><code>socket</code>: Unix domain socket where <code>xenvmd</code> listens for requests from
<code>xenvm</code> tunnelled by <code>xapi</code></li><li><code>host_allocation_quantum</code>: number of megabytes to hand to a host at a time</li><li><code>host_low_water_mark</code>: threshold below which we will hand blocks to a host</li><li><code>devices</code>: local devices containing the PVs</li></ul><p><code>xenvmd</code> continually</p><ul><li>peeks updates from all the <code>to_LVM</code> queues</li><li>calculates how much free space each host still has</li><li>if the size of a host&rsquo;s free pool drops below some threshold:<ul><li>choose some free blocks</li></ul></li><li>if the size of a host&rsquo;s free pool goes above some threshold:<ul><li>request a cap of the host&rsquo;s free pool</li></ul></li><li>writes the change it is going to make to a journal stored in an LV</li><li>pops the updates from the <code>to_LVM</code> queues</li><li>pushes the updates to the <code>from_LVM</code> queues</li><li>pushes updates to the LVM redo-log</li><li>periodically flush the LVM redo-log to the LVM metadata area</li></ul><h1 id=the-membership-monitor>The membership monitor</h1><p>The role of the membership monitor is to keep the list of <code>xenvmd</code> connections
in sync with the <code>PBD.currently_attached</code> fields.</p><p>We shall</p><ul><li>install a <code>host-pre-declare-dead</code> script to use <code>xenvm</code> to send an RPC
to <code>xenvmd</code> to forcibly flush (without acknowledgement) the <code>to_LVM</code> queue
and destroy the LVs.</li><li>modify XenAPI <code>Host.declare_dead</code> to call <code>host-pre-declare-dead</code> before
the VMs are unlocked</li><li>add a <code>host-pre-forget</code> hook type which will be called just before a Host
is forgotten</li><li>install a <code>host-pre-forget</code> script to use <code>xenvm</code> to call <code>xenvmd</code> to
destroy the host&rsquo;s local LVs</li></ul><h1 id=modifications-to-lvhd-sr>Modifications to LVHD SR</h1><ul><li><code>sr_attach</code> should:<ul><li>if an SRmaster, update the <code>MGT</code> major version number to prevent</li><li>Write the xenvmd configuration file (on <em>all</em> hosts, not just SRmaster)</li><li>spawn <code>local_allocator</code></li></ul></li><li><code>sr_detach</code> should:<ul><li>call <code>xenvm</code> to request the shutdown of <code>local_allocator</code></li></ul></li><li><code>vdi_deactivate</code> should:<ul><li>call <code>xenvm</code> to request the flushing of all the <code>to_LVM</code> queues to the
redo log</li></ul></li><li><code>vdi_activate</code> should:<ul><li>if necessary, call <code>xenvm</code> to deflate the LV to the minimum size (with some slack)</li></ul></li></ul><p>Note that it is possible to attach and detach the individual hosts in any order
but when the SRmaster is unplugged then there will be no &ldquo;refilling&rdquo; of the host
local free LVs; it will behave as if the master host has failed.</p><h1 id=modifications-to-xapi>Modifications to xapi</h1><ul><li>Xapi needs to learn how to forward xenvm connections to the SR master.</li><li>Xapi needs to start and stop xenvmd at the appropriate times</li><li>We must disable unplugging the PBDs for shared SRs on the pool master
if any other slave has its PBD plugging. This is actually fixing an
issue that exists today - LVHD SRs require the master PBD to be
plugged to do many operations.</li><li>Xapi should provide a mechanism by which the xenvmd process can be killed
once the last PBD for an SR has been unplugged.</li></ul><h1 id=enabling-thin-provisioning>Enabling thin provisioning</h1><p>Thin provisioning will be automatically enabled on upgrade. When the SRmaster
plugs in <code>PBD</code> the <code>MGT</code> major version number will be bumped to prevent old
hosts from plugging in the SR and getting confused.
When a VDI is activated, it will be deflated to the new low size.</p><h1 id=disabling-thin-provisioning>Disabling thin provisioning</h1><p>We shall make a tool which will</p><ul><li>allow someone to downgrade their pool after enabling thin provisioning</li><li>allow developers to test the upgrade logic without fully downgrading their
hosts</li></ul><p>The tool will</p><ul><li>check if there is enough space to fully inflate all non-snapshot leaves</li><li>unplug all the non-SRmaster <code>PBD</code>s</li><li>unplug the SRmaster <code>PBD</code>. As a side-effect all pending LVM updates will be
written to the LVM metadata.</li><li>modify the <code>MGT</code> volume to have the lower metadata version</li><li>fully inflate all non-snapshot leaves</li></ul><h1 id=walk-through-upgrade>Walk-through: upgrade</h1><p>Rolling upgrade should work in the usual way. As soon as the pool master has been
upgraded, hosts will be able to use thin provisioning when new VDIs are attached.
A VM suspend/resume/reboot or migrate will be needed to turn on thin provisioning
for existing running VMs.</p><h1 id=walk-through-downgrade>Walk-through: downgrade</h1><p>A pool may be safely downgraded to a previous version without thin provisioning
provided that the downgrade tool is run. If the tool hasn&rsquo;t run then the old
pool will refuse to attach the SR because the metadata has been upgraded.</p><h1 id=walk-through-after-a-host-failure>Walk-through: after a host failure</h1><p>If HA is enabled:</p><ul><li><code>xhad</code> elects a new master if necessary</li><li><code>Xapi</code> on the master will start xenvmd processes for shared thin-lvhd SRs</li><li>the <code>xhad</code> tells <code>Xapi</code> which hosts are alive and which have failed.</li><li><code>Xapi</code> runs the <code>host-pre-declare-dead</code> scripts for every failed host</li><li>the <code>host-pre-declare-dead</code> tells <code>xenvmd</code> to flush the <code>to_LVM</code> updates</li><li><code>Xapi</code> unlocks the VMs and restarts them on new hosts.</li></ul><p>If HA is not enabled:</p><ul><li>The admin should verify the host is definitely dead</li><li>If the dead host was the master, a new master must be designated. This will
start the xenvmd processes for the shared thin-lvhd SRs.</li><li>the admin must tell <code>Xapi</code> which hosts have failed with <code>xe host-declare-dead</code></li><li><code>Xapi</code> runs the <code>host-pre-declare-dead</code> scripts for every failed host</li><li>the <code>host-pre-declare-dead</code> tells <code>xenvmd</code> to flush the <code>to_LVM</code> updates</li><li><code>Xapi</code> unlocks the VMs</li><li>the admin may now restart the VMs on new hosts.</li></ul><h1 id=walk-through-co-operative-master-transition>Walk-through: co-operative master transition</h1><p>The admin calls Pool.designate_new_master. This initiates a two-phase
commit of the new master. As part of this, the slaves will restart,
and on restart each host&rsquo;s xapi will kill any xenvmd that should only
run on the pool master. The new designated master will then restart itself
and start up the xenvmd process on itself.</p><h1 id=future-use-of-dm-thin>Future use of dm-thin?</h1><p>Dm-thin also uses 2 local LVs: one for the &ldquo;thin pool&rdquo; and one for the metadata.
After replaying our journal we could potentially delete our host local LVs and
switch over to dm-thin.</p><h1 id=summary-of-the-impact-on-the-admin>Summary of the impact on the admin</h1><ul><li>If the VM workload performs a lot of disk allocation, then the admin <em>should</em>
enable HA.</li><li>The admin <em>must</em> not downgrade the pool without first cleanly detaching the
storage.</li><li>Extra metadata is needed to track thin provisioing, reducing the amount of
space available for user volumes.</li><li>If an SR is completely full then it will not be possible to enable thin
provisioning.</li><li>There will be more fragmentation, but the extent size is large (4MiB) so it
shouldn&rsquo;t be too bad.</li></ul><h1 id=ring-protocols>Ring protocols</h1><p>Each ring consists of 3 sectors of metadata followed by the data area. The
contents of the first 3 sectors are:</p><table><thead><tr><th>Sector, Octet offsets</th><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>0,0-30</td><td>signature</td><td>string</td><td>Signature (&ldquo;mirage shared-block-device 1.0&rdquo;)</td></tr><tr><td>1,0-7</td><td>producer</td><td>uint64</td><td>Pointer to the end of data written by the producer</td></tr><tr><td>1,8</td><td>suspend_ack</td><td>uint8</td><td>Suspend acknowledgement byte</td></tr><tr><td>2,0-7</td><td>consumer</td><td>uint64</td><td>Pointer to the end of data read by the consumer</td></tr><tr><td>2,8</td><td>suspend</td><td>uint8</td><td>Suspend request byte</td></tr></tbody></table><p>Note. producer and consumer pointers are stored in little endian
format.</p><p>The pointers are free running byte offsets rounded up to the next
4-byte boundary, and the position of the actual data is found by
finding the remainder when dividing by the size of the data area. The
producer pointer points to the first free byte, and the consumer
pointer points to the byte after the last data consumed. The actual
payload is preceded by a 4-byte length field, stored in little endian
format. When writing a 1 byte payload, the next value of the producer
pointer will therefore be 8 bytes on from the previous - 4 for the
length (which will contain [0x01,0x00,0x00,0x00]), 1 byte for the
payload, and 3 bytes padding.</p><p>A ring is suspended and resumed by the consumer. To suspend, the
consumer first checks that the producer and consumer agree on the
current suspend status. If they do not, the ring cannot be
suspended. The consumer then writes the byte 0x02 into byte 8 of
sector 2. The consumer must then wait for the producer to acknowledge
the suspend, which it will do by writing 0x02 into byte 8 of sector 1.</p><h2 id=the-fromlvm-ring>The FromLVM ring</h2><p>Two different types of message can be sent on the FromLVM ring.</p><p>The FreeAllocation message contains the blocks for the free pool.
Example message:</p><pre><code>(FreeAllocation((blocks((pv0(12326 12249))(pv0(11 1))))(generation 2)))
</code></pre><p>Pretty-printed:</p><pre><code>(FreeAllocation
    (
        (blocks
            (
                (pv0(12326 12249))
                (pv0(11 1))
            )
        )
        (generation 2)
    )
)
</code></pre><p>This is a message to add two new sets of extents to the free pool. A
span of length 12249 extents starting at extent 12326, and a span of
length 1 starting from extent 11, both within the physical volume
&lsquo;pv0&rsquo;. The generation count of this message is &lsquo;2&rsquo;. The semantics of
the generation is that the local allocator must record the generation
of the last message it received since the FromLVM ring was resumed,
and ignore any message with a generated less than or equal to the last
message received.</p><p>The CapRequest message contains a request to cap the free pool at
a maximum size.
Example message:</p><pre><code>(CapRequest((cap 6127)(name host1-freeme)))
</code></pre><p>Pretty-printed:</p><pre><code>(CapRequest
    (
        (cap 6127)
        (name host1-freeme)
    )
)
</code></pre><p>This is a request to cap the free pool at a maximum size of 6127
extents. The &rsquo;name&rsquo; parameter reflects the name of the LV into which
the extents should be transferred.</p><h2 id=the-tolvm-ring>The ToLVM Ring</h2><p>The ToLVM ring only contains 1 type of message. Example:</p><pre><code>((volume test5)(segments(((start_extent 1)(extent_count 32)(cls(Linear((name pv0)(start_extent 12328))))))))
</code></pre><p>Pretty-printed:</p><pre><code>(
    (volume test5)
    (segments
        (
            (
                (start_extent 1)
                (extent_count 32)
                (cls
                    (Linear
                        (
                            (name pv0)
                            (start_extent 12328)
                        )
                    )
                )
            )
        )
    )
)
</code></pre><p>This message is extending an LV named &rsquo;test5&rsquo; by giving it 32 extents
starting at extent 1, coming from PV &lsquo;pv0&rsquo; starting at extent
12328. The &lsquo;cls&rsquo; field should always be &lsquo;Linear&rsquo; - this is the only
acceptable value.</p><h1 id=cap-requests>Cap requests</h1><p>Xenvmd will try to keep the free pools of the hosts within a range
set as a fraction of free space. There are 3 parameters adjustable
via the config file:</p><ul><li>low_water_mark_factor</li><li>medium_water_mark_factor</li><li>high_water_mark_factor</li></ul><p>These three are all numbers between 0 and 1. Xenvmd will sum the free
size and the sizes of all hosts&rsquo; free pools to find the total
effective free size in the VG, <code>F</code>. It will then subtract the sizes of
any pending desired space from in-flight create or resize calls <code>s</code>. This
will then be divided by the number of hosts connected, <code>n</code>, and
multiplied by the three factors above to find the 3 absolute values
for the high, medium and low watermarks.</p><pre><code>{high, medium, low} * (F - s) / n
</code></pre><p>When xenvmd notices that a host&rsquo;s free pool size has dropped below
the low watermark, it will be topped up such that the size is equal
to the medium watermark. If xenvmd notices that a host&rsquo;s free pool
size is above the high watermark, it will issue a &lsquo;cap request&rsquo; to
the host&rsquo;s local allocator, which will then respond by allocating
from its free pool into the fake LV, which xenvmd will then delete
as soon as it gets the update.</p><p>Xenvmd keeps track of the last update it has sent to the local
allocator, and will not resend the same request twice, unless it
is restarted.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (22.6.0)</span></td></tr></table></header><h1 id=tls-vertification-for-intra-pool-communications>TLS vertification for intra-pool communications</h1><h2 id=overview>Overview</h2><p>Xenserver has used TLS-encrypted communications between xapi daemons in a pool since its first release.
However it does not use TLS certificates to authenticate the servers it connects to.
This allows possible attackers opportunities to impersonate servers when the poolsâ€™ management network is compromised.</p><p>In order to enable certificate verification, certificate exchange as well as proper set up to trust them must be provided by xapi.
This is currently done by allowing users to generate, sign and install the certificates themselves; and then enable the Common Criteria mode.
This requires a CA and has a high barrier of entry.</p><p>Using the same certificates for intra-host communication creates friction between what the user needs and what the host needs.
Instead of trying to reconcile these two uses with one set of certificates, host will serve two certificates: one for API calls from external clients, which is the one that can be changed by the users; and one that is use for intra-pool communications.
The TLS server in the host can select which certificate to serve depending on the service name the client requests when opening a TLS connection.
This mechanism is called Server Name Identification or SNI in short.</p><p>Last but not least the update bearing these changes must not disrupt pool operations while or after being applied.</p><h2 id=glossary>Glossary</h2><table><thead><tr><th>Term</th><th>Meaning</th></tr></thead><tbody><tr><td>SNI</td><td>Server Name Identification. This TLS protocol extension allows a server to select a certificate during the initial TLS handshake depending on a client-provided name. This usually allows a single reverse-proxy to serve several HTTPS websites.</td></tr><tr><td>Host certificate</td><td>Certificate that a host sends clients when the latter initiate a connection with the former. The clients may close the connection depending on the properties of this certificate and whether they have decided to trust it previously.</td></tr><tr><td>Trusted certificate</td><td>Certificate that a computer uses to verify whether a host certificate is valid. If the host certificate&rsquo;s chain of trust does not include a trusted certificate it will be considered invalid.</td></tr><tr><td>Default Certificate</td><td>Xenserver hosts present this certificate to clients which do not request an SNI. Users are allowed to install their own custom certificate.</td></tr><tr><td>Pool Certificate</td><td>Xenserver hosts present this certificate to clients which request <code>xapi:pool</code>as the SNI. They are used for host-to-host communications.</td></tr><tr><td>Common Criteria</td><td>Common Criteria for Information Technology Security Evaluation is a certification on computer security.</td></tr></tbody></table><h1 id=certificates-and-identity-management>Certificates and Identity management</h1><p>Currently Xenserver hosts generate self-signed certificates with the IP or FQDN as their subjects, users may also choose to install certificates.
When installing these certificates only the cryptographic algorithms used to generate the certificates (private key and hash) are validated and no properties about them are required.</p><p>This means that using user-installed certificates for intra-pool communication may prove difficult as restrictions regarding FQDN and chain validation need to be ensured before enabling TLS certificate checking or the pool communications will break down.</p><p>Instead a different certificate is used only for pool communication.
This allows to decouple whatever requirements users might have for the certificates they install to the requirements needed for secure pool communication.
This has several benefits:</p><ul><li>Frees the pool from ensuring a sound hostname resolution on the internal communications.</li><li>Allows the pool to rotate the certificates when it deems necessary. (in particular expiration, or forced invalidation)</li><li>Hosts never share a host certificate, and their private keys never get transmitted.</li></ul><p>In general, the project is able to more safely change the parameters of intra-pool communication without disrupting how users use custom certificates.</p><p>To be able to establish trust in a pool, hosts must distribute the certificates to the rest of the pool members.
Once that is done servers can verify whether they are connecting to another host in the pool by comparing the server certificate with the certificates in the trust root.
Certificate pinning is available and would allow more stringent checks, but it doesn&rsquo;t seem a necessity: hosts in a pool already share secret that allows them to have full control of the pool.</p><p>To be able to select a host certificate depending whether the connections is intra-pool or comes from API clients SNI will be used.
This allows clients to ask for a service when establishing a TLS connection.
This allows the server to choose the certificate they want to offer when negotiating the connection with the client.
The hosts will exploit this to request a particular service when they establish a connection with other hosts in the pool.
When initiating a connection to another host in the pool, a server will create requests for TLS connections with the server_name <code>xapi:pool</code> with the <code>name_type</code> <code>DNS</code>, this goes against RFC-6066 as this <code>server_name</code> is not resolvable.
This still works because we control the implementation in both peers of the connection and can follow the same convention.</p><p>In addition connections to the WLB appliance will continue to be validated using the current scheme of user-installed CA certificates.
This means that hosts connecting to the appliance will need a special case to only trust user-installed certificated when establishing the connection.
Conversely pool connections will ignore these certificates.</p><table><thead><tr><th>Name</th><th>Filesystem location</th><th>User-configurable</th><th>Used for</th></tr></thead><tbody><tr><td>Host Default</td><td>/etc/xensource/xapi-ssl.pem</td><td>yes (using API)</td><td>Hosts serve it to normal API clients</td></tr><tr><td>Host Pool</td><td>/etc/xensource/xapi-pool-tls.pem</td><td>no</td><td>Hosts serve to clients requesting &ldquo;xapi:pool&rdquo; as the SNI</td></tr><tr><td>Trusted Default</td><td>/etc/stunnel/certs/</td><td>yes (using API)</td><td>Certificates that users can install for trusting appliances</td></tr><tr><td>Trusted Pool</td><td>/etc/stunnel/certs-pool/</td><td>no</td><td>Certificates that are managed by the pool for host-to-host communications</td></tr><tr><td>Default Bundle</td><td>/etc/stunnel/xapi-stunnel-ca-bundle.pem</td><td>no</td><td>Bundle of certificates that hosts use to verify appliances (in particular WLB), this is kept in sync with &ldquo;Trusted Default&rdquo;</td></tr><tr><td>Pool Bundle</td><td>/etc/stunnel/xapi-pool-ca-bundle.pem</td><td>no</td><td>Bundle of certificates that hosts use to verify other hosts on pool communications, this is kept in sync with &ldquo;Trusted Pool&rdquo;</td></tr></tbody></table><h2 id=cryptography-of-certificates>Cryptography of certificates</h2><p>The certificates until now have been signed using sha256WithRSAEncryption:</p><ul><li>Pre-8.0 releases use 1024-bit RSA keys.</li><li>8.0, 8.1 and 8.2 use 2048-bit RSA keys.</li></ul><p>The Default Certificates served to API clients will continue to use sha256WithRSAEncryption with 2048-bit RSA keys. The Pool certificates will use the same algorithms for consistency.</p><p>The self-signed certificates until now have used a mix of IP and hostname claims:</p><ul><li>All released versions:<ul><li>Subject and issuer have CN FQDN if the hostname is different from localhost, or CN management IP</li><li>Subject Alternate Names extension contains all the domain names as DNS names</li></ul></li><li>Next release:<ul><li>Subject and issuer have CN management IP</li><li>SAN extension contains all domain names as DNS names and the management IP as IP</li></ul></li></ul><p>The Pool certificates do not contain claims about IPs nor hostnames as this may change during runtime and depending on their validity may make pool communication more brittle.
Instead the only claim they have is that their Issuer and their Subject are CN Host UUID, along with a serial number.</p><p>Self-signed certificates produced until now have had validity periods of 3650 days (~10 years).
The Pool certificates will have the same validity period.</p><h1 id=server-components>Server Components</h1><p>HTTPS Connections between hosts usually involve the xapi daemons and stunnel processes:</p><ul><li>When a xapi daemon needs to initiate a connection with another host it starts an HTTP connection with a local stunnel process.</li><li>The stunnel processes wrap http connections inside a TLS connection, allowing HTTPS to be used when hosts communicate</li></ul><p>This means that stunnel needs to be set up correctly to verify certificates when connecting to other hosts.
Some aspects like CA certificates are already managed, but certificate pinning is not.</p><h1 id=use-cases>Use Cases</h1><p>There are several use cases that need to be modified in order correctly manage trust between hosts.</p><h2 id=opening-a-connection-with-a-pool-host>Opening a connection with a pool host</h2><p>This is the main use case for the feature, the rest of use cases that need changes are modified to support this one.
Currently a Xenserver host connecting to another host within the pool does not try to authenticate the receiving server when opening a TLS connection.
(The receiving server authenticates the originating server by xapi authentication, see below)</p><p>Stunnel will be configured to verify the peer certificate against the CA certificates that are present in the host.
The CA certificates must be correctly set up when a host joins the pool to correctly establish trust.</p><p>The previous behaviour for WLB must be kept as the WLB connection <em>must</em> be checked against the user-friendly CA certificates.</p><h2 id=receiving-an-incoming-tls-connection>Receiving an incoming TLS connection</h2><p>All incoming connections authenticate the client using credentials, this does not need the addition of certificates.
(username and password, pool secret)</p><p>The hosts must present the certificate file to incoming connections so the client can authenticate them.
This is already managed by xapi, it configures stunnel to present the configured host certificate.
The configuration has to be changed so stunnel responds to SNI requests containing the string <code>xapi:pool</code> to serve the internal certificate instead of the client-installed one.</p><h2 id=u1-host-installation>U1. Host Installation</h2><p>On xapi startup an additional certificate is created now for pool operations.
It&rsquo;s added to the trusted pool certificates.
The certificate&rsquo;s only claim is the host&rsquo;s UUID.
No IP nor hostname information is kept as the clients only check for the certificate presence in the trust root.</p><h2 id=u2-pool-join>U2. Pool Join</h2><p>This use-case is delicate as it is the point where trust is established between hosts.
This is done with a call from the joiner to the pool coordinator where the certificate of the coordinator is not verified.
In this call the joiner transmits its certificate to the coordinator and the coordinator returns a list of the pool members&rsquo; UUIDs and certificates.
This means that in the policy used is trust on first use.</p><p>To deal with parallel pool joins, hosts download all the Pool certificates in the pool from the coordinator after all restarts.</p><p>The connection is initiated by a client, just like before, there is no change in the API as all the information needed to start the join is already provided (pool username and password, IP of coordinator)</p><pre class="mermaid align-center">
sequenceDiagram
participant clnt as Client
participant join as Joiner
participant coor as Coordinator
participant memb as Member
clnt-&gt;&gt;join: pool.join coordinator_ip coordinator_username coordinator_password
join-&gt;&gt;coor:login_with_password coordinator_ip coordinator_username coordinator_password

Note over join: pre_join_checks
join-&gt;&gt;join: remote_pool_has_tls_enabled = self_pool_has_tls_enabled
alt are different
Note over join: interrupt join, raise error
end
Note right of join: certificate distribution
coor--&gt;&gt;join:
join-&gt;&gt;coor: pool.internal_certificate_list_content
coor--&gt;&gt;join:

join-&gt;&gt;coor: pool.upload_identity_host_certificate joiner_certificate uuid
coor-&gt;&gt;memb: pool.internal_certificates_sync
memb--&gt;&gt;coor:

loop for every &lt;user CA certificate&gt; in Joiner
join-&gt;&gt;coor: Pool.install_ca_certitificate &lt;user CA certificate&gt;
coor--&gt;&gt;join:
end

loop for every &lt;user CRL&gt; in Joiner
join-&gt;&gt;coor: Pool.install_crl &lt;user CRL&gt;
coor--&gt;&gt;join:
end

join-&gt;&gt;coor: host.add joiner
coor--&gt;&gt;join:

join-&gt;&gt;join: restart_as_slave
join-&gt;&gt;coor: pool.user_certificates_sync
join-&gt;&gt;coor: host.copy_primary_host_certs</pre><h2 id=u3-pool-eject>U3. Pool Eject</h2><p>During pool eject the pool must remove the host certificate of the ejected member from the internal trust root, this must be done by the xapi daemon of the coordinator.</p><p>The ejected member will recreate both server certificates to replicate a new installation.
This can be triggered by deleting the certificates and their private keys in the host before rebooting, the current boot scripts automatically generates a new self-signed certificate if the file is not present.
Additionally, both the user and the internal trust roots will be cleared before rebooting as well.</p><h2 id=u4-pool-upgrade>U4. Pool Upgrade</h2><p>When a pool has finished upgrading to the version with certificate checking the database reflects that the feature is turned off, this is done as part of the database upgrade procedure in xen-api.
The internal certificate is created on restart.
It is added to the internal trusted certificates directory.
The distribution of certificate will happens when the tls verification is turned on, afterwards.</p><h2 id=u5-host-certificate-state-inspection>U5. Host certificate state inspection</h2><p>In order to give information about the validity and useful information of installed user-facing certificates to API clients as well as the certificates used for internal purposes, 2 fields are added to certificate records in xapi&rsquo;s datamodel and database:</p><ul><li>type: indicates which of the 3 kind of certificates is the certificate. If it&rsquo;s a user-installed trusted CA certificate, a server certificate served to clients that do not use SNI, and a server certificate served when the SNI xapi:pool is used. The exact values are ca, host and host-internal, respectively.</li><li>name: the human-readable name given by the user. This fields is only present on trusted CA certificates and allows the pool operators to better recognise the certificates.</li></ul><p>Additionally, now the _host field contains a null reference if the certificate is a corporate CA (a ca certificate).</p><p>The fields will get exposed in the CLI whenever a certificate record is listed, this needs a xapi-cli-server to be modified to show the new field.</p><h2 id=u6-migrating-a-vm-to-another-pool>U6. Migrating a VM to another pool</h2><p>To enable a frictionless migration when pools have tls verification enabled, the host certificate of the host receiving the vm is sent to the sender.
This is done by adding the certificate of the receiving host as well as its pool coordinator to the return value of the function migrate_receive function.
The sender can then add the certificate to the folder of CA certificates that stunnel uses to verify the server in a TLS connection.
When the transaction finishes, whether it fails or succeeds the CA certificate is deleted.</p><p>The certificate is stored in a temporary location so xapi can clean up the file when it starts up, in case after the host fences or power cycles while the migration is in progress.</p><p>Xapi invokes sparse_dd with the filename correct trusted bundle as a parameter so it can verify the vhd-server running on the other host.</p><p>Xapi also invokes xcp-rrdd to migrate the VM metrics.
xcp-rrdd is passed the 2 certificates to verify the remote hosts when sending the metrics.</p><p>Clients should not be aware of this change and require no change.</p><p>Xapi-cli-server, the server of xe embedded into xapi, connects to the remote coordinator using TLS to be able to initiate the migration.
Currently no verification is done. A certificate is required to initiate the connection to verify the remote server.</p><p>In u6.3 and u6.4 no changes seem necessary.</p><h2 id=u7-change-a-hosts-name>U7. Change a host&rsquo;s name</h2><p>The Pool certificates do not depend on hostnames.
Changing the hostnames does not affect TLS certificate verification in a pool.</p><h2 id=u8-installing-a-certificate-corporate-ca>U8. Installing a certificate (corporate CA)</h2><p>Installation of corporate CA can be done with current API.
Certificates are added to the database as CA certificates.</p><h2 id=u9-resetting-a-certificate-to-self-signed-certificate>U9. Resetting a certificate (to self-signed certificate)</h2><p>This needs a reimplementation of the current API to reset host certificate, this time allowing the operation to happen when the host is not on emergency node and to be able to do it remotely.</p><h2 id=u10-enabling-certificate-verification>U10. Enabling certificate verification</h2><p>A new API call is introduced to enable tls certificate verification: Pool.enable_tls_verification.
This is used by the CLI command pool-enable-tls-verification.
The call causes the coordinator of the pool to install the Pool certificates of all the members in its internal trust root.
Then calls the api for each member to install all of these certificates.
After this public key exchange is done, TLS certificate verification is enabled on the members, with the coordinator being the last to enable it.</p><p>When there are issues that block enabling the feature, the call returns an error specific to that problem:</p><ul><li>HA must not be enabled, as it can interrupt the procedure when certificates are distributed</li><li>Pool operations that can disrupt the certificate exchange block this operation: These operations are listed in here</li><li>There was an issue with the certificate exchange in the pool.</li></ul><p>The coordinator enabling verification last is done to ensure that if there is any issue enabling the coordinator host can still connect to members and rollback the setting.</p><p>A new field is added to the pool: tls_verification_enabled. This enables clients to query whether TLS verification is enabled.</p><h2 id=u11-disabling-certificate-verification>U11. Disabling certificate verification</h2><p>A new emergency command is added emergency-host-disable-tls-verification.
This command disables tls-verification for the xapi daemon in a host.
This allows the host to communicate with other hosts in the pool.</p><p>After that, the admin can regenerate the certificates using the new host-refresh-server-certificate in the hosts with invalid certificates, finally they can reenable tls certificate checking using the call emergency-host-reenable-tls-verification.</p><p>The documentation will include instructions for administrators on how to reset certificates and manually installing the host certificates as CA certificates to recover pools.</p><p>This means they will not have to disable TLS and compromise on security.</p><h2 id=u12-being-aware-of-certificate-expiry>U12. Being aware of certificate expiry</h2><p>Stockholm hosts provide alerts 30 days before hosts certificates expire, it must be changed to alert about users&rsquo; CA certificates expiring.</p><p>Pool certificates need to be cycled when the certificate expiry is approaching.
Alerts are introduced to warn the administrator this task must be done, or risk the operation of the pool.
A new API is introduced to create certificates for all members in a pool and replace the existing internal certificates with these.
This call imposes the same requirements in a pool as the pool secret rotation: It cannot be run in a pool unless all the host are online, it can only be started by the coordinator, the coordinator is in a valid state, HA is disabled, no RPU is in progress, and no pool operations are in progress.
The API call is Pool.rotate_internal_certificates.
It is exposed by xe as pool-rotate-internal-certificates.</p><h1 id=changes>Changes</h1><p>Xapi startup has to account for host changes that affect this feature and modify the filesystem and pool database accordingly.</p><ul><li>Public certificate changed: On first boot, after a pool join and when doing emergency repairs the server certificate record of the host may not match to the contents in the filesystem. A check is to be introduced that detects if the database does not associate a certificate with the host or if the certificate&rsquo;s public key in the database and the filesystem are different. If that&rsquo;s the case the database is updated with the certificate in the filesystem.</li><li>Pool certificate not present: In the same way the public certificate served is generated on startup, the internal certificate must be generated if the certificate is not present in the filesystem.</li><li>Pool certificate changed: On first boot, after a pool join and after having done emergency repairs the internal server certificate record may not match the contents of the filesystem. A check is to be introduced that detects if the database does not associate a certificate with the host or if the certificate&rsquo;s public key in the database and the filesystem are different. This check is made aware whether the host is joining a pool or is on first-boot, it does this by counting the amount of hosts in the pool from the database. In the case where it&rsquo;s joining a pool it simply updated the database record with the correct information from the filesystem as the filesystem contents have been put in place before the restart. In the case of first boot the public part of the certificate is copied to the directory and the bundle for internally-trusted certificates: /etc/stunnel/certs-pool/ and /etc/stunnel/xapi-pool-ca-bundle.pem.</li></ul><p>The xapi database records for certificates must be changed according with the additions explained before.</p><h3 id=api>API</h3><p>Additions</p><ul><li>Pool.tls_verification_enabled: this is a field that indicates whether TLS verification is enabled.</li><li>Pool.enable_tls_verification: this call is allowed for role _R_POOL_ADMIN. It&rsquo;s not allowed to run if HA is enabled nor pool operations are in progress. All the hosts in the pool transmit their certificate to the coordinator and the coordinator then distributes the certificates to all members of the pool. Once that is done the coordinator tries to initiate a session with all the pool members with TLS verification enabled. If it&rsquo;s successful TLS verification is enabled for the whole pool, otherwise the error COULD_NOT_VERIFY_HOST [member UUID] is emmited.</li><li>TLS_VERIFICATION_ENABLE_IN_PROGRESS is a new error that is produced when trying to do other pool operations while enabling TLS verification is in progress</li><li>Host.emergency_disable_tls_verification: this called is allowed for role _R_LOCAL_ROOT_ONLY: it&rsquo;s an emergency command and acts locally. It forces connections in xapi to stop verifying the peers on outgoing connections. It generates an alert to warn the administrators of this uncommon state.</li><li>Host.emergency_reenable_tls_verification: this call is allowed for role _R_LOCAL_ROOT_ONLY: it&rsquo;s an emergency command and acts locally. It changes the configuration so xapi verifies connections by default after being switched off with the previous command.</li><li>Pool.install_ca_certificate: rename of Pool.certificate_install, add the ca certificate to the database.</li><li>Pool.uninstall_ca_certificate: rename of Pool.certificate_uninstall, removes the certificate from the database.</li><li>Host.reset_server_certificate: replaces Host.emergency_reset_server_certificate, now it&rsquo;s allowed for role _R_POOL_ADMIN. It adds a record for the generated Default Certificate to the database while removing the previous record, if any.</li><li>Pool.rotate_internal_certificates: This call generates new Pool certificates, and substitutes the previous certificates with these. See the certificate expiry section for more details.</li></ul><p>Modifications:</p><ul><li>Pool.join: certificates must be correctly distributed. API Error POOL_JOINING_HOST_TLS_VERIFICATION_MISMATCH is returned if the tls_verification of the two pools doesn&rsquo;t match.</li><li>Pool.eject: all certificates must be deleted from the ejected host&rsquo;s filesystem and the ejected host&rsquo;s certificate must be deleted from the pool&rsquo;s trust root.</li><li>Host.install_server_certificate: the certificate type host for the record must be added to denote it&rsquo;s a Standard Certificate.</li></ul><p>Deprecations:</p><ul><li>pool.certificate_install</li><li>pool.certificate_uninstall</li><li>pool.certificate_list</li><li>pool.wlb_verify_cert: This setting is superseeded by pool.enable_tls_verification. It cannot be removed, however. When updating from a previous version when this setting is on, TLS connections to WLB must still verify the external host. When the global setting is enabled this setting is ignored.</li><li>host.emergency_reset_server_certificate: host.reset_server_certificate should be used instead as this call does not modify the database.</li></ul><h3 id=cli>CLI</h3><p>Following API additions:</p><ul><li>pool-enable-tls-verification</li><li>pool-install-ca-certificate</li><li>pool-uninstall-ca-certificate</li><li>pool-internal-certificates-rotation</li><li>host-reset-server-certificate</li><li>host-emergency-disable-tls-verification (emits a warning when verification is off and the pool-level is on)</li><li>host-emergency-reenable-tls-verification</li></ul><p>And removals:</p><ul><li>host-emergency-server-certificate</li></ul><h3 id=feature-flags>Feature Flags</h3><p>This feature needs clients to behave differently when initiating pool joins, to allow them to choose behaviour the toolstack will expose a new feature flag &lsquo;Certificate_verification&rsquo;. This flag will be part of the express edition as it&rsquo;s meant to aid detection of a feature and not block access to it.</p><h3 id=alerts>Alerts</h3><p>Several alerts are introduced:</p><ul><li><p>POOL_CA_CERTIFICATE_EXPIRING_30, POOL_CA_CERTIFICATE_EXPIRING_14, POOL_CA_CERTIFICATE_EXPIRING_07, POOL_CA_CERTIFICATE_EXPIRED: Similar to host certificates, now the user-installable pool&rsquo;s CA certificates are monitored for expiry dates and alerts are generated about them. The body for this type of message is:</p><body><message>The trusted TLS server certificate {is expiring soon|has expired}.</message><date>20210302T02:00:01Z</date></body></li><li><p>HOST_INTERNAL_CERTIFICATE_EXPIRING_30, HOST_INTERNAL_CERTIFICATE_EXPIRING_14, HOST_INTERNAL_CERTIFICATE_EXPIRING_07, HOST_INTERNAL_CERTIFICATE_EXPIRED: Similar to host certificates, the newly-introduced hosts&rsquo; internal server certificates are monitored for expiry dates and alerts are generated about them. The body for this type of message is:</p><body><message>The TLS server certificate for internal communications {is expiring soon|has expired}.</message><date>20210302T02:00:01Z</date></body></li><li><p>TLS_VERIFICATION_EMERGENCY_DISABLED: The host is in emergency mode and is not enforcing tls verification anymore, the situation that forced the disabling must be fixed and the verification enabled ASAP.</p><body><host>HOST-UUID</host></body></li><li><p>FAILED_LOGIN_ATTEMPTS: An hourly alert that contains the number of failed attempts and the 3 most common origins for these failed alerts. The body for this type of message is:</p><body><total>35</total>
<known><username>usr5</username><originator>origin5</originator><ip>5.4.3.2</ip><number>10</number><date>20200922T15:03:13Z</date></known>
<known><username>usr4</username><useragent>UA</useragent><number>6</number><date>20200922T15:03:13Z</date></known>
<known><useragent>UA</useragent><ip>4.3.2.1</ip><number>4</number><date>20200922T14:57:11Z</date></known>
<unknown>10</unknown></body></li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (5.6 fp1)</span></td></tr></table></header><h1 id=tunnelling-api-design>Tunnelling API design</h1><p>To isolate network traffic between VMs (e.g. for security reasons) one can use
VLANs. The number of possible VLANs on a network, however, is limited, and
setting up a VLAN requires configuring the physical switches in the network.
GRE tunnels provide a similar, though more flexible solution. This document
proposes a design that integrates the use of tunnelling in the XenAPI. The
design relies on the recent introduction of the Open vSwitch, and
requires an Open vSwitch
(<a href=https://www.opennetworking.org/sdn-resources/openflow rel=external target=_blank>OpenFlow</a>) controller
(further referred to as
<em>the controller</em>) to set up and maintain the actual GRE tunnels.</p><p>We suggest following the way VLANs are modelled in the datamodel. Introducing a
VLAN involves creating a Network object for the VLAN, that VIFs can connect to.
The <code>VLAN.create</code> API call takes references to a PIF and Network to use and a
VLAN tag, and creates a VLAN object and a PIF object. We propose something
similar for tunnels; the resulting objects and relations for two hosts would
look like this:</p><pre><code>PIF (transport) -- Tunnel -- PIF (access) \          / VIF
                                            Network -- VIF
PIF (transport) -- Tunnel -- PIF (access) /          \ VIF
</code></pre><h2 id=xenapi-changes>XenAPI changes</h2><h3 id=new-tunnel-class>New tunnel class</h3><h4 id=fields>Fields</h4><ul><li><code>string uuid</code> (read-only)</li><li><code>PIF ref access_PIF</code> (read-only)</li><li><code>PIF ref transport_PIF</code> (read-only)</li><li><code>(string -> string) map status</code> (read/write); owned by the controller, containing at least the
key <code>active</code>, and <code>key</code> and <code>error</code> when appropriate (see below)</li><li><code>(string -> string) map other_config</code> (read/write)</li></ul><p>New fields in PIF class (automatically linked to the corresponding <code>tunnel</code>
fields):</p><ul><li><code>PIF ref set tunnel_access_PIF_of</code> (read-only)</li><li><code>PIF ref set tunnel_transport_PIF_of</code> (read-only)</li></ul><h4 id=messages>Messages</h4><ul><li><code>tunnel ref create (PIF ref, network ref)</code></li><li><code>void destroy (tunnel ref)</code></li></ul><h3 id=backends>Backends</h3><p>For clients to determine which network backend is in use (to decide whether
tunnelling functionality is enabled) a key <code>network_backend</code> is added to the
<code>Host.software_version</code> map on each host. The value of this key can be:</p><ul><li><code>bridge</code>: the Linux bridging backend is in use;</li><li><code>openvswitch</code>: the [Open vSwitch] backend is in use.</li></ul><h3 id=notes>Notes</h3><ul><li><p>The user is responsible for creating tunnel and network objects, associating
VIFs with the right networks, and configuring the physical PIFs, all using
the XenAPI/CLI/XC.</p></li><li><p>The <code>tunnel.status</code> field is owned by the controller. It
may be possible to define an RBAC role for the controller, such that only the
controller is able to write to it.</p></li><li><p>The <code>tunnel.create</code> message does not take
a tunnel identifier (GRE key). The controller is responsible for assigning
the right keys transparently. When a tunnel has been set up, the controller
will write its key to <code>tunnel.status:key</code>, and it will set
<code>tunnel.status:active</code> to <code>"true"</code> in the same field.</p></li><li><p>In case a tunnel could
not be set up, an error code (to be defined) will be written to
<code>tunnel.status:error</code>, and <code>tunnel.status:active</code> will be <code>"false"</code>.</p></li></ul><h2 id=xapi>Xapi</h2><h3 id=tunnelcreate>tunnel.create</h3><ul><li>Fails with <code>OPENVSWITCH_NOT_ACTIVE</code> if the Open vSwitch networking sub-system
is not active (the host uses linux bridging).</li><li>Fails with <code>IS_TUNNEL_ACCESS_PIF</code> if the specified transport PIF is a tunnel access PIF.</li><li>Takes care of creating and connecting the new tunnel and PIF objects.<ul><li>Sets a random MAC on the access PIF.</li><li>IP configuration of the tunnel
access PIF is left blank. (The IP configuration on a PIF is normally used for
the interface in dom0. In this case, there is no tunnel interface for dom0 to
use. Such functionality may be added in future.)</li><li>The <code>tunnel.status:active</code>
field is initialised to <code>"false"</code>, indicating that no actual tunnelling
infrastructure has been set up yet.</li></ul></li><li>Calls <code>PIF.plug</code> on the new tunnel access PIF.</li></ul><h3 id=tunneldestroy>tunnel.destroy</h3><ul><li>Calls <code>PIF.unplug</code> on the tunnel access PIF. Destroys the <code>tunnel</code> and
tunnel access PIF objects.</li></ul><h3 id=pifplug-on-a-tunnel-access-pif>PIF.plug on a tunnel access PIF</h3><ul><li>Fails with <code>TRANSPORT_PIF_NOT_CONFIGURED</code> if the underlying transport PIF has
<code>PIF.ip_configuration_mode = None</code>, as this interface needs to be configured
for the tunnelling to work. Otherwise, the transport PIF will be plugged.</li><li>Xapi requests <code>interface-reconfigure</code> to &ldquo;bring up&rdquo; the tunnel access PIF,
which causes it to create a local bridge.</li><li>No link will be made between the
new bridge and the physical interface by <code>interface-reconfigure</code>. The
controller is responsible for setting up these links. If the controller is
not available, no links can be created, and the tunnel network degrades to an
internal network (only intra-host connectivity).</li><li><code>PIF.currently_attached</code> is set to <code>true</code>.</li></ul><h3 id=pifunplug-on-a-tunnel-access-pif>PIF.unplug on a tunnel access PIF</h3><ul><li>Xapi requests <code>interface-reconfigure</code> to &ldquo;bring down&rdquo; the tunnel PIF, which
causes it to destroy the local bridge.</li><li><code>PIF.currently_attached</code> is set to <code>false</code>.</li></ul><h3 id=pifunplug-on-a-tunnel-transport-pif>PIF.unplug on a tunnel transport PIF</h3><ul><li>Calls <code>PIF.unplug</code> on the associated tunnel access PIF(s).</li></ul><h3 id=pifforget-on-a-tunnel-access-of-transport-pif>PIF.forget on a tunnel access of transport PIF</h3><ul><li>Fails with <code>PIF_TUNNEL_STILL_EXISTS</code>.</li></ul><h3 id=vlancreate>VLAN.create</h3><ul><li>Tunnels can only exist on top of physical/VLAN/Bond PIFs, and not the other
way around. <code>VLAN.create</code> fails with <code>IS_TUNNEL_ACCESS_PIF</code> if given an
underlying PIF that is a tunnel access PIF.</li></ul><h3 id=pool-join>Pool join</h3><ul><li>As for VLANs, when a host joins a pool, it will inherit the tunnels that are
present on the pool master.</li><li>Any tunnels (tunnel and access PIF objects)
configured on the host are removed, which will leave their networks
disconnected (the networks become internal networks). As a joining host is
always a single host, there is no real use for having had tunnels on it, so
this probably will never be an issue.</li></ul><h2 id=the-controller>The controller</h2><ul><li>The controller tracks the <code>tunnel</code> class to determine which bridges/networks
require GRE tunnelling.<ul><li>On start-up, it calls <code>tunnel.get_all</code> to obtain the information about all
tunnels.</li><li>Registers for events on the <code>tunnel</code> class to stay up-to-date.</li></ul></li><li>A tunnel network is organised as a star topology. The controller is free to
decide which host will be the central host (&ldquo;switching host&rdquo;).</li><li>If the
current switching host goes down, a new one will be selected, and GRE tunnels
will be reconstructed.</li><li>The controller creates GRE tunnels connecting each
existing Open vSwitch bridge that is associated with the same tunnel network,
after assigning the network a unique GRE key.</li><li>The controller destroys GRE
tunnels if associated Open vSwitch bridges are destroyed. If the destroyed
bridge was on the switching host, and other hosts are still using the same
tunnel network, a new switching host will be selected, and GRE tunnels will
be reconstructed.</li><li>The controller sets <code>tunnel.status:active</code> to <code>"true"</code> for
all tunnel links that have been set up, and <code>"false"</code> if links are broken.</li><li>The controller writes an appropriate error code (to be defined) to
<code>tunnel.status:error</code> in case something went wrong.</li><li>When an access PIF is
plugged, and the controller succeeds to set up the tunnelling infrastructure,
it writes the GRE key to <code>tunnel.status:key</code> on the associated tunnel object
(at the same time <code>tunnel.status:active</code> will be set to <code>"true"</code>).</li><li>When the
tunnel infrastructure is not up and running, the controller may remove the
key <code>tunnel.status:key</code> (optional; the key should anyway be disregarded if
<code>tunnel.status:active</code> is <code>"false"</code>).</li></ul><h2 id=cli>CLI</h2><p>New <code>xe</code> commands (analogous to <code>xe vlan-</code>):</p><ul><li><code>tunnel-create</code></li><li><code>tunnel-destroy</code></li><li><code>tunnel-list</code></li><li><code>tunnel-param-get</code></li><li><code>tunnel-param-list</code></li></ul><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (8.2)</span></td></tr></table></header><h1 id=user-installable-host-certificates>User-installable host certificates</h1><h2 id=introduction>Introduction</h2><p>It is often necessary to replace the TLS certificate used to secure
communications to Xenservers hosts, for example to allow a XenAPI user such as
Citrix Virtual Apps and Desktops (CVAD) to validate that the host is genuine
and not impersonating the actual host.</p><p>Historically there has not been a supported mechanism to do this, and as a
result users have had to rely on guides written by third parties that show how
to manually replace the xapi-ssl.pem file on a host. This process is
error-prone, and if a mistake is made, can result in an unusable system.
This design provides a fully supported mechanism to allow replacing the
certificates.</p><h2 id=design-proposal>Design proposal</h2><p>It is expected that an API caller will provide, in a single API call, a private
key, and one or more certificates for use on the host. The key will be provided
in PKCS #8 format, and the certificates in X509 format, both in
base-64-encoded PEM containers.</p><p>Multiple certificates can be provided to cater for the case where an
intermediate certificate or certificates are required for the caller to be able
to verify the certificate back to a trusted root (best practice for Certificate
Authorities is to have an &lsquo;offline&rsquo; root, and issue certificates from an
intermediate Certificate Authority). In this situation, it is expected (and
common practice among other tools) that the first certificate provided in the
chain is the host&rsquo;s unique server certificate, and subsequent certificates form
the chain.</p><p>To detect mistakes a user may make, certain checks will be carried out on the
provided key and certificate(s) before they are used on the host. If all checks
pass, the key and certificate(s) will be written to the host, at which stage a
signal will be sent to stunnel that will cause it to start serving the new
certificate.</p><h2 id=certificate-installation>Certificate Installation</h2><h3 id=api-additions>API Additions</h3><p>Xapi must provide an API call through Host RPC API to install host
certificates:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#66d9ef>let</span> install_server_certificate <span style=color:#f92672>=</span> call
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>lifecycle<span style=color:#f92672>:[</span><span style=color:#a6e22e>Published</span><span style=color:#f92672>,</span> rel_stockholm<span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>name<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;install_server_certificate&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>doc<span style=color:#f92672>:</span><span style=color:#e6db74>&#34;Install the TLS server certificate.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>versioned_params<span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>[{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>Ref</span> <span style=color:#f92672>_</span>host<span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;host&#34;</span><span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The host&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>None</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>;{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>String</span><span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;certificate&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The server certificate, in PEM form&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>None</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>;{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>String</span><span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;private_key&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The unencrypted private key used to sign the certificate, \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    in PKCS#8 form&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>None</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>;{</span> param_type<span style=color:#f92672>=</span><span style=color:#a6e22e>String</span><span style=color:#f92672>;</span> param_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;certificate_chain&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_doc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;The certificate chain, in PEM form&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#f92672>;</span> param_release<span style=color:#f92672>=</span>stockholm_release<span style=color:#f92672>;</span> param_default<span style=color:#f92672>=</span><span style=color:#a6e22e>Some</span> <span style=color:#f92672>(</span><span style=color:#a6e22e>VString</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>)}</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>~</span>allowed_roles<span style=color:#f92672>:_</span>R_POOL_ADMIN
</span></span><span style=display:flex><span>    ()</span></span></code></pre></div><p>This call should be implemented within xapi, using the already-existing crypto
libraries available to it.</p><p>Analogous to the API call, a new CLI call <code>host-server-certificate-install</code>
must be introduced, which takes the parameters <code>certificate</code>, <code>key</code> and
<code>certificate-chain</code> - these parameters are expected to be filenames, from which
the key and certificate(s) must be read, and passed to the
<code>install_server_certificate</code> RPC call.</p><p>The CLI will be defined as:</p><div class="highlight wrap-code"><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ocaml data-lang=ocaml><span style=display:flex><span><span style=color:#e6db74>&#34;host-server-certificate-install&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  reqd<span style=color:#f92672>=[</span><span style=color:#e6db74>&#34;certificate&#34;</span><span style=color:#f92672>;</span> <span style=color:#e6db74>&#34;private-key&#34;</span><span style=color:#f92672>];</span>
</span></span><span style=display:flex><span>  optn<span style=color:#f92672>=[</span><span style=color:#e6db74>&#34;certificate-chain&#34;</span><span style=color:#f92672>];</span>
</span></span><span style=display:flex><span>  help<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Install a server TLS certificate on a host&#34;</span><span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  implementation<span style=color:#f92672>=</span><span style=color:#a6e22e>With_fd</span> Cli_operations.host_install_server_certificate<span style=color:#f92672>;</span>
</span></span><span style=display:flex><span>  flags<span style=color:#f92672>=[</span> <span style=color:#a6e22e>Host_selectors</span> <span style=color:#f92672>];</span>
</span></span><span style=display:flex><span><span style=color:#f92672>};</span></span></span></code></pre></div><h3 id=validation>Validation</h3><p>Xapi must perform the following validation steps on the provided key and
certificate. If any validation step fails, the API call must return an error
with the specified error code, providing any associated text:</p><h3 id=private-key>Private Key</h3><ul><li><p>Validate that it is a pem-encoded PKCS#8 key, use error
<code>SERVER_CERTIFICATE_KEY_INVALID []</code> and exposed as
&ldquo;The provided key is not in a pem-encoded PKCS#8 format.&rdquo;</p></li><li><p>Validate that the algorithm of the key is RSA, use error
<code>SERVER_CERTIFICATE_KEY_ALGORITHM_NOT_SUPPORTED, [&lt;algorithms's ASN.1 OID>]</code>
and exposed as &ldquo;The provided key uses an unsupported algorithm.&rdquo;</p></li><li><p>Validate that the key length is â‰¥ 2048, and â‰¤ 4096 bits, use error
<code>SERVER_CERTIFICATE_KEY_RSA_LENGTH_NOT_SUPPORTED, [length]</code> and exposed as
&ldquo;The provided RSA key does not have a length between 2048 and 4096.&rdquo;</p></li><li><p>The library used does not support multi-prime RSA keys, when it&rsquo;s
encountered use error <code>SERVER_CERTIFICATE_KEY_RSA_MULTI_NOT_SUPPORTED []</code> and
exposed as &ldquo;The provided RSA key is using more than 2 primes, expecting only
2&rdquo;</p></li></ul><h4 id=server-certificate>Server Certificate</h4><ul><li><p>Validate that it is a pem-encoded X509 certificate, use error
<code>SERVER_CERTIFICATE_INVALID []</code> and exposed as &ldquo;The provided certificate is not
in a pem-encoded X509.&rdquo;</p></li><li><p>Validate that the public key of the certificate matches the public key from
the private key, using error <code>SERVER_CERTIFICATE_KEY_MISMATCH []</code> and exposing
it as &ldquo;The provided key does not match the provided certificate&rsquo;s public key.&rdquo;</p></li><li><p>Validate that the certificate is currently valid. (ensure all time
comparisons are done using UTC, and any times presented in errors are using
ISO8601 format):</p><ul><li><p>Ensure the certificate&rsquo;s <code>not_before</code> date is â‰¤ NOW
<code>SERVER_CERTIFICATE_NOT_VALID_YET, [&lt;NOW>; &lt;not_before>]</code> and exposed as
&ldquo;The provided certificate certificate is not valid yet.&rdquo;</p></li><li><p>Ensure the certificate&rsquo;s <code>not_after</code> date is > NOW
<code>SERVER_CERTIFICATE_EXPIRED, [&lt;NOW>; &lt;not_after>]</code> and exposed as &ldquo;The
provided certificate has expired.&rdquo;</p></li></ul></li><li><p>Validate that the certificate signature algorithm is SHA-256
<code>SERVER_CERTIFICATE_SIGNATURE_NOT_SUPPORTED []</code> and exposed as
&ldquo;The provided certificate is not using the SHA256 (SHA2) signature algorithm.&rdquo;</p></li></ul><h4 id=intermediate-certificates>Intermediate Certificates</h4><ul><li>Validate that it is an X509 certificate, use
<code>SERVER_CERTIFICATE_CHAIN_INVALID []</code> and exposed as &ldquo;The provided
intermediate certificates are not in a pem-encoded X509.&rdquo;</li></ul><h3 id=filesystem-interaction>Filesystem Interaction</h3><p>If validation has been completed successfully, a temporary file must be created
with permissions 0x400 containing the key and certificate(s), in that order,
separated by an empty line.</p><p>This file must then be atomically moved to /etc/xensource/xapi-ssl.pem in
order to ensure the integrity of the contents. This may be done using rename
with the origin and destination in the same mount-point.</p><h2 id=alerting>Alerting</h2><p>A daily task must be added. This task must check the expiry date of the first
certificate present in /etc/xensource/xapi-ssl.pem, and if it is within 30
days of expiry, generate a <code>message</code> to alert the administrator that the
certificate is due to expire shortly.</p><p>The body of the message should contain:</p><div class="highlight wrap-code"><pre tabindex=0><code>&lt;body&gt;
  &lt;message&gt;
    The TLS server certificate is expiring soon
  &lt;/message&gt;
  &lt;date&gt;
    &lt;expiry date in ISO8601 &#39;YYYY-MM-DDThh:mm:ssZ&#39; format&gt;`
  &lt;/date&gt;
&lt;/body&gt;</code></pre></div><p>The priority of the message should be based on the number of days to expiry as
follows:</p><table><thead><tr><th>Number of days</th><th>Priority</th></tr></thead><tbody><tr><td>0-7</td><td>1</td></tr><tr><td>8-14</td><td>2</td></tr><tr><td>14+</td><td>3</td></tr></tbody></table><p>The other fields of the message should be:</p><table><thead><tr><th>Field</th><th>Value</th></tr></thead><tbody><tr><td>name</td><td>HOST_SERVER_CERTIFICATE_EXPIRING</td></tr><tr><td>class</td><td>Host</td></tr><tr><td>obj-uuid</td><td>&lt; Host UUID ></td></tr></tbody></table><p>Any existing <code>HOST_SERVER_CERTIFICATE_EXPIRING</code> messages with this host&rsquo;s UUID
should be removed to avoid a build-up of messages.</p><p>Additionally, the task may also produce messages for expired server
certificates which must use the name <code>HOST_SERVER_CERTIFICATE_EXPIRED</code>.
This kind of message must contain the message &ldquo;The TLS server certificate has
expired.&rdquo; as well as the expiry date, like the expiring messages.
They also may replace the existing expiring messages in a host.</p><h2 id=expose-certificate-metadata>Expose Certificate metadata</h2><p>Currently xapi exposes a CLI command to print the certificate being used to
verify external hosts. We would like to also expose through the API and the
CLI useful metadata about the certificates in use by each host.</p><p>The new class is meant to cover server certificates and trusted certificates.</p><h3 id=schema>Schema</h3><p>A new class, Certificate, will be added with the following schema:</p><table><thead><tr><th>Field</th><th>Type</th><th>Notes</th></tr></thead><tbody><tr><td>uuid</td><td></td><td></td></tr><tr><td>type</td><td>CA</td><td>Certificate trusted by all hosts</td></tr><tr><td></td><td>Host</td><td>Certificate that the host presents to normal clients</td></tr><tr><td>name</td><td>String</td><td>Name, only present for trusted certificates</td></tr><tr><td>host</td><td>Ref _host</td><td>Host where the certificate is installed</td></tr><tr><td>not_before</td><td>DateTime</td><td>Date after which the certificate is valid</td></tr><tr><td>not_after</td><td>DateTime</td><td>Date before which the certificate is valid</td></tr><tr><td>fingerprint_sha256</td><td>String</td><td>The certificate&rsquo;s SHA256 fingerprint / hash</td></tr><tr><td>fingerprint_sha1</td><td>String</td><td>The certificate&rsquo;s SHA1 fingerprint / hash</td></tr></tbody></table><h3 id=cli--api>CLI / API</h3><p>There are currently-existing CLI parameters for certificates:
<code>pool-certificate-{install,uninstall,list,sync}</code>,
<code>pool-crl-{install,uninstall,list}</code> and <code>host-get-server-certificate</code>.</p><p>The new command must show the metadata of installed server certificates in
the pool.
It must be able to show all of them in the same call, and be able to filter
the certificates per-host.</p><p>To make it easy to separate it from the previous calls and to reflect that
certificates are a class type in xapi the call will be named <code>certificate-list</code>
and it will accept the parameter <code>host-uuid=&lt;uuid></code>.</p><h2 id=recovery-mechanism>Recovery mechanism</h2><p>In the case a certificate is let to expire TLS clients connecting to the host
will refuse to establish the connection.
This means that the host is going to be unable to be managed using the xapi
API (Xencenter, or a CVAD control plane)</p><p>There needs to be a mechanism to recover from this situation.
A CLI command must be provided to install a self-signed certificate, in the
same way it is generated during the setup process at the moment.
The command will be <code>host-emergency-reset-server-certificate</code>.
This command is never to be forwarded to another host and will call openssl to
create a new RSA private key</p><p>The command must notify stunnel to make sure stunnel uses the newly-created
certificate.</p><h1 id=miscellaneous>Miscellaneous</h1><p>The auto-generated <code>xapi-ssl.pem</code> currently contains Diffie-Hellman (DH)
Parameters, specifically 512 bits worth. We no longer support any ciphers which
require DH parameters, so these are no longer needed, and it is acceptable for
them to be lost as part of installing a new certificate/key pair.</p><p>The generation should also be modified to avoid creating these for new
installations.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr><tr><td>Review</td><td><a href=http://github.com/xapi-project/xapi-project.github.io/issues/156>#156</a></td></tr><tr><th colspan=2>Revision history</th></tr><tr><td><span class="label label-default">v1</span></td><td>Initial version</td></tr></table></header><h1 id=vgpu-type-identifiers>VGPU type identifiers</h1><h2 id=introduction>Introduction</h2><p>When xapi starts, it may create a number of VGPU_type objects. These act as
VGPU presets, and exactly which VGPU_type objects are created depends on the
installed hardware and in certain cases the presence of certain files in dom0.</p><p>When deciding which VGPU_type objects need to be created, xapi needs to
determine whether a suitable VGPU_type object already exists, as there should
never be duplicates. At the moment the combination of vendor name and model name
is used as a primary key, but this is not ideal as these values are subject to
change. We therefore need a way of creating a primary key to uniquely identify
VGPU_type objects.</p><h2 id=identifier>Identifier</h2><p>We will add a new read-only field to the database:</p><ul><li><code>VGPU_type.identifier (string)</code></li></ul><p>This field will contain a string representation of the parameters required to
uniquely identify a VGPU_type. The parameters required can be summed up with the
following OCaml type:</p><div class="highlight wrap-code"><pre tabindex=0><code>type nvidia_id = {
  pdev_id : int;
  psubdev_id : int option;
  vdev_id : int;
  vsubdev_id : int;
}

type gvt_g_id = {
  pdev_id : int;
  low_gm_sz : int64;
  high_gm_sz : int64;
  fence_sz : int64;
  monitor_config_file : string option;
}

type t =
  | Passthrough
  | Nvidia of nvidia_id
  | GVT_g of gvt_g_id</code></pre></div><p>When converting this type to a string, the string will always be prefixed with
<code>0001:</code> enabling future versioning of the serialisation format.</p><p>For passthrough, the string will simply be:</p><p><code>0001:passthrough</code></p><p>For NVIDIA, the string will be <code>nvidia</code> followed by the four device IDs
serialised as four-digit hex values, separated by commas. If <code>psubdev_id</code> is
<code>None</code>, the empty string will be used e.g.</p><div class="highlight wrap-code"><pre tabindex=0><code>Nvidia {
  pdev_id = 0x11bf;
  psubdev_id = None;
  vdev_id = 0x11b0;
  vsubdev_id = 0x109d;
}</code></pre></div><p>would map to</p><p><code>0001:nvidia,11bf,,11b0,109d</code></p><p>For GVT-g, the string will be <code>gvt-g</code> followed by the physical device ID encoded
as four-digit hex, followed by <code>low_gm_sz</code>, <code>high_gm_sz</code> and <code>fence_sz</code> encoded
as hex, followed by <code>monitor_config_file</code> (or the empty string if it is <code>None</code>)
e.g.</p><div class="highlight wrap-code"><pre tabindex=0><code>GVT_g {
  pdev_id = 0x162a;
  low_gm_sz = 128L;
  high_gm_sz = 384L;
  fence_sz = 4L;
  monitor_config_file = None;
}</code></pre></div><p>would map to</p><p><code>0001:gvt-g,162a,80,180,4,,</code></p><p>Having this string in the database will allow us to do a simple lookup to test
whether a certain VGPU_type already exists. Although it is not currently
required, this string can also be converted back to the type from which it was
generated.</p><p>When deciding whether to create VGPU_type objects, xapi will generate the
identifier string and use it to look for existing VGPU_type objects in the
database. If none are found, xapi will look for existing VGPU_type objects with
the tuple of model name and vendor name. If still none are found, xapi will
create a new VGPU_type object.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v1</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-success">released (7.0)</span></td></tr></table></header><h1 id=virtual-hardware-platform-version>Virtual Hardware Platform Version</h1><h3 id=background-and-goal>Background and goal</h3><p>Some VMs can only be run on hosts of sufficiently recent versions.</p><p>We want a clean way to ensure that xapi only tries to run a guest VM on a host that supports the &ldquo;virtual hardware platform&rdquo; required by the VM.</p><h3 id=suggested-design>Suggested design</h3><ul><li>In the datamodel, VM has a new integer field &ldquo;hardware_platform_version&rdquo; which defaults to zero.</li><li>In the datamodel, Host has a corresponding new integer-list field &ldquo;virtual_hardware_platform_versions&rdquo; which defaults to list containing a single zero element (i.e. <code>[0]</code> or <code>[0L]</code> in OCaml notation). The zero represents the implicit version supported by older hosts that lack the code to handle the Virtual Hardware Platform Version concept.</li><li>When a host boots it populates its own entry from a hardcoded value, currently <code>[0; 1]</code> i.e. a list containing the two integer elements <code>0</code> and <code>1</code>. (Alternatively this could come from a config file.)<ul><li>If this new version-handling functionality is introduced in a hotfix, at some point the pool master will have the new functionality while at least one slave does not. An old slave-host that does not yet have software to handle this feature will not set its DB entry, which will therefore remain as <code>[0]</code> (maintained in the DB by the master).</li></ul></li><li>The existing test for whether a VM can run on (or migrate to) a host must include a check that the VM&rsquo;s virtual hardware platform version is in the host&rsquo;s list of supported versions.</li><li>When a VM is made to start using a feature that is available only in a certain virtual hardware platform version, xapi must set the VM&rsquo;s hardware_platform_version to the maximum of that version-number and its current value (i.e. raise if needed).</li></ul><p>For the version we could consider some type other than integer, but a strict ordering is needed.</p><h3 id=first-use-case>First use-case</h3><p>Version 1 denotes support for a certain feature:</p><blockquote><p>When a VM starts, if a certain flag is set in VM.platform then XenServer will provide an emulated PCI device which will trigger the guest Windows OS to seek drivers for the device, or updates for those drivers. Thus updated drivers can be obtained through the standard Windows Update mechanism.</p></blockquote><p>If the PCI device is removed, the guest OS will fail to boot. A VM using this feature must not be migrated to or started on a XenServer that lacks support for the feature.</p><p>Therefore at VM start, we can look at whether this feature is being used; if it is, then if the VM&rsquo;s Virtual Hardware Platform Version is less than 1 we should raise it to 1.</p><h3 id=limitation>Limitation</h3><p>Consider a VM that requires version 1 or higher. Suppose it is exported, then imported into an old host that does not support this feature. Then the host will not check the versions but will attempt to run the VM, which will then have difficulties.</p><p>The only way to prevent this would be to make a backwards-incompatible change to the VM metadata (e.g. a new item in an enum) so that the old hosts cannot read it, but that seems like a bad idea.</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v2</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=xenprep>XenPrep</h1><h3 id=background>Background</h3><p>Windows guests should have XenServer-specific drivers installed. As of mid-2015 these have been always been installed and upgraded by an essentially manual process involving an ISO carrying the drivers. We have a plan to enable automation through the standard Windows Update mechanism. This will involve a new additional virtual PCI device being provided to the VM, to trigger Windows Update to fetch drivers for the device.</p><p>There are many existing Windows guests that have drivers installed already. These drivers must be uninstalled before the new drivers are installed (and ideally before the new PCI device is added). To make this easier, we are planning a XenAPI call that will cause the removal of the old drivers and the addition of the new PCI device.</p><p>Since this is only to help with updating old guests, the call may well be removed at some point in the future.</p><h3 id=brief-high-level-design>Brief high-level design</h3><p>The XenAPI call will be called <code>VM.xenprep_start</code>. It will update the VM record to note that the process has started, and will insert a special ISO into the VM&rsquo;s virtual CD drive.</p><p>That ISO will contain a tool which will be set up to auto-run (if auto-run is enabled in the guest). The tool will:</p><ol><li>Lock the CD drive so other Windows programs cannot eject the disc.</li><li>Uninstall the old drivers.</li><li>Eject the CD to signal success.</li><li>Shut down the VM.</li></ol><p>XenServer will interpret the ejection of the CD as a success signal, and when the VM shuts down without the special ISO in the drive, XenServer will:</p><ol><li>Update the VM record:</li></ol><ul><li>Remove the mark that shows that the xenprep process is in progress</li><li>Give it the new PCI device: set <code>VM.auto_update_drivers</code> to <code>true</code>.</li><li>If <code>VM.virtual_hardware_platform_version</code> is less than 2, then set it to 2.</li></ul><ol start=2><li>Start the VM.</li></ol><h3 id=more-details-of-the-xapi-project-parts>More details of the xapi-project parts</h3><p>(The tool that runs in the guest is out of scope for this document.)</p><h4 id=start>Start</h4><p>The XenAPI call <code>VM.xenprep_start</code> will throw a power-state error if the VM is not running.
For RBAC roles, it will be available to &ldquo;VM Operator&rdquo; and above.</p><p>It will:</p><ol><li>Insert the xenprep ISO into the VM&rsquo;s virtual CD drive.</li><li>Write <code>VM.other_config</code> key <code>xenprep_progress=ISO_inserted</code> to record the fact that the xenprep process has been initiated.</li></ol><p>If <code>xenprep_start</code> is called on a VM already undergoing xenprep, the call will return successfully but will not do anything.</p><p>If the VM does not have an empty virtual CD drive, the call will fail with a suitable error.</p><h4 id=cancellation>Cancellation</h4><p>While xenprep is in progress, any request to eject the xenprep ISO (except from inside the guest) will be rejected with a new error &ldquo;VBD_XENPREP_CD_IN_USE&rdquo;.</p><p>There will be a new XenAPI call <code>VM.xenprep_abort</code> which will:</p><ol><li>Remove the <code>xenprep_progress</code> entry from <code>VM.other_config</code>.</li><li>Make a best-effort attempt to eject the CD. (The guest might prevent ejection.)</li></ol><p>This is not intended for cancellation while the xenprep tool is running, but rather for use before it starts, for example if auto-run is disabled or if the VM has a non-Windows OS.</p><h4 id=completion>Completion</h4><p>Aim: when the guest shuts down after ejecting the CD, XenServer will start the guest again with the new PCI device.</p><p>Xapi works through the queue of events it receives from xenopsd. It is possible that by the time xapi processes the cd-eject event, the guest might have shut down already.</p><p>When the shutdown (not reboot) event is handled, we shall check whether we need to do anything xenprep-related. If</p><ul><li>The VM <code>other_config</code> map has <code>xenprep_progress</code> as either of <code>ISO_inserted</code> or <code>shutdown</code>, and</li><li>The xenprep ISO is no longer in the drive</li></ul><p>then we must (in the specified order)</p><ol><li>Update the VM record:</li><li>In <code>VM.other_config</code> set <code>xenprep_progress=shutdown</code></li><li>If <code>VM.virtual_hardware_platform_version</code> is less than 2, then set it to 2.</li><li>Give it the new PCI device: set <code>VM.auto_update_drivers</code> to <code>true</code>.</li><li>Initiate VM start.</li><li>Remove <code>xenprep_progress</code> from <code>VM.other_config</code></li></ol><p>The most relevant code is probably the <code>update_vm</code> function in <code>ocaml/xapi/xapi_xenops.ml</code> in the <code>xen-api</code> repo (or in some function called from there).</p><script>for(let e of document.querySelectorAll(".inline-type"))e.innerHTML=renderType(e.innerHTML)</script><footer class=footline></footer></article></section></div></main></div><script src=/new-docs/js/clipboard.min.js?1741171267 defer></script><script src=/new-docs/js/perfect-scrollbar.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-color.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-dispatch.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-drag.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-ease.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-interpolate.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-selection.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-timer.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-transition.min.js?1741171267 defer></script><script src=/new-docs/js/d3/d3-zoom.min.js?1741171267 defer></script><script src=/new-docs/js/js-yaml.min.js?1741171267 defer></script><script src=/new-docs/js/mermaid.min.js?1741171267 defer></script><script>window.themeUseMermaid=JSON.parse('{ "fontFamily": "Roboto Flex", "securityLevel": "loose" }')</script><script src=/new-docs/js/theme.js?1741171267 defer></script><script>function apply_image_invert_filter(e){document.querySelectorAll("img").forEach(function(t){if(t.classList.contains("no-invert"))return;t.style="filter: invert("+e+");"})}function darkThemeUsed(){const t=window.getComputedStyle(document.querySelector("body")),n=t.getPropertyValue("background-color");var e=n.match(/\d+/g).map(function(e){return parseInt(e,10)});return e.length===3&&.2126*e[0]+.7152*e[1]+.0722*e[2]<165}const invertToDarkGray=.85;darkThemeUsed()&&apply_image_invert_filter(invertToDarkGray),document.addEventListener("themeVariantLoaded",function(e){apply_image_invert_filter(e.detail.variant.endsWith("dark")?invertToDarkGray:0)})</script></body></html>