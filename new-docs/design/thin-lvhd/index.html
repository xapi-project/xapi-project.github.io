<!doctype html><html lang=en-us dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=html><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.127.0"><meta name=generator content="Relearn 7.3.2"><meta name=description content="LVHD is a block-based storage system built on top of Xapi and LVM. LVHD disks are represented as LVM LVs with vhd-format data inside. When a disk is snapshotted, the LVM LV is “deflated” to the minimum-possible size, just big enough to store the current vhd data. All other disks are stored “inflated” i.e. consuming the maximum amount of storage space. This proposal describes how we could add dynamic thin-provisioning to LVHD such that"><meta name=author content><meta name=twitter:card content="summary"><meta name=twitter:title content="thin LVHD storage :: XAPI Toolstack Developer Documentation"><meta name=twitter:description content="LVHD is a block-based storage system built on top of Xapi and LVM. LVHD disks are represented as LVM LVs with vhd-format data inside. When a disk is snapshotted, the LVM LV is “deflated” to the minimum-possible size, just big enough to store the current vhd data. All other disks are stored “inflated” i.e. consuming the maximum amount of storage space. This proposal describes how we could add dynamic thin-provisioning to LVHD such that"><meta property="og:url" content="https://xapi-project.github.io/new-docs/design/thin-lvhd/index.html"><meta property="og:site_name" content="XAPI Toolstack Developer Documentation"><meta property="og:title" content="thin LVHD storage :: XAPI Toolstack Developer Documentation"><meta property="og:description" content="LVHD is a block-based storage system built on top of Xapi and LVM. LVHD disks are represented as LVM LVs with vhd-format data inside. When a disk is snapshotted, the LVM LV is “deflated” to the minimum-possible size, just big enough to store the current vhd data. All other disks are stored “inflated” i.e. consuming the maximum amount of storage space. This proposal describes how we could add dynamic thin-provisioning to LVHD such that"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="Design Documents"><meta itemprop=name content="thin LVHD storage :: XAPI Toolstack Developer Documentation"><meta itemprop=description content="LVHD is a block-based storage system built on top of Xapi and LVM. LVHD disks are represented as LVM LVs with vhd-format data inside. When a disk is snapshotted, the LVM LV is “deflated” to the minimum-possible size, just big enough to store the current vhd data. All other disks are stored “inflated” i.e. consuming the maximum amount of storage space. This proposal describes how we could add dynamic thin-provisioning to LVHD such that"><meta itemprop=wordCount content="5200"><title>thin LVHD storage :: XAPI Toolstack Developer Documentation</title>
<link href=/new-docs/images/favicon.png?1739805295 rel=icon type=image/png><link href=/new-docs/css/fontawesome-all.min.css?1739805295 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/fontawesome-all.min.css?1739805295 rel=stylesheet></noscript><link href=/new-docs/css/auto-complete.css?1739805295 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/new-docs/css/auto-complete.css?1739805295 rel=stylesheet></noscript><link href=/new-docs/css/perfect-scrollbar.min.css?1739805295 rel=stylesheet><link href=/new-docs/css/theme.min.css?1739805295 rel=stylesheet><link href=/new-docs/css/format-html.min.css?1739805295 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="../..",window.relearn.relBaseUri="../../..",window.relearn.absBaseUri="https://xapi-project.github.io/new-docs",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["auto","zen-light","zen-dark","red","blue","green","learn","neon","relearn-light","relearn-bright","relearn-dark"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script><link rel=stylesheet href=https://xapi-project.github.io/new-docs/css/misc.css></head><body class="mobile-support html" data-url=/new-docs/design/thin-lvhd/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button><div class=topbar-content><div class=topbar-content-wrapper><nav class=TableOfContents><ul><li><a href=#note-on-running-out-of-blocks>Note on running out of blocks</a></li></ul><ul><li><a href=#logical-messages-in-the-queues>Logical messages in the queues</a></li><li><a href=#starting-up-the-local_allocator>Starting up the local_allocator</a></li><li><a href=#starting-xenvmd>Starting xenvmd</a></li><li><a href=#shutting-down-the-local_allocator>Shutting down the local_allocator</a></li><li><a href=#downgrading-metadata>Downgrading metadata</a></li><li><a href=#queues-as-rings>Queues as rings</a></li><li><a href=#journals-as-queues>Journals as queues</a></li><li><a href=#suspending-and-resuming-queues>Suspending and resuming queues</a></li><li><a href=#modelling-the-suspendresume-protocol>Modelling the suspend/resume protocol</a></li></ul><ul><li><a href=#the-extend-request>The extend request</a></li><li><a href=#the-extend-response>The extend response</a></li></ul><ul><li><a href=#shutting-down-the-local-allocator>Shutting down the local-allocator</a></li></ul><ul><li><a href=#the-fromlvm-ring>The FromLVM ring</a></li><li><a href=#the-tolvm-ring>The ToLVM Ring</a></li></ul></nav></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/new-docs/index.html><span itemprop=name>XAPI Toolstack Developer Guide</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/new-docs/design/index.html><span itemprop=name>Designs</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>thin LVHD storage</span><meta itemprop=position content="3"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-edit" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=https://github.com/xapi-project/xen-api/edit/master/doc/content/design/thin-lvhd/index.md target=_blank title="Edit (CTRL+ALT+w)"><i class="fa-fw fas fa-pen"></i></a></div><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/design/sr-level-rrds/index.html title="SR-Level RRDs (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/new-docs/design/pool-certificates/index.html title="TLS vertification for intra-pool communications (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable design" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline><table class=revision-table><tr><th colspan=2>Design document</th></tr><tr style=background-color:#fff><td>Revision</td><td><span class="label label-default">v3</span></td></tr><tr style=background-color:#fff><td>Status</td><td><span class="label
label-danger">proposed</span></td></tr></table></header><h1 id=thin-lvhd-storage>thin LVHD storage</h1><p>LVHD is a block-based storage system built on top of Xapi and LVM. LVHD
disks are represented as LVM LVs with vhd-format data inside. When a
disk is snapshotted, the LVM LV is &ldquo;deflated&rdquo; to the minimum-possible
size, just big enough to store the current vhd data. All other disks are
stored &ldquo;inflated&rdquo; i.e. consuming the maximum amount of storage space.
This proposal describes how we could add dynamic thin-provisioning to
LVHD such that</p><ul><li>disks only consume the space they need (plus an adjustable small
overhead)</li><li>when a disk needs more space, the allocation can be done <em>locally</em>
in the common-case; in particular there is no network RPC needed</li><li>when the resource pool master host has failed, allocations can still
continue, up to some limit, allowing time for the master host to be
recovered; in particular there is no need for very low HA timeouts.</li><li>we can (in future) support in-kernel block allocation through the
device mapper dm-thin target.</li></ul><p>The following diagram shows the &ldquo;Allocation plane&rdquo;:</p><p><img alt="Allocation plane" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/thin-lvhd/allocation-plane.png style=height:auto;width:auto></p><p>All VM disk writes are channelled through <code>tapdisk</code> which keeps track
of the remaining reserved space within the device mapper device. When
the free space drops below a &ldquo;low-water mark&rdquo;, tapdisk sends a message
to a local per-SR daemon called <code>local-allocator</code> and requests more
space.</p><p>The <code>local-allocator</code> maintains a free pool of blocks available for
allocation locally (hence the name). It will pick some blocks and
transactionally send the update to the <code>xenvmd</code> process running
on the SRmaster via the shared ring (labelled <code>ToLVM queue</code> in the diagram)
and update the device mapper tables locally.</p><p>There is one <code>xenvmd</code> process per SR on the SRmaster. <code>xenvmd</code> receives
local allocations from all the host shared rings (labelled <code>ToLVM queue</code>
in the diagram) and combines them together, appending them to a redo-log
also on shared storage. When <code>xenvmd</code> notices that a host&rsquo;s free space
(represented in the metadata as another LV) is low it allocates new free blocks
and pushes these to the host via another shared ring (labelled <code>FromLVM queue</code>
in the diagram).</p><p>The <code>xenvmd</code> process maintains a cache of the current VG metadata for
fast query and update. All updates are appended to the redo-log to ensure
they operate in O(1) time. The redo log updates are periodically flushed
to the primary LVM metadata.</p><p>Since the operations are stored in the redo-log and will only be removed
after the real metadata has been written, the implication is that it is
possible for the operations to be performed more than once. This will
occur if the xenvmd process exits between flushing to the real metadata
and acknowledging the operations as completed. For this to work as expected,
every individual operation stored in the redo-log <em>must</em> be idempotent.</p><h2 id=note-on-running-out-of-blocks>Note on running out of blocks</h2><p>Note that, while the host has plenty of free blocks, local allocations should
be fast. If the master fails and the local free pool starts running out
and <code>tapdisk</code> asks for more blocks, then the local allocator won&rsquo;t be able
to provide them.
<code>tapdisk</code> should start to slow
I/O in order to provide the local allocator more time.
Eventually if <code>tapdisk</code> runs
out of space before the local allocator can satisfy the request then
guest I/O will block. Note Windows VMs will start to crash if guest
I/O blocks for more than 70s. Linux VMs, no matter PV or HVM, may suffer
from &ldquo;block for more than 120 seconds&rdquo; issue due to slow I/O. This
known issue is that, slow I/O during dirty pages writeback/flush may
cause memory starvation, then other userland process or kernel threads
would be blocked.</p><p>The following diagram shows the control-plane:</p><p><img alt="control plane" class="noborder lazy nolightbox shadow figure-image" loading=lazy src=/new-docs/design/thin-lvhd/control-plane.png style=height:auto;width:auto></p><p>When thin-provisioning is enabled we will be modifying the LVM metadata at
an increased rate. We will cache the current metadata in the <code>xenvmd</code> process
and funnel all queries through it, rather than &ldquo;peeking&rdquo; at the metadata
on-disk. Note it will still be possible to peek at the on-disk metadata but it
will be out-of-date. Peeking can still be used to query the PV state of the volume
group.</p><p>The <code>xenvm</code> CLI uses a simple
RPC interface to query the <code>xenvmd</code> process, tunnelled through <code>xapi</code> over
the management network. The RPC interface can be used for</p><ul><li>activating volumes locally: <code>xenvm</code> will query the LV segments and program
device mapper</li><li>deactivating volumes locally</li><li>listing LVs, PVs etc</li></ul><p>Note that current LVHD requires the management network for these control-plane
functions.</p><p>When the SM backend wishes to query or update volume group metadata it should use the
<code>xenvm</code> CLI while thin-provisioning is enabled.</p><p>The <code>xenvmd</code> process shall use a redo-log to ensure that metadata updates are
persisted in constant time and flushed lazily to the regular metadata area.</p><p>Tunnelling through xapi will be done by POSTing to the localhost URI</p><pre><code>/services/xenvmd/&lt;SR uuid&gt;
</code></pre><p>Xapi will the either proxy the request transparently to the SRmaster, or issue an
http level redirect that the xenvm CLI would need to follow.</p><p>If the xenvmd process is not running on the host on which it should
be, xapi will start it.</p><h1 id=components-roles-and-responsibilities>Components: roles and responsibilities</h1><p><code>xenvmd</code>:</p><ul><li>one per plugged SRmaster PBD</li><li>owns the LVM metadata</li><li>provides a fast query/update API so we can (for example) create lots of LVs very fast</li><li>allocates free blocks to hosts when they are running low</li><li>receives block allocations from hosts and incorporates them in the LVM metadata</li><li>can safely flush all updates and downgrade to regular LVM</li></ul><p><code>xenvm</code>:</p><ul><li>a CLI which talks the <code>xenvmd</code> protocol to query / update LVs</li><li>can be run on any host, calls (except &ldquo;format&rdquo; and &ldquo;upgrade&rdquo;) are forwarded by <code>xapi</code></li><li>can &ldquo;format&rdquo; a LUN to prepare it for <code>xenvmd</code></li><li>can &ldquo;upgrade&rdquo; a LUN to prepare it for <code>xenvmd</code></li></ul><p><code>local_allocator</code>:</p><ul><li>one per plugged PBD</li><li>exposes a simple interface to <code>tapdisk</code> for requesting more space</li><li>receives free block allocations via a queue on the shared disk from <code>xenvmd</code></li><li>sends block allocations to <code>xenvmd</code> and updates the device mapper target locally</li></ul><p><code>tapdisk</code>:</p><ul><li>monitors the free space inside LVs and requests more space when running out</li><li>slows down I/O when nearly out of space</li></ul><p><code>xapi</code>:</p><ul><li>provides authenticated communication tunnels</li><li>ensures the xenvmd daemons are only running on the correct hosts.</li></ul><p><code>SM</code>:</p><ul><li>writes the configuration file for xenvmd (though doesn&rsquo;t start it)</li><li>has an on/off switch for thin-provisioning</li><li>can use either normal LVM or the <code>xenvm</code> CLI</li></ul><p><code>membership_monitor</code></p><ul><li>configures and manages the connections between <code>xenvmd</code> and the <code>local_allocator</code></li></ul><h1 id=queues-on-the-shared-disk>Queues on the shared disk</h1><p>The <code>local_allocator</code> communicates with <code>xenvmd</code> via a pair
of queues on the shared disk. Using the disk rather than the network means
that VMs will continue to run even if the management network is not working.
In particular</p><ul><li>if the (management) network fails, VMs continue to run on SAN storage</li><li>if a host changes IP address, nothing needs to be reconfigured</li><li>if xapi fails, VMs continue to run.</li></ul><h2 id=logical-messages-in-the-queues>Logical messages in the queues</h2><p>The <code>local_allocator</code> needs to tell the <code>xenvmd</code> which blocks have
been allocated to which guest LV. <code>xenvmd</code> needs to tell the
<code>local_allocator</code> which blocks have become free. Since we are based on
LVM, a &ldquo;block&rdquo; is an extent, and an &ldquo;allocation&rdquo; is a segment i.e. the
placing of a physical extent at a logical extent in the logical volume.</p><p>The <code>local_allocator</code> needs to send a message with logical contents:</p><ul><li><code>volume</code>: a human-readable name of the LV</li><li><code>segments</code>: a list of LVM segments which says
&ldquo;place physical extent x at logical extent y using a linear mapping&rdquo;.</li></ul><p>Note this message is idempotent.</p><p>The <code>xenvmd</code> needs to send a message with logical contents:</p><ul><li><code>extents</code>: a list of physical extents which are free for the host to use</li></ul><p>Although
for internal housekeeping <code>xenvmd</code> will want to assign these
physical extents to logical extents within the host&rsquo;s free LV, the
<code>local_allocator</code>
doesn&rsquo;t need to know the logical extents. It only needs to know
the set of blocks which it is free to allocate.</p><h2 id=starting-up-the-local_allocator>Starting up the local_allocator</h2><p>What happens when a <code>local_allocator</code> (re)starts, after a</p><ul><li>process crash, respawn</li><li>host crash, reboot?</li></ul><p>When the <code>local_allocator</code> starts up, there are 2 cases:</p><ol><li>the host has just rebooted, there are no attached disks and no running VMs</li><li>the process has just crashed, there are attached disks and running VMs</li></ol><p>Case 1 is uninteresting. In case 2 there may have been an allocation in
progress when the process crashed and this must be completed. Therefore
the operation is journalled in a local filesystem in a directory which
is deliberately deleted on host reboot (Case 1). The allocation operation
consists of:</p><ol><li><code>push</code>ing the allocation to <code>xenvmd</code> on the SRmaster</li><li>updating the device mapper</li></ol><p>Note that both parts of the allocation operation are idempotent and hence
the whole operation is idempotent. The journalling will guarantee it executes
at-least-once.</p><p>When the <code>local_allocator</code> starts up it needs to discover the list of
free blocks. Rather than have 2 code paths, it&rsquo;s best to treat everything
as if it is a cold start (i.e. no local caches already populated) and to
ask the master to resync the free block list. The resync is performed by
executing a &ldquo;suspend&rdquo; and &ldquo;resume&rdquo; of the free block queue, and requiring
the remote allocator to:</p><ul><li><code>pop</code> all block allocations and incorporate these updates</li><li>send the complete set of free blocks &ldquo;now&rdquo; (i.e. while the queue is
suspended) to the local allocator.</li></ul><h2 id=starting-xenvmd>Starting xenvmd</h2><p><code>xenvmd</code> needs to know</p><ul><li>the device containing the volume group</li><li>the hosts to &ldquo;connect&rdquo; to via the shared queues</li></ul><p>The device containing the volume group should be written to a config
file when the SR is plugged.</p><p><code>xenvmd</code> does not remember which hosts it is listening to across crashes,
restarts or master failovers. The <code>membership_monitor</code> will keep the
<code>xenvmd</code> list in sync with the <code>PBD.currently_attached</code> fields.</p><h2 id=shutting-down-the-local_allocator>Shutting down the local_allocator</h2><p>The <code>local_allocator</code> should be able to crash at any time and recover
afterwards. If the user requests a <code>PBD.unplug</code> we can perform a
clean shutdown by:</p><ul><li>signalling <code>xenvmd</code> to suspend the block allocation queue</li><li>arranging for the <code>local_allocator</code> to acknowledge the suspension and exit</li><li>when the <code>xenvmd</code> sees the acknowlegement, we know that the
<code>local_allocator</code> is offline and it doesn&rsquo;t need to poll the queue any more</li></ul><h2 id=downgrading-metadata>Downgrading metadata</h2><p><code>xenvmd</code> can be terminated at any time and restarted, since all compound
operations are journalled.</p><p>Downgrade is a special case of shutdown.
To downgrade, we need to stop all hosts allocating and ensure all updates
are flushed to the global LVM metadata. <code>xenvmd</code> can shutdown
by:</p><ul><li>shutting down all <code>local_allocator</code>s (see previous section)</li><li>flushing all outstanding block allocations to the LVM redo log</li><li>flushing the LVM redo log to the global LVM metadata</li></ul><h2 id=queues-as-rings>Queues as rings</h2><p>We can use a simple ring protocol to represent the queues on the disk.
Each queue will have a single consumer and single producer and reside within
a single logical volume.</p><p>To make diagnostics simpler, we can require the ring to only support <code>push</code>
and <code>pop</code> of <em>whole</em> messages i.e. there can be no partial reads or partial
writes. This means that the <code>producer</code> and <code>consumer</code> pointers will always
point to valid message boundaries.</p><p>One possible format used by the <a href=https://github.com/mirage/shared-block-ring/blob/master/lib/ring.ml rel=external target=_blank>prototype</a> is as follows:</p><ul><li>sector 0: a magic string</li><li>sector 1: producer state</li><li>sector 2: consumer state</li><li>sector 3&mldr;: data</li></ul><p>Within the producer state sector we can have:</p><ul><li>octets 0-7: producer offset: a little-endian 64-bit integer</li><li>octet 8: 1 means &ldquo;suspend acknowledged&rdquo;; 0 otherwise</li></ul><p>Within the consumer state sector we can have:</p><ul><li>octets 0-7: consumer offset: a little-endian 64-bit integer</li><li>octet 8: 1 means &ldquo;suspend requested&rdquo;; 0 otherwise</li></ul><p>The consumer and producer pointers point to message boundaries. Each
message is prefixed with a 4 byte length and padded to the next 4-byte
boundary.</p><p>To push a message onto the ring we need to</p><ul><li>check whether the message is too big to ever fit: this is a permanent
error</li><li>check whether the message is too big to fit given the current free
space: this is a transient error</li><li>write the message into the ring</li><li>advance the producer pointer</li></ul><p>To pop a message from the ring we need to</p><ul><li>check whether there is unconsumed space: if not this is a transient
error</li><li>read the message from the ring and process it</li><li>advance the consumer pointer</li></ul><h2 id=journals-as-queues>Journals as queues</h2><p>When we journal an operation we want to guarantee to execute it never
<em>or</em> at-least-once. We can re-use the queue implementation by <code>push</code>ing
a description of the work item to the queue and waiting for the
item to be <code>pop</code>ped, processed and finally consumed by advancing the
consumer pointer. The journal code needs to check for unconsumed data
during startup, and to process it before continuing.</p><h2 id=suspending-and-resuming-queues>Suspending and resuming queues</h2><p>During startup (resync the free blocks) and shutdown (flush the allocations)
we need to suspend and resume queues. The ring protocol can be extended
to allow the <em>consumer</em> to suspend the ring by:</p><ul><li>the consumer asserts the &ldquo;suspend requested&rdquo; bit</li><li>the producer <code>push</code> function checks the bit and writes &ldquo;suspend acknowledged&rdquo;</li><li>the producer also periodically polls the queue state and writes
&ldquo;suspend acknowledged&rdquo; (to catch the case where no items are to be pushed)</li><li>after the producer has acknowledged it will guarantee to <code>push</code> no more
items</li><li>when the consumer polls the producer&rsquo;s state and spots the &ldquo;suspend acknowledged&rdquo;,
it concludes that the queue is now suspended.</li></ul><p>The key detail is that the handshake on the ring causes the two sides
to synchronise and both agree that the ring is now suspended/ resumed.</p><h2 id=modelling-the-suspendresume-protocol>Modelling the suspend/resume protocol</h2><p>To check that the suspend/resume protocol works well enough to be used
to resynchronise the free blocks list on a slave, a simple
<a href=/new-docs/design/thin-lvhd/queue.pml>promela model</a> was created. We model the queue state as
2 boolean flags:</p><div class="highlight wrap-code"><pre tabindex=0><code>bool suspend /* suspend requested */
bool suspend_ack /* suspend acknowledged *./</code></pre></div><p>and an abstract representation of the data within the ring:</p><div class="highlight wrap-code"><pre tabindex=0><code>/* the queue may have no data (none); a delta or a full sync.
   the full sync is performed immediately on resume. */
mtype = { sync delta none }
mtype inflight_data = none</code></pre></div><p>There is a &ldquo;producer&rdquo; and a &ldquo;consumer&rdquo; process which run forever,
exchanging data and suspending and resuming whenever they want.
The special data item <code>sync</code> is only sent immediately after a resume
and we check that we never desynchronise with asserts:</p><div class="highlight wrap-code"><pre tabindex=0><code>  :: (inflight_data != none) -&gt;
    /* In steady state we receive deltas */
    assert (suspend_ack == false);
    assert (inflight_data == delta);
    inflight_data = none</code></pre></div><p>i.e. when we are receiving data normally (outside of the suspend/resume
code) we aren&rsquo;t suspended and we expect deltas, not full syncs.</p><p>The model-checker <a href=http://spinroot.com/spin/whatispin.html rel=external target=_blank>spin</a>
verifies this property holds.</p><h1 id=interaction-with-ha>Interaction with HA</h1><p>Consider what will happen if a host fails when HA is disabled:</p><ul><li>if the host is a slave: the VMs running on the host will crash but
no other host is affected.</li><li>if the host is a master: allocation requests from running VMs will
continue provided enough free blocks are cached on the hosts. If a
host eventually runs out of free blocks, then guest I/O will start to
block and VMs may eventually crash.</li></ul><p>Therefore we <em>recommend</em> that users enable HA and only disable it
for short periods of time. Note that, unlike other thin-provisioning
implementations, we will allow HA to be disabled.</p><h1 id=host-local-lvs>Host-local LVs</h1><p>When a host calls SMAPI <code>sr_attach</code>, it will use <code>xenvm</code> to tell <code>xenvmd</code> on the
SRmaster to connect to the <code>local_allocator</code> on the host. The <code>xenvmd</code>
daemon will create the volumes for queues and a volume to represent the
&ldquo;free blocks&rdquo; which a host is allowed to allocate.</p><h1 id=monitoring>Monitoring</h1><p>The <code>xenvmd</code> process should export RRD datasources over shared
memory named</p><ul><li><code>sr_&lt;SR uuid>_&lt;host uuid>_free</code>: the number of free blocks in
the local cache. It&rsquo;s useful to look at this and verify that it doesn&rsquo;t
usually hit zero, since that&rsquo;s when allocations will start to block.
For this reason we should use the <code>MIN</code> consolidation function.</li><li><code>sr_&lt;SR uuid>_&lt;host uuid>_requests</code>: a counter of the number
of satisfied allocation requests. If this number is too high then the quantum
of allocation should be increased. For this reason we should use the
<code>MAX</code> consolidation function.</li><li><code>sr_&lt;SR uuid>_&lt;host uuid>_allocations</code>: a counter of the number of
bytes being allocated. If the allocation rate is too high compared with
the number of free blocks divided by the HA timeout period then the
<code>SRmaster-allocator</code> should be reconfigured to supply more blocks with the host.</li></ul><h1 id=modifications-to-tapdisk>Modifications to tapdisk</h1><p>TODO: to be updated by Germano</p><p><code>tapdisk</code> will be modified to</p><ul><li>on open: discover the current maximum size of the file/LV (for a file
we assume there is no limit for now)</li><li>read a low-water mark value from a config file <code>/etc/tapdisk3.conf</code></li><li>read a very-low-water mark value from a config file <code>/etc/tapdisk3.conf</code></li><li>read a Unix domain socket path from a config file <code>/etc/tapdisk3.conf</code></li><li>when there is less free space available than the low-water mark: connect
to Unix domain socket and write an &ldquo;extend&rdquo; request</li><li>upon receiving the &ldquo;extend&rdquo; response, re-read the maximum size of the
file/LV</li><li>when there is less free space available than the very-low-water mark:
start to slow I/O responses and write a single &rsquo;error&rsquo; line to the log.</li></ul><h2 id=the-extend-request>The extend request</h2><p>TODO: to be updated by Germano</p><p>The request has the following format:</p><table><thead><tr><th>Octet offsets</th><th>Name</th><th>Description</th></tr></thead><tbody><tr><td>0,1</td><td>tl</td><td>Total length (including this field) of message (in network byte order)</td></tr><tr><td>2</td><td>type</td><td>The value &lsquo;0&rsquo; indicating an extend request</td></tr><tr><td>3</td><td>nl</td><td>The length of the LV name in octets, including NULL terminator</td></tr><tr><td>4,..,4+nl-1</td><td>name</td><td>The LV name</td></tr><tr><td>4+nl,..,12+nl-1</td><td>vdi_size</td><td>The virtual size of the logical VDI (in network byte order)</td></tr><tr><td>12+nl,..,20+nl-1</td><td>lv_size</td><td>The current size of the LV (in network byte order)</td></tr><tr><td>20+nl,..,28+nl-1</td><td>cur_size</td><td>The current size of the vhd metadata (in network byte order)</td></tr></tbody></table><h2 id=the-extend-response>The extend response</h2><p>The response is a single byte value &ldquo;0&rdquo; which is a signal to re-examime
the LV size. The request will block indefinitely until it succeeds. The
request will block for a long time if</p><ul><li>the SR has genuinely run out of space. The admin should observe the
existing free space graphs/alerts and perform an SR resize.</li><li>the master has failed and HA is disabled. The admin should re-enable
HA or fix the problem manually.</li></ul><h1 id=the-local_allocator>The local_allocator</h1><p>There is one <code>local_allocator</code> process per plugged PBD.
The process will be
spawned by the SM <code>sr_attach</code> call, and shutdown from the <code>sr_detach</code> call.</p><p>The <code>local_allocator</code> accepts the following configuration (via a config file):</p><ul><li><code>socket</code>: path to a local Unix domain socket. This is where the <code>local_allocator</code>
listens for requests from <code>tapdisk</code></li><li><code>allocation_quantum</code>: number of megabytes to allocate to each tapdisk on request</li><li><code>local_journal</code>: path to a block device or file used for local journalling. This
should be deleted on reboot.</li><li><code>free_pool</code>: name of the LV used to store the host&rsquo;s free blocks</li><li><code>devices</code>: list of local block devices containing the PVs</li><li><code>to_LVM</code>: name of the LV containing the queue of block allocations sent to <code>xenvmd</code></li><li><code>from_LVM</code>: name of the LV containing the queue of messages sent from <code>xenvmd</code>.
There are two types of messages:<ol><li>Free blocks to put into the free pool</li><li>Cap requests to remove blocks from the free pool.</li></ol></li></ul><p>When the <code>local_allocator</code> process starts up it will read the host local
journal and</p><ul><li>re-execute any pending allocation requests from tapdisk</li><li>suspend and resume the <code>from_LVM</code> queue to trigger a full retransmit
of free blocks from <code>xenvmd</code></li></ul><p>The procedure for handling an allocation request from tapdisk is:</p><ol><li>if there aren&rsquo;t enough free blocks in the free pool, wait polling the
<code>from_LVM</code> queue</li><li>choose a range of blocks to assign to the tapdisk LV from the free LV</li><li>write the operation (i.e. exactly what we are about to do) to the journal.
This ensures that it will be repeated if the allocator crashes and restarts.
Note that, since the operation may be repeated multiple times, it must be
idempotent.</li><li>push the block assignment to the <code>toLVM</code> queue</li><li>suspend the device mapper device</li><li>add/modify the device mapper target</li><li>resume the device mapper device</li><li>remove the operation from the local journal (i.e. there&rsquo;s no need to repeat
it now)</li><li>reply to tapdisk</li></ol><h2 id=shutting-down-the-local-allocator>Shutting down the local-allocator</h2><p>The SM <code>sr_detach</code> called from <code>PBD.unplug</code> will use the <code>xenvm</code> CLI to request
that <code>xenvmd</code> disconnects from a host. The procedure is:</p><ol><li>SM calls <code>xenvm disconnect host</code></li><li><code>xenvm</code> sends an RPC to <code>xenvmd</code> tunnelled through <code>xapi</code></li><li><code>xenvmd</code> suspends the <code>to_LVM</code> queue</li><li>the <code>local_allocator</code> acknowledges the suspend and exits</li><li><code>xenvmd</code> flushes all updates from the <code>to_LVM</code> queue and stops listening</li></ol><h1 id=xenvmd>xenvmd</h1><p><code>xenvmd</code> is a daemon running per SRmaster PBD, started in <code>sr_attach</code> and
terminated in <code>sr_detach</code>. <code>xenvmd</code> has a config file containing:</p><ul><li><code>socket</code>: Unix domain socket where <code>xenvmd</code> listens for requests from
<code>xenvm</code> tunnelled by <code>xapi</code></li><li><code>host_allocation_quantum</code>: number of megabytes to hand to a host at a time</li><li><code>host_low_water_mark</code>: threshold below which we will hand blocks to a host</li><li><code>devices</code>: local devices containing the PVs</li></ul><p><code>xenvmd</code> continually</p><ul><li>peeks updates from all the <code>to_LVM</code> queues</li><li>calculates how much free space each host still has</li><li>if the size of a host&rsquo;s free pool drops below some threshold:<ul><li>choose some free blocks</li></ul></li><li>if the size of a host&rsquo;s free pool goes above some threshold:<ul><li>request a cap of the host&rsquo;s free pool</li></ul></li><li>writes the change it is going to make to a journal stored in an LV</li><li>pops the updates from the <code>to_LVM</code> queues</li><li>pushes the updates to the <code>from_LVM</code> queues</li><li>pushes updates to the LVM redo-log</li><li>periodically flush the LVM redo-log to the LVM metadata area</li></ul><h1 id=the-membership-monitor>The membership monitor</h1><p>The role of the membership monitor is to keep the list of <code>xenvmd</code> connections
in sync with the <code>PBD.currently_attached</code> fields.</p><p>We shall</p><ul><li>install a <code>host-pre-declare-dead</code> script to use <code>xenvm</code> to send an RPC
to <code>xenvmd</code> to forcibly flush (without acknowledgement) the <code>to_LVM</code> queue
and destroy the LVs.</li><li>modify XenAPI <code>Host.declare_dead</code> to call <code>host-pre-declare-dead</code> before
the VMs are unlocked</li><li>add a <code>host-pre-forget</code> hook type which will be called just before a Host
is forgotten</li><li>install a <code>host-pre-forget</code> script to use <code>xenvm</code> to call <code>xenvmd</code> to
destroy the host&rsquo;s local LVs</li></ul><h1 id=modifications-to-lvhd-sr>Modifications to LVHD SR</h1><ul><li><code>sr_attach</code> should:<ul><li>if an SRmaster, update the <code>MGT</code> major version number to prevent</li><li>Write the xenvmd configuration file (on <em>all</em> hosts, not just SRmaster)</li><li>spawn <code>local_allocator</code></li></ul></li><li><code>sr_detach</code> should:<ul><li>call <code>xenvm</code> to request the shutdown of <code>local_allocator</code></li></ul></li><li><code>vdi_deactivate</code> should:<ul><li>call <code>xenvm</code> to request the flushing of all the <code>to_LVM</code> queues to the
redo log</li></ul></li><li><code>vdi_activate</code> should:<ul><li>if necessary, call <code>xenvm</code> to deflate the LV to the minimum size (with some slack)</li></ul></li></ul><p>Note that it is possible to attach and detach the individual hosts in any order
but when the SRmaster is unplugged then there will be no &ldquo;refilling&rdquo; of the host
local free LVs; it will behave as if the master host has failed.</p><h1 id=modifications-to-xapi>Modifications to xapi</h1><ul><li>Xapi needs to learn how to forward xenvm connections to the SR master.</li><li>Xapi needs to start and stop xenvmd at the appropriate times</li><li>We must disable unplugging the PBDs for shared SRs on the pool master
if any other slave has its PBD plugging. This is actually fixing an
issue that exists today - LVHD SRs require the master PBD to be
plugged to do many operations.</li><li>Xapi should provide a mechanism by which the xenvmd process can be killed
once the last PBD for an SR has been unplugged.</li></ul><h1 id=enabling-thin-provisioning>Enabling thin provisioning</h1><p>Thin provisioning will be automatically enabled on upgrade. When the SRmaster
plugs in <code>PBD</code> the <code>MGT</code> major version number will be bumped to prevent old
hosts from plugging in the SR and getting confused.
When a VDI is activated, it will be deflated to the new low size.</p><h1 id=disabling-thin-provisioning>Disabling thin provisioning</h1><p>We shall make a tool which will</p><ul><li>allow someone to downgrade their pool after enabling thin provisioning</li><li>allow developers to test the upgrade logic without fully downgrading their
hosts</li></ul><p>The tool will</p><ul><li>check if there is enough space to fully inflate all non-snapshot leaves</li><li>unplug all the non-SRmaster <code>PBD</code>s</li><li>unplug the SRmaster <code>PBD</code>. As a side-effect all pending LVM updates will be
written to the LVM metadata.</li><li>modify the <code>MGT</code> volume to have the lower metadata version</li><li>fully inflate all non-snapshot leaves</li></ul><h1 id=walk-through-upgrade>Walk-through: upgrade</h1><p>Rolling upgrade should work in the usual way. As soon as the pool master has been
upgraded, hosts will be able to use thin provisioning when new VDIs are attached.
A VM suspend/resume/reboot or migrate will be needed to turn on thin provisioning
for existing running VMs.</p><h1 id=walk-through-downgrade>Walk-through: downgrade</h1><p>A pool may be safely downgraded to a previous version without thin provisioning
provided that the downgrade tool is run. If the tool hasn&rsquo;t run then the old
pool will refuse to attach the SR because the metadata has been upgraded.</p><h1 id=walk-through-after-a-host-failure>Walk-through: after a host failure</h1><p>If HA is enabled:</p><ul><li><code>xhad</code> elects a new master if necessary</li><li><code>Xapi</code> on the master will start xenvmd processes for shared thin-lvhd SRs</li><li>the <code>xhad</code> tells <code>Xapi</code> which hosts are alive and which have failed.</li><li><code>Xapi</code> runs the <code>host-pre-declare-dead</code> scripts for every failed host</li><li>the <code>host-pre-declare-dead</code> tells <code>xenvmd</code> to flush the <code>to_LVM</code> updates</li><li><code>Xapi</code> unlocks the VMs and restarts them on new hosts.</li></ul><p>If HA is not enabled:</p><ul><li>The admin should verify the host is definitely dead</li><li>If the dead host was the master, a new master must be designated. This will
start the xenvmd processes for the shared thin-lvhd SRs.</li><li>the admin must tell <code>Xapi</code> which hosts have failed with <code>xe host-declare-dead</code></li><li><code>Xapi</code> runs the <code>host-pre-declare-dead</code> scripts for every failed host</li><li>the <code>host-pre-declare-dead</code> tells <code>xenvmd</code> to flush the <code>to_LVM</code> updates</li><li><code>Xapi</code> unlocks the VMs</li><li>the admin may now restart the VMs on new hosts.</li></ul><h1 id=walk-through-co-operative-master-transition>Walk-through: co-operative master transition</h1><p>The admin calls Pool.designate_new_master. This initiates a two-phase
commit of the new master. As part of this, the slaves will restart,
and on restart each host&rsquo;s xapi will kill any xenvmd that should only
run on the pool master. The new designated master will then restart itself
and start up the xenvmd process on itself.</p><h1 id=future-use-of-dm-thin>Future use of dm-thin?</h1><p>Dm-thin also uses 2 local LVs: one for the &ldquo;thin pool&rdquo; and one for the metadata.
After replaying our journal we could potentially delete our host local LVs and
switch over to dm-thin.</p><h1 id=summary-of-the-impact-on-the-admin>Summary of the impact on the admin</h1><ul><li>If the VM workload performs a lot of disk allocation, then the admin <em>should</em>
enable HA.</li><li>The admin <em>must</em> not downgrade the pool without first cleanly detaching the
storage.</li><li>Extra metadata is needed to track thin provisioing, reducing the amount of
space available for user volumes.</li><li>If an SR is completely full then it will not be possible to enable thin
provisioning.</li><li>There will be more fragmentation, but the extent size is large (4MiB) so it
shouldn&rsquo;t be too bad.</li></ul><h1 id=ring-protocols>Ring protocols</h1><p>Each ring consists of 3 sectors of metadata followed by the data area. The
contents of the first 3 sectors are:</p><table><thead><tr><th>Sector, Octet offsets</th><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>0,0-30</td><td>signature</td><td>string</td><td>Signature (&ldquo;mirage shared-block-device 1.0&rdquo;)</td></tr><tr><td>1,0-7</td><td>producer</td><td>uint64</td><td>Pointer to the end of data written by the producer</td></tr><tr><td>1,8</td><td>suspend_ack</td><td>uint8</td><td>Suspend acknowledgement byte</td></tr><tr><td>2,0-7</td><td>consumer</td><td>uint64</td><td>Pointer to the end of data read by the consumer</td></tr><tr><td>2,8</td><td>suspend</td><td>uint8</td><td>Suspend request byte</td></tr></tbody></table><p>Note. producer and consumer pointers are stored in little endian
format.</p><p>The pointers are free running byte offsets rounded up to the next
4-byte boundary, and the position of the actual data is found by
finding the remainder when dividing by the size of the data area. The
producer pointer points to the first free byte, and the consumer
pointer points to the byte after the last data consumed. The actual
payload is preceded by a 4-byte length field, stored in little endian
format. When writing a 1 byte payload, the next value of the producer
pointer will therefore be 8 bytes on from the previous - 4 for the
length (which will contain [0x01,0x00,0x00,0x00]), 1 byte for the
payload, and 3 bytes padding.</p><p>A ring is suspended and resumed by the consumer. To suspend, the
consumer first checks that the producer and consumer agree on the
current suspend status. If they do not, the ring cannot be
suspended. The consumer then writes the byte 0x02 into byte 8 of
sector 2. The consumer must then wait for the producer to acknowledge
the suspend, which it will do by writing 0x02 into byte 8 of sector 1.</p><h2 id=the-fromlvm-ring>The FromLVM ring</h2><p>Two different types of message can be sent on the FromLVM ring.</p><p>The FreeAllocation message contains the blocks for the free pool.
Example message:</p><pre><code>(FreeAllocation((blocks((pv0(12326 12249))(pv0(11 1))))(generation 2)))
</code></pre><p>Pretty-printed:</p><pre><code>(FreeAllocation
    (
        (blocks
            (
                (pv0(12326 12249))
                (pv0(11 1))
            )
        )
        (generation 2)
    )
)
</code></pre><p>This is a message to add two new sets of extents to the free pool. A
span of length 12249 extents starting at extent 12326, and a span of
length 1 starting from extent 11, both within the physical volume
&lsquo;pv0&rsquo;. The generation count of this message is &lsquo;2&rsquo;. The semantics of
the generation is that the local allocator must record the generation
of the last message it received since the FromLVM ring was resumed,
and ignore any message with a generated less than or equal to the last
message received.</p><p>The CapRequest message contains a request to cap the free pool at
a maximum size.
Example message:</p><pre><code>(CapRequest((cap 6127)(name host1-freeme)))
</code></pre><p>Pretty-printed:</p><pre><code>(CapRequest
    (
        (cap 6127)
        (name host1-freeme)
    )
)
</code></pre><p>This is a request to cap the free pool at a maximum size of 6127
extents. The &rsquo;name&rsquo; parameter reflects the name of the LV into which
the extents should be transferred.</p><h2 id=the-tolvm-ring>The ToLVM Ring</h2><p>The ToLVM ring only contains 1 type of message. Example:</p><pre><code>((volume test5)(segments(((start_extent 1)(extent_count 32)(cls(Linear((name pv0)(start_extent 12328))))))))
</code></pre><p>Pretty-printed:</p><pre><code>(
    (volume test5)
    (segments
        (
            (
                (start_extent 1)
                (extent_count 32)
                (cls
                    (Linear
                        (
                            (name pv0)
                            (start_extent 12328)
                        )
                    )
                )
            )
        )
    )
)
</code></pre><p>This message is extending an LV named &rsquo;test5&rsquo; by giving it 32 extents
starting at extent 1, coming from PV &lsquo;pv0&rsquo; starting at extent
12328. The &lsquo;cls&rsquo; field should always be &lsquo;Linear&rsquo; - this is the only
acceptable value.</p><h1 id=cap-requests>Cap requests</h1><p>Xenvmd will try to keep the free pools of the hosts within a range
set as a fraction of free space. There are 3 parameters adjustable
via the config file:</p><ul><li>low_water_mark_factor</li><li>medium_water_mark_factor</li><li>high_water_mark_factor</li></ul><p>These three are all numbers between 0 and 1. Xenvmd will sum the free
size and the sizes of all hosts&rsquo; free pools to find the total
effective free size in the VG, <code>F</code>. It will then subtract the sizes of
any pending desired space from in-flight create or resize calls <code>s</code>. This
will then be divided by the number of hosts connected, <code>n</code>, and
multiplied by the three factors above to find the 3 absolute values
for the high, medium and low watermarks.</p><pre><code>{high, medium, low} * (F - s) / n
</code></pre><p>When xenvmd notices that a host&rsquo;s free pool size has dropped below
the low watermark, it will be topped up such that the size is equal
to the medium watermark. If xenvmd notices that a host&rsquo;s free pool
size is above the high watermark, it will issue a &lsquo;cap request&rsquo; to
the host&rsquo;s local allocator, which will then respond by allocating
from its free pool into the fake LV, which xenvmd will then delete
as soon as it gets the update.</p><p>Xenvmd keeps track of the last update it has sent to the local
allocator, and will not resend the same request twice, unless it
is restarted.</p><footer class=footline></footer></article></div></main></div><aside id=R-sidebar class=default-animation><div id=R-header-topbar class=default-animation></div><div id=R-header-wrapper class=default-animation><div id=R-header class=default-animation><img src=https://xapi-project.github.io/new-docs//images/xapi-project.png></div><script>window.index_js_url="/new-docs/searchindex.en.js?1739805295"</script><search><form action=/new-docs/search/index.html method=get><div class="searchbox default-animation"><button class=search-detail type=submit title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
<label class=a11y-only for=R-search-by>Search</label>
<input data-search-input id=R-search-by name=search-by class=search-by type=search placeholder=Search...>
<button class=search-clear type=button data-search-clear title="Clear search"><i class="fas fa-times" title="Clear search"></i></button></div></form></search><script>var contentLangs=["en"]</script><script src=/new-docs/js/auto-complete.js?1739805295 defer></script><script src=/new-docs/js/lunr/lunr.min.js?1739805295 defer></script><script src=/new-docs/js/lunr/lunr.stemmer.support.min.js?1739805295 defer></script><script src=/new-docs/js/lunr/lunr.multi.min.js?1739805295 defer></script><script src=/new-docs/js/lunr/lunr.en.min.js?1739805295 defer></script><script src=/new-docs/js/search.js?1739805295 defer></script></div><div id=R-homelinks class="default-animation homelinks"><ul><li><a class=padding href=/new-docs/index.html><i class="fa-fw fas fa-home"></i> Home</a></li></ul><hr class=padding></div><div id=R-content-wrapper class=highlightable><div id=R-shortcutmenu-home class=R-sidebarmenu><ul class="enlarge morespace collapsible-menu"><li data-nav-id=/new-docs/toolstack/index.html><input type=checkbox id=R-section-cdb5571004e49cdb09f50d87f536567a aria-controls=R-subsections-cdb5571004e49cdb09f50d87f536567a><label for=R-section-cdb5571004e49cdb09f50d87f536567a><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu The Toolstack</span></label><a class=padding href=/new-docs/toolstack/index.html>The Toolstack</a><ul id=R-subsections-cdb5571004e49cdb09f50d87f536567a class=collapsible-menu><li data-nav-id=/new-docs/toolstack/responsibilities/index.html><a class=padding href=/new-docs/toolstack/responsibilities/index.html>Responsibilities</a></li><li data-nav-id=/new-docs/toolstack/high-level/index.html><input type=checkbox id=R-section-dde4854617c1f5d00e4646f449a8ec77 aria-controls=R-subsections-dde4854617c1f5d00e4646f449a8ec77><label for=R-section-dde4854617c1f5d00e4646f449a8ec77><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu High-level architecture</span></label><a class=padding href=/new-docs/toolstack/high-level/index.html>High-level architecture</a><ul id=R-subsections-dde4854617c1f5d00e4646f449a8ec77 class=collapsible-menu><li data-nav-id=/new-docs/toolstack/high-level/environment/index.html><a class=padding href=/new-docs/toolstack/high-level/environment/index.html>Environment</a></li><li data-nav-id=/new-docs/toolstack/high-level/daemons/index.html><a class=padding href=/new-docs/toolstack/high-level/daemons/index.html>Daemons</a></li><li data-nav-id=/new-docs/toolstack/high-level/interfaces/index.html><a class=padding href=/new-docs/toolstack/high-level/interfaces/index.html>Interfaces</a></li></ul></li><li data-nav-id=/new-docs/toolstack/features/index.html><input type=checkbox id=R-section-5d4df6b6fbc8d5af47c300ab507cc521 aria-controls=R-subsections-5d4df6b6fbc8d5af47c300ab507cc521><label for=R-section-5d4df6b6fbc8d5af47c300ab507cc521><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Features</span></label><a class=padding href=/new-docs/toolstack/features/index.html>Features</a><ul id=R-subsections-5d4df6b6fbc8d5af47c300ab507cc521 class=collapsible-menu><li data-nav-id=/new-docs/toolstack/features/DR/index.html><a class=padding href=/new-docs/toolstack/features/DR/index.html>Disaster Recovery</a></li><li data-nav-id=/new-docs/toolstack/features/events/index.html><a class=padding href=/new-docs/toolstack/features/events/index.html>Event handling</a></li><li data-nav-id=/new-docs/toolstack/features/HA/index.html><a class=padding href=/new-docs/toolstack/features/HA/index.html>High-Availability</a></li><li data-nav-id=/new-docs/toolstack/features/MVD/index.html><a class=padding href=/new-docs/toolstack/features/MVD/index.html>Multi-version drivers</a></li><li data-nav-id=/new-docs/toolstack/features/NUMA/index.html><a class=padding href=/new-docs/toolstack/features/NUMA/index.html>NUMA</a></li><li data-nav-id=/new-docs/toolstack/features/snapshots/index.html><a class=padding href=/new-docs/toolstack/features/snapshots/index.html>Snapshots</a></li><li data-nav-id=/new-docs/toolstack/features/VGPU/index.html><a class=padding href=/new-docs/toolstack/features/VGPU/index.html>vGPU</a></li><li data-nav-id=/new-docs/toolstack/features/XSM/index.html><a class=padding href=/new-docs/toolstack/features/XSM/index.html>Xapi Storage Migration</a></li></ul></li></ul></li><li data-nav-id=/new-docs/xapi/index.html><input type=checkbox id=R-section-67e12bf3d95935fbf56dca0e788c11f2 aria-controls=R-subsections-67e12bf3d95935fbf56dca0e788c11f2><label for=R-section-67e12bf3d95935fbf56dca0e788c11f2><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Xapi</span></label><a class=padding href=/new-docs/xapi/index.html>Xapi</a><ul id=R-subsections-67e12bf3d95935fbf56dca0e788c11f2 class=collapsible-menu><li data-nav-id=/new-docs/xapi/guides/index.html><input type=checkbox id=R-section-cf20b1aa06d1cc447ea310de2b605d51 aria-controls=R-subsections-cf20b1aa06d1cc447ea310de2b605d51><label for=R-section-cf20b1aa06d1cc447ea310de2b605d51><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Guides</span></label><a class=padding href=/new-docs/xapi/guides/index.html>Guides</a><ul id=R-subsections-cf20b1aa06d1cc447ea310de2b605d51 class=collapsible-menu><li data-nav-id=/new-docs/xapi/guides/howtos/index.html><input type=checkbox id=R-section-3ffc2fd44b0a843a38dd54cb698aadf0 aria-controls=R-subsections-3ffc2fd44b0a843a38dd54cb698aadf0><label for=R-section-3ffc2fd44b0a843a38dd54cb698aadf0><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu How to add....</span></label><a class=padding href=/new-docs/xapi/guides/howtos/index.html>How to add....</a><ul id=R-subsections-3ffc2fd44b0a843a38dd54cb698aadf0 class=collapsible-menu><li data-nav-id=/new-docs/xapi/guides/howtos/add-class/index.html><a class=padding href=/new-docs/xapi/guides/howtos/add-class/index.html>Adding a Class to the API</a></li><li data-nav-id=/new-docs/xapi/guides/howtos/add-field/index.html><a class=padding href=/new-docs/xapi/guides/howtos/add-field/index.html>Adding a field to the API</a></li><li data-nav-id=/new-docs/xapi/guides/howtos/add-function/index.html><a class=padding href=/new-docs/xapi/guides/howtos/add-function/index.html>Adding a function to the API</a></li><li data-nav-id=/new-docs/xapi/guides/howtos/add-api-extension/index.html><a class=padding href=/new-docs/xapi/guides/howtos/add-api-extension/index.html>Adding a XenAPI extension</a></li></ul></li></ul></li><li data-nav-id=/new-docs/xapi/cli/index.html><a class=padding href=/new-docs/xapi/cli/index.html>CLI</a></li><li data-nav-id=/new-docs/xapi/database/index.html><input type=checkbox id=R-section-44b9d28660dda49c2b7949290a3e1b89 aria-controls=R-subsections-44b9d28660dda49c2b7949290a3e1b89><label for=R-section-44b9d28660dda49c2b7949290a3e1b89><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Database</span></label><a class=padding href=/new-docs/xapi/database/index.html>Database</a><ul id=R-subsections-44b9d28660dda49c2b7949290a3e1b89 class=collapsible-menu><li data-nav-id=/new-docs/xapi/database/redo-log/index.html><a class=padding href=/new-docs/xapi/database/redo-log/index.html>Metadata-on-LUN</a></li></ul></li><li data-nav-id=/new-docs/xapi/internals/index.html><input type=checkbox id=R-section-e47da3b3a01078022dd7e240a34b6bc4 aria-controls=R-subsections-e47da3b3a01078022dd7e240a34b6bc4><label for=R-section-e47da3b3a01078022dd7e240a34b6bc4><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Internals</span></label><a class=padding href=/new-docs/xapi/internals/index.html>Internals</a><ul id=R-subsections-e47da3b3a01078022dd7e240a34b6bc4 class=collapsible-menu><li data-nav-id=/new-docs/xapi/internals/generated/index.html><a class=padding href=/new-docs/xapi/internals/generated/index.html>Generated Parts of Xapi</a></li></ul></li><li data-nav-id=/new-docs/xapi/memory/index.html><a class=padding href=/new-docs/xapi/memory/index.html>Memory</a></li><li data-nav-id=/new-docs/xapi/storage/index.html><input type=checkbox id=R-section-d5e50434ccd4672b573b8b9cfa1e675a aria-controls=R-subsections-d5e50434ccd4672b573b8b9cfa1e675a><label for=R-section-d5e50434ccd4672b573b8b9cfa1e675a><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Storage</span></label><a class=padding href=/new-docs/xapi/storage/index.html>Storage</a><ul id=R-subsections-d5e50434ccd4672b573b8b9cfa1e675a class=collapsible-menu><li data-nav-id=/new-docs/xapi/storage/sxm/index.html><a class=padding href=/new-docs/xapi/storage/sxm/index.html>Storage migration</a></li></ul></li><li data-nav-id=/new-docs/xapi/walkthroughs/index.html><input type=checkbox id=R-section-ae42e6008c3827e3783a76feed214d3a aria-controls=R-subsections-ae42e6008c3827e3783a76feed214d3a><label for=R-section-ae42e6008c3827e3783a76feed214d3a><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Walk-throughs</span></label><a class=padding href=/new-docs/xapi/walkthroughs/index.html>Walk-throughs</a><ul id=R-subsections-ae42e6008c3827e3783a76feed214d3a class=collapsible-menu><li data-nav-id=/new-docs/xapi/walkthroughs/migration_overview/index.html><a class=padding href=/new-docs/xapi/walkthroughs/migration_overview/index.html>How XAPI handles migration request</a></li></ul></li></ul></li><li data-nav-id=/new-docs/xenopsd/index.html><input type=checkbox id=R-section-eaf872be43aeecdfeb5e6f8d151be1a6 aria-controls=R-subsections-eaf872be43aeecdfeb5e6f8d151be1a6><label for=R-section-eaf872be43aeecdfeb5e6f8d151be1a6><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Xenopsd</span></label><a class=padding href=/new-docs/xenopsd/index.html>Xenopsd</a><ul id=R-subsections-eaf872be43aeecdfeb5e6f8d151be1a6 class=collapsible-menu><li data-nav-id=/new-docs/xenopsd/architecture/index.html><a class=padding href=/new-docs/xenopsd/architecture/index.html>Architecture</a></li><li data-nav-id=/new-docs/xenopsd/design/index.html><input type=checkbox id=R-section-c4ba0778266b15d8b43f3f7369b55454 aria-controls=R-subsections-c4ba0778266b15d8b43f3f7369b55454><label for=R-section-c4ba0778266b15d8b43f3f7369b55454><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Design</span></label><a class=padding href=/new-docs/xenopsd/design/index.html>Design</a><ul id=R-subsections-c4ba0778266b15d8b43f3f7369b55454 class=collapsible-menu><li data-nav-id=/new-docs/xenopsd/design/Events/index.html><a class=padding href=/new-docs/xenopsd/design/Events/index.html>Events</a></li><li data-nav-id=/new-docs/xenopsd/design/hooks/index.html><a class=padding href=/new-docs/xenopsd/design/hooks/index.html>Hooks</a></li><li data-nav-id=/new-docs/xenopsd/design/pvs-proxy-ovs/index.html><a class=padding href=/new-docs/xenopsd/design/pvs-proxy-ovs/index.html>PVS Proxy OVS Rules</a></li><li data-nav-id=/new-docs/xenopsd/design/suspend-image-considerations/index.html><a class=padding href=/new-docs/xenopsd/design/suspend-image-considerations/index.html>Requirements for suspend image framing</a></li><li data-nav-id=/new-docs/xenopsd/design/suspend-image-framing-format/index.html><a class=padding href=/new-docs/xenopsd/design/suspend-image-framing-format/index.html>Suspend image framing format</a></li><li data-nav-id=/new-docs/xenopsd/design/Tasks/index.html><a class=padding href=/new-docs/xenopsd/design/Tasks/index.html>Tasks</a></li></ul></li><li data-nav-id=/new-docs/xenopsd/features/index.html><a class=padding href=/new-docs/xenopsd/features/index.html>Features</a></li><li data-nav-id=/new-docs/xenopsd/walkthroughs/index.html><input type=checkbox id=R-section-82a272712946f38915b8e2cefa80a829 aria-controls=R-subsections-82a272712946f38915b8e2cefa80a829><label for=R-section-82a272712946f38915b8e2cefa80a829><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Walk-throughs</span></label><a class=padding href=/new-docs/xenopsd/walkthroughs/index.html>Walk-throughs</a><ul id=R-subsections-82a272712946f38915b8e2cefa80a829 class=collapsible-menu><li data-nav-id=/new-docs/xenopsd/walkthroughs/VM.start/index.html><a class=padding href=/new-docs/xenopsd/walkthroughs/VM.start/index.html>Starting a VM</a></li><li data-nav-id=/new-docs/xenopsd/walkthroughs/VM.build/index.html><input type=checkbox id=R-section-cb0e73c14bd7f69383fe23bf9475f55d aria-controls=R-subsections-cb0e73c14bd7f69383fe23bf9475f55d><label for=R-section-cb0e73c14bd7f69383fe23bf9475f55d><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Building a VM</span></label><a class=padding href=/new-docs/xenopsd/walkthroughs/VM.build/index.html>Building a VM</a><ul id=R-subsections-cb0e73c14bd7f69383fe23bf9475f55d class=collapsible-menu><li data-nav-id=/new-docs/xenopsd/walkthroughs/VM.build/VM_build/index.html><a class=padding href=/new-docs/xenopsd/walkthroughs/VM.build/VM_build/index.html>VM_build μ-op</a></li><li data-nav-id=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html><a class=padding href=/new-docs/xenopsd/walkthroughs/VM.build/Domain.build/index.html>Domain.build</a></li><li data-nav-id=/new-docs/xenopsd/walkthroughs/VM.build/xenguest/index.html><a class=padding href=/new-docs/xenopsd/walkthroughs/VM.build/xenguest/index.html>xenguest</a></li></ul></li><li data-nav-id=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html><a class=padding href=/new-docs/xenopsd/walkthroughs/VM.migrate/index.html>Migrating a VM</a></li><li data-nav-id=/new-docs/xenopsd/walkthroughs/live-migration/index.html><a class=padding href=/new-docs/xenopsd/walkthroughs/live-migration/index.html>Live Migration</a></li></ul></li></ul></li><li data-nav-id=/new-docs/xcp-networkd/index.html><a class=padding href=/new-docs/xcp-networkd/index.html>Networkd</a></li><li data-nav-id=/new-docs/squeezed/index.html><input type=checkbox id=R-section-2b908e757f8297a064be7fe32eedad70 aria-controls=R-subsections-2b908e757f8297a064be7fe32eedad70><label for=R-section-2b908e757f8297a064be7fe32eedad70><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Squeezed</span></label><a class=padding href=/new-docs/squeezed/index.html>Squeezed</a><ul id=R-subsections-2b908e757f8297a064be7fe32eedad70 class=collapsible-menu><li data-nav-id=/new-docs/squeezed/architecture/index.html><a class=padding href=/new-docs/squeezed/architecture/index.html>Architecture</a></li><li data-nav-id=/new-docs/squeezed/design/index.html><a class=padding href=/new-docs/squeezed/design/index.html>Design</a></li></ul></li><li data-nav-id=/new-docs/xapi-guard/index.html><a class=padding href=/new-docs/xapi-guard/index.html>Xapi-guard</a></li><li class=parent data-nav-id=/new-docs/design/index.html><input type=checkbox id=R-section-584881683b3574eff66f4a8e3d60d561 aria-controls=R-subsections-584881683b3574eff66f4a8e3d60d561 checked><label for=R-section-584881683b3574eff66f4a8e3d60d561><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Designs</span></label><a class=padding href=/new-docs/design/index.html>Designs</a><ul id=R-subsections-584881683b3574eff66f4a8e3d60d561 class=collapsible-menu><li data-nav-id=/new-docs/design/aggr-storage-reboots/index.html><a class=padding href=/new-docs/design/aggr-storage-reboots/index.html>Aggregated Local Storage and Host Reboots</a></li><li data-nav-id=/new-docs/design/backtraces/index.html><a class=padding href=/new-docs/design/backtraces/index.html>Backtrace support</a></li><li data-nav-id=/new-docs/design/bonding-improvements/index.html><a class=padding href=/new-docs/design/bonding-improvements/index.html>Bonding Improvements design</a></li><li data-nav-id=/new-docs/design/coverage/index.html><a class=padding href=/new-docs/design/coverage/index.html>Code Coverage Profiling</a></li><li data-nav-id=/new-docs/design/cpu-levelling-v2/index.html><a class=padding href=/new-docs/design/cpu-levelling-v2/index.html>CPU feature levelling 2.0</a></li><li data-nav-id=/new-docs/design/distributed-database/index.html><a class=padding href=/new-docs/design/distributed-database/index.html>Distributed database</a></li><li data-nav-id=/new-docs/design/emergency-network-reset/index.html><a class=padding href=/new-docs/design/emergency-network-reset/index.html>Emergency Network Reset Design</a></li><li data-nav-id=/new-docs/design/fcoe-nics/index.html><a class=padding href=/new-docs/design/fcoe-nics/index.html>FCoE capable NICs</a></li><li data-nav-id=/new-docs/design/gpu-passthrough/index.html><a class=padding href=/new-docs/design/gpu-passthrough/index.html>GPU pass-through support</a></li><li data-nav-id=/new-docs/design/gpu-support-evolution/index.html><a class=padding href=/new-docs/design/gpu-support-evolution/index.html>GPU support evolution</a></li><li data-nav-id=/new-docs/design/pif-properties/index.html><a class=padding href=/new-docs/design/pif-properties/index.html>GRO and other properties of PIFs</a></li><li data-nav-id=/new-docs/design/heterogeneous-pools/index.html><a class=padding href=/new-docs/design/heterogeneous-pools/index.html>Heterogeneous pools</a></li><li data-nav-id=/new-docs/design/snapshot-revert/index.html><a class=padding href=/new-docs/design/snapshot-revert/index.html>Improving snapshot revert behaviour</a></li><li data-nav-id=/new-docs/design/integrated-gpu-passthrough/index.html><a class=padding href=/new-docs/design/integrated-gpu-passthrough/index.html>Integrated GPU passthrough support</a></li><li data-nav-id=/new-docs/design/local-database/index.html><a class=padding href=/new-docs/design/local-database/index.html>Local database</a></li><li data-nav-id=/new-docs/design/management-interface-on-vlan/index.html><a class=padding href=/new-docs/design/management-interface-on-vlan/index.html>Management Interface on VLAN</a></li><li data-nav-id=/new-docs/design/multiple-cluster-managers/index.html><a class=padding href=/new-docs/design/multiple-cluster-managers/index.html>Multiple Cluster Managers</a></li><li data-nav-id=/new-docs/design/multiple-device-emulators/index.html><a class=padding href=/new-docs/design/multiple-device-emulators/index.html>Multiple device emulators</a></li><li data-nav-id=/new-docs/design/ocfs2/index.html><a class=padding href=/new-docs/design/ocfs2/index.html>OCFS2 storage</a></li><li data-nav-id=/new-docs/design/patches-in-vdis/index.html><a class=padding href=/new-docs/design/patches-in-vdis/index.html>patches in VDIs</a></li><li data-nav-id=/new-docs/design/pci-passthrough/index.html><a class=padding href=/new-docs/design/pci-passthrough/index.html>PCI passthrough support</a></li><li data-nav-id=/new-docs/design/pool-wide-ssh/index.html><a class=padding href=/new-docs/design/pool-wide-ssh/index.html>Pool-wide SSH</a></li><li data-nav-id=/new-docs/design/xenopsd_events/index.html><a class=padding href=/new-docs/design/xenopsd_events/index.html>Process events from xenopsd in a timely manner</a></li><li data-nav-id=/new-docs/design/RDP/index.html><a class=padding href=/new-docs/design/RDP/index.html>RDP control</a></li><li data-nav-id=/new-docs/design/archival-redesign/index.html><a class=padding href=/new-docs/design/archival-redesign/index.html>RRDD archival redesign</a></li><li data-nav-id=/new-docs/design/plugin-protocol-v2/index.html><a class=padding href=/new-docs/design/plugin-protocol-v2/index.html>RRDD plugin protocol v2</a></li><li data-nav-id=/new-docs/design/plugin-protocol-v3/index.html><a class=padding href=/new-docs/design/plugin-protocol-v3/index.html>RRDD plugin protocol v3</a></li><li data-nav-id=/new-docs/design/schedule-snapshot/index.html><a class=padding href=/new-docs/design/schedule-snapshot/index.html>Schedule Snapshot Design</a></li><li data-nav-id=/new-docs/design/smapiv3/index.html><a class=padding href=/new-docs/design/smapiv3/index.html>SMAPIv3</a></li><li data-nav-id=/new-docs/design/emulated-pci-spec/index.html><a class=padding href=/new-docs/design/emulated-pci-spec/index.html>Specifying Emulated PCI Devices</a></li><li data-nav-id=/new-docs/design/sr-level-rrds/index.html><a class=padding href=/new-docs/design/sr-level-rrds/index.html>SR-Level RRDs</a></li><li class=active data-nav-id=/new-docs/design/thin-lvhd/index.html><a class=padding href=/new-docs/design/thin-lvhd/index.html>thin LVHD storage</a></li><li data-nav-id=/new-docs/design/pool-certificates/index.html><a class=padding href=/new-docs/design/pool-certificates/index.html>TLS vertification for intra-pool communications</a></li><li data-nav-id=/new-docs/design/tunnelling/index.html><a class=padding href=/new-docs/design/tunnelling/index.html>Tunnelling API design</a></li><li data-nav-id=/new-docs/design/user-certificates/index.html><a class=padding href=/new-docs/design/user-certificates/index.html>User-installable host certificates</a></li><li data-nav-id=/new-docs/design/vgpu-type-identifiers/index.html><a class=padding href=/new-docs/design/vgpu-type-identifiers/index.html>VGPU type identifiers</a></li><li data-nav-id=/new-docs/design/virt-hw-platform-vn/index.html><a class=padding href=/new-docs/design/virt-hw-platform-vn/index.html>Virtual Hardware Platform Version</a></li><li data-nav-id=/new-docs/design/xenprep/index.html><a class=padding href=/new-docs/design/xenprep/index.html>XenPrep</a></li></ul></li><li data-nav-id=/new-docs/python/index.html><a class=padding href=/new-docs/python/index.html>Python</a></li><li data-nav-id=/new-docs/xcp-rrdd/index.html><input type=checkbox id=R-section-38712d4ba58933e242c69e06308c5176 aria-controls=R-subsections-38712d4ba58933e242c69e06308c5176><label for=R-section-38712d4ba58933e242c69e06308c5176><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu RRDD</span></label><a class=padding href=/new-docs/xcp-rrdd/index.html>RRDD</a><ul id=R-subsections-38712d4ba58933e242c69e06308c5176 class=collapsible-menu><li data-nav-id=/new-docs/xcp-rrdd/futures/archival-redesign/index.html><a class=padding href=/new-docs/xcp-rrdd/futures/archival-redesign/index.html>RRDD archival redesign</a></li><li data-nav-id=/new-docs/xcp-rrdd/design/plugin-protocol-v2/index.html><a class=padding href=/new-docs/xcp-rrdd/design/plugin-protocol-v2/index.html>RRDD plugin protocol v2</a></li><li data-nav-id=/new-docs/xcp-rrdd/futures/sr-level-rrds/index.html><a class=padding href=/new-docs/xcp-rrdd/futures/sr-level-rrds/index.html>SR-Level RRDs</a></li></ul></li><li data-nav-id=/new-docs/xen-api/index.html><input type=checkbox id=R-section-85499c3ba3e3f693ce7955637d671edd aria-controls=R-subsections-85499c3ba3e3f693ce7955637d671edd><label for=R-section-85499c3ba3e3f693ce7955637d671edd><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu XenAPI</span></label><a class=padding href=/new-docs/xen-api/index.html>XenAPI</a><ul id=R-subsections-85499c3ba3e3f693ce7955637d671edd class=collapsible-menu><li data-nav-id=/new-docs/xen-api/basics/index.html><a class=padding href=/new-docs/xen-api/basics/index.html>XenAPI Basics</a></li><li data-nav-id=/new-docs/xen-api/wire-protocol/index.html><a class=padding href=/new-docs/xen-api/wire-protocol/index.html>Wire Protocol</a></li><li data-nav-id=/new-docs/xen-api/overview/index.html><a class=padding href=/new-docs/xen-api/overview/index.html>Overview of the XenAPI</a></li><li data-nav-id=/new-docs/xen-api/evolution/index.html><a class=padding href=/new-docs/xen-api/evolution/index.html>API evolution</a></li><li data-nav-id=/new-docs/xen-api/usage/index.html><a class=padding href=/new-docs/xen-api/usage/index.html>Using the API</a></li><li data-nav-id=/new-docs/xen-api/classes/index.html><input type=checkbox id=R-section-c18ca4c7a790a34a3710541914952da2 aria-controls=R-subsections-c18ca4c7a790a34a3710541914952da2><label for=R-section-c18ca4c7a790a34a3710541914952da2><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu XenAPI Reference</span></label><a class=padding href=/new-docs/xen-api/classes/index.html>XenAPI Reference</a><ul id=R-subsections-c18ca4c7a790a34a3710541914952da2 class=collapsible-menu><li data-nav-id=/new-docs/xen-api/classes/auth/index.html><a class=padding href=/new-docs/xen-api/classes/auth/index.html>auth</a></li><li data-nav-id=/new-docs/xen-api/classes/blob/index.html><a class=padding href=/new-docs/xen-api/classes/blob/index.html>blob</a></li><li data-nav-id=/new-docs/xen-api/classes/bond/index.html><a class=padding href=/new-docs/xen-api/classes/bond/index.html>Bond</a></li><li data-nav-id=/new-docs/xen-api/classes/certificate/index.html><a class=padding href=/new-docs/xen-api/classes/certificate/index.html>Certificate</a></li><li data-nav-id=/new-docs/xen-api/classes/cluster/index.html><a class=padding href=/new-docs/xen-api/classes/cluster/index.html>Cluster</a></li><li data-nav-id=/new-docs/xen-api/classes/cluster_host/index.html><a class=padding href=/new-docs/xen-api/classes/cluster_host/index.html>Cluster_host</a></li><li data-nav-id=/new-docs/xen-api/classes/console/index.html><a class=padding href=/new-docs/xen-api/classes/console/index.html>console</a></li><li data-nav-id=/new-docs/xen-api/classes/crashdump/index.html><a class=padding href=/new-docs/xen-api/classes/crashdump/index.html>crashdump</a></li><li data-nav-id=/new-docs/xen-api/classes/data_source/index.html><a class=padding href=/new-docs/xen-api/classes/data_source/index.html>data_source</a></li><li data-nav-id=/new-docs/xen-api/classes/dr_task/index.html><a class=padding href=/new-docs/xen-api/classes/dr_task/index.html>DR_task</a></li><li data-nav-id=/new-docs/xen-api/classes/event/index.html><a class=padding href=/new-docs/xen-api/classes/event/index.html>event</a></li><li data-nav-id=/new-docs/xen-api/classes/feature/index.html><a class=padding href=/new-docs/xen-api/classes/feature/index.html>Feature</a></li><li data-nav-id=/new-docs/xen-api/classes/gpu_group/index.html><a class=padding href=/new-docs/xen-api/classes/gpu_group/index.html>GPU_group</a></li><li data-nav-id=/new-docs/xen-api/classes/host/index.html><a class=padding href=/new-docs/xen-api/classes/host/index.html>host</a></li><li data-nav-id=/new-docs/xen-api/classes/host_cpu/index.html><a class=padding href=/new-docs/xen-api/classes/host_cpu/index.html>host_cpu</a></li><li data-nav-id=/new-docs/xen-api/classes/host_crashdump/index.html><a class=padding href=/new-docs/xen-api/classes/host_crashdump/index.html>host_crashdump</a></li><li data-nav-id=/new-docs/xen-api/classes/host_metrics/index.html><a class=padding href=/new-docs/xen-api/classes/host_metrics/index.html>host_metrics</a></li><li data-nav-id=/new-docs/xen-api/classes/host_patch/index.html><a class=padding href=/new-docs/xen-api/classes/host_patch/index.html>host_patch</a></li><li data-nav-id=/new-docs/xen-api/classes/lvhd/index.html><a class=padding href=/new-docs/xen-api/classes/lvhd/index.html>LVHD</a></li><li data-nav-id=/new-docs/xen-api/classes/message/index.html><a class=padding href=/new-docs/xen-api/classes/message/index.html>message</a></li><li data-nav-id=/new-docs/xen-api/classes/network/index.html><a class=padding href=/new-docs/xen-api/classes/network/index.html>network</a></li><li data-nav-id=/new-docs/xen-api/classes/network_sriov/index.html><a class=padding href=/new-docs/xen-api/classes/network_sriov/index.html>network_sriov</a></li><li data-nav-id=/new-docs/xen-api/classes/observer/index.html><a class=padding href=/new-docs/xen-api/classes/observer/index.html>Observer</a></li><li data-nav-id=/new-docs/xen-api/classes/pbd/index.html><a class=padding href=/new-docs/xen-api/classes/pbd/index.html>PBD</a></li><li data-nav-id=/new-docs/xen-api/classes/pci/index.html><a class=padding href=/new-docs/xen-api/classes/pci/index.html>PCI</a></li><li data-nav-id=/new-docs/xen-api/classes/pgpu/index.html><a class=padding href=/new-docs/xen-api/classes/pgpu/index.html>PGPU</a></li><li data-nav-id=/new-docs/xen-api/classes/pif/index.html><a class=padding href=/new-docs/xen-api/classes/pif/index.html>PIF</a></li><li data-nav-id=/new-docs/xen-api/classes/pif_metrics/index.html><a class=padding href=/new-docs/xen-api/classes/pif_metrics/index.html>PIF_metrics</a></li><li data-nav-id=/new-docs/xen-api/classes/pool/index.html><a class=padding href=/new-docs/xen-api/classes/pool/index.html>pool</a></li><li data-nav-id=/new-docs/xen-api/classes/pool_patch/index.html><a class=padding href=/new-docs/xen-api/classes/pool_patch/index.html>pool_patch</a></li><li data-nav-id=/new-docs/xen-api/classes/pool_update/index.html><a class=padding href=/new-docs/xen-api/classes/pool_update/index.html>pool_update</a></li><li data-nav-id=/new-docs/xen-api/classes/probe_result/index.html><a class=padding href=/new-docs/xen-api/classes/probe_result/index.html>probe_result</a></li><li data-nav-id=/new-docs/xen-api/classes/pusb/index.html><a class=padding href=/new-docs/xen-api/classes/pusb/index.html>PUSB</a></li><li data-nav-id=/new-docs/xen-api/classes/pvs_cache_storage/index.html><a class=padding href=/new-docs/xen-api/classes/pvs_cache_storage/index.html>PVS_cache_storage</a></li><li data-nav-id=/new-docs/xen-api/classes/pvs_proxy/index.html><a class=padding href=/new-docs/xen-api/classes/pvs_proxy/index.html>PVS_proxy</a></li><li data-nav-id=/new-docs/xen-api/classes/pvs_server/index.html><a class=padding href=/new-docs/xen-api/classes/pvs_server/index.html>PVS_server</a></li><li data-nav-id=/new-docs/xen-api/classes/pvs_site/index.html><a class=padding href=/new-docs/xen-api/classes/pvs_site/index.html>PVS_site</a></li><li data-nav-id=/new-docs/xen-api/classes/repository/index.html><a class=padding href=/new-docs/xen-api/classes/repository/index.html>Repository</a></li><li data-nav-id=/new-docs/xen-api/classes/role/index.html><a class=padding href=/new-docs/xen-api/classes/role/index.html>role</a></li><li data-nav-id=/new-docs/xen-api/classes/sdn_controller/index.html><a class=padding href=/new-docs/xen-api/classes/sdn_controller/index.html>SDN_controller</a></li><li data-nav-id=/new-docs/xen-api/classes/secret/index.html><a class=padding href=/new-docs/xen-api/classes/secret/index.html>secret</a></li><li data-nav-id=/new-docs/xen-api/classes/session/index.html><a class=padding href=/new-docs/xen-api/classes/session/index.html>session</a></li><li data-nav-id=/new-docs/xen-api/classes/sm/index.html><a class=padding href=/new-docs/xen-api/classes/sm/index.html>SM</a></li><li data-nav-id=/new-docs/xen-api/classes/sr/index.html><a class=padding href=/new-docs/xen-api/classes/sr/index.html>SR</a></li><li data-nav-id=/new-docs/xen-api/classes/sr_stat/index.html><a class=padding href=/new-docs/xen-api/classes/sr_stat/index.html>sr_stat</a></li><li data-nav-id=/new-docs/xen-api/classes/subject/index.html><a class=padding href=/new-docs/xen-api/classes/subject/index.html>subject</a></li><li data-nav-id=/new-docs/xen-api/classes/task/index.html><a class=padding href=/new-docs/xen-api/classes/task/index.html>task</a></li><li data-nav-id=/new-docs/xen-api/classes/tunnel/index.html><a class=padding href=/new-docs/xen-api/classes/tunnel/index.html>tunnel</a></li><li data-nav-id=/new-docs/xen-api/classes/usb_group/index.html><a class=padding href=/new-docs/xen-api/classes/usb_group/index.html>USB_group</a></li><li data-nav-id=/new-docs/xen-api/classes/user/index.html><a class=padding href=/new-docs/xen-api/classes/user/index.html>user</a></li><li data-nav-id=/new-docs/xen-api/classes/vbd/index.html><a class=padding href=/new-docs/xen-api/classes/vbd/index.html>VBD</a></li><li data-nav-id=/new-docs/xen-api/classes/vbd_metrics/index.html><a class=padding href=/new-docs/xen-api/classes/vbd_metrics/index.html>VBD_metrics</a></li><li data-nav-id=/new-docs/xen-api/classes/vdi/index.html><a class=padding href=/new-docs/xen-api/classes/vdi/index.html>VDI</a></li><li data-nav-id=/new-docs/xen-api/classes/vdi_nbd_server_info/index.html><a class=padding href=/new-docs/xen-api/classes/vdi_nbd_server_info/index.html>vdi_nbd_server_info</a></li><li data-nav-id=/new-docs/xen-api/classes/vgpu/index.html><a class=padding href=/new-docs/xen-api/classes/vgpu/index.html>VGPU</a></li><li data-nav-id=/new-docs/xen-api/classes/vgpu_type/index.html><a class=padding href=/new-docs/xen-api/classes/vgpu_type/index.html>VGPU_type</a></li><li data-nav-id=/new-docs/xen-api/classes/vif/index.html><a class=padding href=/new-docs/xen-api/classes/vif/index.html>VIF</a></li><li data-nav-id=/new-docs/xen-api/classes/vif_metrics/index.html><a class=padding href=/new-docs/xen-api/classes/vif_metrics/index.html>VIF_metrics</a></li><li data-nav-id=/new-docs/xen-api/classes/vlan/index.html><a class=padding href=/new-docs/xen-api/classes/vlan/index.html>VLAN</a></li><li data-nav-id=/new-docs/xen-api/classes/vm/index.html><a class=padding href=/new-docs/xen-api/classes/vm/index.html>VM</a></li><li data-nav-id=/new-docs/xen-api/classes/vm_appliance/index.html><a class=padding href=/new-docs/xen-api/classes/vm_appliance/index.html>VM_appliance</a></li><li data-nav-id=/new-docs/xen-api/classes/vm_guest_metrics/index.html><a class=padding href=/new-docs/xen-api/classes/vm_guest_metrics/index.html>VM_guest_metrics</a></li><li data-nav-id=/new-docs/xen-api/classes/vm_metrics/index.html><a class=padding href=/new-docs/xen-api/classes/vm_metrics/index.html>VM_metrics</a></li><li data-nav-id=/new-docs/xen-api/classes/vmpp/index.html><a class=padding href=/new-docs/xen-api/classes/vmpp/index.html>VMPP</a></li><li data-nav-id=/new-docs/xen-api/classes/vmss/index.html><a class=padding href=/new-docs/xen-api/classes/vmss/index.html>VMSS</a></li><li data-nav-id=/new-docs/xen-api/classes/vtpm/index.html><a class=padding href=/new-docs/xen-api/classes/vtpm/index.html>VTPM</a></li><li data-nav-id=/new-docs/xen-api/classes/vusb/index.html><a class=padding href=/new-docs/xen-api/classes/vusb/index.html>VUSB</a></li></ul></li><li data-nav-id=/new-docs/xen-api/releases/index.html><input type=checkbox id=R-section-163f98f7fb9ab44b225cce7a0165c198 aria-controls=R-subsections-163f98f7fb9ab44b225cce7a0165c198><label for=R-section-163f98f7fb9ab44b225cce7a0165c198><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu XenAPI Releases</span></label><a class=padding href=/new-docs/xen-api/releases/index.html>XenAPI Releases</a><ul id=R-subsections-163f98f7fb9ab44b225cce7a0165c198 class=collapsible-menu><li data-nav-id=/new-docs/xen-api/releases/24.16.0/index.html><a class=padding href=/new-docs/xen-api/releases/24.16.0/index.html>XAPI 24.16.0</a></li><li data-nav-id=/new-docs/xen-api/releases/24.14.0/index.html><a class=padding href=/new-docs/xen-api/releases/24.14.0/index.html>XAPI 24.14.0</a></li><li data-nav-id=/new-docs/xen-api/releases/24.10.0/index.html><a class=padding href=/new-docs/xen-api/releases/24.10.0/index.html>XAPI 24.10.0</a></li><li data-nav-id=/new-docs/xen-api/releases/24.3.0/index.html><a class=padding href=/new-docs/xen-api/releases/24.3.0/index.html>XAPI 24.3.0</a></li><li data-nav-id=/new-docs/xen-api/releases/24.0.0/index.html><a class=padding href=/new-docs/xen-api/releases/24.0.0/index.html>XAPI 24.0.0</a></li><li data-nav-id=/new-docs/xen-api/releases/23.30.0/index.html><a class=padding href=/new-docs/xen-api/releases/23.30.0/index.html>XAPI 23.30.0</a></li><li data-nav-id=/new-docs/xen-api/releases/23.27.0/index.html><a class=padding href=/new-docs/xen-api/releases/23.27.0/index.html>XAPI 23.27.0</a></li><li data-nav-id=/new-docs/xen-api/releases/23.25.0/index.html><a class=padding href=/new-docs/xen-api/releases/23.25.0/index.html>XAPI 23.25.0</a></li><li data-nav-id=/new-docs/xen-api/releases/23.18.0/index.html><a class=padding href=/new-docs/xen-api/releases/23.18.0/index.html>XAPI 23.18.0</a></li><li data-nav-id=/new-docs/xen-api/releases/23.14.0/index.html><a class=padding href=/new-docs/xen-api/releases/23.14.0/index.html>XAPI 23.14.0</a></li><li data-nav-id=/new-docs/xen-api/releases/23.9.0/index.html><a class=padding href=/new-docs/xen-api/releases/23.9.0/index.html>XAPI 23.9.0</a></li><li data-nav-id=/new-docs/xen-api/releases/23.1.0/index.html><a class=padding href=/new-docs/xen-api/releases/23.1.0/index.html>XAPI 23.1.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.37.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.37.0/index.html>XAPI 22.37.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.33.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.33.0/index.html>XAPI 22.33.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.27.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.27.0/index.html>XAPI 22.27.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.26.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.26.0/index.html>XAPI 22.26.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.20.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.20.0/index.html>XAPI 22.20.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.19.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.19.0/index.html>XAPI 22.19.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.16.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.16.0/index.html>XAPI 22.16.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.12.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.12.0/index.html>XAPI 22.12.0</a></li><li data-nav-id=/new-docs/xen-api/releases/22.5.0/index.html><a class=padding href=/new-docs/xen-api/releases/22.5.0/index.html>XAPI 22.5.0</a></li><li data-nav-id=/new-docs/xen-api/releases/21.4.0/index.html><a class=padding href=/new-docs/xen-api/releases/21.4.0/index.html>XAPI 21.4.0</a></li><li data-nav-id=/new-docs/xen-api/releases/21.3.0/index.html><a class=padding href=/new-docs/xen-api/releases/21.3.0/index.html>XAPI 21.3.0</a></li><li data-nav-id=/new-docs/xen-api/releases/21.2.0/index.html><a class=padding href=/new-docs/xen-api/releases/21.2.0/index.html>XAPI 21.2.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.329.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.329.0/index.html>XAPI 1.329.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.318.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.318.0/index.html>XAPI 1.318.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.313.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.313.0/index.html>XAPI 1.313.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.307.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.307.0/index.html>XAPI 1.307.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.304.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.304.0/index.html>XAPI 1.304.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.303.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.303.0/index.html>XAPI 1.303.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.301.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.301.0/index.html>XAPI 1.301.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.298.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.298.0/index.html>XAPI 1.298.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.297.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.297.0/index.html>XAPI 1.297.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.294.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.294.0/index.html>XAPI 1.294.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.290.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.290.0/index.html>XAPI 1.290.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.271.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.271.0/index.html>XAPI 1.271.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.257.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.257.0/index.html>XAPI 1.257.0</a></li><li data-nav-id=/new-docs/xen-api/releases/1.250.0/index.html><a class=padding href=/new-docs/xen-api/releases/1.250.0/index.html>XAPI 1.250.0</a></li><li data-nav-id=/new-docs/xen-api/releases/nile-preview/index.html><a class=padding href=/new-docs/xen-api/releases/nile-preview/index.html>XenServer 8 Preview</a></li><li data-nav-id=/new-docs/xen-api/releases/stockholm_psr/index.html><a class=padding href=/new-docs/xen-api/releases/stockholm_psr/index.html>Citrix Hypervisor 8.2 Hotfix 2</a></li><li data-nav-id=/new-docs/xen-api/releases/stockholm/index.html><a class=padding href=/new-docs/xen-api/releases/stockholm/index.html>Citrix Hypervisor 8.2</a></li><li data-nav-id=/new-docs/xen-api/releases/quebec/index.html><a class=padding href=/new-docs/xen-api/releases/quebec/index.html>Citrix Hypervisor 8.1</a></li><li data-nav-id=/new-docs/xen-api/releases/naples/index.html><a class=padding href=/new-docs/xen-api/releases/naples/index.html>Citrix Hypervisor 8.0</a></li><li data-nav-id=/new-docs/xen-api/releases/lima/index.html><a class=padding href=/new-docs/xen-api/releases/lima/index.html>XenServer 7.6</a></li><li data-nav-id=/new-docs/xen-api/releases/kolkata/index.html><a class=padding href=/new-docs/xen-api/releases/kolkata/index.html>XenServer 7.5</a></li><li data-nav-id=/new-docs/xen-api/releases/jura/index.html><a class=padding href=/new-docs/xen-api/releases/jura/index.html>XenServer 7.4</a></li><li data-nav-id=/new-docs/xen-api/releases/inverness/index.html><a class=padding href=/new-docs/xen-api/releases/inverness/index.html>XenServer 7.3</a></li><li data-nav-id=/new-docs/xen-api/releases/falcon/index.html><a class=padding href=/new-docs/xen-api/releases/falcon/index.html>XenServer 7.2</a></li><li data-nav-id=/new-docs/xen-api/releases/ely/index.html><a class=padding href=/new-docs/xen-api/releases/ely/index.html>XenServer 7.1</a></li><li data-nav-id=/new-docs/xen-api/releases/dundee/index.html><a class=padding href=/new-docs/xen-api/releases/dundee/index.html>XenServer 7.0</a></li><li data-nav-id=/new-docs/xen-api/releases/indigo/index.html><a class=padding href=/new-docs/xen-api/releases/indigo/index.html>XenServer 6.5 SP1 Hotfix 31</a></li><li data-nav-id=/new-docs/xen-api/releases/cream/index.html><a class=padding href=/new-docs/xen-api/releases/cream/index.html>XenServer 6.5 SP1</a></li><li data-nav-id=/new-docs/xen-api/releases/creedence/index.html><a class=padding href=/new-docs/xen-api/releases/creedence/index.html>XenServer 6.5</a></li><li data-nav-id=/new-docs/xen-api/releases/clearwater-whetstone/index.html><a class=padding href=/new-docs/xen-api/releases/clearwater-whetstone/index.html>XenServer 6.2 SP1 Hotfix 11</a></li><li data-nav-id=/new-docs/xen-api/releases/clearwater-felton/index.html><a class=padding href=/new-docs/xen-api/releases/clearwater-felton/index.html>XenServer 6.2 SP1 Hotfix 4</a></li><li data-nav-id=/new-docs/xen-api/releases/vgpu-productisation/index.html><a class=padding href=/new-docs/xen-api/releases/vgpu-productisation/index.html>XenServer 6.2 SP1</a></li><li data-nav-id=/new-docs/xen-api/releases/vgpu-tech-preview/index.html><a class=padding href=/new-docs/xen-api/releases/vgpu-tech-preview/index.html>XenServer 6.2 SP1 Tech-Preview</a></li><li data-nav-id=/new-docs/xen-api/releases/clearwater/index.html><a class=padding href=/new-docs/xen-api/releases/clearwater/index.html>XenServer 6.2</a></li><li data-nav-id=/new-docs/xen-api/releases/tampa/index.html><a class=padding href=/new-docs/xen-api/releases/tampa/index.html>XenServer 6.1</a></li><li data-nav-id=/new-docs/xen-api/releases/boston/index.html><a class=padding href=/new-docs/xen-api/releases/boston/index.html>XenServer 6.0</a></li><li data-nav-id=/new-docs/xen-api/releases/cowley/index.html><a class=padding href=/new-docs/xen-api/releases/cowley/index.html>XenServer 5.6 FP1</a></li><li data-nav-id=/new-docs/xen-api/releases/midnight-ride/index.html><a class=padding href=/new-docs/xen-api/releases/midnight-ride/index.html>XenServer 5.6</a></li><li data-nav-id=/new-docs/xen-api/releases/george/index.html><a class=padding href=/new-docs/xen-api/releases/george/index.html>XenServer 5.5</a></li><li data-nav-id=/new-docs/xen-api/releases/orlando-update-1/index.html><a class=padding href=/new-docs/xen-api/releases/orlando-update-1/index.html>XenServer 5.0 Update 1</a></li><li data-nav-id=/new-docs/xen-api/releases/orlando/index.html><a class=padding href=/new-docs/xen-api/releases/orlando/index.html>XenServer 5.0</a></li><li data-nav-id=/new-docs/xen-api/releases/symc/index.html><a class=padding href=/new-docs/xen-api/releases/symc/index.html>XenServer 4.1.1</a></li><li data-nav-id=/new-docs/xen-api/releases/miami/index.html><a class=padding href=/new-docs/xen-api/releases/miami/index.html>XenServer 4.1</a></li><li data-nav-id=/new-docs/xen-api/releases/rio/index.html><a class=padding href=/new-docs/xen-api/releases/rio/index.html>XenServer 4.0</a></li></ul></li><li data-nav-id=/new-docs/xen-api/topics/index.html><input type=checkbox id=R-section-7bfdf54ffdbca634ba4315d64a0b4174 aria-controls=R-subsections-7bfdf54ffdbca634ba4315d64a0b4174><label for=R-section-7bfdf54ffdbca634ba4315d64a0b4174><i class="fa-fw fas fa-chevron-right"></i><span class=a11y-only>Submenu Topics</span></label><a class=padding href=/new-docs/xen-api/topics/index.html>Topics</a><ul id=R-subsections-7bfdf54ffdbca634ba4315d64a0b4174 class=collapsible-menu><li data-nav-id=/new-docs/xen-api/topics/udhcp/index.html><a class=padding href=/new-docs/xen-api/topics/udhcp/index.html>API for configuring the udhcp server in Dom0</a></li><li data-nav-id=/new-docs/xen-api/topics/guest-agents/index.html><a class=padding href=/new-docs/xen-api/topics/guest-agents/index.html>Guest agents</a></li><li data-nav-id=/new-docs/xen-api/topics/memory/index.html><a class=padding href=/new-docs/xen-api/topics/memory/index.html>Memory</a></li><li data-nav-id=/new-docs/xen-api/topics/metrics/index.html><a class=padding href=/new-docs/xen-api/topics/metrics/index.html>Metrics</a></li><li data-nav-id=/new-docs/xen-api/topics/snapshots/index.html><a class=padding href=/new-docs/xen-api/topics/snapshots/index.html>Snapshots</a></li><li data-nav-id=/new-docs/xen-api/topics/consoles/index.html><a class=padding href=/new-docs/xen-api/topics/consoles/index.html>VM consoles</a></li><li data-nav-id=/new-docs/xen-api/topics/importexport/index.html><a class=padding href=/new-docs/xen-api/topics/importexport/index.html>VM import/export</a></li><li data-nav-id=/new-docs/xen-api/topics/vm-lifecycle/index.html><a class=padding href=/new-docs/xen-api/topics/vm-lifecycle/index.html>VM Lifecycle</a></li><li data-nav-id=/new-docs/xen-api/topics/xencenter/index.html><a class=padding href=/new-docs/xen-api/topics/xencenter/index.html>XenCenter</a></li></ul></li></ul></li></ul></div><div class="padding footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showFooter"></div><div id=R-menu-footer><hr class="padding default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showFooter"><div id=R-prefooter class="footerLangSwitch footerVariantSwitch footerVisitedLinks showVariantSwitch"><ul><li id=R-select-language-container class=footerLangSwitch><div class="padding menu-control"><i class="fa-fw fas fa-language"></i>
<span>&nbsp;</span><div class=control-style><label class=a11y-only for=R-select-language>Language</label>
<select id=R-select-language onchange="location=this.querySelector(this.value).dataset.url"><option id=R-select-language-en value=#R-select-language-en data-url=/new-docs/design/thin-lvhd/index.html lang=en-us selected></option></select></div><div class=clear></div></div></li><li id=R-select-variant-container class="footerVariantSwitch showVariantSwitch"><div class="padding menu-control"><i class="fa-fw fas fa-paint-brush"></i>
<span>&nbsp;</span><div class=control-style><label class=a11y-only for=R-select-variant>Theme</label>
<select id=R-select-variant onchange=window.relearn.changeVariant(this.value)><option id=R-select-variant-auto value=auto selected>Auto</option><option id=R-select-variant-zen-light value=zen-light>Zen Light</option><option id=R-select-variant-zen-dark value=zen-dark>Zen Dark</option><option id=R-select-variant-red value=red>Red</option><option id=R-select-variant-blue value=blue>Blue</option><option id=R-select-variant-green value=green>Green</option><option id=R-select-variant-learn value=learn>Learn</option><option id=R-select-variant-neon value=neon>Neon</option><option id=R-select-variant-relearn-light value=relearn-light>Relearn Light</option><option id=R-select-variant-relearn-bright value=relearn-bright>Relearn Bright</option><option id=R-select-variant-relearn-dark value=relearn-dark>Relearn Dark</option></select></div><div class=clear></div></div><script>window.relearn.markVariant()</script></li><li class=footerVisitedLinks><div class="padding menu-control"><i class="fa-fw fas fa-history"></i>
<span>&nbsp;</span><div class=control-style><button onclick=clearHistory()>Clear History</button></div><div class=clear></div></div></li></ul></div><div id=R-footer class="footerFooter showFooter"><p>Built with <a href=https://github.com/McShelby/hugo-theme-relearn title=love><i class="fas fa-heart"></i></a> by <a href=https://gohugo.io/>Hugo</a></p></div></div></div></aside><script src=/new-docs/js/clipboard.min.js?1739805295 defer></script><script src=/new-docs/js/perfect-scrollbar.min.js?1739805295 defer></script><script src=/new-docs/js/theme.js?1739805295 defer></script><script>function apply_image_invert_filter(e){document.querySelectorAll("img").forEach(function(t){if(t.classList.contains("no-invert"))return;t.style="filter: invert("+e+");"})}function darkThemeUsed(){const t=window.getComputedStyle(document.querySelector("body")),n=t.getPropertyValue("background-color");var e=n.match(/\d+/g).map(function(e){return parseInt(e,10)});return e.length===3&&.2126*e[0]+.7152*e[1]+.0722*e[2]<165}const invertToDarkGray=.85;darkThemeUsed()&&apply_image_invert_filter(invertToDarkGray),document.addEventListener("themeVariantLoaded",function(e){apply_image_invert_filter(e.detail.variant.endsWith("dark")?invertToDarkGray:0)})</script></body></html>